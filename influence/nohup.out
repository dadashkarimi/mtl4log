/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-13 14:17:18.885145: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-13 14:17:33.969484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 437.12MiB
2018-02-13 14:17:33.969544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[<1x359 sparse matrix of type '<type 'numpy.int64'>'
	with 13 stored elements in Compressed Sparse Row format>]
Total number of parameters: 359
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6562918e-08
Norm of the params: 6.092818
Orig loss: 0.00266. Accuracy: 1.000
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14746325
Train loss (w/o reg) on all data: 0.14003877
Test loss (w/o reg) on all data: 0.04943
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2762089e-06
Norm of the params: 12.185622
Flipped loss: 0.04943. Accuracy: 0.997
### Flips: 205, rs: 0, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012215031
Train loss (w/o reg) on all data: 0.009216191
Test loss (w/o reg) on all data: 0.0056127
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.047849e-07
Norm of the params: 7.744469
     Influence (LOO): fixed 162 labels. Loss 0.00561. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601366
Test loss (w/o reg) on all data: 0.002656104
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8070396e-08
Norm of the params: 6.092788
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14394565
Train loss (w/o reg) on all data: 0.13649075
Test loss (w/o reg) on all data: 0.047974426
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.2253835e-06
Norm of the params: 12.21057
              Random: fixed   6 labels. Loss 0.04797. Accuracy 0.998.
### Flips: 205, rs: 0, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504633
Test loss (w/o reg) on all data: 0.0026118031
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.24132e-08
Norm of the params: 5.9419866
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731076e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13681322
Train loss (w/o reg) on all data: 0.12925468
Test loss (w/o reg) on all data: 0.045991246
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9184587e-06
Norm of the params: 12.29515
              Random: fixed  17 labels. Loss 0.04599. Accuracy 0.999.
### Flips: 205, rs: 0, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504508
Test loss (w/o reg) on all data: 0.002611776
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1765437e-08
Norm of the params: 5.942008
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.76235e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13053265
Train loss (w/o reg) on all data: 0.122811876
Test loss (w/o reg) on all data: 0.043263417
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.964521e-06
Norm of the params: 12.426399
              Random: fixed  24 labels. Loss 0.04326. Accuracy 0.998.
### Flips: 205, rs: 0, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504505
Test loss (w/o reg) on all data: 0.0026117684
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4754028e-08
Norm of the params: 5.942009
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.75303e-09
Norm of the params: 6.092823
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12423544
Train loss (w/o reg) on all data: 0.1162994
Test loss (w/o reg) on all data: 0.039731827
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.012726e-06
Norm of the params: 12.598443
              Random: fixed  33 labels. Loss 0.03973. Accuracy 0.999.
### Flips: 205, rs: 0, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504557
Test loss (w/o reg) on all data: 0.002611784
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.36520475e-08
Norm of the params: 5.942
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.76273e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1198003
Train loss (w/o reg) on all data: 0.11195653
Test loss (w/o reg) on all data: 0.03841893
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8871073e-06
Norm of the params: 12.524995
              Random: fixed  39 labels. Loss 0.03842. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8473217e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8927218e-08
Norm of the params: 6.0928073
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114245586
Train loss (w/o reg) on all data: 0.10591586
Test loss (w/o reg) on all data: 0.037875254
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.95876e-06
Norm of the params: 12.90715
              Random: fixed  45 labels. Loss 0.03788. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15214607
Train loss (w/o reg) on all data: 0.1443241
Test loss (w/o reg) on all data: 0.058973573
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.065919e-06
Norm of the params: 12.5075865
Flipped loss: 0.05897. Accuracy: 0.995
### Flips: 205, rs: 1, checks: 205
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027910754
Train loss (w/o reg) on all data: 0.023743287
Test loss (w/o reg) on all data: 0.009311504
Train acc on all data:  0.9934354485776805
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5787713e-07
Norm of the params: 9.129586
     Influence (LOO): fixed 156 labels. Loss 0.00931. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1455481e-08
Norm of the params: 6.09282
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14677769
Train loss (w/o reg) on all data: 0.13937938
Test loss (w/o reg) on all data: 0.053774808
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.26402e-06
Norm of the params: 12.164136
              Random: fixed  10 labels. Loss 0.05377. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008329673
Train loss (w/o reg) on all data: 0.005911144
Test loss (w/o reg) on all data: 0.0048076273
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.277178e-08
Norm of the params: 6.954896
     Influence (LOO): fixed 179 labels. Loss 0.00481. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.90094e-08
Norm of the params: 6.092823
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13806294
Train loss (w/o reg) on all data: 0.13103472
Test loss (w/o reg) on all data: 0.049991384
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.7577604e-06
Norm of the params: 11.855993
              Random: fixed  24 labels. Loss 0.04999. Accuracy 0.997.
### Flips: 205, rs: 1, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560451
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7418213e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4440071e-08
Norm of the params: 6.0928183
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1299691
Train loss (w/o reg) on all data: 0.12297804
Test loss (w/o reg) on all data: 0.044716805
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9901445e-05
Norm of the params: 11.824608
              Random: fixed  33 labels. Loss 0.04472. Accuracy 0.997.
### Flips: 205, rs: 1, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6702189e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.002656051
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1170272e-08
Norm of the params: 6.0928216
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12288923
Train loss (w/o reg) on all data: 0.11595607
Test loss (w/o reg) on all data: 0.042318493
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8257866e-06
Norm of the params: 11.775536
              Random: fixed  43 labels. Loss 0.04232. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7162398e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0041727e-08
Norm of the params: 6.0928135
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11641246
Train loss (w/o reg) on all data: 0.109613016
Test loss (w/o reg) on all data: 0.040204614
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.110032e-06
Norm of the params: 11.661425
              Random: fixed  53 labels. Loss 0.04020. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5200216e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1836029e-08
Norm of the params: 6.0928144
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10961706
Train loss (w/o reg) on all data: 0.10268221
Test loss (w/o reg) on all data: 0.03775513
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3572558e-06
Norm of the params: 11.776973
              Random: fixed  61 labels. Loss 0.03776. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14758961
Train loss (w/o reg) on all data: 0.13962196
Test loss (w/o reg) on all data: 0.05584788
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.443878e-05
Norm of the params: 12.623514
Flipped loss: 0.05585. Accuracy: 0.997
### Flips: 205, rs: 2, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021763504
Train loss (w/o reg) on all data: 0.018040495
Test loss (w/o reg) on all data: 0.0083019305
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8211537e-07
Norm of the params: 8.629031
     Influence (LOO): fixed 155 labels. Loss 0.00830. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2552403e-08
Norm of the params: 6.0928288
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14333582
Train loss (w/o reg) on all data: 0.13537374
Test loss (w/o reg) on all data: 0.05429086
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0916929e-06
Norm of the params: 12.619097
              Random: fixed   7 labels. Loss 0.05429. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008255255
Train loss (w/o reg) on all data: 0.005896547
Test loss (w/o reg) on all data: 0.003465293
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.164515e-07
Norm of the params: 6.8683457
     Influence (LOO): fixed 171 labels. Loss 0.00347. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560843
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6402997e-08
Norm of the params: 6.092812
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13855897
Train loss (w/o reg) on all data: 0.13072585
Test loss (w/o reg) on all data: 0.0524415
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.340019e-06
Norm of the params: 12.516489
              Random: fixed  14 labels. Loss 0.05244. Accuracy 0.997.
### Flips: 205, rs: 2, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955855
Test loss (w/o reg) on all data: 0.002633726
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.094854e-08
Norm of the params: 5.928658
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2106033e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13371286
Train loss (w/o reg) on all data: 0.12592302
Test loss (w/o reg) on all data: 0.05091204
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.931628e-06
Norm of the params: 12.481858
              Random: fixed  21 labels. Loss 0.05091. Accuracy 0.997.
### Flips: 205, rs: 2, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955864
Test loss (w/o reg) on all data: 0.0026337446
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5478593e-08
Norm of the params: 5.928656
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206395e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12626632
Train loss (w/o reg) on all data: 0.11861563
Test loss (w/o reg) on all data: 0.049003042
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.34264865e-05
Norm of the params: 12.369867
              Random: fixed  32 labels. Loss 0.04900. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253034
Train loss (w/o reg) on all data: 0.002495588
Test loss (w/o reg) on all data: 0.0026337574
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1525635e-08
Norm of the params: 5.928653
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220603e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117915064
Train loss (w/o reg) on all data: 0.110238835
Test loss (w/o reg) on all data: 0.04886133
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5116971e-05
Norm of the params: 12.390504
              Random: fixed  42 labels. Loss 0.04886. Accuracy 0.994.
### Flips: 205, rs: 2, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3557391e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0715513e-08
Norm of the params: 6.0928144
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10396526
Train loss (w/o reg) on all data: 0.09599339
Test loss (w/o reg) on all data: 0.04627632
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5502225e-06
Norm of the params: 12.626848
              Random: fixed  58 labels. Loss 0.04628. Accuracy 0.991.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14673169
Train loss (w/o reg) on all data: 0.13939223
Test loss (w/o reg) on all data: 0.060583636
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.144917e-06
Norm of the params: 12.115662
Flipped loss: 0.06058. Accuracy: 0.992
### Flips: 205, rs: 3, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025572121
Train loss (w/o reg) on all data: 0.021881562
Test loss (w/o reg) on all data: 0.009345742
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5266289e-06
Norm of the params: 8.591344
     Influence (LOO): fixed 149 labels. Loss 0.00935. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601347
Test loss (w/o reg) on all data: 0.002656123
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.323685e-08
Norm of the params: 6.09279
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14357509
Train loss (w/o reg) on all data: 0.13630389
Test loss (w/o reg) on all data: 0.05832157
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.479907e-05
Norm of the params: 12.059191
              Random: fixed   6 labels. Loss 0.05832. Accuracy 0.993.
### Flips: 205, rs: 3, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011170188
Train loss (w/o reg) on all data: 0.008652756
Test loss (w/o reg) on all data: 0.0041082655
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8400772e-08
Norm of the params: 7.0956783
     Influence (LOO): fixed 165 labels. Loss 0.00411. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013065
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5604008e-08
Norm of the params: 6.0927978
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13720451
Train loss (w/o reg) on all data: 0.12967515
Test loss (w/o reg) on all data: 0.057211593
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.2613713e-06
Norm of the params: 12.271406
              Random: fixed  14 labels. Loss 0.05721. Accuracy 0.991.
### Flips: 205, rs: 3, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047741877
Train loss (w/o reg) on all data: 0.0029861776
Test loss (w/o reg) on all data: 0.0027003486
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0289414e-08
Norm of the params: 5.979984
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0851126e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1285666
Train loss (w/o reg) on all data: 0.12119982
Test loss (w/o reg) on all data: 0.052910924
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.457695e-06
Norm of the params: 12.138184
              Random: fixed  27 labels. Loss 0.05291. Accuracy 0.993.
### Flips: 205, rs: 3, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004774188
Train loss (w/o reg) on all data: 0.0029861876
Test loss (w/o reg) on all data: 0.0027003814
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.444252e-08
Norm of the params: 5.9799676
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.083742e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1209854
Train loss (w/o reg) on all data: 0.11361735
Test loss (w/o reg) on all data: 0.049155857
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.0027495e-06
Norm of the params: 12.139233
              Random: fixed  37 labels. Loss 0.04916. Accuracy 0.992.
### Flips: 205, rs: 3, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047741877
Train loss (w/o reg) on all data: 0.0029861764
Test loss (w/o reg) on all data: 0.0027003256
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.33182e-08
Norm of the params: 5.979985
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0851837e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11461708
Train loss (w/o reg) on all data: 0.107158825
Test loss (w/o reg) on all data: 0.046072364
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6623574e-06
Norm of the params: 12.213313
              Random: fixed  46 labels. Loss 0.04607. Accuracy 0.994.
### Flips: 205, rs: 3, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05870575e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.872463e-09
Norm of the params: 6.092814
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10629926
Train loss (w/o reg) on all data: 0.09862622
Test loss (w/o reg) on all data: 0.042743556
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7396175e-06
Norm of the params: 12.387928
              Random: fixed  55 labels. Loss 0.04274. Accuracy 0.994.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1527687
Train loss (w/o reg) on all data: 0.14443466
Test loss (w/o reg) on all data: 0.060442235
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5907648e-05
Norm of the params: 12.910489
Flipped loss: 0.06044. Accuracy: 0.995
### Flips: 205, rs: 4, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026102923
Train loss (w/o reg) on all data: 0.022025216
Test loss (w/o reg) on all data: 0.00781811
Train acc on all data:  0.9941648431801605
Test acc on all data:   1.0
Norm of the mean of gradients: 6.00827e-07
Norm of the params: 9.030732
     Influence (LOO): fixed 153 labels. Loss 0.00782. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037321304
Train loss (w/o reg) on all data: 0.001472317
Test loss (w/o reg) on all data: 0.0043935357
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.041491e-08
Norm of the params: 6.722817
                Loss: fixed 176 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14565831
Train loss (w/o reg) on all data: 0.13731441
Test loss (w/o reg) on all data: 0.05695057
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.365775e-05
Norm of the params: 12.918127
              Random: fixed  10 labels. Loss 0.05695. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01216815
Train loss (w/o reg) on all data: 0.00939793
Test loss (w/o reg) on all data: 0.0041964795
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 8.704036e-08
Norm of the params: 7.4434133
     Influence (LOO): fixed 169 labels. Loss 0.00420. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0346076e-08
Norm of the params: 6.0928254
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13808686
Train loss (w/o reg) on all data: 0.12973608
Test loss (w/o reg) on all data: 0.05404239
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1688368e-05
Norm of the params: 12.923446
              Random: fixed  21 labels. Loss 0.05404. Accuracy 0.996.
### Flips: 205, rs: 4, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052877134
Train loss (w/o reg) on all data: 0.0033059153
Test loss (w/o reg) on all data: 0.0031720628
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.770521e-08
Norm of the params: 6.29571
     Influence (LOO): fixed 175 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010126
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0928022e-08
Norm of the params: 6.092846
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13504784
Train loss (w/o reg) on all data: 0.12656166
Test loss (w/o reg) on all data: 0.052925978
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9111193e-06
Norm of the params: 13.027803
              Random: fixed  25 labels. Loss 0.05293. Accuracy 0.996.
### Flips: 205, rs: 4, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504559
Test loss (w/o reg) on all data: 0.0026117736
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7469621e-08
Norm of the params: 5.9419994
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.747265e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827441
Train loss (w/o reg) on all data: 0.11985343
Test loss (w/o reg) on all data: 0.051056806
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.5213125e-06
Norm of the params: 12.9776535
              Random: fixed  35 labels. Loss 0.05106. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504502
Test loss (w/o reg) on all data: 0.0026117524
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.848492e-08
Norm of the params: 5.942009
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.063494e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12234576
Train loss (w/o reg) on all data: 0.11383327
Test loss (w/o reg) on all data: 0.04927255
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.0876636e-06
Norm of the params: 13.047982
              Random: fixed  42 labels. Loss 0.04927. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504603
Test loss (w/o reg) on all data: 0.0026117875
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.546194e-08
Norm of the params: 5.941992
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8543715e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11623016
Train loss (w/o reg) on all data: 0.10789985
Test loss (w/o reg) on all data: 0.046549577
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.3869954e-06
Norm of the params: 12.9075985
              Random: fixed  51 labels. Loss 0.04655. Accuracy 0.994.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15645537
Train loss (w/o reg) on all data: 0.14860697
Test loss (w/o reg) on all data: 0.053917103
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.316464e-06
Norm of the params: 12.5286875
Flipped loss: 0.05392. Accuracy: 0.999
### Flips: 205, rs: 5, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031957533
Train loss (w/o reg) on all data: 0.026946709
Test loss (w/o reg) on all data: 0.009561849
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 2.998839e-07
Norm of the params: 10.010818
     Influence (LOO): fixed 156 labels. Loss 0.00956. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004204713
Train loss (w/o reg) on all data: 0.0018032985
Test loss (w/o reg) on all data: 0.0026193098
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1118668e-08
Norm of the params: 6.930244
                Loss: fixed 184 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15046507
Train loss (w/o reg) on all data: 0.14279473
Test loss (w/o reg) on all data: 0.052390873
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.12266935e-05
Norm of the params: 12.385756
              Random: fixed  10 labels. Loss 0.05239. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 410
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013089961
Train loss (w/o reg) on all data: 0.009844487
Test loss (w/o reg) on all data: 0.005514727
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3653731e-07
Norm of the params: 8.0566435
     Influence (LOO): fixed 175 labels. Loss 0.00551. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7011557e-08
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14281358
Train loss (w/o reg) on all data: 0.13502558
Test loss (w/o reg) on all data: 0.049484648
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2637541e-05
Norm of the params: 12.4803915
              Random: fixed  22 labels. Loss 0.04948. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158226
Train loss (w/o reg) on all data: 0.0018504526
Test loss (w/o reg) on all data: 0.0026117773
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0336637e-08
Norm of the params: 5.9420033
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.730823e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13648514
Train loss (w/o reg) on all data: 0.1284526
Test loss (w/o reg) on all data: 0.048000865
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2247367e-06
Norm of the params: 12.67481
              Random: fixed  31 labels. Loss 0.04800. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504582
Test loss (w/o reg) on all data: 0.0026117833
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7926473e-08
Norm of the params: 5.941996
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728658e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1274223
Train loss (w/o reg) on all data: 0.11936968
Test loss (w/o reg) on all data: 0.046057448
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.282631e-06
Norm of the params: 12.690647
              Random: fixed  44 labels. Loss 0.04606. Accuracy 0.998.
### Flips: 205, rs: 5, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850457
Test loss (w/o reg) on all data: 0.0026117756
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3221379e-08
Norm of the params: 5.941997
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728813e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12568733
Train loss (w/o reg) on all data: 0.11751579
Test loss (w/o reg) on all data: 0.04585979
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3054959e-06
Norm of the params: 12.784011
              Random: fixed  46 labels. Loss 0.04586. Accuracy 0.998.
### Flips: 205, rs: 5, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504573
Test loss (w/o reg) on all data: 0.0026117803
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.965797e-08
Norm of the params: 5.941998
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.729064e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12146976
Train loss (w/o reg) on all data: 0.113652214
Test loss (w/o reg) on all data: 0.04462981
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.9107748e-06
Norm of the params: 12.504032
              Random: fixed  52 labels. Loss 0.04463. Accuracy 0.998.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15137523
Train loss (w/o reg) on all data: 0.14406559
Test loss (w/o reg) on all data: 0.051338986
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8263864e-06
Norm of the params: 12.0910225
Flipped loss: 0.05134. Accuracy: 0.998
### Flips: 205, rs: 6, checks: 205
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031134473
Train loss (w/o reg) on all data: 0.027011639
Test loss (w/o reg) on all data: 0.008974958
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3233275e-07
Norm of the params: 9.080566
     Influence (LOO): fixed 150 labels. Loss 0.00897. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035984488
Train loss (w/o reg) on all data: 0.0013201104
Test loss (w/o reg) on all data: 0.0026171214
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1239665e-08
Norm of the params: 6.750316
                Loss: fixed 175 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14406028
Train loss (w/o reg) on all data: 0.13690503
Test loss (w/o reg) on all data: 0.047983944
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4252889e-05
Norm of the params: 11.962654
              Random: fixed  11 labels. Loss 0.04798. Accuracy 0.997.
### Flips: 205, rs: 6, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072451057
Train loss (w/o reg) on all data: 0.0045594047
Test loss (w/o reg) on all data: 0.0030997014
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.004338e-08
Norm of the params: 7.328985
     Influence (LOO): fixed 172 labels. Loss 0.00310. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601282
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8235575e-08
Norm of the params: 6.092801
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13776314
Train loss (w/o reg) on all data: 0.13048138
Test loss (w/o reg) on all data: 0.045633167
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3191477e-06
Norm of the params: 12.067943
              Random: fixed  20 labels. Loss 0.04563. Accuracy 0.996.
### Flips: 205, rs: 6, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6525914e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7186096e-08
Norm of the params: 6.0928164
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13209423
Train loss (w/o reg) on all data: 0.12503184
Test loss (w/o reg) on all data: 0.043742575
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1471793e-06
Norm of the params: 11.884778
              Random: fixed  29 labels. Loss 0.04374. Accuracy 0.998.
### Flips: 205, rs: 6, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010714
Test loss (w/o reg) on all data: 0.0026560419
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.577511e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6113153e-08
Norm of the params: 6.0928183
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057384
Train loss (w/o reg) on all data: 0.11314392
Test loss (w/o reg) on all data: 0.039867368
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.191733e-06
Norm of the params: 12.190095
              Random: fixed  43 labels. Loss 0.03987. Accuracy 0.999.
### Flips: 205, rs: 6, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.282446e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3689336e-08
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117737435
Train loss (w/o reg) on all data: 0.11043406
Test loss (w/o reg) on all data: 0.038877927
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 8.71363e-06
Norm of the params: 12.085836
              Random: fixed  48 labels. Loss 0.03888. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7728928e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.054155e-08
Norm of the params: 6.0928197
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11022441
Train loss (w/o reg) on all data: 0.102998056
Test loss (w/o reg) on all data: 0.036190104
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 9.382165e-06
Norm of the params: 12.021943
              Random: fixed  58 labels. Loss 0.03619. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15016375
Train loss (w/o reg) on all data: 0.14224552
Test loss (w/o reg) on all data: 0.05962749
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9316598e-06
Norm of the params: 12.584305
Flipped loss: 0.05963. Accuracy: 0.996
### Flips: 205, rs: 7, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023539642
Train loss (w/o reg) on all data: 0.01933927
Test loss (w/o reg) on all data: 0.007721191
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 5.481149e-07
Norm of the params: 9.165555
     Influence (LOO): fixed 159 labels. Loss 0.00772. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6309177e-08
Norm of the params: 6.0928164
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1423648
Train loss (w/o reg) on all data: 0.13457808
Test loss (w/o reg) on all data: 0.055801634
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.0029536e-06
Norm of the params: 12.479365
              Random: fixed  13 labels. Loss 0.05580. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010779976
Train loss (w/o reg) on all data: 0.008207416
Test loss (w/o reg) on all data: 0.0047272174
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.08192424e-07
Norm of the params: 7.172949
     Influence (LOO): fixed 173 labels. Loss 0.00473. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560435
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4729762e-08
Norm of the params: 6.092827
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1367524
Train loss (w/o reg) on all data: 0.12907888
Test loss (w/o reg) on all data: 0.052163392
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.139683e-06
Norm of the params: 12.388314
              Random: fixed  22 labels. Loss 0.05216. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052877134
Train loss (w/o reg) on all data: 0.0033059306
Test loss (w/o reg) on all data: 0.0031721182
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.701231e-08
Norm of the params: 6.295686
     Influence (LOO): fixed 178 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010114
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1165887e-08
Norm of the params: 6.092846
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13119881
Train loss (w/o reg) on all data: 0.12395802
Test loss (w/o reg) on all data: 0.049260106
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2134077e-05
Norm of the params: 12.033937
              Random: fixed  30 labels. Loss 0.04926. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504579
Test loss (w/o reg) on all data: 0.0026117868
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5632403e-08
Norm of the params: 5.941996
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731034e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1272409
Train loss (w/o reg) on all data: 0.11992321
Test loss (w/o reg) on all data: 0.048865005
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6754446e-06
Norm of the params: 12.097674
              Random: fixed  35 labels. Loss 0.04887. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504614
Test loss (w/o reg) on all data: 0.0026117987
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.625872e-08
Norm of the params: 5.9419904
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.761564e-09
Norm of the params: 6.0928226
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11807056
Train loss (w/o reg) on all data: 0.111068785
Test loss (w/o reg) on all data: 0.04492802
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8688417e-06
Norm of the params: 11.833654
              Random: fixed  49 labels. Loss 0.04493. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5915399e-08
Norm of the params: 6.0928125
     Influence (LOO): fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5217665e-09
Norm of the params: 6.0928164
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11266378
Train loss (w/o reg) on all data: 0.10594401
Test loss (w/o reg) on all data: 0.041050155
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.189788e-06
Norm of the params: 11.592906
              Random: fixed  58 labels. Loss 0.04105. Accuracy 0.998.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15190074
Train loss (w/o reg) on all data: 0.14401592
Test loss (w/o reg) on all data: 0.055768475
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.34834045e-05
Norm of the params: 12.557719
Flipped loss: 0.05577. Accuracy: 0.994
### Flips: 205, rs: 8, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025170423
Train loss (w/o reg) on all data: 0.021468578
Test loss (w/o reg) on all data: 0.007006257
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1514344e-07
Norm of the params: 8.60447
     Influence (LOO): fixed 154 labels. Loss 0.00701. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009613
Test loss (w/o reg) on all data: 0.0026560265
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0787174e-08
Norm of the params: 6.092854
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14482458
Train loss (w/o reg) on all data: 0.13673638
Test loss (w/o reg) on all data: 0.052890673
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.8962655e-06
Norm of the params: 12.718651
              Random: fixed  11 labels. Loss 0.05289. Accuracy 0.995.
### Flips: 205, rs: 8, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011204874
Train loss (w/o reg) on all data: 0.008550508
Test loss (w/o reg) on all data: 0.0040084324
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.511886e-07
Norm of the params: 7.2861047
     Influence (LOO): fixed 169 labels. Loss 0.00401. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3575199e-08
Norm of the params: 6.092823
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14018668
Train loss (w/o reg) on all data: 0.13223648
Test loss (w/o reg) on all data: 0.051358733
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.21216e-06
Norm of the params: 12.609684
              Random: fixed  18 labels. Loss 0.05136. Accuracy 0.994.
### Flips: 205, rs: 8, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955876
Test loss (w/o reg) on all data: 0.0026337167
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8867007e-08
Norm of the params: 5.928655
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2201604e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13116667
Train loss (w/o reg) on all data: 0.12345488
Test loss (w/o reg) on all data: 0.04593494
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4616976e-05
Norm of the params: 12.419161
              Random: fixed  32 labels. Loss 0.04593. Accuracy 0.997.
### Flips: 205, rs: 8, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337288
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9655855e-09
Norm of the params: 5.9286604
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220678e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12608457
Train loss (w/o reg) on all data: 0.118441775
Test loss (w/o reg) on all data: 0.042867105
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4512483e-06
Norm of the params: 12.363488
              Random: fixed  39 labels. Loss 0.04287. Accuracy 0.997.
### Flips: 205, rs: 8, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955845
Test loss (w/o reg) on all data: 0.002633702
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.742841e-08
Norm of the params: 5.9286604
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2201357e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11831042
Train loss (w/o reg) on all data: 0.11089893
Test loss (w/o reg) on all data: 0.039526746
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0673094e-05
Norm of the params: 12.174968
              Random: fixed  50 labels. Loss 0.03953. Accuracy 0.999.
### Flips: 205, rs: 8, checks: 1230
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955843
Test loss (w/o reg) on all data: 0.0026337327
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.054915e-09
Norm of the params: 5.92866
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206782e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10580227
Train loss (w/o reg) on all data: 0.098210245
Test loss (w/o reg) on all data: 0.034743622
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.154321e-06
Norm of the params: 12.322356
              Random: fixed  64 labels. Loss 0.03474. Accuracy 0.998.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15368278
Train loss (w/o reg) on all data: 0.14664002
Test loss (w/o reg) on all data: 0.059793707
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7101175e-06
Norm of the params: 11.868242
Flipped loss: 0.05979. Accuracy: 0.994
### Flips: 205, rs: 9, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027419899
Train loss (w/o reg) on all data: 0.023185026
Test loss (w/o reg) on all data: 0.009511523
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.748997e-06
Norm of the params: 9.203122
     Influence (LOO): fixed 157 labels. Loss 0.00951. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3345499e-08
Norm of the params: 6.0928073
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1496464
Train loss (w/o reg) on all data: 0.14263406
Test loss (w/o reg) on all data: 0.056860767
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3801005e-06
Norm of the params: 11.842586
              Random: fixed   7 labels. Loss 0.05686. Accuracy 0.996.
### Flips: 205, rs: 9, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01236745
Train loss (w/o reg) on all data: 0.009524697
Test loss (w/o reg) on all data: 0.0048036193
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.00504e-07
Norm of the params: 7.540229
     Influence (LOO): fixed 173 labels. Loss 0.00480. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8414939e-08
Norm of the params: 6.092809
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14375621
Train loss (w/o reg) on all data: 0.13683271
Test loss (w/o reg) on all data: 0.05482786
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5334024e-05
Norm of the params: 11.767325
              Random: fixed  16 labels. Loss 0.05483. Accuracy 0.998.
### Flips: 205, rs: 9, checks: 615
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075198645
Train loss (w/o reg) on all data: 0.0051086047
Test loss (w/o reg) on all data: 0.0039977697
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6631958e-07
Norm of the params: 6.944436
     Influence (LOO): fixed 177 labels. Loss 0.00400. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7551924e-08
Norm of the params: 6.092808
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13994014
Train loss (w/o reg) on all data: 0.13303263
Test loss (w/o reg) on all data: 0.054212846
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.8131426e-06
Norm of the params: 11.753735
              Random: fixed  21 labels. Loss 0.05421. Accuracy 0.997.
### Flips: 205, rs: 9, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9564993e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0902038e-08
Norm of the params: 6.0928164
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1322828
Train loss (w/o reg) on all data: 0.12537248
Test loss (w/o reg) on all data: 0.05189293
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0515862e-06
Norm of the params: 11.756113
              Random: fixed  32 labels. Loss 0.05189. Accuracy 0.998.
### Flips: 205, rs: 9, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3011957e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0498732e-08
Norm of the params: 6.0928154
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12004843
Train loss (w/o reg) on all data: 0.11360133
Test loss (w/o reg) on all data: 0.04881934
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.594907e-06
Norm of the params: 11.35527
              Random: fixed  49 labels. Loss 0.04882. Accuracy 0.997.
### Flips: 205, rs: 9, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010097
Test loss (w/o reg) on all data: 0.002656031
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0356432e-08
Norm of the params: 6.092845
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601342
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.856148e-08
Norm of the params: 6.092792
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114170305
Train loss (w/o reg) on all data: 0.107728325
Test loss (w/o reg) on all data: 0.045190167
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.8514555e-06
Norm of the params: 11.350756
              Random: fixed  58 labels. Loss 0.04519. Accuracy 0.997.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15537003
Train loss (w/o reg) on all data: 0.14846717
Test loss (w/o reg) on all data: 0.053709332
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.2653744e-06
Norm of the params: 11.749772
Flipped loss: 0.05371. Accuracy: 0.997
### Flips: 205, rs: 10, checks: 205
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020039259
Train loss (w/o reg) on all data: 0.016562084
Test loss (w/o reg) on all data: 0.006494309
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 7.348645e-07
Norm of the params: 8.339273
     Influence (LOO): fixed 163 labels. Loss 0.00649. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601118
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.57337e-08
Norm of the params: 6.092829
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14898719
Train loss (w/o reg) on all data: 0.1423499
Test loss (w/o reg) on all data: 0.051429026
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8103767e-06
Norm of the params: 11.521542
              Random: fixed  11 labels. Loss 0.05143. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004444846
Train loss (w/o reg) on all data: 0.0023350522
Test loss (w/o reg) on all data: 0.0028284572
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0102033e-07
Norm of the params: 6.495835
     Influence (LOO): fixed 177 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096007605
Test loss (w/o reg) on all data: 0.002656007
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.00118e-08
Norm of the params: 6.0928874
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14307219
Train loss (w/o reg) on all data: 0.13644025
Test loss (w/o reg) on all data: 0.04910121
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.69739e-06
Norm of the params: 11.516892
              Random: fixed  20 labels. Loss 0.04910. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601339
Test loss (w/o reg) on all data: 0.0026560933
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1682347e-08
Norm of the params: 6.092793
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2512832e-08
Norm of the params: 6.09281
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14082384
Train loss (w/o reg) on all data: 0.13423471
Test loss (w/o reg) on all data: 0.048013657
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.403219e-06
Norm of the params: 11.479661
              Random: fixed  23 labels. Loss 0.04801. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601331
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1393377e-08
Norm of the params: 6.092794
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2383267e-08
Norm of the params: 6.09281
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13271481
Train loss (w/o reg) on all data: 0.12602916
Test loss (w/o reg) on all data: 0.0456534
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.52501e-06
Norm of the params: 11.563428
              Random: fixed  34 labels. Loss 0.04565. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.098753e-09
Norm of the params: 6.0928216
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.942134e-09
Norm of the params: 6.0928197
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12521608
Train loss (w/o reg) on all data: 0.118542805
Test loss (w/o reg) on all data: 0.042287946
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.5357934e-06
Norm of the params: 11.552731
              Random: fixed  45 labels. Loss 0.04229. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.098854e-09
Norm of the params: 6.0928135
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1068353e-08
Norm of the params: 6.0928187
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12149283
Train loss (w/o reg) on all data: 0.114869125
Test loss (w/o reg) on all data: 0.040981498
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5020336e-06
Norm of the params: 11.509738
              Random: fixed  49 labels. Loss 0.04098. Accuracy 0.998.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14722434
Train loss (w/o reg) on all data: 0.13970323
Test loss (w/o reg) on all data: 0.060312495
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.211264e-06
Norm of the params: 12.264671
Flipped loss: 0.06031. Accuracy: 0.993
### Flips: 205, rs: 11, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02437058
Train loss (w/o reg) on all data: 0.020810982
Test loss (w/o reg) on all data: 0.009848819
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2777393e-07
Norm of the params: 8.437534
     Influence (LOO): fixed 159 labels. Loss 0.00985. Accuracy 0.999.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031825108
Train loss (w/o reg) on all data: 0.0011578884
Test loss (w/o reg) on all data: 0.003246974
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.310772e-08
Norm of the params: 6.363368
                Loss: fixed 180 labels. Loss 0.00325. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14096715
Train loss (w/o reg) on all data: 0.13297358
Test loss (w/o reg) on all data: 0.05782353
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.3894627e-06
Norm of the params: 12.644019
              Random: fixed   9 labels. Loss 0.05782. Accuracy 0.992.
### Flips: 205, rs: 11, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009134753
Train loss (w/o reg) on all data: 0.0068351137
Test loss (w/o reg) on all data: 0.0034082844
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1862254e-08
Norm of the params: 6.7817984
     Influence (LOO): fixed 176 labels. Loss 0.00341. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601259
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.271511e-08
Norm of the params: 6.0928054
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12887998
Train loss (w/o reg) on all data: 0.12104137
Test loss (w/o reg) on all data: 0.05299015
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.623192e-06
Norm of the params: 12.520865
              Random: fixed  25 labels. Loss 0.05299. Accuracy 0.993.
### Flips: 205, rs: 11, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024956064
Test loss (w/o reg) on all data: 0.0026337847
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7576255e-08
Norm of the params: 5.928623
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2207159e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12494074
Train loss (w/o reg) on all data: 0.117078386
Test loss (w/o reg) on all data: 0.051795468
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1931561e-05
Norm of the params: 12.539815
              Random: fixed  31 labels. Loss 0.05180. Accuracy 0.994.
### Flips: 205, rs: 11, checks: 820
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337274
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.943602e-08
Norm of the params: 5.928661
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206416e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118966684
Train loss (w/o reg) on all data: 0.111153394
Test loss (w/o reg) on all data: 0.04831586
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.5408661e-06
Norm of the params: 12.500633
              Random: fixed  40 labels. Loss 0.04832. Accuracy 0.994.
### Flips: 205, rs: 11, checks: 1025
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955852
Test loss (w/o reg) on all data: 0.0026337323
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0936147e-08
Norm of the params: 5.9286585
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220678e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11410168
Train loss (w/o reg) on all data: 0.10652218
Test loss (w/o reg) on all data: 0.045787532
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.9980674e-06
Norm of the params: 12.312184
              Random: fixed  48 labels. Loss 0.04579. Accuracy 0.995.
### Flips: 205, rs: 11, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601316
Test loss (w/o reg) on all data: 0.0026561206
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0141744e-08
Norm of the params: 6.092797
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5608543e-08
Norm of the params: 6.0928116
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10782409
Train loss (w/o reg) on all data: 0.100119635
Test loss (w/o reg) on all data: 0.04399001
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.9323453e-06
Norm of the params: 12.413263
              Random: fixed  57 labels. Loss 0.04399. Accuracy 0.994.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14600107
Train loss (w/o reg) on all data: 0.13802862
Test loss (w/o reg) on all data: 0.056865886
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.3806594e-06
Norm of the params: 12.627308
Flipped loss: 0.05687. Accuracy: 0.994
### Flips: 205, rs: 12, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024331708
Train loss (w/o reg) on all data: 0.020762222
Test loss (w/o reg) on all data: 0.007927941
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2692048e-07
Norm of the params: 8.449243
     Influence (LOO): fixed 156 labels. Loss 0.00793. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600375
Test loss (w/o reg) on all data: 0.0026559534
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.159342e-08
Norm of the params: 6.0929503
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1380284
Train loss (w/o reg) on all data: 0.13016012
Test loss (w/o reg) on all data: 0.052108757
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.332967e-05
Norm of the params: 12.544541
              Random: fixed  12 labels. Loss 0.05211. Accuracy 0.997.
### Flips: 205, rs: 12, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075422074
Train loss (w/o reg) on all data: 0.005263719
Test loss (w/o reg) on all data: 0.0035499514
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.562765e-08
Norm of the params: 6.7505383
     Influence (LOO): fixed 173 labels. Loss 0.00355. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1997786e-08
Norm of the params: 6.092831
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13115759
Train loss (w/o reg) on all data: 0.123343796
Test loss (w/o reg) on all data: 0.050956942
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0822065e-06
Norm of the params: 12.501042
              Random: fixed  22 labels. Loss 0.05096. Accuracy 0.995.
### Flips: 205, rs: 12, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601149
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.845333e-09
Norm of the params: 6.092823
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.993965e-09
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12722006
Train loss (w/o reg) on all data: 0.11925713
Test loss (w/o reg) on all data: 0.049603254
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.404998e-06
Norm of the params: 12.619777
              Random: fixed  28 labels. Loss 0.04960. Accuracy 0.995.
### Flips: 205, rs: 12, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0276655e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5622895e-08
Norm of the params: 6.0928135
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11915162
Train loss (w/o reg) on all data: 0.111008614
Test loss (w/o reg) on all data: 0.046897274
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.026539e-06
Norm of the params: 12.761667
              Random: fixed  37 labels. Loss 0.04690. Accuracy 0.996.
### Flips: 205, rs: 12, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8422766e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0793106e-08
Norm of the params: 6.0928197
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11443847
Train loss (w/o reg) on all data: 0.10678701
Test loss (w/o reg) on all data: 0.04406971
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.458585e-06
Norm of the params: 12.370492
              Random: fixed  45 labels. Loss 0.04407. Accuracy 0.997.
### Flips: 205, rs: 12, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3332833e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.596531e-09
Norm of the params: 6.0928173
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10610682
Train loss (w/o reg) on all data: 0.09865291
Test loss (w/o reg) on all data: 0.039242588
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.123753e-06
Norm of the params: 12.209758
              Random: fixed  56 labels. Loss 0.03924. Accuracy 0.997.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15917075
Train loss (w/o reg) on all data: 0.15092692
Test loss (w/o reg) on all data: 0.05835071
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.807252e-06
Norm of the params: 12.840426
Flipped loss: 0.05835. Accuracy: 0.997
### Flips: 205, rs: 13, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024086323
Train loss (w/o reg) on all data: 0.020344052
Test loss (w/o reg) on all data: 0.0074513964
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.968774e-07
Norm of the params: 8.651324
     Influence (LOO): fixed 162 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034720325
Train loss (w/o reg) on all data: 0.0013089796
Test loss (w/o reg) on all data: 0.0026516
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.131236e-08
Norm of the params: 6.577314
                Loss: fixed 181 labels. Loss 0.00265. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15527901
Train loss (w/o reg) on all data: 0.146837
Test loss (w/o reg) on all data: 0.057903506
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.3910917e-06
Norm of the params: 12.993853
              Random: fixed   5 labels. Loss 0.05790. Accuracy 0.995.
### Flips: 205, rs: 13, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011810001
Train loss (w/o reg) on all data: 0.009169164
Test loss (w/o reg) on all data: 0.004081058
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1368695e-07
Norm of the params: 7.2675133
     Influence (LOO): fixed 175 labels. Loss 0.00408. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601385
Test loss (w/o reg) on all data: 0.0026561038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.439357e-08
Norm of the params: 6.0927844
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14265278
Train loss (w/o reg) on all data: 0.13369419
Test loss (w/o reg) on all data: 0.05544995
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4836933e-06
Norm of the params: 13.385509
              Random: fixed  21 labels. Loss 0.05545. Accuracy 0.995.
### Flips: 205, rs: 13, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044110185
Train loss (w/o reg) on all data: 0.002656308
Test loss (w/o reg) on all data: 0.0030831222
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1809856e-08
Norm of the params: 5.924037
     Influence (LOO): fixed 180 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.684556e-09
Norm of the params: 6.0928206
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14004824
Train loss (w/o reg) on all data: 0.13114545
Test loss (w/o reg) on all data: 0.053470578
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.367604e-06
Norm of the params: 13.343752
              Random: fixed  25 labels. Loss 0.05347. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 820
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004411018
Train loss (w/o reg) on all data: 0.0026563173
Test loss (w/o reg) on all data: 0.0030831401
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9995932e-08
Norm of the params: 5.92402
     Influence (LOO): fixed 180 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.683457e-09
Norm of the params: 6.0928206
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13247743
Train loss (w/o reg) on all data: 0.1237754
Test loss (w/o reg) on all data: 0.051727798
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.309022e-06
Norm of the params: 13.19244
              Random: fixed  35 labels. Loss 0.05173. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504439
Test loss (w/o reg) on all data: 0.0026117289
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8722512e-08
Norm of the params: 5.9420204
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.878847e-09
Norm of the params: 6.092823
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12565787
Train loss (w/o reg) on all data: 0.11670604
Test loss (w/o reg) on all data: 0.049633615
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.355258e-06
Norm of the params: 13.38045
              Random: fixed  42 labels. Loss 0.04963. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504409
Test loss (w/o reg) on all data: 0.002611739
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5161303e-08
Norm of the params: 5.9420257
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.748127e-09
Norm of the params: 6.092822
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117641255
Train loss (w/o reg) on all data: 0.10889488
Test loss (w/o reg) on all data: 0.04387686
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5740514e-05
Norm of the params: 13.22602
              Random: fixed  55 labels. Loss 0.04388. Accuracy 0.997.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1491856
Train loss (w/o reg) on all data: 0.14189848
Test loss (w/o reg) on all data: 0.05630681
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.4673806e-06
Norm of the params: 12.072372
Flipped loss: 0.05631. Accuracy: 0.997
### Flips: 205, rs: 14, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025843825
Train loss (w/o reg) on all data: 0.021613788
Test loss (w/o reg) on all data: 0.0076286662
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3473125e-07
Norm of the params: 9.197866
     Influence (LOO): fixed 153 labels. Loss 0.00763. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3971882e-08
Norm of the params: 6.0928154
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14161445
Train loss (w/o reg) on all data: 0.13429968
Test loss (w/o reg) on all data: 0.054148473
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3725695e-06
Norm of the params: 12.0952635
              Random: fixed  11 labels. Loss 0.05415. Accuracy 0.997.
### Flips: 205, rs: 14, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007972587
Train loss (w/o reg) on all data: 0.005465749
Test loss (w/o reg) on all data: 0.0034979412
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.438387e-08
Norm of the params: 7.0807304
     Influence (LOO): fixed 171 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1442318e-08
Norm of the params: 6.092814
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13505554
Train loss (w/o reg) on all data: 0.12779638
Test loss (w/o reg) on all data: 0.049734958
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.834761e-06
Norm of the params: 12.049206
              Random: fixed  22 labels. Loss 0.04973. Accuracy 0.996.
### Flips: 205, rs: 14, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.632353e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2325296e-09
Norm of the params: 6.092816
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13206862
Train loss (w/o reg) on all data: 0.12490506
Test loss (w/o reg) on all data: 0.04730367
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9558298e-06
Norm of the params: 11.969593
              Random: fixed  27 labels. Loss 0.04730. Accuracy 0.997.
### Flips: 205, rs: 14, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.540817e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.704617e-09
Norm of the params: 6.0928164
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12618925
Train loss (w/o reg) on all data: 0.11921868
Test loss (w/o reg) on all data: 0.04579937
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.0371093e-06
Norm of the params: 11.807266
              Random: fixed  37 labels. Loss 0.04580. Accuracy 0.995.
### Flips: 205, rs: 14, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3562348e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2952092e-08
Norm of the params: 6.092823
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11661656
Train loss (w/o reg) on all data: 0.1093715
Test loss (w/o reg) on all data: 0.043280184
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.676076e-06
Norm of the params: 12.037492
              Random: fixed  47 labels. Loss 0.04328. Accuracy 0.996.
### Flips: 205, rs: 14, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8462718e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.796023e-08
Norm of the params: 6.0928164
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11175411
Train loss (w/o reg) on all data: 0.10439996
Test loss (w/o reg) on all data: 0.04185074
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6606167e-06
Norm of the params: 12.127786
              Random: fixed  53 labels. Loss 0.04185. Accuracy 0.997.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1519027
Train loss (w/o reg) on all data: 0.1435581
Test loss (w/o reg) on all data: 0.060315073
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9572419e-05
Norm of the params: 12.918676
Flipped loss: 0.06032. Accuracy: 0.997
### Flips: 205, rs: 15, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01978244
Train loss (w/o reg) on all data: 0.015877172
Test loss (w/o reg) on all data: 0.007533836
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0411002e-07
Norm of the params: 8.837725
     Influence (LOO): fixed 162 labels. Loss 0.00753. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.176813e-08
Norm of the params: 6.0928206
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14775518
Train loss (w/o reg) on all data: 0.13943878
Test loss (w/o reg) on all data: 0.05812758
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.2572494e-06
Norm of the params: 12.896816
              Random: fixed   7 labels. Loss 0.05813. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 410
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044808155
Train loss (w/o reg) on all data: 0.0023802605
Test loss (w/o reg) on all data: 0.0032515945
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.124035e-08
Norm of the params: 6.4815974
     Influence (LOO): fixed 177 labels. Loss 0.00325. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4448122e-08
Norm of the params: 6.09282
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14161864
Train loss (w/o reg) on all data: 0.13319428
Test loss (w/o reg) on all data: 0.056494296
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.560936e-06
Norm of the params: 12.98026
              Random: fixed  15 labels. Loss 0.05649. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5402471e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601312
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.793714e-08
Norm of the params: 6.092797
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1387628
Train loss (w/o reg) on all data: 0.13066918
Test loss (w/o reg) on all data: 0.05575968
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0318819e-05
Norm of the params: 12.722918
              Random: fixed  20 labels. Loss 0.05576. Accuracy 0.996.
### Flips: 205, rs: 15, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5276376e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0380908e-08
Norm of the params: 6.092812
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1335524
Train loss (w/o reg) on all data: 0.12542297
Test loss (w/o reg) on all data: 0.054443516
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.381982e-06
Norm of the params: 12.751031
              Random: fixed  28 labels. Loss 0.05444. Accuracy 0.996.
### Flips: 205, rs: 15, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8999367e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5777406e-08
Norm of the params: 6.092827
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123914376
Train loss (w/o reg) on all data: 0.115511775
Test loss (w/o reg) on all data: 0.048473377
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0667861e-06
Norm of the params: 12.963489
              Random: fixed  39 labels. Loss 0.04847. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.0026560468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2011568e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5203314e-08
Norm of the params: 6.0928235
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119277515
Train loss (w/o reg) on all data: 0.11094209
Test loss (w/o reg) on all data: 0.04688304
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6078586e-06
Norm of the params: 12.911565
              Random: fixed  45 labels. Loss 0.04688. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15457976
Train loss (w/o reg) on all data: 0.14759602
Test loss (w/o reg) on all data: 0.05155092
Train acc on all data:  0.9567225869195235
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.010005e-05
Norm of the params: 11.818406
Flipped loss: 0.05155. Accuracy: 0.998
### Flips: 205, rs: 16, checks: 205
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022360174
Train loss (w/o reg) on all data: 0.018753361
Test loss (w/o reg) on all data: 0.006905674
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.926149e-07
Norm of the params: 8.493306
     Influence (LOO): fixed 163 labels. Loss 0.00691. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5620297e-08
Norm of the params: 6.0928316
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14772604
Train loss (w/o reg) on all data: 0.14062703
Test loss (w/o reg) on all data: 0.04884862
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.378445e-06
Norm of the params: 11.91555
              Random: fixed   9 labels. Loss 0.04885. Accuracy 0.998.
### Flips: 205, rs: 16, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009440983
Train loss (w/o reg) on all data: 0.0069445204
Test loss (w/o reg) on all data: 0.003374648
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 9.4254744e-08
Norm of the params: 7.0660625
     Influence (LOO): fixed 175 labels. Loss 0.00337. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7662058e-08
Norm of the params: 6.0928173
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14206852
Train loss (w/o reg) on all data: 0.13488036
Test loss (w/o reg) on all data: 0.04568456
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1030344e-06
Norm of the params: 11.990121
              Random: fixed  18 labels. Loss 0.04568. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005343829
Train loss (w/o reg) on all data: 0.0033799442
Test loss (w/o reg) on all data: 0.0027326322
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9451867e-08
Norm of the params: 6.2671924
     Influence (LOO): fixed 178 labels. Loss 0.00273. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3384983e-08
Norm of the params: 6.09282
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13601278
Train loss (w/o reg) on all data: 0.12858194
Test loss (w/o reg) on all data: 0.043699507
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1702295e-06
Norm of the params: 12.190843
              Random: fixed  25 labels. Loss 0.04370. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.001850456
Test loss (w/o reg) on all data: 0.0026117815
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.949185e-08
Norm of the params: 5.9419994
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728317e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12930982
Train loss (w/o reg) on all data: 0.12164675
Test loss (w/o reg) on all data: 0.04113787
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9927166e-06
Norm of the params: 12.379881
              Random: fixed  33 labels. Loss 0.04114. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504529
Test loss (w/o reg) on all data: 0.0026117729
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5420595e-08
Norm of the params: 5.942004
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.759053e-09
Norm of the params: 6.0928226
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120397106
Train loss (w/o reg) on all data: 0.11276833
Test loss (w/o reg) on all data: 0.03800741
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7659414e-06
Norm of the params: 12.352146
              Random: fixed  44 labels. Loss 0.03801. Accuracy 1.000.
### Flips: 205, rs: 16, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504587
Test loss (w/o reg) on all data: 0.0026117875
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9627092e-08
Norm of the params: 5.941995
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7315195e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110241205
Train loss (w/o reg) on all data: 0.10261454
Test loss (w/o reg) on all data: 0.035128932
Train acc on all data:  0.9708242159008024
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1670683e-06
Norm of the params: 12.350443
              Random: fixed  58 labels. Loss 0.03513. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15350856
Train loss (w/o reg) on all data: 0.1464743
Test loss (w/o reg) on all data: 0.058764905
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.6474446e-06
Norm of the params: 11.8610735
Flipped loss: 0.05876. Accuracy: 0.993
### Flips: 205, rs: 17, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024084397
Train loss (w/o reg) on all data: 0.020789312
Test loss (w/o reg) on all data: 0.0078227585
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.08591e-07
Norm of the params: 8.117985
     Influence (LOO): fixed 160 labels. Loss 0.00782. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.242222e-08
Norm of the params: 6.0928216
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14685203
Train loss (w/o reg) on all data: 0.13973992
Test loss (w/o reg) on all data: 0.055284563
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8026864e-06
Norm of the params: 11.9265375
              Random: fixed   7 labels. Loss 0.05528. Accuracy 0.995.
### Flips: 205, rs: 17, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010026994
Train loss (w/o reg) on all data: 0.0076727653
Test loss (w/o reg) on all data: 0.004125128
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4389918e-07
Norm of the params: 6.86182
     Influence (LOO): fixed 175 labels. Loss 0.00413. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601092
Test loss (w/o reg) on all data: 0.0026560402
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9777484e-08
Norm of the params: 6.092833
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13897248
Train loss (w/o reg) on all data: 0.13157168
Test loss (w/o reg) on all data: 0.05158632
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8236109e-06
Norm of the params: 12.166176
              Random: fixed  18 labels. Loss 0.05159. Accuracy 0.993.
### Flips: 205, rs: 17, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061893994
Train loss (w/o reg) on all data: 0.004204819
Test loss (w/o reg) on all data: 0.0038925172
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.693286e-08
Norm of the params: 6.3001275
     Influence (LOO): fixed 178 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2742347e-08
Norm of the params: 6.0928173
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13163696
Train loss (w/o reg) on all data: 0.12426247
Test loss (w/o reg) on all data: 0.049715716
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.9035483e-06
Norm of the params: 12.144541
              Random: fixed  29 labels. Loss 0.04972. Accuracy 0.993.
### Flips: 205, rs: 17, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004397688
Train loss (w/o reg) on all data: 0.0025526027
Test loss (w/o reg) on all data: 0.0035622478
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.868599e-09
Norm of the params: 6.074677
     Influence (LOO): fixed 179 labels. Loss 0.00356. Accuracy 0.999.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1447471e-08
Norm of the params: 6.0928082
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12805043
Train loss (w/o reg) on all data: 0.12061098
Test loss (w/o reg) on all data: 0.04831364
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.337688e-06
Norm of the params: 12.197911
              Random: fixed  34 labels. Loss 0.04831. Accuracy 0.994.
### Flips: 205, rs: 17, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013705
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9293073e-08
Norm of the params: 6.0927877
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.070051e-08
Norm of the params: 6.0928173
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124070965
Train loss (w/o reg) on all data: 0.11673909
Test loss (w/o reg) on all data: 0.04645923
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.651792e-06
Norm of the params: 12.109399
              Random: fixed  40 labels. Loss 0.04646. Accuracy 0.996.
### Flips: 205, rs: 17, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601051
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1677824e-08
Norm of the params: 6.0928397
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.103543e-08
Norm of the params: 6.0928264
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118922755
Train loss (w/o reg) on all data: 0.11161943
Test loss (w/o reg) on all data: 0.044428345
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9139286e-06
Norm of the params: 12.0858
              Random: fixed  47 labels. Loss 0.04443. Accuracy 0.995.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1538879
Train loss (w/o reg) on all data: 0.146827
Test loss (w/o reg) on all data: 0.051355463
Train acc on all data:  0.9581813761244834
Test acc on all data:   1.0
Norm of the mean of gradients: 5.189194e-06
Norm of the params: 11.883518
Flipped loss: 0.05136. Accuracy: 1.000
### Flips: 205, rs: 18, checks: 205
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024114968
Train loss (w/o reg) on all data: 0.020244388
Test loss (w/o reg) on all data: 0.007813525
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.2884025e-07
Norm of the params: 8.798387
     Influence (LOO): fixed 157 labels. Loss 0.00781. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7351394e-08
Norm of the params: 6.09282
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14567252
Train loss (w/o reg) on all data: 0.13859652
Test loss (w/o reg) on all data: 0.0478819
Train acc on all data:  0.9608558230002431
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6328377e-06
Norm of the params: 11.896219
              Random: fixed  13 labels. Loss 0.04788. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077338563
Train loss (w/o reg) on all data: 0.005121056
Test loss (w/o reg) on all data: 0.0038490607
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5207006e-07
Norm of the params: 7.228832
     Influence (LOO): fixed 171 labels. Loss 0.00385. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.778143e-08
Norm of the params: 6.0928197
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14080445
Train loss (w/o reg) on all data: 0.13373698
Test loss (w/o reg) on all data: 0.0462636
Train acc on all data:  0.963044006807683
Test acc on all data:   1.0
Norm of the mean of gradients: 2.06149e-06
Norm of the params: 11.889045
              Random: fixed  20 labels. Loss 0.04626. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4257764e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7127785e-08
Norm of the params: 6.0928187
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13211213
Train loss (w/o reg) on all data: 0.124742106
Test loss (w/o reg) on all data: 0.043496743
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 8.61242e-06
Norm of the params: 12.140866
              Random: fixed  30 labels. Loss 0.04350. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601053
Test loss (w/o reg) on all data: 0.0026560314
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7223174e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2990206e-08
Norm of the params: 6.0928135
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12793559
Train loss (w/o reg) on all data: 0.120807976
Test loss (w/o reg) on all data: 0.0420324
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5027188e-06
Norm of the params: 11.939524
              Random: fixed  37 labels. Loss 0.04203. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5084886e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5925325e-08
Norm of the params: 6.0928183
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12239237
Train loss (w/o reg) on all data: 0.11523071
Test loss (w/o reg) on all data: 0.040481117
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6194302e-06
Norm of the params: 11.968008
              Random: fixed  44 labels. Loss 0.04048. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601246
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3077852e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0470602e-08
Norm of the params: 6.0928173
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11665807
Train loss (w/o reg) on all data: 0.10945505
Test loss (w/o reg) on all data: 0.03808704
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.4323086e-06
Norm of the params: 12.002518
              Random: fixed  52 labels. Loss 0.03809. Accuracy 0.998.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14850424
Train loss (w/o reg) on all data: 0.14103451
Test loss (w/o reg) on all data: 0.05388513
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6955247e-06
Norm of the params: 12.222709
Flipped loss: 0.05389. Accuracy: 0.998
### Flips: 205, rs: 19, checks: 205
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017814392
Train loss (w/o reg) on all data: 0.014555738
Test loss (w/o reg) on all data: 0.0065638553
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7830885e-07
Norm of the params: 8.072985
     Influence (LOO): fixed 158 labels. Loss 0.00656. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033283643
Train loss (w/o reg) on all data: 0.0012077944
Test loss (w/o reg) on all data: 0.002752565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9595896e-08
Norm of the params: 6.5124035
                Loss: fixed 170 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14598416
Train loss (w/o reg) on all data: 0.13866454
Test loss (w/o reg) on all data: 0.052571524
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7053056e-06
Norm of the params: 12.099269
              Random: fixed   4 labels. Loss 0.05257. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011825
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5324364e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.897586e-09
Norm of the params: 6.0928187
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14014596
Train loss (w/o reg) on all data: 0.13259806
Test loss (w/o reg) on all data: 0.05051935
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.1253182e-06
Norm of the params: 12.286499
              Random: fixed  12 labels. Loss 0.05052. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.584796e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0589744e-08
Norm of the params: 6.0928164
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1331778
Train loss (w/o reg) on all data: 0.12565489
Test loss (w/o reg) on all data: 0.0473626
Train acc on all data:  0.9647459275468028
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0884344e-06
Norm of the params: 12.266143
              Random: fixed  22 labels. Loss 0.04736. Accuracy 1.000.
### Flips: 205, rs: 19, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0990945e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3383194e-08
Norm of the params: 6.092814
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12564749
Train loss (w/o reg) on all data: 0.11799055
Test loss (w/o reg) on all data: 0.044407878
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.268679e-06
Norm of the params: 12.374925
              Random: fixed  32 labels. Loss 0.04441. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9186758e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2791973e-08
Norm of the params: 6.0928144
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11683214
Train loss (w/o reg) on all data: 0.10929491
Test loss (w/o reg) on all data: 0.040663514
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 6.500554e-06
Norm of the params: 12.2778015
              Random: fixed  44 labels. Loss 0.04066. Accuracy 1.000.
### Flips: 205, rs: 19, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2668292e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.48866715e-08
Norm of the params: 6.0928173
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10905245
Train loss (w/o reg) on all data: 0.101687245
Test loss (w/o reg) on all data: 0.036972947
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0735138e-06
Norm of the params: 12.136886
              Random: fixed  56 labels. Loss 0.03697. Accuracy 0.999.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674938
Train loss (w/o reg) on all data: 0.13941208
Test loss (w/o reg) on all data: 0.05953477
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.513124e-06
Norm of the params: 12.113879
Flipped loss: 0.05953. Accuracy: 0.991
### Flips: 205, rs: 20, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019693159
Train loss (w/o reg) on all data: 0.01590908
Test loss (w/o reg) on all data: 0.0064801835
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6907003e-07
Norm of the params: 8.699516
     Influence (LOO): fixed 156 labels. Loss 0.00648. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032601203
Train loss (w/o reg) on all data: 0.0011785391
Test loss (w/o reg) on all data: 0.0035768996
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.831603e-08
Norm of the params: 6.4522576
                Loss: fixed 171 labels. Loss 0.00358. Accuracy 0.999.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14177418
Train loss (w/o reg) on all data: 0.13422298
Test loss (w/o reg) on all data: 0.057748925
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.5391993e-06
Norm of the params: 12.289175
              Random: fixed   7 labels. Loss 0.05775. Accuracy 0.991.
### Flips: 205, rs: 20, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005544957
Train loss (w/o reg) on all data: 0.0031628942
Test loss (w/o reg) on all data: 0.0036075618
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.711655e-08
Norm of the params: 6.9022646
     Influence (LOO): fixed 170 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0922905e-08
Norm of the params: 6.092822
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13313074
Train loss (w/o reg) on all data: 0.12567414
Test loss (w/o reg) on all data: 0.05443859
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.044582e-06
Norm of the params: 12.211957
              Random: fixed  19 labels. Loss 0.05444. Accuracy 0.992.
### Flips: 205, rs: 20, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.04709e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17505365e-08
Norm of the params: 6.092818
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12409037
Train loss (w/o reg) on all data: 0.1165605
Test loss (w/o reg) on all data: 0.04984649
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.7867363e-06
Norm of the params: 12.271819
              Random: fixed  33 labels. Loss 0.04985. Accuracy 0.993.
### Flips: 205, rs: 20, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4569557e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1261345e-08
Norm of the params: 6.0928144
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12013184
Train loss (w/o reg) on all data: 0.11243568
Test loss (w/o reg) on all data: 0.047257084
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6320724e-05
Norm of the params: 12.406582
              Random: fixed  39 labels. Loss 0.04726. Accuracy 0.995.
### Flips: 205, rs: 20, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011173
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3321073e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4818721e-08
Norm of the params: 6.09281
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11342108
Train loss (w/o reg) on all data: 0.10570815
Test loss (w/o reg) on all data: 0.044515193
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9025184e-06
Norm of the params: 12.420088
              Random: fixed  48 labels. Loss 0.04452. Accuracy 0.996.
### Flips: 205, rs: 20, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560472
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9288548e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7897562e-08
Norm of the params: 6.092803
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10837091
Train loss (w/o reg) on all data: 0.10061339
Test loss (w/o reg) on all data: 0.042197425
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4526598e-05
Norm of the params: 12.455935
              Random: fixed  54 labels. Loss 0.04220. Accuracy 0.996.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1524428
Train loss (w/o reg) on all data: 0.14548843
Test loss (w/o reg) on all data: 0.055862892
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.4019283e-06
Norm of the params: 11.793533
Flipped loss: 0.05586. Accuracy: 0.996
### Flips: 205, rs: 21, checks: 205
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023490254
Train loss (w/o reg) on all data: 0.019839881
Test loss (w/o reg) on all data: 0.0078148525
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3009766e-07
Norm of the params: 8.54444
     Influence (LOO): fixed 158 labels. Loss 0.00781. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3168264e-08
Norm of the params: 6.0928035
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14612143
Train loss (w/o reg) on all data: 0.1390317
Test loss (w/o reg) on all data: 0.052810404
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.8252347e-06
Norm of the params: 11.907755
              Random: fixed   8 labels. Loss 0.05281. Accuracy 0.996.
### Flips: 205, rs: 21, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007722269
Train loss (w/o reg) on all data: 0.0053861677
Test loss (w/o reg) on all data: 0.0033958124
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5971805e-08
Norm of the params: 6.835351
     Influence (LOO): fixed 173 labels. Loss 0.00340. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9222263e-08
Norm of the params: 6.0928335
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13928033
Train loss (w/o reg) on all data: 0.13184024
Test loss (w/o reg) on all data: 0.04977057
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.540087e-06
Norm of the params: 12.198429
              Random: fixed  17 labels. Loss 0.04977. Accuracy 0.999.
### Flips: 205, rs: 21, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005203963
Train loss (w/o reg) on all data: 0.0031816773
Test loss (w/o reg) on all data: 0.003155274
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.494782e-08
Norm of the params: 6.359694
     Influence (LOO): fixed 175 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.626557e-08
Norm of the params: 6.092824
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13131359
Train loss (w/o reg) on all data: 0.123763196
Test loss (w/o reg) on all data: 0.046663627
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.082331e-05
Norm of the params: 12.28853
              Random: fixed  29 labels. Loss 0.04666. Accuracy 0.999.
### Flips: 205, rs: 21, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504589
Test loss (w/o reg) on all data: 0.0026117754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4751369e-08
Norm of the params: 5.941995
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.72891e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827243
Train loss (w/o reg) on all data: 0.120875485
Test loss (w/o reg) on all data: 0.045686223
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.324908e-06
Norm of the params: 12.163008
              Random: fixed  33 labels. Loss 0.04569. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158243
Train loss (w/o reg) on all data: 0.0018504628
Test loss (w/o reg) on all data: 0.002611797
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.510842e-08
Norm of the params: 5.941989
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.03876e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12168763
Train loss (w/o reg) on all data: 0.11425366
Test loss (w/o reg) on all data: 0.042780984
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.08405275e-05
Norm of the params: 12.193416
              Random: fixed  41 labels. Loss 0.04278. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850459
Test loss (w/o reg) on all data: 0.0026117868
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6962058e-08
Norm of the params: 5.9419937
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.867316e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11437007
Train loss (w/o reg) on all data: 0.10699672
Test loss (w/o reg) on all data: 0.040296935
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.050395e-05
Norm of the params: 12.143597
              Random: fixed  52 labels. Loss 0.04030. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15215708
Train loss (w/o reg) on all data: 0.14384404
Test loss (w/o reg) on all data: 0.05285658
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.8808832e-06
Norm of the params: 12.894215
Flipped loss: 0.05286. Accuracy: 0.999
### Flips: 205, rs: 22, checks: 205
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022134848
Train loss (w/o reg) on all data: 0.018623319
Test loss (w/o reg) on all data: 0.007503975
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 5.671178e-07
Norm of the params: 8.38037
     Influence (LOO): fixed 160 labels. Loss 0.00750. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601081
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7303657e-08
Norm of the params: 6.092834
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14607418
Train loss (w/o reg) on all data: 0.13782579
Test loss (w/o reg) on all data: 0.05047724
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.0680967e-06
Norm of the params: 12.843972
              Random: fixed  10 labels. Loss 0.05048. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008224226
Train loss (w/o reg) on all data: 0.0054489607
Test loss (w/o reg) on all data: 0.0040640174
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 6.636406e-08
Norm of the params: 7.4501877
     Influence (LOO): fixed 172 labels. Loss 0.00406. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5046318e-08
Norm of the params: 6.0928173
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14002043
Train loss (w/o reg) on all data: 0.13164835
Test loss (w/o reg) on all data: 0.04766421
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.683769e-06
Norm of the params: 12.939928
              Random: fixed  18 labels. Loss 0.04766. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044750604
Train loss (w/o reg) on all data: 0.0023959503
Test loss (w/o reg) on all data: 0.003228123
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8390233e-08
Norm of the params: 6.4484262
     Influence (LOO): fixed 176 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1842617e-08
Norm of the params: 6.0928183
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13174908
Train loss (w/o reg) on all data: 0.12354091
Test loss (w/o reg) on all data: 0.045059264
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8402094e-05
Norm of the params: 12.812632
              Random: fixed  30 labels. Loss 0.04506. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601222
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5931592e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3282896e-08
Norm of the params: 6.0928154
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124434136
Train loss (w/o reg) on all data: 0.11613761
Test loss (w/o reg) on all data: 0.042394865
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.983371e-06
Norm of the params: 12.881404
              Random: fixed  39 labels. Loss 0.04239. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601432
Test loss (w/o reg) on all data: 0.0026561
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9631754e-08
Norm of the params: 6.0927763
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601283
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.832678e-08
Norm of the params: 6.092801
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12337101
Train loss (w/o reg) on all data: 0.115055665
Test loss (w/o reg) on all data: 0.042083763
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8862352e-06
Norm of the params: 12.896003
              Random: fixed  41 labels. Loss 0.04208. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013176
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3374115e-08
Norm of the params: 6.0927954
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012396
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9224592e-08
Norm of the params: 6.0928082
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11844201
Train loss (w/o reg) on all data: 0.1103691
Test loss (w/o reg) on all data: 0.039559923
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3135006e-06
Norm of the params: 12.706618
              Random: fixed  49 labels. Loss 0.03956. Accuracy 0.999.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1563925
Train loss (w/o reg) on all data: 0.14929827
Test loss (w/o reg) on all data: 0.054498468
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.147867e-06
Norm of the params: 11.911533
Flipped loss: 0.05450. Accuracy: 0.997
### Flips: 205, rs: 23, checks: 205
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023606693
Train loss (w/o reg) on all data: 0.019577892
Test loss (w/o reg) on all data: 0.0073490622
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3012096e-06
Norm of the params: 8.976414
     Influence (LOO): fixed 158 labels. Loss 0.00735. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960096
Test loss (w/o reg) on all data: 0.0026560093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.126295e-08
Norm of the params: 6.0928535
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15226199
Train loss (w/o reg) on all data: 0.14522699
Test loss (w/o reg) on all data: 0.05369074
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.0083514e-06
Norm of the params: 11.861706
              Random: fixed   5 labels. Loss 0.05369. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 410
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012583791
Train loss (w/o reg) on all data: 0.009779329
Test loss (w/o reg) on all data: 0.004227249
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4272103e-07
Norm of the params: 7.4892745
     Influence (LOO): fixed 170 labels. Loss 0.00423. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601126
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19674715e-08
Norm of the params: 6.092826
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14455415
Train loss (w/o reg) on all data: 0.13744967
Test loss (w/o reg) on all data: 0.050884604
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.2681574e-05
Norm of the params: 11.920136
              Random: fixed  15 labels. Loss 0.05088. Accuracy 0.998.
### Flips: 205, rs: 23, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.00263374
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2536286e-08
Norm of the params: 5.928661
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206072e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13496986
Train loss (w/o reg) on all data: 0.12756452
Test loss (w/o reg) on all data: 0.048502274
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.01080295e-05
Norm of the params: 12.169912
              Random: fixed  28 labels. Loss 0.04850. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955887
Test loss (w/o reg) on all data: 0.002633732
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9300872e-08
Norm of the params: 5.928653
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2202066e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12886952
Train loss (w/o reg) on all data: 0.12190286
Test loss (w/o reg) on all data: 0.043978903
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0303042e-06
Norm of the params: 11.803945
              Random: fixed  38 labels. Loss 0.04398. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955869
Test loss (w/o reg) on all data: 0.0026337395
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1966693e-08
Norm of the params: 5.928655
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206445e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12046012
Train loss (w/o reg) on all data: 0.11374559
Test loss (w/o reg) on all data: 0.040882304
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2403834e-05
Norm of the params: 11.588381
              Random: fixed  50 labels. Loss 0.04088. Accuracy 0.995.
### Flips: 205, rs: 23, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955862
Test loss (w/o reg) on all data: 0.002633741
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1626343e-08
Norm of the params: 5.9286575
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206423e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11541239
Train loss (w/o reg) on all data: 0.10875023
Test loss (w/o reg) on all data: 0.036202747
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.923951e-06
Norm of the params: 11.543104
              Random: fixed  58 labels. Loss 0.03620. Accuracy 0.999.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15707289
Train loss (w/o reg) on all data: 0.14941399
Test loss (w/o reg) on all data: 0.05264052
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.3926203e-06
Norm of the params: 12.376513
Flipped loss: 0.05264. Accuracy: 0.996
### Flips: 205, rs: 24, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022013575
Train loss (w/o reg) on all data: 0.018774675
Test loss (w/o reg) on all data: 0.006296098
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 1.062807e-06
Norm of the params: 8.048477
     Influence (LOO): fixed 164 labels. Loss 0.00630. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560312
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7852696e-08
Norm of the params: 6.0928135
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15077044
Train loss (w/o reg) on all data: 0.14299463
Test loss (w/o reg) on all data: 0.050253477
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.6338853e-06
Norm of the params: 12.4706135
              Random: fixed   9 labels. Loss 0.05025. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01008534
Train loss (w/o reg) on all data: 0.007626568
Test loss (w/o reg) on all data: 0.0036699192
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5011245e-07
Norm of the params: 7.012521
     Influence (LOO): fixed 177 labels. Loss 0.00367. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6648931e-08
Norm of the params: 6.092802
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14398524
Train loss (w/o reg) on all data: 0.1362255
Test loss (w/o reg) on all data: 0.04824341
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.398177e-06
Norm of the params: 12.457722
              Random: fixed  18 labels. Loss 0.04824. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059347358
Train loss (w/o reg) on all data: 0.0039639543
Test loss (w/o reg) on all data: 0.0031905966
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.643278e-08
Norm of the params: 6.2781863
     Influence (LOO): fixed 180 labels. Loss 0.00319. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1025606e-08
Norm of the params: 6.092812
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13955954
Train loss (w/o reg) on all data: 0.13190183
Test loss (w/o reg) on all data: 0.047152217
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.097326e-05
Norm of the params: 12.375542
              Random: fixed  24 labels. Loss 0.04715. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495589
Test loss (w/o reg) on all data: 0.0026337483
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4547457e-08
Norm of the params: 5.928652
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206786e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1334579
Train loss (w/o reg) on all data: 0.12573394
Test loss (w/o reg) on all data: 0.043785457
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9369044e-06
Norm of the params: 12.428967
              Random: fixed  33 labels. Loss 0.04379. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955864
Test loss (w/o reg) on all data: 0.0026337455
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6321754e-08
Norm of the params: 5.928657
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206375e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12306404
Train loss (w/o reg) on all data: 0.11572788
Test loss (w/o reg) on all data: 0.038446747
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.601585e-06
Norm of the params: 12.112938
              Random: fixed  46 labels. Loss 0.03845. Accuracy 0.997.
### Flips: 205, rs: 24, checks: 1230
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955922
Test loss (w/o reg) on all data: 0.0026337502
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8581776e-08
Norm of the params: 5.928646
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206441e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11481981
Train loss (w/o reg) on all data: 0.10731756
Test loss (w/o reg) on all data: 0.03607945
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4843766e-06
Norm of the params: 12.249288
              Random: fixed  57 labels. Loss 0.03608. Accuracy 0.997.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149245
Train loss (w/o reg) on all data: 0.14129798
Test loss (w/o reg) on all data: 0.052479126
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.297086e-05
Norm of the params: 12.607147
Flipped loss: 0.05248. Accuracy: 0.999
### Flips: 205, rs: 25, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022829743
Train loss (w/o reg) on all data: 0.01959975
Test loss (w/o reg) on all data: 0.00679966
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7965235e-07
Norm of the params: 8.037403
     Influence (LOO): fixed 154 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7693596e-08
Norm of the params: 6.0928154
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14308906
Train loss (w/o reg) on all data: 0.13517466
Test loss (w/o reg) on all data: 0.049661923
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.121804e-06
Norm of the params: 12.581246
              Random: fixed   9 labels. Loss 0.04966. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011982778
Train loss (w/o reg) on all data: 0.009404367
Test loss (w/o reg) on all data: 0.004437299
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4059846e-07
Norm of the params: 7.1811028
     Influence (LOO): fixed 165 labels. Loss 0.00444. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4087346e-08
Norm of the params: 6.0928063
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13575596
Train loss (w/o reg) on all data: 0.12765515
Test loss (w/o reg) on all data: 0.04661248
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4515324e-06
Norm of the params: 12.728561
              Random: fixed  18 labels. Loss 0.04661. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049584694
Train loss (w/o reg) on all data: 0.0031965852
Test loss (w/o reg) on all data: 0.0030820032
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7978317e-08
Norm of the params: 5.9361334
     Influence (LOO): fixed 171 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560363
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.117875e-08
Norm of the params: 6.092826
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13286054
Train loss (w/o reg) on all data: 0.12466574
Test loss (w/o reg) on all data: 0.045655828
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5066654e-06
Norm of the params: 12.802184
              Random: fixed  22 labels. Loss 0.04566. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495587
Test loss (w/o reg) on all data: 0.002633734
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.669143e-08
Norm of the params: 5.9286556
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206983e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486732
Train loss (w/o reg) on all data: 0.11663292
Test loss (w/o reg) on all data: 0.043952625
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9882364e-06
Norm of the params: 12.83308
              Random: fixed  32 labels. Loss 0.04395. Accuracy 0.998.
### Flips: 205, rs: 25, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253034
Train loss (w/o reg) on all data: 0.0024955892
Test loss (w/o reg) on all data: 0.0026337388
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6424226e-08
Norm of the params: 5.9286504
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206367e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116093144
Train loss (w/o reg) on all data: 0.10787307
Test loss (w/o reg) on all data: 0.040869415
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8846427e-06
Norm of the params: 12.821914
              Random: fixed  44 labels. Loss 0.04087. Accuracy 0.998.
### Flips: 205, rs: 25, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024956
Test loss (w/o reg) on all data: 0.0026337663
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2700733e-08
Norm of the params: 5.928634
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206393e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111387074
Train loss (w/o reg) on all data: 0.103204854
Test loss (w/o reg) on all data: 0.038362898
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4158984e-06
Norm of the params: 12.792358
              Random: fixed  51 labels. Loss 0.03836. Accuracy 0.997.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15231165
Train loss (w/o reg) on all data: 0.14423004
Test loss (w/o reg) on all data: 0.05365009
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 9.318822e-06
Norm of the params: 12.713471
Flipped loss: 0.05365. Accuracy: 1.000
### Flips: 205, rs: 26, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021986462
Train loss (w/o reg) on all data: 0.018138109
Test loss (w/o reg) on all data: 0.0061742286
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4177316e-07
Norm of the params: 8.7730875
     Influence (LOO): fixed 159 labels. Loss 0.00617. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1246674e-08
Norm of the params: 6.092824
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15110828
Train loss (w/o reg) on all data: 0.14305823
Test loss (w/o reg) on all data: 0.05293997
Train acc on all data:  0.9574519815220034
Test acc on all data:   1.0
Norm of the mean of gradients: 4.187163e-06
Norm of the params: 12.688621
              Random: fixed   2 labels. Loss 0.05294. Accuracy 1.000.
### Flips: 205, rs: 26, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006315386
Train loss (w/o reg) on all data: 0.004266466
Test loss (w/o reg) on all data: 0.0030553227
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8286038e-08
Norm of the params: 6.4014378
     Influence (LOO): fixed 175 labels. Loss 0.00306. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5009359e-08
Norm of the params: 6.092823
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14387055
Train loss (w/o reg) on all data: 0.13596365
Test loss (w/o reg) on all data: 0.050668757
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6891087e-06
Norm of the params: 12.5752945
              Random: fixed  11 labels. Loss 0.05067. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504544
Test loss (w/o reg) on all data: 0.0026117754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5849056e-08
Norm of the params: 5.942002
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.736335e-09
Norm of the params: 6.092822
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1377862
Train loss (w/o reg) on all data: 0.13005044
Test loss (w/o reg) on all data: 0.047196418
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.832724e-06
Norm of the params: 12.438456
              Random: fixed  21 labels. Loss 0.04720. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504563
Test loss (w/o reg) on all data: 0.0026117552
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.499046e-08
Norm of the params: 5.941999
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.877148e-09
Norm of the params: 6.092823
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13268241
Train loss (w/o reg) on all data: 0.12514776
Test loss (w/o reg) on all data: 0.04551049
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.1192127e-06
Norm of the params: 12.275708
              Random: fixed  29 labels. Loss 0.04551. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850451
Test loss (w/o reg) on all data: 0.0026117729
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.161472e-09
Norm of the params: 5.942007
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.748401e-09
Norm of the params: 6.0928226
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12498231
Train loss (w/o reg) on all data: 0.117363654
Test loss (w/o reg) on all data: 0.042006064
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3049103e-06
Norm of the params: 12.34395
              Random: fixed  40 labels. Loss 0.04201. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158245
Train loss (w/o reg) on all data: 0.001850468
Test loss (w/o reg) on all data: 0.0026118055
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1013283e-07
Norm of the params: 5.9419804
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.318285e-09
Norm of the params: 6.0928216
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11933407
Train loss (w/o reg) on all data: 0.11148314
Test loss (w/o reg) on all data: 0.03876385
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.913852e-06
Norm of the params: 12.530707
              Random: fixed  48 labels. Loss 0.03876. Accuracy 0.999.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14791891
Train loss (w/o reg) on all data: 0.13914073
Test loss (w/o reg) on all data: 0.057765994
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0497197e-06
Norm of the params: 13.250046
Flipped loss: 0.05777. Accuracy: 0.998
### Flips: 205, rs: 27, checks: 205
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027919877
Train loss (w/o reg) on all data: 0.023289967
Test loss (w/o reg) on all data: 0.011536727
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7944156e-06
Norm of the params: 9.622796
     Influence (LOO): fixed 151 labels. Loss 0.01154. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2970449e-08
Norm of the params: 6.092827
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1444006
Train loss (w/o reg) on all data: 0.13557099
Test loss (w/o reg) on all data: 0.054627012
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1424938e-06
Norm of the params: 13.288803
              Random: fixed   7 labels. Loss 0.05463. Accuracy 0.998.
### Flips: 205, rs: 27, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009190837
Train loss (w/o reg) on all data: 0.006267865
Test loss (w/o reg) on all data: 0.004813948
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1799963e-07
Norm of the params: 7.645877
     Influence (LOO): fixed 173 labels. Loss 0.00481. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5480905e-08
Norm of the params: 6.0928235
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13978696
Train loss (w/o reg) on all data: 0.13096584
Test loss (w/o reg) on all data: 0.051766984
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.934628e-06
Norm of the params: 13.2824
              Random: fixed  14 labels. Loss 0.05177. Accuracy 0.998.
### Flips: 205, rs: 27, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560368
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5889847e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2311472e-08
Norm of the params: 6.09282
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13237263
Train loss (w/o reg) on all data: 0.123935096
Test loss (w/o reg) on all data: 0.04620541
Train acc on all data:  0.963287138341843
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6571397e-06
Norm of the params: 12.99041
              Random: fixed  26 labels. Loss 0.04621. Accuracy 1.000.
### Flips: 205, rs: 27, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2517765e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6401635e-09
Norm of the params: 6.0928164
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12741716
Train loss (w/o reg) on all data: 0.11925719
Test loss (w/o reg) on all data: 0.04474575
Train acc on all data:  0.9649890590809628
Test acc on all data:   1.0
Norm of the mean of gradients: 4.538861e-06
Norm of the params: 12.77496
              Random: fixed  33 labels. Loss 0.04475. Accuracy 1.000.
### Flips: 205, rs: 27, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601263
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6718272e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6552063e-08
Norm of the params: 6.0928154
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120117486
Train loss (w/o reg) on all data: 0.11227598
Test loss (w/o reg) on all data: 0.04139336
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.1176723e-06
Norm of the params: 12.523186
              Random: fixed  44 labels. Loss 0.04139. Accuracy 0.999.
### Flips: 205, rs: 27, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960111
Test loss (w/o reg) on all data: 0.002656037
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5199931e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4841617e-08
Norm of the params: 6.0928197
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11380543
Train loss (w/o reg) on all data: 0.10584418
Test loss (w/o reg) on all data: 0.0391335
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.157427e-06
Norm of the params: 12.618438
              Random: fixed  52 labels. Loss 0.03913. Accuracy 0.999.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15432999
Train loss (w/o reg) on all data: 0.14618419
Test loss (w/o reg) on all data: 0.05598647
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6282688e-06
Norm of the params: 12.763853
Flipped loss: 0.05599. Accuracy: 0.996
### Flips: 205, rs: 28, checks: 205
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029001717
Train loss (w/o reg) on all data: 0.024803732
Test loss (w/o reg) on all data: 0.0084646335
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2745827e-07
Norm of the params: 9.162951
     Influence (LOO): fixed 158 labels. Loss 0.00846. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4850593e-08
Norm of the params: 6.092813
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641382
Train loss (w/o reg) on all data: 0.1382602
Test loss (w/o reg) on all data: 0.053372744
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.254207e-06
Norm of the params: 12.769977
              Random: fixed  12 labels. Loss 0.05337. Accuracy 0.995.
### Flips: 205, rs: 28, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00926818
Train loss (w/o reg) on all data: 0.006729458
Test loss (w/o reg) on all data: 0.0041651586
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3247583e-07
Norm of the params: 7.1256185
     Influence (LOO): fixed 180 labels. Loss 0.00417. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560829
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8422418e-08
Norm of the params: 6.092816
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14200687
Train loss (w/o reg) on all data: 0.13373877
Test loss (w/o reg) on all data: 0.051658027
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1111896e-06
Norm of the params: 12.859321
              Random: fixed  18 labels. Loss 0.05166. Accuracy 0.996.
### Flips: 205, rs: 28, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560382
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1649123e-08
Norm of the params: 6.092827
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0737202e-08
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1357198
Train loss (w/o reg) on all data: 0.12775639
Test loss (w/o reg) on all data: 0.04904387
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.672101e-06
Norm of the params: 12.620155
              Random: fixed  26 labels. Loss 0.04904. Accuracy 0.993.
### Flips: 205, rs: 28, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012774
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2771107e-08
Norm of the params: 6.092802
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7824375e-08
Norm of the params: 6.092812
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12787609
Train loss (w/o reg) on all data: 0.11982573
Test loss (w/o reg) on all data: 0.04521261
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.488376e-06
Norm of the params: 12.68886
              Random: fixed  38 labels. Loss 0.04521. Accuracy 0.994.
### Flips: 205, rs: 28, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3102523e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.04054e-09
Norm of the params: 6.0928144
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12545997
Train loss (w/o reg) on all data: 0.11735973
Test loss (w/o reg) on all data: 0.04467833
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.642898e-06
Norm of the params: 12.728111
              Random: fixed  41 labels. Loss 0.04468. Accuracy 0.995.
### Flips: 205, rs: 28, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012943
Test loss (w/o reg) on all data: 0.0026560905
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4897362e-08
Norm of the params: 6.0928
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5916312e-08
Norm of the params: 6.0928097
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11620463
Train loss (w/o reg) on all data: 0.10826476
Test loss (w/o reg) on all data: 0.04163079
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5287707e-06
Norm of the params: 12.601482
              Random: fixed  53 labels. Loss 0.04163. Accuracy 0.995.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15801403
Train loss (w/o reg) on all data: 0.15047182
Test loss (w/o reg) on all data: 0.058964565
Train acc on all data:  0.9542912715779237
Test acc on all data:   1.0
Norm of the mean of gradients: 8.669933e-06
Norm of the params: 12.281859
Flipped loss: 0.05896. Accuracy: 1.000
### Flips: 205, rs: 29, checks: 205
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031075552
Train loss (w/o reg) on all data: 0.02667748
Test loss (w/o reg) on all data: 0.011665696
Train acc on all data:  0.9934354485776805
Test acc on all data:   1.0
Norm of the mean of gradients: 5.288962e-07
Norm of the params: 9.378776
     Influence (LOO): fixed 162 labels. Loss 0.01167. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2255799e-08
Norm of the params: 6.0928254
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15248086
Train loss (w/o reg) on all data: 0.1449311
Test loss (w/o reg) on all data: 0.05549891
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.3410214e-06
Norm of the params: 12.288013
              Random: fixed  10 labels. Loss 0.05550. Accuracy 0.999.
### Flips: 205, rs: 29, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00979028
Train loss (w/o reg) on all data: 0.0072648316
Test loss (w/o reg) on all data: 0.004573706
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2546237e-07
Norm of the params: 7.106966
     Influence (LOO): fixed 183 labels. Loss 0.00457. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.41020235e-08
Norm of the params: 6.092818
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14625071
Train loss (w/o reg) on all data: 0.13857917
Test loss (w/o reg) on all data: 0.05359805
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.951495e-06
Norm of the params: 12.386713
              Random: fixed  18 labels. Loss 0.05360. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005400412
Train loss (w/o reg) on all data: 0.0033894903
Test loss (w/o reg) on all data: 0.0030398264
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7605718e-08
Norm of the params: 6.3418007
     Influence (LOO): fixed 187 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0643679e-08
Norm of the params: 6.0928187
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13712192
Train loss (w/o reg) on all data: 0.12950017
Test loss (w/o reg) on all data: 0.04938849
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.4829745e-06
Norm of the params: 12.34646
              Random: fixed  31 labels. Loss 0.04939. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.423926e-08
Norm of the params: 6.092815
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.931866e-08
Norm of the params: 6.0928173
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13426757
Train loss (w/o reg) on all data: 0.12693255
Test loss (w/o reg) on all data: 0.048061226
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.1214944e-06
Norm of the params: 12.111991
              Random: fixed  36 labels. Loss 0.04806. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3219263e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6735454e-08
Norm of the params: 6.0928187
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128005
Train loss (w/o reg) on all data: 0.12045381
Test loss (w/o reg) on all data: 0.044965915
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4125386e-06
Norm of the params: 12.289175
              Random: fixed  46 labels. Loss 0.04497. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.00826e-08
Norm of the params: 6.092799
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8773079e-08
Norm of the params: 6.092812
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11858481
Train loss (w/o reg) on all data: 0.11075645
Test loss (w/o reg) on all data: 0.0413227
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0099508e-06
Norm of the params: 12.51268
              Random: fixed  57 labels. Loss 0.04132. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14261132
Train loss (w/o reg) on all data: 0.13404131
Test loss (w/o reg) on all data: 0.058534976
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8091993e-06
Norm of the params: 13.091999
Flipped loss: 0.05853. Accuracy: 0.994
### Flips: 205, rs: 30, checks: 205
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020355858
Train loss (w/o reg) on all data: 0.016649308
Test loss (w/o reg) on all data: 0.006942716
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0339433e-07
Norm of the params: 8.609935
     Influence (LOO): fixed 160 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031857602
Train loss (w/o reg) on all data: 0.0010902117
Test loss (w/o reg) on all data: 0.0027034166
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.731106e-07
Norm of the params: 6.4738684
                Loss: fixed 175 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13658328
Train loss (w/o reg) on all data: 0.12823322
Test loss (w/o reg) on all data: 0.055203356
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.4946775e-06
Norm of the params: 12.922888
              Random: fixed   9 labels. Loss 0.05520. Accuracy 0.994.
### Flips: 205, rs: 30, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0085628405
Train loss (w/o reg) on all data: 0.006310287
Test loss (w/o reg) on all data: 0.0038882298
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.00196246e-07
Norm of the params: 6.7120104
     Influence (LOO): fixed 172 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560882
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7112415e-08
Norm of the params: 6.0928044
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162075
Train loss (w/o reg) on all data: 0.12343766
Test loss (w/o reg) on all data: 0.05184074
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.1104336e-06
Norm of the params: 12.793042
              Random: fixed  17 labels. Loss 0.05184. Accuracy 0.996.
### Flips: 205, rs: 30, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615823
Train loss (w/o reg) on all data: 0.0018504523
Test loss (w/o reg) on all data: 0.0026117724
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2073078e-08
Norm of the params: 5.9420047
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.74498e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12947248
Train loss (w/o reg) on all data: 0.12136256
Test loss (w/o reg) on all data: 0.051247254
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.96093e-06
Norm of the params: 12.735718
              Random: fixed  20 labels. Loss 0.05125. Accuracy 0.995.
### Flips: 205, rs: 30, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615823
Train loss (w/o reg) on all data: 0.0018504527
Test loss (w/o reg) on all data: 0.0026117747
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0352521e-08
Norm of the params: 5.9420037
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.745494e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11850791
Train loss (w/o reg) on all data: 0.11037582
Test loss (w/o reg) on all data: 0.04511506
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.558581e-06
Norm of the params: 12.753106
              Random: fixed  37 labels. Loss 0.04512. Accuracy 0.997.
### Flips: 205, rs: 30, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504536
Test loss (w/o reg) on all data: 0.0026117996
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8830297e-08
Norm of the params: 5.942003
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7303125e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11394794
Train loss (w/o reg) on all data: 0.105638504
Test loss (w/o reg) on all data: 0.04336075
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2775692e-06
Norm of the params: 12.89142
              Random: fixed  44 labels. Loss 0.04336. Accuracy 0.997.
### Flips: 205, rs: 30, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011383
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.917458e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3103853e-08
Norm of the params: 6.0928173
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110772684
Train loss (w/o reg) on all data: 0.10254843
Test loss (w/o reg) on all data: 0.042212542
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.655376e-06
Norm of the params: 12.825178
              Random: fixed  48 labels. Loss 0.04221. Accuracy 0.997.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14750078
Train loss (w/o reg) on all data: 0.13956234
Test loss (w/o reg) on all data: 0.053526364
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.3489587e-06
Norm of the params: 12.600349
Flipped loss: 0.05353. Accuracy: 0.997
### Flips: 205, rs: 31, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027288599
Train loss (w/o reg) on all data: 0.022889892
Test loss (w/o reg) on all data: 0.008111198
Train acc on all data:  0.9941648431801605
Test acc on all data:   1.0
Norm of the mean of gradients: 2.989673e-07
Norm of the params: 9.379454
     Influence (LOO): fixed 152 labels. Loss 0.00811. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.08826e-08
Norm of the params: 6.092821
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14429766
Train loss (w/o reg) on all data: 0.13635384
Test loss (w/o reg) on all data: 0.05243178
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2449345e-05
Norm of the params: 12.604618
              Random: fixed   5 labels. Loss 0.05243. Accuracy 0.997.
### Flips: 205, rs: 31, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010945506
Train loss (w/o reg) on all data: 0.008077371
Test loss (w/o reg) on all data: 0.003983864
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0144685e-07
Norm of the params: 7.5738173
     Influence (LOO): fixed 170 labels. Loss 0.00398. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7759023e-08
Norm of the params: 6.092809
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13390812
Train loss (w/o reg) on all data: 0.12602922
Test loss (w/o reg) on all data: 0.045337025
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.377179e-06
Norm of the params: 12.553011
              Random: fixed  21 labels. Loss 0.04534. Accuracy 0.997.
### Flips: 205, rs: 31, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044750604
Train loss (w/o reg) on all data: 0.0023959526
Test loss (w/o reg) on all data: 0.0032281077
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0208835e-08
Norm of the params: 6.448423
     Influence (LOO): fixed 175 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1841352e-08
Norm of the params: 6.0928183
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12852432
Train loss (w/o reg) on all data: 0.12086745
Test loss (w/o reg) on all data: 0.04332405
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4783162e-06
Norm of the params: 12.374869
              Random: fixed  30 labels. Loss 0.04332. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009474
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0892463e-08
Norm of the params: 6.0928564
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1248845e-08
Norm of the params: 6.092819
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12304264
Train loss (w/o reg) on all data: 0.115890026
Test loss (w/o reg) on all data: 0.04150952
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.274395e-05
Norm of the params: 11.96045
              Random: fixed  39 labels. Loss 0.04151. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600932
Test loss (w/o reg) on all data: 0.0026560274
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4669135e-08
Norm of the params: 6.092859
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2052258e-08
Norm of the params: 6.0928106
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10948162
Train loss (w/o reg) on all data: 0.10213783
Test loss (w/o reg) on all data: 0.03665744
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.223631e-06
Norm of the params: 12.119234
              Random: fixed  56 labels. Loss 0.03666. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1461306e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.835559e-09
Norm of the params: 6.092818
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10288662
Train loss (w/o reg) on all data: 0.095536985
Test loss (w/o reg) on all data: 0.033570196
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1719372e-06
Norm of the params: 12.124056
              Random: fixed  65 labels. Loss 0.03357. Accuracy 0.999.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14232507
Train loss (w/o reg) on all data: 0.13498518
Test loss (w/o reg) on all data: 0.05103116
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.982418e-06
Norm of the params: 12.116015
Flipped loss: 0.05103. Accuracy: 0.998
### Flips: 205, rs: 32, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014407062
Train loss (w/o reg) on all data: 0.011409637
Test loss (w/o reg) on all data: 0.0057144025
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0348999e-06
Norm of the params: 7.742642
     Influence (LOO): fixed 156 labels. Loss 0.00571. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1890927e-08
Norm of the params: 6.0928087
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13691807
Train loss (w/o reg) on all data: 0.12933688
Test loss (w/o reg) on all data: 0.049914252
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.8341923e-06
Norm of the params: 12.313564
              Random: fixed   7 labels. Loss 0.04991. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076951394
Train loss (w/o reg) on all data: 0.005304308
Test loss (w/o reg) on all data: 0.0034335647
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2287182e-07
Norm of the params: 6.9149566
     Influence (LOO): fixed 163 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5703334e-08
Norm of the params: 6.0928235
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12833728
Train loss (w/o reg) on all data: 0.120584816
Test loss (w/o reg) on all data: 0.046653148
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9815252e-05
Norm of the params: 12.451873
              Random: fixed  19 labels. Loss 0.04665. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504359
Test loss (w/o reg) on all data: 0.0026117344
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.685418e-08
Norm of the params: 5.942033
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.878408e-09
Norm of the params: 6.092823
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122740634
Train loss (w/o reg) on all data: 0.115008414
Test loss (w/o reg) on all data: 0.044663128
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.068064e-06
Norm of the params: 12.435612
              Random: fixed  27 labels. Loss 0.04466. Accuracy 0.996.
### Flips: 205, rs: 32, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504601
Test loss (w/o reg) on all data: 0.002611799
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7504776e-08
Norm of the params: 5.9419923
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762386e-09
Norm of the params: 6.092822
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11889994
Train loss (w/o reg) on all data: 0.1110321
Test loss (w/o reg) on all data: 0.042282846
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.1224596e-06
Norm of the params: 12.544197
              Random: fixed  32 labels. Loss 0.04228. Accuracy 0.996.
### Flips: 205, rs: 32, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504587
Test loss (w/o reg) on all data: 0.0026117794
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2521882e-08
Norm of the params: 5.9419947
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762567e-09
Norm of the params: 6.092822
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112118766
Train loss (w/o reg) on all data: 0.10405958
Test loss (w/o reg) on all data: 0.03999034
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.995834e-06
Norm of the params: 12.695817
              Random: fixed  40 labels. Loss 0.03999. Accuracy 0.998.
### Flips: 205, rs: 32, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9376595e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.480201e-09
Norm of the params: 6.0928206
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109910205
Train loss (w/o reg) on all data: 0.10197274
Test loss (w/o reg) on all data: 0.03985508
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.867268e-06
Norm of the params: 12.599577
              Random: fixed  43 labels. Loss 0.03986. Accuracy 0.997.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14955904
Train loss (w/o reg) on all data: 0.14183101
Test loss (w/o reg) on all data: 0.052554905
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.183711e-06
Norm of the params: 12.432233
Flipped loss: 0.05255. Accuracy: 0.999
### Flips: 205, rs: 33, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025530916
Train loss (w/o reg) on all data: 0.02162569
Test loss (w/o reg) on all data: 0.0088130925
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 5.728396e-07
Norm of the params: 8.837677
     Influence (LOO): fixed 153 labels. Loss 0.00881. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034638732
Train loss (w/o reg) on all data: 0.0012580276
Test loss (w/o reg) on all data: 0.002846322
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4863928e-08
Norm of the params: 6.6420565
                Loss: fixed 175 labels. Loss 0.00285. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14115745
Train loss (w/o reg) on all data: 0.13349944
Test loss (w/o reg) on all data: 0.049533855
Train acc on all data:  0.9603695599319232
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0872365e-05
Norm of the params: 12.375787
              Random: fixed  13 labels. Loss 0.04953. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010438705
Train loss (w/o reg) on all data: 0.0076318583
Test loss (w/o reg) on all data: 0.004696422
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8130287e-07
Norm of the params: 7.4924574
     Influence (LOO): fixed 170 labels. Loss 0.00470. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3336745e-08
Norm of the params: 6.0928197
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13431644
Train loss (w/o reg) on all data: 0.12659676
Test loss (w/o reg) on all data: 0.046788964
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2888973e-06
Norm of the params: 12.425526
              Random: fixed  22 labels. Loss 0.04679. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504484
Test loss (w/o reg) on all data: 0.0026117587
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.769814e-08
Norm of the params: 5.9420114
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.63992e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12644744
Train loss (w/o reg) on all data: 0.11891763
Test loss (w/o reg) on all data: 0.043715898
Train acc on all data:  0.9659615852176027
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0798785e-06
Norm of the params: 12.271762
              Random: fixed  34 labels. Loss 0.04372. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504615
Test loss (w/o reg) on all data: 0.0026117803
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5625675e-08
Norm of the params: 5.94199
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.729528e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057563
Train loss (w/o reg) on all data: 0.11325657
Test loss (w/o reg) on all data: 0.04182936
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.259625e-06
Norm of the params: 12.098809
              Random: fixed  43 labels. Loss 0.04183. Accuracy 0.999.
### Flips: 205, rs: 33, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504601
Test loss (w/o reg) on all data: 0.0026117787
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.840639e-08
Norm of the params: 5.9419923
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.548096e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11421097
Train loss (w/o reg) on all data: 0.10685645
Test loss (w/o reg) on all data: 0.039756276
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1045245e-06
Norm of the params: 12.128082
              Random: fixed  51 labels. Loss 0.03976. Accuracy 0.998.
### Flips: 205, rs: 33, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7522236e-08
Norm of the params: 6.0928335
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.106109e-08
Norm of the params: 6.092824
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10977122
Train loss (w/o reg) on all data: 0.10262023
Test loss (w/o reg) on all data: 0.038107358
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.7757775e-06
Norm of the params: 11.959088
              Random: fixed  60 labels. Loss 0.03811. Accuracy 0.998.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15682897
Train loss (w/o reg) on all data: 0.14925909
Test loss (w/o reg) on all data: 0.0554334
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.6115707e-06
Norm of the params: 12.304369
Flipped loss: 0.05543. Accuracy: 0.999
### Flips: 205, rs: 34, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024126211
Train loss (w/o reg) on all data: 0.02039363
Test loss (w/o reg) on all data: 0.009346639
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7283935e-07
Norm of the params: 8.640117
     Influence (LOO): fixed 161 labels. Loss 0.00935. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034086392
Train loss (w/o reg) on all data: 0.0011744701
Test loss (w/o reg) on all data: 0.00259981
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3106889e-08
Norm of the params: 6.6845627
                Loss: fixed 182 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15171422
Train loss (w/o reg) on all data: 0.14403154
Test loss (w/o reg) on all data: 0.0531209
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0699011e-05
Norm of the params: 12.395714
              Random: fixed   7 labels. Loss 0.05312. Accuracy 0.999.
### Flips: 205, rs: 34, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012925005
Train loss (w/o reg) on all data: 0.009879616
Test loss (w/o reg) on all data: 0.0057810023
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3431447e-07
Norm of the params: 7.8043427
     Influence (LOO): fixed 172 labels. Loss 0.00578. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601297
Test loss (w/o reg) on all data: 0.0026561033
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0215295e-07
Norm of the params: 6.0928
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14696701
Train loss (w/o reg) on all data: 0.13948074
Test loss (w/o reg) on all data: 0.05209439
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.943453e-06
Norm of the params: 12.236237
              Random: fixed  13 labels. Loss 0.05209. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158243
Train loss (w/o reg) on all data: 0.0018504595
Test loss (w/o reg) on all data: 0.0026117929
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2485765e-08
Norm of the params: 5.941994
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.730752e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14155614
Train loss (w/o reg) on all data: 0.13406329
Test loss (w/o reg) on all data: 0.04959619
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0872277e-06
Norm of the params: 12.241619
              Random: fixed  20 labels. Loss 0.04960. Accuracy 0.997.
### Flips: 205, rs: 34, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504461
Test loss (w/o reg) on all data: 0.0026117738
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6355774e-08
Norm of the params: 5.942015
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762713e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13799669
Train loss (w/o reg) on all data: 0.13061965
Test loss (w/o reg) on all data: 0.04865494
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.6786795e-06
Norm of the params: 12.146646
              Random: fixed  27 labels. Loss 0.04865. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504618
Test loss (w/o reg) on all data: 0.0026117857
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7828473e-08
Norm of the params: 5.9419894
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.764824e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13059017
Train loss (w/o reg) on all data: 0.12326585
Test loss (w/o reg) on all data: 0.045447633
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.090757e-05
Norm of the params: 12.103155
              Random: fixed  38 labels. Loss 0.04545. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504753
Test loss (w/o reg) on all data: 0.0026118155
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3968466e-08
Norm of the params: 5.9419665
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.485679e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11986478
Train loss (w/o reg) on all data: 0.112273775
Test loss (w/o reg) on all data: 0.042020883
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.1080802e-06
Norm of the params: 12.32153
              Random: fixed  51 labels. Loss 0.04202. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1437891
Train loss (w/o reg) on all data: 0.1362636
Test loss (w/o reg) on all data: 0.04910796
Train acc on all data:  0.9598832968636032
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5903222e-05
Norm of the params: 12.26826
Flipped loss: 0.04911. Accuracy: 1.000
### Flips: 205, rs: 35, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016544513
Train loss (w/o reg) on all data: 0.013273043
Test loss (w/o reg) on all data: 0.0074469377
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.250707e-07
Norm of the params: 8.088846
     Influence (LOO): fixed 150 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033229268
Train loss (w/o reg) on all data: 0.0011508467
Test loss (w/o reg) on all data: 0.0027215527
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0271202e-08
Norm of the params: 6.5910244
                Loss: fixed 164 labels. Loss 0.00272. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13342851
Train loss (w/o reg) on all data: 0.12569225
Test loss (w/o reg) on all data: 0.04585224
Train acc on all data:  0.963044006807683
Test acc on all data:   1.0
Norm of the mean of gradients: 7.002721e-06
Norm of the params: 12.438869
              Random: fixed  13 labels. Loss 0.04585. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008086189
Train loss (w/o reg) on all data: 0.005982866
Test loss (w/o reg) on all data: 0.0035166158
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9128077e-08
Norm of the params: 6.4858656
     Influence (LOO): fixed 160 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0578272e-08
Norm of the params: 6.0928197
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123731166
Train loss (w/o reg) on all data: 0.11617768
Test loss (w/o reg) on all data: 0.04255239
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7862272e-06
Norm of the params: 12.291044
              Random: fixed  28 labels. Loss 0.04255. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004958469
Train loss (w/o reg) on all data: 0.00319655
Test loss (w/o reg) on all data: 0.0030819448
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8320072e-08
Norm of the params: 5.936192
     Influence (LOO): fixed 162 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560363
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1337e-08
Norm of the params: 6.092826
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119389966
Train loss (w/o reg) on all data: 0.11189491
Test loss (w/o reg) on all data: 0.041776624
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9031693e-06
Norm of the params: 12.243406
              Random: fixed  34 labels. Loss 0.04178. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337267
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.708223e-08
Norm of the params: 5.9286604
     Influence (LOO): fixed 163 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220679e-08
Norm of the params: 6.0928364
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11398816
Train loss (w/o reg) on all data: 0.10645762
Test loss (w/o reg) on all data: 0.039467815
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.863687e-06
Norm of the params: 12.272359
              Random: fixed  42 labels. Loss 0.03947. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955897
Test loss (w/o reg) on all data: 0.0026337346
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5994373e-08
Norm of the params: 5.928651
     Influence (LOO): fixed 163 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206786e-08
Norm of the params: 6.0928364
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10604519
Train loss (w/o reg) on all data: 0.09839299
Test loss (w/o reg) on all data: 0.03616611
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1329645e-06
Norm of the params: 12.371089
              Random: fixed  52 labels. Loss 0.03617. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05554e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2948558e-08
Norm of the params: 6.092819
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10128443
Train loss (w/o reg) on all data: 0.0937064
Test loss (w/o reg) on all data: 0.03422315
Train acc on all data:  0.9744711889132021
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9924863e-06
Norm of the params: 12.310997
              Random: fixed  59 labels. Loss 0.03422. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1490481
Train loss (w/o reg) on all data: 0.14188944
Test loss (w/o reg) on all data: 0.05307124
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.386671e-06
Norm of the params: 11.96551
Flipped loss: 0.05307. Accuracy: 0.999
### Flips: 205, rs: 36, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026248686
Train loss (w/o reg) on all data: 0.022200681
Test loss (w/o reg) on all data: 0.008307454
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6998668e-07
Norm of the params: 8.997783
     Influence (LOO): fixed 155 labels. Loss 0.00831. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7423571e-08
Norm of the params: 6.0928087
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14057972
Train loss (w/o reg) on all data: 0.13334899
Test loss (w/o reg) on all data: 0.04967067
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.497477e-06
Norm of the params: 12.025579
              Random: fixed  11 labels. Loss 0.04967. Accuracy 0.999.
### Flips: 205, rs: 36, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010936346
Train loss (w/o reg) on all data: 0.008243079
Test loss (w/o reg) on all data: 0.004195696
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.702127e-07
Norm of the params: 7.3393
     Influence (LOO): fixed 170 labels. Loss 0.00420. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0769257e-08
Norm of the params: 6.092809
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13654345
Train loss (w/o reg) on all data: 0.12941308
Test loss (w/o reg) on all data: 0.04835813
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3980455e-06
Norm of the params: 11.9418335
              Random: fixed  18 labels. Loss 0.04836. Accuracy 0.999.
### Flips: 205, rs: 36, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037485412
Train loss (w/o reg) on all data: 0.001961789
Test loss (w/o reg) on all data: 0.003131893
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1360674e-08
Norm of the params: 5.9778795
     Influence (LOO): fixed 176 labels. Loss 0.00313. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3848758e-08
Norm of the params: 6.092821
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12943149
Train loss (w/o reg) on all data: 0.12245324
Test loss (w/o reg) on all data: 0.04634614
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7588332e-05
Norm of the params: 11.813763
              Random: fixed  29 labels. Loss 0.04635. Accuracy 0.998.
### Flips: 205, rs: 36, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6090288e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2670304e-08
Norm of the params: 6.0928164
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12228916
Train loss (w/o reg) on all data: 0.11511959
Test loss (w/o reg) on all data: 0.043396477
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.501661e-06
Norm of the params: 11.974614
              Random: fixed  40 labels. Loss 0.04340. Accuracy 0.998.
### Flips: 205, rs: 36, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601364
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7989996e-08
Norm of the params: 6.092789
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8580282e-08
Norm of the params: 6.0928164
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11256223
Train loss (w/o reg) on all data: 0.10545327
Test loss (w/o reg) on all data: 0.040321626
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9616706e-06
Norm of the params: 11.923894
              Random: fixed  52 labels. Loss 0.04032. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960109
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3412248e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.324502e-09
Norm of the params: 6.092819
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10715308
Train loss (w/o reg) on all data: 0.100291125
Test loss (w/o reg) on all data: 0.038548026
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5376237e-06
Norm of the params: 11.714908
              Random: fixed  60 labels. Loss 0.03855. Accuracy 0.997.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15037176
Train loss (w/o reg) on all data: 0.14322904
Test loss (w/o reg) on all data: 0.058860786
Train acc on all data:  0.9550206661804036
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.1491246e-06
Norm of the params: 11.95217
Flipped loss: 0.05886. Accuracy: 0.993
### Flips: 205, rs: 37, checks: 205
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02317163
Train loss (w/o reg) on all data: 0.019280186
Test loss (w/o reg) on all data: 0.007608566
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9641858e-07
Norm of the params: 8.822068
     Influence (LOO): fixed 165 labels. Loss 0.00761. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003648779
Train loss (w/o reg) on all data: 0.0012964649
Test loss (w/o reg) on all data: 0.0026098024
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6037633e-08
Norm of the params: 6.859029
                Loss: fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14463307
Train loss (w/o reg) on all data: 0.13754736
Test loss (w/o reg) on all data: 0.056057442
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8717028e-06
Norm of the params: 11.904379
              Random: fixed   9 labels. Loss 0.05606. Accuracy 0.993.
### Flips: 205, rs: 37, checks: 410
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006950848
Train loss (w/o reg) on all data: 0.0047367346
Test loss (w/o reg) on all data: 0.0032268448
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2376516e-07
Norm of the params: 6.654493
     Influence (LOO): fixed 181 labels. Loss 0.00323. Accuracy 0.999.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4120439e-08
Norm of the params: 6.092798
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13675772
Train loss (w/o reg) on all data: 0.12959231
Test loss (w/o reg) on all data: 0.0536065
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.306609e-06
Norm of the params: 11.971128
              Random: fixed  20 labels. Loss 0.05361. Accuracy 0.992.
### Flips: 205, rs: 37, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00585163
Train loss (w/o reg) on all data: 0.003902881
Test loss (w/o reg) on all data: 0.00263413
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.995331e-08
Norm of the params: 6.2429943
     Influence (LOO): fixed 182 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012536
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8650067e-08
Norm of the params: 6.092806
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12924357
Train loss (w/o reg) on all data: 0.12198695
Test loss (w/o reg) on all data: 0.050088834
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.079621e-06
Norm of the params: 12.0470915
              Random: fixed  32 labels. Loss 0.05009. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024956204
Test loss (w/o reg) on all data: 0.002633803
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5012472e-07
Norm of the params: 5.9286
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2211122e-08
Norm of the params: 6.092837
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120071955
Train loss (w/o reg) on all data: 0.11289175
Test loss (w/o reg) on all data: 0.046491623
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.36374e-06
Norm of the params: 11.983493
              Random: fixed  45 labels. Loss 0.04649. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955992
Test loss (w/o reg) on all data: 0.0026337588
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6811865e-08
Norm of the params: 5.928636
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206757e-08
Norm of the params: 6.0928364
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11366162
Train loss (w/o reg) on all data: 0.10675054
Test loss (w/o reg) on all data: 0.043876275
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.031673e-06
Norm of the params: 11.756762
              Random: fixed  55 labels. Loss 0.04388. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495593
Test loss (w/o reg) on all data: 0.0026337535
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.195757e-08
Norm of the params: 5.928645
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206537e-08
Norm of the params: 6.0928364
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1073278
Train loss (w/o reg) on all data: 0.10059153
Test loss (w/o reg) on all data: 0.040455338
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.1217086e-06
Norm of the params: 11.607122
              Random: fixed  65 labels. Loss 0.04046. Accuracy 0.995.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15258206
Train loss (w/o reg) on all data: 0.14449997
Test loss (w/o reg) on all data: 0.054844934
Train acc on all data:  0.9567225869195235
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.5578597e-06
Norm of the params: 12.713846
Flipped loss: 0.05484. Accuracy: 0.996
### Flips: 205, rs: 38, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02774588
Train loss (w/o reg) on all data: 0.024021132
Test loss (w/o reg) on all data: 0.008100698
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 9.632937e-07
Norm of the params: 8.631045
     Influence (LOO): fixed 157 labels. Loss 0.00810. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9867554e-08
Norm of the params: 6.092816
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14695767
Train loss (w/o reg) on all data: 0.13922475
Test loss (w/o reg) on all data: 0.05010067
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.165784e-06
Norm of the params: 12.436169
              Random: fixed  11 labels. Loss 0.05010. Accuracy 0.997.
### Flips: 205, rs: 38, checks: 410
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011193203
Train loss (w/o reg) on all data: 0.008365347
Test loss (w/o reg) on all data: 0.004557652
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4087607e-07
Norm of the params: 7.520446
     Influence (LOO): fixed 174 labels. Loss 0.00456. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0064675e-08
Norm of the params: 6.092818
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14122464
Train loss (w/o reg) on all data: 0.13329704
Test loss (w/o reg) on all data: 0.04753311
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.084417e-06
Norm of the params: 12.591741
              Random: fixed  20 labels. Loss 0.04753. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005185454
Train loss (w/o reg) on all data: 0.0031549383
Test loss (w/o reg) on all data: 0.002764501
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.811367e-08
Norm of the params: 6.372623
     Influence (LOO): fixed 180 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.0026560843
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6614562e-08
Norm of the params: 6.0928035
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13330211
Train loss (w/o reg) on all data: 0.12532657
Test loss (w/o reg) on all data: 0.04536608
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5725385e-06
Norm of the params: 12.629758
              Random: fixed  31 labels. Loss 0.04537. Accuracy 0.997.
### Flips: 205, rs: 38, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158229
Train loss (w/o reg) on all data: 0.0018504655
Test loss (w/o reg) on all data: 0.0026118115
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1845156e-07
Norm of the params: 5.941982
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.355286e-09
Norm of the params: 6.0928216
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126138
Train loss (w/o reg) on all data: 0.11773671
Test loss (w/o reg) on all data: 0.042688157
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0474584e-06
Norm of the params: 12.96248
              Random: fixed  40 labels. Loss 0.04269. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504472
Test loss (w/o reg) on all data: 0.002611764
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9746907e-08
Norm of the params: 5.9420147
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.759256e-09
Norm of the params: 6.0928226
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11818192
Train loss (w/o reg) on all data: 0.10953548
Test loss (w/o reg) on all data: 0.040184274
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.555101e-06
Norm of the params: 13.150241
              Random: fixed  50 labels. Loss 0.04018. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504434
Test loss (w/o reg) on all data: 0.0026117489
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3866026e-08
Norm of the params: 5.94202
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.760823e-09
Norm of the params: 6.0928226
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11225788
Train loss (w/o reg) on all data: 0.103635505
Test loss (w/o reg) on all data: 0.03792237
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0194194e-06
Norm of the params: 13.13193
              Random: fixed  59 labels. Loss 0.03792. Accuracy 0.996.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14908299
Train loss (w/o reg) on all data: 0.1408788
Test loss (w/o reg) on all data: 0.058730535
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4942915e-05
Norm of the params: 12.809526
Flipped loss: 0.05873. Accuracy: 0.993
### Flips: 205, rs: 39, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026079673
Train loss (w/o reg) on all data: 0.022140503
Test loss (w/o reg) on all data: 0.0084669385
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9962476e-07
Norm of the params: 8.876001
     Influence (LOO): fixed 158 labels. Loss 0.00847. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041751335
Train loss (w/o reg) on all data: 0.001507792
Test loss (w/o reg) on all data: 0.0029857864
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0793803e-08
Norm of the params: 7.303891
                Loss: fixed 178 labels. Loss 0.00299. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14340506
Train loss (w/o reg) on all data: 0.13530831
Test loss (w/o reg) on all data: 0.05607891
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.601407e-06
Norm of the params: 12.725375
              Random: fixed   9 labels. Loss 0.05608. Accuracy 0.993.
### Flips: 205, rs: 39, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008578552
Train loss (w/o reg) on all data: 0.006254995
Test loss (w/o reg) on all data: 0.0034803962
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2295357e-07
Norm of the params: 6.816974
     Influence (LOO): fixed 175 labels. Loss 0.00348. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003409193
Train loss (w/o reg) on all data: 0.0011793401
Test loss (w/o reg) on all data: 0.00290908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.854557e-08
Norm of the params: 6.678103
                Loss: fixed 179 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13937418
Train loss (w/o reg) on all data: 0.13112715
Test loss (w/o reg) on all data: 0.05495811
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.819819e-06
Norm of the params: 12.842925
              Random: fixed  14 labels. Loss 0.05496. Accuracy 0.994.
### Flips: 205, rs: 39, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004411018
Train loss (w/o reg) on all data: 0.0026563052
Test loss (w/o reg) on all data: 0.0030831718
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3375827e-08
Norm of the params: 5.924041
     Influence (LOO): fixed 178 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.684781e-09
Norm of the params: 6.0928206
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13471098
Train loss (w/o reg) on all data: 0.12645623
Test loss (w/o reg) on all data: 0.052003134
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.13408e-06
Norm of the params: 12.848935
              Random: fixed  22 labels. Loss 0.05200. Accuracy 0.995.
### Flips: 205, rs: 39, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504543
Test loss (w/o reg) on all data: 0.0026117642
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8237366e-08
Norm of the params: 5.942003
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13198864
Train loss (w/o reg) on all data: 0.123981364
Test loss (w/o reg) on all data: 0.05043054
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.708718e-06
Norm of the params: 12.654862
              Random: fixed  27 labels. Loss 0.05043. Accuracy 0.995.
### Flips: 205, rs: 39, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.001850454
Test loss (w/o reg) on all data: 0.002611766
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7538698e-08
Norm of the params: 5.9420037
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.756744e-09
Norm of the params: 6.092823
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12661248
Train loss (w/o reg) on all data: 0.11854873
Test loss (w/o reg) on all data: 0.047692012
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2984175e-06
Norm of the params: 12.699419
              Random: fixed  35 labels. Loss 0.04769. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504568
Test loss (w/o reg) on all data: 0.0026117817
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1200028e-08
Norm of the params: 5.941998
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.747286e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11931413
Train loss (w/o reg) on all data: 0.11162967
Test loss (w/o reg) on all data: 0.04260809
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.93369e-06
Norm of the params: 12.397139
              Random: fixed  46 labels. Loss 0.04261. Accuracy 0.997.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25523645
Train loss (w/o reg) on all data: 0.24803415
Test loss (w/o reg) on all data: 0.10766977
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8555374e-05
Norm of the params: 12.001919
Flipped loss: 0.10767. Accuracy: 0.998
### Flips: 410, rs: 0, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13130566
Train loss (w/o reg) on all data: 0.122753516
Test loss (w/o reg) on all data: 0.05256282
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7597678e-06
Norm of the params: 13.078341
     Influence (LOO): fixed 193 labels. Loss 0.05256. Accuracy 0.998.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09930131
Train loss (w/o reg) on all data: 0.08495253
Test loss (w/o reg) on all data: 0.048298262
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.3725288e-06
Norm of the params: 16.940353
                Loss: fixed 205 labels. Loss 0.04830. Accuracy 0.991.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24459328
Train loss (w/o reg) on all data: 0.2372749
Test loss (w/o reg) on all data: 0.102220505
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5454294e-05
Norm of the params: 12.098246
              Random: fixed  20 labels. Loss 0.10222. Accuracy 0.998.
### Flips: 410, rs: 0, checks: 410
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06630761
Train loss (w/o reg) on all data: 0.060275584
Test loss (w/o reg) on all data: 0.021161921
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0043522e-06
Norm of the params: 10.983651
     Influence (LOO): fixed 288 labels. Loss 0.02116. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051846434
Train loss (w/o reg) on all data: 0.002095114
Test loss (w/o reg) on all data: 0.0044906554
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7256527e-07
Norm of the params: 7.860699
                Loss: fixed 360 labels. Loss 0.00449. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23756522
Train loss (w/o reg) on all data: 0.2303418
Test loss (w/o reg) on all data: 0.09703169
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.9505205e-06
Norm of the params: 12.019497
              Random: fixed  36 labels. Loss 0.09703. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 615
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036535148
Train loss (w/o reg) on all data: 0.031735066
Test loss (w/o reg) on all data: 0.012031098
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9959552e-06
Norm of the params: 9.798042
     Influence (LOO): fixed 327 labels. Loss 0.01203. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.0026560964
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1399366e-08
Norm of the params: 6.092804
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23084138
Train loss (w/o reg) on all data: 0.22359115
Test loss (w/o reg) on all data: 0.0939563
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.2520763e-06
Norm of the params: 12.041786
              Random: fixed  48 labels. Loss 0.09396. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 820
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028479658
Train loss (w/o reg) on all data: 0.024165418
Test loss (w/o reg) on all data: 0.009117891
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2868575e-06
Norm of the params: 9.28896
     Influence (LOO): fixed 337 labels. Loss 0.00912. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560875
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1252175e-08
Norm of the params: 6.0928054
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22095974
Train loss (w/o reg) on all data: 0.21359016
Test loss (w/o reg) on all data: 0.09003835
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.549912e-05
Norm of the params: 12.140495
              Random: fixed  64 labels. Loss 0.09004. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021611078
Train loss (w/o reg) on all data: 0.018191414
Test loss (w/o reg) on all data: 0.007345348
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0442336e-07
Norm of the params: 8.270022
     Influence (LOO): fixed 346 labels. Loss 0.00735. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.999378e-08
Norm of the params: 6.092819
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21417393
Train loss (w/o reg) on all data: 0.20689332
Test loss (w/o reg) on all data: 0.085499056
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.1139148e-06
Norm of the params: 12.066983
              Random: fixed  79 labels. Loss 0.08550. Accuracy 0.999.
### Flips: 410, rs: 0, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013939117
Train loss (w/o reg) on all data: 0.010914678
Test loss (w/o reg) on all data: 0.00548902
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6708872e-07
Norm of the params: 7.7774544
     Influence (LOO): fixed 353 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5042599e-08
Norm of the params: 6.0928173
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20234562
Train loss (w/o reg) on all data: 0.19471772
Test loss (w/o reg) on all data: 0.081512906
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2574812e-05
Norm of the params: 12.351438
              Random: fixed  97 labels. Loss 0.08151. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25862622
Train loss (w/o reg) on all data: 0.2525715
Test loss (w/o reg) on all data: 0.09979753
Train acc on all data:  0.9136883053732069
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2164931e-05
Norm of the params: 11.004312
Flipped loss: 0.09980. Accuracy: 0.994
### Flips: 410, rs: 1, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13420631
Train loss (w/o reg) on all data: 0.12676823
Test loss (w/o reg) on all data: 0.05078655
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.1096272e-06
Norm of the params: 12.196789
     Influence (LOO): fixed 185 labels. Loss 0.05079. Accuracy 0.996.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09785621
Train loss (w/o reg) on all data: 0.08566165
Test loss (w/o reg) on all data: 0.04991578
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.8039811e-06
Norm of the params: 15.617014
                Loss: fixed 205 labels. Loss 0.04992. Accuracy 0.987.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24757746
Train loss (w/o reg) on all data: 0.24141659
Test loss (w/o reg) on all data: 0.09514955
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1550451e-05
Norm of the params: 11.100333
              Random: fixed  20 labels. Loss 0.09515. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 410
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06053633
Train loss (w/o reg) on all data: 0.05448155
Test loss (w/o reg) on all data: 0.02110532
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4570435e-06
Norm of the params: 11.004343
     Influence (LOO): fixed 293 labels. Loss 0.02111. Accuracy 0.997.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003922875
Train loss (w/o reg) on all data: 0.00144176
Test loss (w/o reg) on all data: 0.0034984222
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9747233e-08
Norm of the params: 7.0443096
                Loss: fixed 362 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23996203
Train loss (w/o reg) on all data: 0.23399083
Test loss (w/o reg) on all data: 0.09056494
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0508943e-05
Norm of the params: 10.9281225
              Random: fixed  37 labels. Loss 0.09056. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 615
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036397137
Train loss (w/o reg) on all data: 0.031761892
Test loss (w/o reg) on all data: 0.011444766
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1436892e-06
Norm of the params: 9.628336
     Influence (LOO): fixed 329 labels. Loss 0.01144. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7778722e-08
Norm of the params: 6.0928116
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2336551
Train loss (w/o reg) on all data: 0.22759132
Test loss (w/o reg) on all data: 0.08852755
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.9304886e-06
Norm of the params: 11.012509
              Random: fixed  49 labels. Loss 0.08853. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 820
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023217816
Train loss (w/o reg) on all data: 0.019642439
Test loss (w/o reg) on all data: 0.006884773
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 8.530848e-07
Norm of the params: 8.456213
     Influence (LOO): fixed 344 labels. Loss 0.00688. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013024
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7376364e-08
Norm of the params: 6.0927978
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22070703
Train loss (w/o reg) on all data: 0.21448787
Test loss (w/o reg) on all data: 0.08339383
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.577973e-06
Norm of the params: 11.152724
              Random: fixed  71 labels. Loss 0.08339. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015237164
Train loss (w/o reg) on all data: 0.012444638
Test loss (w/o reg) on all data: 0.005293291
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 5.861574e-07
Norm of the params: 7.4733205
     Influence (LOO): fixed 352 labels. Loss 0.00529. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3153007e-08
Norm of the params: 6.0928044
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21171792
Train loss (w/o reg) on all data: 0.20561786
Test loss (w/o reg) on all data: 0.078739166
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.901641e-06
Norm of the params: 11.045415
              Random: fixed  89 labels. Loss 0.07874. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010898982
Train loss (w/o reg) on all data: 0.008480981
Test loss (w/o reg) on all data: 0.0042993464
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6052916e-07
Norm of the params: 6.9541373
     Influence (LOO): fixed 356 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2835355e-08
Norm of the params: 6.092811
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20208895
Train loss (w/o reg) on all data: 0.19585897
Test loss (w/o reg) on all data: 0.07495119
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.503241e-06
Norm of the params: 11.1624155
              Random: fixed 105 labels. Loss 0.07495. Accuracy 0.995.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25830045
Train loss (w/o reg) on all data: 0.25230798
Test loss (w/o reg) on all data: 0.10343826
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.5170375e-06
Norm of the params: 10.947587
Flipped loss: 0.10344. Accuracy: 0.998
### Flips: 410, rs: 2, checks: 205
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13530183
Train loss (w/o reg) on all data: 0.12746467
Test loss (w/o reg) on all data: 0.04925188
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1548345e-06
Norm of the params: 12.519711
     Influence (LOO): fixed 183 labels. Loss 0.04925. Accuracy 0.998.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094670855
Train loss (w/o reg) on all data: 0.081186466
Test loss (w/o reg) on all data: 0.050859205
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.2890628e-06
Norm of the params: 16.422173
                Loss: fixed 205 labels. Loss 0.05086. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2480415
Train loss (w/o reg) on all data: 0.24215403
Test loss (w/o reg) on all data: 0.09616817
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.189141e-06
Norm of the params: 10.85124
              Random: fixed  22 labels. Loss 0.09617. Accuracy 0.999.
### Flips: 410, rs: 2, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05551047
Train loss (w/o reg) on all data: 0.049580667
Test loss (w/o reg) on all data: 0.01952885
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2793679e-06
Norm of the params: 10.890179
     Influence (LOO): fixed 293 labels. Loss 0.01953. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00494643
Train loss (w/o reg) on all data: 0.0018304065
Test loss (w/o reg) on all data: 0.002939558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.274901e-07
Norm of the params: 7.8943324
                Loss: fixed 353 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24026887
Train loss (w/o reg) on all data: 0.2343221
Test loss (w/o reg) on all data: 0.09334626
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2539467e-05
Norm of the params: 10.905753
              Random: fixed  36 labels. Loss 0.09335. Accuracy 0.999.
### Flips: 410, rs: 2, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03403237
Train loss (w/o reg) on all data: 0.029644413
Test loss (w/o reg) on all data: 0.011336462
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 5.446441e-07
Norm of the params: 9.367986
     Influence (LOO): fixed 321 labels. Loss 0.01134. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004213181
Train loss (w/o reg) on all data: 0.0015274356
Test loss (w/o reg) on all data: 0.0027694916
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.117165e-08
Norm of the params: 7.3290453
                Loss: fixed 354 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22682106
Train loss (w/o reg) on all data: 0.22058284
Test loss (w/o reg) on all data: 0.08764404
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.3158637e-06
Norm of the params: 11.1698065
              Random: fixed  57 labels. Loss 0.08764. Accuracy 0.998.
### Flips: 410, rs: 2, checks: 820
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018264985
Train loss (w/o reg) on all data: 0.01457352
Test loss (w/o reg) on all data: 0.007419482
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2779386e-07
Norm of the params: 8.592399
     Influence (LOO): fixed 339 labels. Loss 0.00742. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6456484e-08
Norm of the params: 6.092818
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21983181
Train loss (w/o reg) on all data: 0.21347345
Test loss (w/o reg) on all data: 0.08282021
Train acc on all data:  0.9326525650376853
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.436444e-06
Norm of the params: 11.276834
              Random: fixed  71 labels. Loss 0.08282. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 1025
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0137434825
Train loss (w/o reg) on all data: 0.01055903
Test loss (w/o reg) on all data: 0.0056614727
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6235527e-07
Norm of the params: 7.9805417
     Influence (LOO): fixed 345 labels. Loss 0.00566. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096010946
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1958495e-08
Norm of the params: 6.092833
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20902833
Train loss (w/o reg) on all data: 0.20278026
Test loss (w/o reg) on all data: 0.07799059
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3779326e-05
Norm of the params: 11.17861
              Random: fixed  89 labels. Loss 0.07799. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 1230
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011455864
Train loss (w/o reg) on all data: 0.008622796
Test loss (w/o reg) on all data: 0.005164671
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5674208e-07
Norm of the params: 7.527373
     Influence (LOO): fixed 348 labels. Loss 0.00516. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656045
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3022347e-08
Norm of the params: 6.092825
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19902143
Train loss (w/o reg) on all data: 0.19267622
Test loss (w/o reg) on all data: 0.07267795
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2772614e-06
Norm of the params: 11.265173
              Random: fixed 105 labels. Loss 0.07268. Accuracy 0.998.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25349134
Train loss (w/o reg) on all data: 0.24708457
Test loss (w/o reg) on all data: 0.10995053
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.5197585e-05
Norm of the params: 11.319681
Flipped loss: 0.10995. Accuracy: 0.989
### Flips: 410, rs: 3, checks: 205
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1303013
Train loss (w/o reg) on all data: 0.122977465
Test loss (w/o reg) on all data: 0.051216707
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.129854e-06
Norm of the params: 12.102751
     Influence (LOO): fixed 188 labels. Loss 0.05122. Accuracy 0.997.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09316138
Train loss (w/o reg) on all data: 0.07860265
Test loss (w/o reg) on all data: 0.055279415
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6503562e-06
Norm of the params: 17.06384
                Loss: fixed 205 labels. Loss 0.05528. Accuracy 0.986.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2458947
Train loss (w/o reg) on all data: 0.23933038
Test loss (w/o reg) on all data: 0.10506093
Train acc on all data:  0.9180646729880866
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.194149e-06
Norm of the params: 11.45803
              Random: fixed  14 labels. Loss 0.10506. Accuracy 0.991.
### Flips: 410, rs: 3, checks: 410
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057633705
Train loss (w/o reg) on all data: 0.05172163
Test loss (w/o reg) on all data: 0.021165796
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 1.156927e-06
Norm of the params: 10.873893
     Influence (LOO): fixed 289 labels. Loss 0.02117. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036910544
Train loss (w/o reg) on all data: 0.0013854748
Test loss (w/o reg) on all data: 0.003928767
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4304754e-08
Norm of the params: 6.7905517
                Loss: fixed 354 labels. Loss 0.00393. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23601922
Train loss (w/o reg) on all data: 0.22933416
Test loss (w/o reg) on all data: 0.0976306
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.935454e-06
Norm of the params: 11.562927
              Random: fixed  34 labels. Loss 0.09763. Accuracy 0.990.
### Flips: 410, rs: 3, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030733708
Train loss (w/o reg) on all data: 0.02622241
Test loss (w/o reg) on all data: 0.010734413
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6199736e-07
Norm of the params: 9.4987335
     Influence (LOO): fixed 326 labels. Loss 0.01073. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215604
Train loss (w/o reg) on all data: 0.0011097618
Test loss (w/o reg) on all data: 0.003636158
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3475931e-08
Norm of the params: 6.1835246
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22426838
Train loss (w/o reg) on all data: 0.21722792
Test loss (w/o reg) on all data: 0.091780216
Train acc on all data:  0.9273036712861659
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.142248e-06
Norm of the params: 11.866299
              Random: fixed  53 labels. Loss 0.09178. Accuracy 0.991.
### Flips: 410, rs: 3, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023745526
Train loss (w/o reg) on all data: 0.0198362
Test loss (w/o reg) on all data: 0.008700648
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9584197e-07
Norm of the params: 8.842314
     Influence (LOO): fixed 334 labels. Loss 0.00870. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215606
Train loss (w/o reg) on all data: 0.001109767
Test loss (w/o reg) on all data: 0.0036361704
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4006531e-08
Norm of the params: 6.1835155
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21144046
Train loss (w/o reg) on all data: 0.20444487
Test loss (w/o reg) on all data: 0.08263318
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3539791e-05
Norm of the params: 11.828428
              Random: fixed  76 labels. Loss 0.08263. Accuracy 0.994.
### Flips: 410, rs: 3, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017742459
Train loss (w/o reg) on all data: 0.014550173
Test loss (w/o reg) on all data: 0.0062349266
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 8.42047e-07
Norm of the params: 7.990351
     Influence (LOO): fixed 342 labels. Loss 0.00623. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003021561
Train loss (w/o reg) on all data: 0.0011097529
Test loss (w/o reg) on all data: 0.003636184
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7293419e-08
Norm of the params: 6.18354
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19539934
Train loss (w/o reg) on all data: 0.18812588
Test loss (w/o reg) on all data: 0.076274164
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.129854e-06
Norm of the params: 12.0610695
              Random: fixed 101 labels. Loss 0.07627. Accuracy 0.994.
### Flips: 410, rs: 3, checks: 1230
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014840838
Train loss (w/o reg) on all data: 0.011649381
Test loss (w/o reg) on all data: 0.005507901
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 5.057419e-07
Norm of the params: 7.989313
     Influence (LOO): fixed 345 labels. Loss 0.00551. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.992155e-08
Norm of the params: 6.092813
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18813469
Train loss (w/o reg) on all data: 0.1809695
Test loss (w/o reg) on all data: 0.07208275
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.1074637e-06
Norm of the params: 11.970946
              Random: fixed 115 labels. Loss 0.07208. Accuracy 0.995.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2551855
Train loss (w/o reg) on all data: 0.2477695
Test loss (w/o reg) on all data: 0.10506089
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.12041e-06
Norm of the params: 12.178689
Flipped loss: 0.10506. Accuracy: 0.997
### Flips: 410, rs: 4, checks: 205
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13221157
Train loss (w/o reg) on all data: 0.12461631
Test loss (w/o reg) on all data: 0.047705904
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.995539e-06
Norm of the params: 12.324985
     Influence (LOO): fixed 187 labels. Loss 0.04771. Accuracy 0.999.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08951993
Train loss (w/o reg) on all data: 0.07578612
Test loss (w/o reg) on all data: 0.049734544
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.6029096e-06
Norm of the params: 16.57336
                Loss: fixed 205 labels. Loss 0.04973. Accuracy 0.990.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24005543
Train loss (w/o reg) on all data: 0.23264185
Test loss (w/o reg) on all data: 0.097127825
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2331367e-05
Norm of the params: 12.17668
              Random: fixed  28 labels. Loss 0.09713. Accuracy 0.996.
### Flips: 410, rs: 4, checks: 410
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056454163
Train loss (w/o reg) on all data: 0.051382735
Test loss (w/o reg) on all data: 0.017217526
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1321572e-06
Norm of the params: 10.071175
     Influence (LOO): fixed 289 labels. Loss 0.01722. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044024875
Train loss (w/o reg) on all data: 0.0016265044
Test loss (w/o reg) on all data: 0.0034203057
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.397427e-08
Norm of the params: 7.451152
                Loss: fixed 346 labels. Loss 0.00342. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23010626
Train loss (w/o reg) on all data: 0.22256093
Test loss (w/o reg) on all data: 0.09229041
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.927061e-06
Norm of the params: 12.284414
              Random: fixed  45 labels. Loss 0.09229. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03412841
Train loss (w/o reg) on all data: 0.029648872
Test loss (w/o reg) on all data: 0.011435192
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7835804e-07
Norm of the params: 9.46524
     Influence (LOO): fixed 313 labels. Loss 0.01144. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600618
Test loss (w/o reg) on all data: 0.0026560167
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.364404e-08
Norm of the params: 6.09291
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22020397
Train loss (w/o reg) on all data: 0.21263748
Test loss (w/o reg) on all data: 0.087850556
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.411059e-06
Norm of the params: 12.301616
              Random: fixed  62 labels. Loss 0.08785. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019171745
Train loss (w/o reg) on all data: 0.015713526
Test loss (w/o reg) on all data: 0.0061681555
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0371539e-07
Norm of the params: 8.316513
     Influence (LOO): fixed 332 labels. Loss 0.00617. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6603077e-08
Norm of the params: 6.0928073
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20818183
Train loss (w/o reg) on all data: 0.20061333
Test loss (w/o reg) on all data: 0.08026649
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.987085e-06
Norm of the params: 12.303241
              Random: fixed  85 labels. Loss 0.08027. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018230796
Train loss (w/o reg) on all data: 0.01486977
Test loss (w/o reg) on all data: 0.0057230974
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1517665e-07
Norm of the params: 8.198813
     Influence (LOO): fixed 333 labels. Loss 0.00572. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560838
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4861585e-08
Norm of the params: 6.0928073
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1981974
Train loss (w/o reg) on all data: 0.19081151
Test loss (w/o reg) on all data: 0.07478207
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.068085e-06
Norm of the params: 12.153907
              Random: fixed 105 labels. Loss 0.07478. Accuracy 0.996.
### Flips: 410, rs: 4, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014088709
Train loss (w/o reg) on all data: 0.010883041
Test loss (w/o reg) on all data: 0.004572537
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7848454e-07
Norm of the params: 8.007083
     Influence (LOO): fixed 337 labels. Loss 0.00457. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2982443e-08
Norm of the params: 6.0928173
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19057003
Train loss (w/o reg) on all data: 0.18311961
Test loss (w/o reg) on all data: 0.071328945
Train acc on all data:  0.9452954048140044
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4941088e-05
Norm of the params: 12.2069
              Random: fixed 119 labels. Loss 0.07133. Accuracy 0.996.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26561734
Train loss (w/o reg) on all data: 0.25907508
Test loss (w/o reg) on all data: 0.11129985
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.073754e-06
Norm of the params: 11.438757
Flipped loss: 0.11130. Accuracy: 0.992
### Flips: 410, rs: 5, checks: 205
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13873862
Train loss (w/o reg) on all data: 0.1309015
Test loss (w/o reg) on all data: 0.053143278
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.152817e-06
Norm of the params: 12.519679
     Influence (LOO): fixed 192 labels. Loss 0.05314. Accuracy 0.997.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10560593
Train loss (w/o reg) on all data: 0.09196085
Test loss (w/o reg) on all data: 0.0562249
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.5275636e-06
Norm of the params: 16.519737
                Loss: fixed 205 labels. Loss 0.05622. Accuracy 0.986.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2563793
Train loss (w/o reg) on all data: 0.24945195
Test loss (w/o reg) on all data: 0.106644936
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.252399e-06
Norm of the params: 11.770599
              Random: fixed  15 labels. Loss 0.10664. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 410
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065506056
Train loss (w/o reg) on all data: 0.0597877
Test loss (w/o reg) on all data: 0.022748234
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6896762e-06
Norm of the params: 10.694259
     Influence (LOO): fixed 298 labels. Loss 0.02275. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053693987
Train loss (w/o reg) on all data: 0.0020991317
Test loss (w/o reg) on all data: 0.004326598
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.037658e-08
Norm of the params: 8.0873575
                Loss: fixed 360 labels. Loss 0.00433. Accuracy 0.999.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24615759
Train loss (w/o reg) on all data: 0.2392386
Test loss (w/o reg) on all data: 0.100449346
Train acc on all data:  0.9212253829321663
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0163299e-05
Norm of the params: 11.763491
              Random: fixed  35 labels. Loss 0.10045. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034503758
Train loss (w/o reg) on all data: 0.030089479
Test loss (w/o reg) on all data: 0.010770337
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1389803e-07
Norm of the params: 9.396042
     Influence (LOO): fixed 335 labels. Loss 0.01077. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003956434
Train loss (w/o reg) on all data: 0.0014328536
Test loss (w/o reg) on all data: 0.0027366797
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2798365e-08
Norm of the params: 7.1043367
                Loss: fixed 364 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23716065
Train loss (w/o reg) on all data: 0.23020992
Test loss (w/o reg) on all data: 0.096931435
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.235566e-05
Norm of the params: 11.790448
              Random: fixed  50 labels. Loss 0.09693. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023849845
Train loss (w/o reg) on all data: 0.01994175
Test loss (w/o reg) on all data: 0.00794856
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0907866e-07
Norm of the params: 8.840922
     Influence (LOO): fixed 346 labels. Loss 0.00795. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033851552
Train loss (w/o reg) on all data: 0.0011814169
Test loss (w/o reg) on all data: 0.0026828814
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0536918e-08
Norm of the params: 6.638883
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22469766
Train loss (w/o reg) on all data: 0.21768865
Test loss (w/o reg) on all data: 0.091372
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.3592995e-06
Norm of the params: 11.839772
              Random: fixed  71 labels. Loss 0.09137. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017598504
Train loss (w/o reg) on all data: 0.01417165
Test loss (w/o reg) on all data: 0.0069286297
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0208931e-07
Norm of the params: 8.278712
     Influence (LOO): fixed 351 labels. Loss 0.00693. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003385156
Train loss (w/o reg) on all data: 0.0011814034
Test loss (w/o reg) on all data: 0.0026828905
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6899983e-08
Norm of the params: 6.638904
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21205805
Train loss (w/o reg) on all data: 0.20444314
Test loss (w/o reg) on all data: 0.08569933
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.9351654e-06
Norm of the params: 12.340912
              Random: fixed  91 labels. Loss 0.08570. Accuracy 0.991.
### Flips: 410, rs: 5, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013721576
Train loss (w/o reg) on all data: 0.010820518
Test loss (w/o reg) on all data: 0.005500337
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.75208e-07
Norm of the params: 7.617162
     Influence (LOO): fixed 356 labels. Loss 0.00550. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033851555
Train loss (w/o reg) on all data: 0.0011814303
Test loss (w/o reg) on all data: 0.0026828966
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3006895e-08
Norm of the params: 6.638863
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20286204
Train loss (w/o reg) on all data: 0.19502361
Test loss (w/o reg) on all data: 0.08227229
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1330026e-05
Norm of the params: 12.520727
              Random: fixed 106 labels. Loss 0.08227. Accuracy 0.990.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24926001
Train loss (w/o reg) on all data: 0.24184582
Test loss (w/o reg) on all data: 0.09720137
Train acc on all data:  0.9180646729880866
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1146099e-05
Norm of the params: 12.177189
Flipped loss: 0.09720. Accuracy: 0.997
### Flips: 410, rs: 6, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12681569
Train loss (w/o reg) on all data: 0.11874727
Test loss (w/o reg) on all data: 0.044391397
Train acc on all data:  0.962314612205203
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8505285e-06
Norm of the params: 12.703083
     Influence (LOO): fixed 187 labels. Loss 0.04439. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088542975
Train loss (w/o reg) on all data: 0.07494849
Test loss (w/o reg) on all data: 0.040766653
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9264747e-06
Norm of the params: 16.489079
                Loss: fixed 205 labels. Loss 0.04077. Accuracy 0.991.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24193045
Train loss (w/o reg) on all data: 0.23454101
Test loss (w/o reg) on all data: 0.093500435
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0772815e-06
Norm of the params: 12.156843
              Random: fixed  13 labels. Loss 0.09350. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053272627
Train loss (w/o reg) on all data: 0.04768785
Test loss (w/o reg) on all data: 0.017203182
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1489393e-06
Norm of the params: 10.568611
     Influence (LOO): fixed 287 labels. Loss 0.01720. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030876363
Train loss (w/o reg) on all data: 0.0011732262
Test loss (w/o reg) on all data: 0.0034910596
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6848682e-08
Norm of the params: 6.1877465
                Loss: fixed 343 labels. Loss 0.00349. Accuracy 0.999.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2329195
Train loss (w/o reg) on all data: 0.22504129
Test loss (w/o reg) on all data: 0.09042545
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1392383e-05
Norm of the params: 12.552464
              Random: fixed  27 labels. Loss 0.09043. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 615
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030077577
Train loss (w/o reg) on all data: 0.025683774
Test loss (w/o reg) on all data: 0.009943739
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1032486e-06
Norm of the params: 9.374224
     Influence (LOO): fixed 316 labels. Loss 0.00994. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601049
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5826461e-08
Norm of the params: 6.0928392
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22026765
Train loss (w/o reg) on all data: 0.21239218
Test loss (w/o reg) on all data: 0.08405248
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.671283e-06
Norm of the params: 12.5502825
              Random: fixed  48 labels. Loss 0.08405. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 820
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020893287
Train loss (w/o reg) on all data: 0.01740151
Test loss (w/o reg) on all data: 0.007111695
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 4.371058e-07
Norm of the params: 8.356766
     Influence (LOO): fixed 326 labels. Loss 0.00711. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9108068e-08
Norm of the params: 6.0928183
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21022092
Train loss (w/o reg) on all data: 0.20205383
Test loss (w/o reg) on all data: 0.08028014
Train acc on all data:  0.9331388281060053
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0005826e-05
Norm of the params: 12.780521
              Random: fixed  64 labels. Loss 0.08028. Accuracy 1.000.
### Flips: 410, rs: 6, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019053467
Train loss (w/o reg) on all data: 0.015647057
Test loss (w/o reg) on all data: 0.00679639
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0359059e-06
Norm of the params: 8.253981
     Influence (LOO): fixed 328 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.543897e-08
Norm of the params: 6.092816
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2014307
Train loss (w/o reg) on all data: 0.19335607
Test loss (w/o reg) on all data: 0.07522045
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.4694367e-05
Norm of the params: 12.707971
              Random: fixed  81 labels. Loss 0.07522. Accuracy 0.999.
### Flips: 410, rs: 6, checks: 1230
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011262059
Train loss (w/o reg) on all data: 0.008743714
Test loss (w/o reg) on all data: 0.0044752113
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0401663e-07
Norm of the params: 7.096966
     Influence (LOO): fixed 336 labels. Loss 0.00448. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010976
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9536435e-08
Norm of the params: 6.0928316
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18912181
Train loss (w/o reg) on all data: 0.18111242
Test loss (w/o reg) on all data: 0.07033256
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.2229e-06
Norm of the params: 12.656529
              Random: fixed 100 labels. Loss 0.07033. Accuracy 0.998.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25872687
Train loss (w/o reg) on all data: 0.25095102
Test loss (w/o reg) on all data: 0.105985746
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4203169e-05
Norm of the params: 12.470644
Flipped loss: 0.10599. Accuracy: 0.991
### Flips: 410, rs: 7, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13766958
Train loss (w/o reg) on all data: 0.12919296
Test loss (w/o reg) on all data: 0.04855617
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.3462067e-06
Norm of the params: 13.020459
     Influence (LOO): fixed 186 labels. Loss 0.04856. Accuracy 0.995.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10214672
Train loss (w/o reg) on all data: 0.08661491
Test loss (w/o reg) on all data: 0.04867358
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.740381e-06
Norm of the params: 17.624878
                Loss: fixed 205 labels. Loss 0.04867. Accuracy 0.987.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2478135
Train loss (w/o reg) on all data: 0.23996277
Test loss (w/o reg) on all data: 0.10053801
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.5274736e-06
Norm of the params: 12.530542
              Random: fixed  20 labels. Loss 0.10054. Accuracy 0.991.
### Flips: 410, rs: 7, checks: 410
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063230895
Train loss (w/o reg) on all data: 0.057293825
Test loss (w/o reg) on all data: 0.019736791
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9419075e-06
Norm of the params: 10.896855
     Influence (LOO): fixed 289 labels. Loss 0.01974. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0045051407
Train loss (w/o reg) on all data: 0.0016956896
Test loss (w/o reg) on all data: 0.0033519873
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.430138e-08
Norm of the params: 7.495934
                Loss: fixed 356 labels. Loss 0.00335. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2377275
Train loss (w/o reg) on all data: 0.23010406
Test loss (w/o reg) on all data: 0.09568931
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.0220887e-06
Norm of the params: 12.347826
              Random: fixed  40 labels. Loss 0.09569. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 615
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033242594
Train loss (w/o reg) on all data: 0.02833054
Test loss (w/o reg) on all data: 0.010517066
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7256322e-07
Norm of the params: 9.911663
     Influence (LOO): fixed 326 labels. Loss 0.01052. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012856
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5317483e-08
Norm of the params: 6.0928
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22850865
Train loss (w/o reg) on all data: 0.2207529
Test loss (w/o reg) on all data: 0.09081966
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.646561e-06
Norm of the params: 12.454522
              Random: fixed  57 labels. Loss 0.09082. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024869734
Train loss (w/o reg) on all data: 0.020640604
Test loss (w/o reg) on all data: 0.0076960605
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1692375e-06
Norm of the params: 9.19688
     Influence (LOO): fixed 337 labels. Loss 0.00770. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.185351e-08
Norm of the params: 6.0928173
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2198059
Train loss (w/o reg) on all data: 0.21175839
Test loss (w/o reg) on all data: 0.08887551
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.8488076e-06
Norm of the params: 12.686609
              Random: fixed  72 labels. Loss 0.08888. Accuracy 0.991.
### Flips: 410, rs: 7, checks: 1025
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016052507
Train loss (w/o reg) on all data: 0.013031181
Test loss (w/o reg) on all data: 0.005488721
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.704746e-07
Norm of the params: 7.7734494
     Influence (LOO): fixed 347 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601069
Test loss (w/o reg) on all data: 0.002656012
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17659134e-07
Norm of the params: 6.0928364
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20982409
Train loss (w/o reg) on all data: 0.20171875
Test loss (w/o reg) on all data: 0.08469228
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2670647e-05
Norm of the params: 12.732116
              Random: fixed  87 labels. Loss 0.08469. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 1230
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008600752
Train loss (w/o reg) on all data: 0.006122649
Test loss (w/o reg) on all data: 0.0037523431
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3459811e-07
Norm of the params: 7.040032
     Influence (LOO): fixed 354 labels. Loss 0.00375. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012966
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4376943e-08
Norm of the params: 6.092799
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19675033
Train loss (w/o reg) on all data: 0.18882538
Test loss (w/o reg) on all data: 0.076124586
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3295345e-05
Norm of the params: 12.589634
              Random: fixed 111 labels. Loss 0.07612. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2674558
Train loss (w/o reg) on all data: 0.2608183
Test loss (w/o reg) on all data: 0.11405583
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7288901e-05
Norm of the params: 11.5217
Flipped loss: 0.11406. Accuracy: 0.991
### Flips: 410, rs: 8, checks: 205
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1404631
Train loss (w/o reg) on all data: 0.1328421
Test loss (w/o reg) on all data: 0.051402498
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 8.682295e-06
Norm of the params: 12.345849
     Influence (LOO): fixed 186 labels. Loss 0.05140. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10980301
Train loss (w/o reg) on all data: 0.09603023
Test loss (w/o reg) on all data: 0.06303755
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.542552e-06
Norm of the params: 16.596857
                Loss: fixed 205 labels. Loss 0.06304. Accuracy 0.980.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25829247
Train loss (w/o reg) on all data: 0.2515882
Test loss (w/o reg) on all data: 0.10501762
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0833722e-05
Norm of the params: 11.579516
              Random: fixed  19 labels. Loss 0.10502. Accuracy 0.994.
### Flips: 410, rs: 8, checks: 410
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07308923
Train loss (w/o reg) on all data: 0.066555835
Test loss (w/o reg) on all data: 0.023928728
Train acc on all data:  0.9795769511305616
Test acc on all data:   1.0
Norm of the mean of gradients: 2.723342e-06
Norm of the params: 11.431002
     Influence (LOO): fixed 283 labels. Loss 0.02393. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004793172
Train loss (w/o reg) on all data: 0.0018280229
Test loss (w/o reg) on all data: 0.0028480175
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.389044e-08
Norm of the params: 7.7008424
                Loss: fixed 364 labels. Loss 0.00285. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2475658
Train loss (w/o reg) on all data: 0.2404353
Test loss (w/o reg) on all data: 0.10136002
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.726356e-06
Norm of the params: 11.941944
              Random: fixed  39 labels. Loss 0.10136. Accuracy 0.995.
### Flips: 410, rs: 8, checks: 615
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042018835
Train loss (w/o reg) on all data: 0.036631417
Test loss (w/o reg) on all data: 0.012820052
Train acc on all data:  0.9888159494286409
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0993377e-06
Norm of the params: 10.38019
     Influence (LOO): fixed 322 labels. Loss 0.01282. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362238
Train loss (w/o reg) on all data: 0.0011012006
Test loss (w/o reg) on all data: 0.0029767947
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9011715e-08
Norm of the params: 6.2209697
                Loss: fixed 367 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23607427
Train loss (w/o reg) on all data: 0.2288576
Test loss (w/o reg) on all data: 0.096494295
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3410675e-05
Norm of the params: 12.013875
              Random: fixed  58 labels. Loss 0.09649. Accuracy 0.994.
### Flips: 410, rs: 8, checks: 820
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03043916
Train loss (w/o reg) on all data: 0.02555912
Test loss (w/o reg) on all data: 0.0091750305
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3935888e-06
Norm of the params: 9.879313
     Influence (LOO): fixed 335 labels. Loss 0.00918. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362234
Train loss (w/o reg) on all data: 0.001101186
Test loss (w/o reg) on all data: 0.0029767468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1771277e-08
Norm of the params: 6.220993
                Loss: fixed 367 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22651874
Train loss (w/o reg) on all data: 0.21927977
Test loss (w/o reg) on all data: 0.08926321
Train acc on all data:  0.9307075127644056
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.9776097e-06
Norm of the params: 12.032431
              Random: fixed  76 labels. Loss 0.08926. Accuracy 0.995.
### Flips: 410, rs: 8, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023780392
Train loss (w/o reg) on all data: 0.01936193
Test loss (w/o reg) on all data: 0.0074190605
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4123352e-06
Norm of the params: 9.400492
     Influence (LOO): fixed 346 labels. Loss 0.00742. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1787191e-08
Norm of the params: 6.0928173
                Loss: fixed 368 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21547769
Train loss (w/o reg) on all data: 0.20797038
Test loss (w/o reg) on all data: 0.08328582
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.703114e-06
Norm of the params: 12.253419
              Random: fixed  95 labels. Loss 0.08329. Accuracy 0.997.
### Flips: 410, rs: 8, checks: 1230
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013425832
Train loss (w/o reg) on all data: 0.009664701
Test loss (w/o reg) on all data: 0.0062467805
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0603454e-07
Norm of the params: 8.673097
     Influence (LOO): fixed 356 labels. Loss 0.00625. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601315
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3917169e-08
Norm of the params: 6.0927963
                Loss: fixed 368 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20315722
Train loss (w/o reg) on all data: 0.19516347
Test loss (w/o reg) on all data: 0.078828804
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0556141e-05
Norm of the params: 12.644164
              Random: fixed 114 labels. Loss 0.07883. Accuracy 0.997.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25410336
Train loss (w/o reg) on all data: 0.2469687
Test loss (w/o reg) on all data: 0.1127436
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.9427892e-06
Norm of the params: 11.945424
Flipped loss: 0.11274. Accuracy: 0.991
### Flips: 410, rs: 9, checks: 205
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162269
Train loss (w/o reg) on all data: 0.12396108
Test loss (w/o reg) on all data: 0.048317563
Train acc on all data:  0.9598832968636032
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0023801e-06
Norm of the params: 12.378696
     Influence (LOO): fixed 183 labels. Loss 0.04832. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09498345
Train loss (w/o reg) on all data: 0.082210645
Test loss (w/o reg) on all data: 0.06307003
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.093052e-06
Norm of the params: 15.982995
                Loss: fixed 205 labels. Loss 0.06307. Accuracy 0.984.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2445014
Train loss (w/o reg) on all data: 0.23753862
Test loss (w/o reg) on all data: 0.10675287
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.586392e-05
Norm of the params: 11.8006525
              Random: fixed  19 labels. Loss 0.10675. Accuracy 0.989.
### Flips: 410, rs: 9, checks: 410
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065983824
Train loss (w/o reg) on all data: 0.060406175
Test loss (w/o reg) on all data: 0.021034718
Train acc on all data:  0.9815220034038414
Test acc on all data:   1.0
Norm of the mean of gradients: 2.002588e-06
Norm of the params: 10.561868
     Influence (LOO): fixed 276 labels. Loss 0.02103. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039115977
Train loss (w/o reg) on all data: 0.0014237999
Test loss (w/o reg) on all data: 0.0030563816
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.00728e-08
Norm of the params: 7.0537906
                Loss: fixed 350 labels. Loss 0.00306. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23593026
Train loss (w/o reg) on all data: 0.22913688
Test loss (w/o reg) on all data: 0.09965847
Train acc on all data:  0.9238998298079261
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.696591e-06
Norm of the params: 11.656222
              Random: fixed  35 labels. Loss 0.09966. Accuracy 0.993.
### Flips: 410, rs: 9, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031906396
Train loss (w/o reg) on all data: 0.027977232
Test loss (w/o reg) on all data: 0.009951627
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 9.510859e-07
Norm of the params: 8.864721
     Influence (LOO): fixed 319 labels. Loss 0.00995. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003568763
Train loss (w/o reg) on all data: 0.0012876174
Test loss (w/o reg) on all data: 0.002966254
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1328484e-08
Norm of the params: 6.7544737
                Loss: fixed 351 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22412612
Train loss (w/o reg) on all data: 0.21708584
Test loss (w/o reg) on all data: 0.0941211
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.6286766e-06
Norm of the params: 11.866148
              Random: fixed  57 labels. Loss 0.09412. Accuracy 0.993.
### Flips: 410, rs: 9, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020153126
Train loss (w/o reg) on all data: 0.016690176
Test loss (w/o reg) on all data: 0.0070985416
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.017998e-07
Norm of the params: 8.322199
     Influence (LOO): fixed 332 labels. Loss 0.00710. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.088401e-08
Norm of the params: 6.09283
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21554361
Train loss (w/o reg) on all data: 0.20835356
Test loss (w/o reg) on all data: 0.09257937
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3659386e-05
Norm of the params: 11.991699
              Random: fixed  70 labels. Loss 0.09258. Accuracy 0.994.
### Flips: 410, rs: 9, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017565424
Train loss (w/o reg) on all data: 0.014431402
Test loss (w/o reg) on all data: 0.006150506
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 4.199295e-07
Norm of the params: 7.9170966
     Influence (LOO): fixed 336 labels. Loss 0.00615. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2509418e-08
Norm of the params: 6.092819
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20713645
Train loss (w/o reg) on all data: 0.20003118
Test loss (w/o reg) on all data: 0.087262034
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.3190353e-06
Norm of the params: 11.920808
              Random: fixed  85 labels. Loss 0.08726. Accuracy 0.995.
### Flips: 410, rs: 9, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013036148
Train loss (w/o reg) on all data: 0.010354011
Test loss (w/o reg) on all data: 0.004653919
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 5.91036e-07
Norm of the params: 7.324119
     Influence (LOO): fixed 342 labels. Loss 0.00465. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9263565e-09
Norm of the params: 6.0928245
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19235231
Train loss (w/o reg) on all data: 0.1848912
Test loss (w/o reg) on all data: 0.081101246
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.415073e-05
Norm of the params: 12.215661
              Random: fixed 108 labels. Loss 0.08110. Accuracy 0.991.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26166177
Train loss (w/o reg) on all data: 0.25554898
Test loss (w/o reg) on all data: 0.10660727
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.526223e-06
Norm of the params: 11.056919
Flipped loss: 0.10661. Accuracy: 0.994
### Flips: 410, rs: 10, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13290103
Train loss (w/o reg) on all data: 0.1253525
Test loss (w/o reg) on all data: 0.0482012
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7715037e-06
Norm of the params: 12.287008
     Influence (LOO): fixed 192 labels. Loss 0.04820. Accuracy 0.999.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101995654
Train loss (w/o reg) on all data: 0.08994317
Test loss (w/o reg) on all data: 0.0536952
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.055021e-06
Norm of the params: 15.525776
                Loss: fixed 205 labels. Loss 0.05370. Accuracy 0.988.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25438538
Train loss (w/o reg) on all data: 0.2480981
Test loss (w/o reg) on all data: 0.102665134
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0276858e-05
Norm of the params: 11.2136345
              Random: fixed  14 labels. Loss 0.10267. Accuracy 0.994.
### Flips: 410, rs: 10, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06467296
Train loss (w/o reg) on all data: 0.058659736
Test loss (w/o reg) on all data: 0.020363286
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0978672e-06
Norm of the params: 10.966516
     Influence (LOO): fixed 285 labels. Loss 0.02036. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036582989
Train loss (w/o reg) on all data: 0.0013216871
Test loss (w/o reg) on all data: 0.0027576704
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8408412e-08
Norm of the params: 6.8360977
                Loss: fixed 356 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24275084
Train loss (w/o reg) on all data: 0.2359959
Test loss (w/o reg) on all data: 0.09833892
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.877843e-06
Norm of the params: 11.623201
              Random: fixed  32 labels. Loss 0.09834. Accuracy 0.993.
### Flips: 410, rs: 10, checks: 615
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03681906
Train loss (w/o reg) on all data: 0.032404795
Test loss (w/o reg) on all data: 0.011715905
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.0538866e-07
Norm of the params: 9.396024
     Influence (LOO): fixed 321 labels. Loss 0.01172. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601095
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0348352e-08
Norm of the params: 6.092832
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23393074
Train loss (w/o reg) on all data: 0.22699465
Test loss (w/o reg) on all data: 0.09423596
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.3707542e-05
Norm of the params: 11.778026
              Random: fixed  48 labels. Loss 0.09424. Accuracy 0.993.
### Flips: 410, rs: 10, checks: 820
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024954291
Train loss (w/o reg) on all data: 0.021547956
Test loss (w/o reg) on all data: 0.008080009
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3724845e-07
Norm of the params: 8.253889
     Influence (LOO): fixed 334 labels. Loss 0.00808. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012495
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8419045e-08
Norm of the params: 6.0928063
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22339454
Train loss (w/o reg) on all data: 0.21634896
Test loss (w/o reg) on all data: 0.09078423
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3810866e-05
Norm of the params: 11.87062
              Random: fixed  67 labels. Loss 0.09078. Accuracy 0.994.
### Flips: 410, rs: 10, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015792727
Train loss (w/o reg) on all data: 0.012723757
Test loss (w/o reg) on all data: 0.0064069196
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.941751e-07
Norm of the params: 7.8345027
     Influence (LOO): fixed 344 labels. Loss 0.00641. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0992249e-08
Norm of the params: 6.092819
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20364778
Train loss (w/o reg) on all data: 0.19620498
Test loss (w/o reg) on all data: 0.08355314
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8256566e-06
Norm of the params: 12.200655
              Random: fixed  97 labels. Loss 0.08355. Accuracy 0.995.
### Flips: 410, rs: 10, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012424065
Train loss (w/o reg) on all data: 0.009636871
Test loss (w/o reg) on all data: 0.004619457
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3255829e-07
Norm of the params: 7.4661837
     Influence (LOO): fixed 348 labels. Loss 0.00462. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.196809e-08
Norm of the params: 6.0928087
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19383927
Train loss (w/o reg) on all data: 0.18615887
Test loss (w/o reg) on all data: 0.079391055
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.514407e-06
Norm of the params: 12.393868
              Random: fixed 114 labels. Loss 0.07939. Accuracy 0.994.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25515696
Train loss (w/o reg) on all data: 0.24793547
Test loss (w/o reg) on all data: 0.102636695
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4302984e-05
Norm of the params: 12.017884
Flipped loss: 0.10264. Accuracy: 0.993
### Flips: 410, rs: 11, checks: 205
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12655441
Train loss (w/o reg) on all data: 0.1190021
Test loss (w/o reg) on all data: 0.047797136
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.78982e-06
Norm of the params: 12.290098
     Influence (LOO): fixed 188 labels. Loss 0.04780. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09531691
Train loss (w/o reg) on all data: 0.08221996
Test loss (w/o reg) on all data: 0.047168892
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0274392e-05
Norm of the params: 16.184532
                Loss: fixed 205 labels. Loss 0.04717. Accuracy 0.989.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2443588
Train loss (w/o reg) on all data: 0.23716421
Test loss (w/o reg) on all data: 0.096290275
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.1137855e-05
Norm of the params: 11.995483
              Random: fixed  22 labels. Loss 0.09629. Accuracy 0.993.
### Flips: 410, rs: 11, checks: 410
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059211984
Train loss (w/o reg) on all data: 0.05353714
Test loss (w/o reg) on all data: 0.020565156
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.6329395e-07
Norm of the params: 10.653491
     Influence (LOO): fixed 283 labels. Loss 0.02057. Accuracy 0.997.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004058118
Train loss (w/o reg) on all data: 0.0016741744
Test loss (w/o reg) on all data: 0.006336596
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5692857e-08
Norm of the params: 6.9049892
                Loss: fixed 347 labels. Loss 0.00634. Accuracy 0.998.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2344961
Train loss (w/o reg) on all data: 0.22747758
Test loss (w/o reg) on all data: 0.0914016
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.1676875e-06
Norm of the params: 11.847798
              Random: fixed  40 labels. Loss 0.09140. Accuracy 0.995.
### Flips: 410, rs: 11, checks: 615
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030928846
Train loss (w/o reg) on all data: 0.026270214
Test loss (w/o reg) on all data: 0.010373851
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.609001e-07
Norm of the params: 9.652597
     Influence (LOO): fixed 319 labels. Loss 0.01037. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004080574
Train loss (w/o reg) on all data: 0.0017832168
Test loss (w/o reg) on all data: 0.0051734773
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1398664e-08
Norm of the params: 6.778433
                Loss: fixed 348 labels. Loss 0.00517. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22787213
Train loss (w/o reg) on all data: 0.22073781
Test loss (w/o reg) on all data: 0.08766448
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0156698e-05
Norm of the params: 11.945133
              Random: fixed  52 labels. Loss 0.08766. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02236653
Train loss (w/o reg) on all data: 0.018633006
Test loss (w/o reg) on all data: 0.0072230105
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6567432e-07
Norm of the params: 8.641209
     Influence (LOO): fixed 333 labels. Loss 0.00722. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1219576e-08
Norm of the params: 6.092824
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21544637
Train loss (w/o reg) on all data: 0.20827948
Test loss (w/o reg) on all data: 0.08111696
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4135468e-05
Norm of the params: 11.972383
              Random: fixed  73 labels. Loss 0.08112. Accuracy 0.995.
### Flips: 410, rs: 11, checks: 1025
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017240398
Train loss (w/o reg) on all data: 0.014130356
Test loss (w/o reg) on all data: 0.005700778
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9994022e-07
Norm of the params: 7.8867507
     Influence (LOO): fixed 338 labels. Loss 0.00570. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601339
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.287558e-08
Norm of the params: 6.0927916
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20350209
Train loss (w/o reg) on all data: 0.19624797
Test loss (w/o reg) on all data: 0.07689924
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.2690447e-06
Norm of the params: 12.045013
              Random: fixed  92 labels. Loss 0.07690. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 1230
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01315003
Train loss (w/o reg) on all data: 0.01035781
Test loss (w/o reg) on all data: 0.0046353536
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 9.9758076e-08
Norm of the params: 7.4729104
     Influence (LOO): fixed 342 labels. Loss 0.00464. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601273
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6800117e-08
Norm of the params: 6.092802
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19384868
Train loss (w/o reg) on all data: 0.18633437
Test loss (w/o reg) on all data: 0.07459577
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.225175e-06
Norm of the params: 12.2591305
              Random: fixed 108 labels. Loss 0.07460. Accuracy 0.993.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2561247
Train loss (w/o reg) on all data: 0.2500148
Test loss (w/o reg) on all data: 0.10450805
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5483089e-05
Norm of the params: 11.054316
Flipped loss: 0.10451. Accuracy: 0.992
### Flips: 410, rs: 12, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12846212
Train loss (w/o reg) on all data: 0.120937705
Test loss (w/o reg) on all data: 0.050013848
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.488433e-06
Norm of the params: 12.267364
     Influence (LOO): fixed 188 labels. Loss 0.05001. Accuracy 0.997.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09707989
Train loss (w/o reg) on all data: 0.08341408
Test loss (w/o reg) on all data: 0.05775892
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.9610209e-06
Norm of the params: 16.532278
                Loss: fixed 205 labels. Loss 0.05776. Accuracy 0.983.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663137
Train loss (w/o reg) on all data: 0.24050388
Test loss (w/o reg) on all data: 0.09825076
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.159725e-06
Norm of the params: 11.070228
              Random: fixed  19 labels. Loss 0.09825. Accuracy 0.993.
### Flips: 410, rs: 12, checks: 410
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06347528
Train loss (w/o reg) on all data: 0.057422508
Test loss (w/o reg) on all data: 0.020379012
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4107118e-06
Norm of the params: 11.0025215
     Influence (LOO): fixed 283 labels. Loss 0.02038. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004505326
Train loss (w/o reg) on all data: 0.0016892343
Test loss (w/o reg) on all data: 0.005855336
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0074363e-07
Norm of the params: 7.504788
                Loss: fixed 353 labels. Loss 0.00586. Accuracy 0.999.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23414852
Train loss (w/o reg) on all data: 0.22796604
Test loss (w/o reg) on all data: 0.093013085
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.8036455e-06
Norm of the params: 11.119781
              Random: fixed  44 labels. Loss 0.09301. Accuracy 0.993.
### Flips: 410, rs: 12, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042376745
Train loss (w/o reg) on all data: 0.037634432
Test loss (w/o reg) on all data: 0.013100368
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 9.598125e-07
Norm of the params: 9.738904
     Influence (LOO): fixed 312 labels. Loss 0.01310. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004271215
Train loss (w/o reg) on all data: 0.0016281672
Test loss (w/o reg) on all data: 0.003601487
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.5815905e-08
Norm of the params: 7.2705526
                Loss: fixed 354 labels. Loss 0.00360. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2217754
Train loss (w/o reg) on all data: 0.21548787
Test loss (w/o reg) on all data: 0.08561192
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.7275934e-06
Norm of the params: 11.2138605
              Random: fixed  66 labels. Loss 0.08561. Accuracy 0.995.
### Flips: 410, rs: 12, checks: 820
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02369006
Train loss (w/o reg) on all data: 0.01997855
Test loss (w/o reg) on all data: 0.0083575435
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7803244e-07
Norm of the params: 8.615695
     Influence (LOO): fixed 335 labels. Loss 0.00836. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560915
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9174466e-08
Norm of the params: 6.0928097
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21592812
Train loss (w/o reg) on all data: 0.20938471
Test loss (w/o reg) on all data: 0.08066438
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8172295e-05
Norm of the params: 11.439769
              Random: fixed  76 labels. Loss 0.08066. Accuracy 0.994.
### Flips: 410, rs: 12, checks: 1025
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016811008
Train loss (w/o reg) on all data: 0.013684295
Test loss (w/o reg) on all data: 0.0052719093
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6085605e-07
Norm of the params: 7.90786
     Influence (LOO): fixed 343 labels. Loss 0.00527. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010877
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9911107e-08
Norm of the params: 6.0928326
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20539494
Train loss (w/o reg) on all data: 0.1988753
Test loss (w/o reg) on all data: 0.075201236
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.1807724e-06
Norm of the params: 11.418975
              Random: fixed  96 labels. Loss 0.07520. Accuracy 0.995.
### Flips: 410, rs: 12, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009572148
Train loss (w/o reg) on all data: 0.006916615
Test loss (w/o reg) on all data: 0.0037767359
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2859555e-07
Norm of the params: 7.287707
     Influence (LOO): fixed 350 labels. Loss 0.00378. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560246
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.517469e-08
Norm of the params: 6.092826
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19344065
Train loss (w/o reg) on all data: 0.18667638
Test loss (w/o reg) on all data: 0.070779584
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4189727e-06
Norm of the params: 11.631213
              Random: fixed 114 labels. Loss 0.07078. Accuracy 0.996.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2554243
Train loss (w/o reg) on all data: 0.24870035
Test loss (w/o reg) on all data: 0.108607136
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.4648518e-05
Norm of the params: 11.596494
Flipped loss: 0.10861. Accuracy: 0.995
### Flips: 410, rs: 13, checks: 205
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12880795
Train loss (w/o reg) on all data: 0.12102279
Test loss (w/o reg) on all data: 0.04963486
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.7286795e-06
Norm of the params: 12.478102
     Influence (LOO): fixed 186 labels. Loss 0.04963. Accuracy 0.998.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09589897
Train loss (w/o reg) on all data: 0.083383754
Test loss (w/o reg) on all data: 0.056857537
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1535018e-06
Norm of the params: 15.821008
                Loss: fixed 205 labels. Loss 0.05686. Accuracy 0.985.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24577297
Train loss (w/o reg) on all data: 0.2390783
Test loss (w/o reg) on all data: 0.10295658
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.355284e-06
Norm of the params: 11.571235
              Random: fixed  19 labels. Loss 0.10296. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 410
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0669365
Train loss (w/o reg) on all data: 0.061555587
Test loss (w/o reg) on all data: 0.022117797
Train acc on all data:  0.9810357403355215
Test acc on all data:   1.0
Norm of the mean of gradients: 1.172899e-06
Norm of the params: 10.373925
     Influence (LOO): fixed 273 labels. Loss 0.02212. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003617968
Train loss (w/o reg) on all data: 0.0013908554
Test loss (w/o reg) on all data: 0.0029409816
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2568818e-08
Norm of the params: 6.6739984
                Loss: fixed 349 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23608287
Train loss (w/o reg) on all data: 0.22953415
Test loss (w/o reg) on all data: 0.094850324
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6326856e-05
Norm of the params: 11.444402
              Random: fixed  37 labels. Loss 0.09485. Accuracy 0.996.
### Flips: 410, rs: 13, checks: 615
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036632586
Train loss (w/o reg) on all data: 0.0324258
Test loss (w/o reg) on all data: 0.011334552
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 5.593343e-07
Norm of the params: 9.172556
     Influence (LOO): fixed 315 labels. Loss 0.01133. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362243
Train loss (w/o reg) on all data: 0.0011012148
Test loss (w/o reg) on all data: 0.0029768138
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7874469e-08
Norm of the params: 6.2209473
                Loss: fixed 350 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23033363
Train loss (w/o reg) on all data: 0.22369859
Test loss (w/o reg) on all data: 0.09022912
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.2341354e-06
Norm of the params: 11.519588
              Random: fixed  50 labels. Loss 0.09023. Accuracy 0.995.
### Flips: 410, rs: 13, checks: 820
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01843165
Train loss (w/o reg) on all data: 0.015194491
Test loss (w/o reg) on all data: 0.006452514
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3603594e-07
Norm of the params: 8.046317
     Influence (LOO): fixed 336 labels. Loss 0.00645. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560486
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9027102e-08
Norm of the params: 6.0928125
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22090831
Train loss (w/o reg) on all data: 0.21414483
Test loss (w/o reg) on all data: 0.08575598
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4823215e-05
Norm of the params: 11.63055
              Random: fixed  66 labels. Loss 0.08576. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 1025
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014430757
Train loss (w/o reg) on all data: 0.011547027
Test loss (w/o reg) on all data: 0.0058039357
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2356868e-07
Norm of the params: 7.5943794
     Influence (LOO): fixed 340 labels. Loss 0.00580. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.47910795e-08
Norm of the params: 6.092808
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20787986
Train loss (w/o reg) on all data: 0.20097925
Test loss (w/o reg) on all data: 0.08022348
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.2874624e-06
Norm of the params: 11.747854
              Random: fixed  88 labels. Loss 0.08022. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013546163
Train loss (w/o reg) on all data: 0.010669395
Test loss (w/o reg) on all data: 0.0057120523
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6591786e-07
Norm of the params: 7.5852075
     Influence (LOO): fixed 341 labels. Loss 0.00571. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096015376
Test loss (w/o reg) on all data: 0.0026561131
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4990547e-08
Norm of the params: 6.092759
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19406033
Train loss (w/o reg) on all data: 0.1869886
Test loss (w/o reg) on all data: 0.07466652
Train acc on all data:  0.9416484318016046
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4725274e-06
Norm of the params: 11.89262
              Random: fixed 109 labels. Loss 0.07467. Accuracy 0.995.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25434342
Train loss (w/o reg) on all data: 0.24799095
Test loss (w/o reg) on all data: 0.102974676
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.5080834e-06
Norm of the params: 11.271613
Flipped loss: 0.10297. Accuracy: 0.993
### Flips: 410, rs: 14, checks: 205
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12938707
Train loss (w/o reg) on all data: 0.12173038
Test loss (w/o reg) on all data: 0.04972999
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.957315e-06
Norm of the params: 12.374723
     Influence (LOO): fixed 188 labels. Loss 0.04973. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097147174
Train loss (w/o reg) on all data: 0.08471329
Test loss (w/o reg) on all data: 0.049278818
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6902798e-06
Norm of the params: 15.769521
                Loss: fixed 205 labels. Loss 0.04928. Accuracy 0.989.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24214324
Train loss (w/o reg) on all data: 0.23557962
Test loss (w/o reg) on all data: 0.096113525
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.410586e-05
Norm of the params: 11.45742
              Random: fixed  25 labels. Loss 0.09611. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06260011
Train loss (w/o reg) on all data: 0.056250922
Test loss (w/o reg) on all data: 0.021169204
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4106426e-06
Norm of the params: 11.268713
     Influence (LOO): fixed 282 labels. Loss 0.02117. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043190126
Train loss (w/o reg) on all data: 0.0016405802
Test loss (w/o reg) on all data: 0.003020673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.246847e-08
Norm of the params: 7.3190603
                Loss: fixed 352 labels. Loss 0.00302. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23401707
Train loss (w/o reg) on all data: 0.227464
Test loss (w/o reg) on all data: 0.09008491
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.020781e-06
Norm of the params: 11.448204
              Random: fixed  41 labels. Loss 0.09008. Accuracy 0.997.
### Flips: 410, rs: 14, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040599015
Train loss (w/o reg) on all data: 0.035313033
Test loss (w/o reg) on all data: 0.013175004
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6580778e-06
Norm of the params: 10.282007
     Influence (LOO): fixed 314 labels. Loss 0.01318. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034945277
Train loss (w/o reg) on all data: 0.0012287492
Test loss (w/o reg) on all data: 0.0029084976
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3436157e-08
Norm of the params: 6.7316837
                Loss: fixed 354 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22319196
Train loss (w/o reg) on all data: 0.21632883
Test loss (w/o reg) on all data: 0.08774332
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.00975185e-05
Norm of the params: 11.715915
              Random: fixed  59 labels. Loss 0.08774. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025757672
Train loss (w/o reg) on all data: 0.021627147
Test loss (w/o reg) on all data: 0.009161429
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0693747e-07
Norm of the params: 9.089032
     Influence (LOO): fixed 330 labels. Loss 0.00916. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1715699e-08
Norm of the params: 6.0928183
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2129719
Train loss (w/o reg) on all data: 0.2060425
Test loss (w/o reg) on all data: 0.08376145
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8294768e-05
Norm of the params: 11.772345
              Random: fixed  77 labels. Loss 0.08376. Accuracy 0.992.
### Flips: 410, rs: 14, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019448884
Train loss (w/o reg) on all data: 0.01581744
Test loss (w/o reg) on all data: 0.007358261
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2809988e-06
Norm of the params: 8.522257
     Influence (LOO): fixed 338 labels. Loss 0.00736. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7702996e-08
Norm of the params: 6.0928125
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2020965
Train loss (w/o reg) on all data: 0.19493039
Test loss (w/o reg) on all data: 0.07819982
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.27694175e-05
Norm of the params: 11.971736
              Random: fixed  94 labels. Loss 0.07820. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 1230
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015013838
Train loss (w/o reg) on all data: 0.011860354
Test loss (w/o reg) on all data: 0.005357086
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7546425e-07
Norm of the params: 7.941643
     Influence (LOO): fixed 343 labels. Loss 0.00536. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7812539e-08
Norm of the params: 6.092808
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19239068
Train loss (w/o reg) on all data: 0.1853513
Test loss (w/o reg) on all data: 0.07469279
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6153217e-05
Norm of the params: 11.865402
              Random: fixed 111 labels. Loss 0.07469. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24474128
Train loss (w/o reg) on all data: 0.23825666
Test loss (w/o reg) on all data: 0.103660144
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2817045e-05
Norm of the params: 11.388247
Flipped loss: 0.10366. Accuracy: 0.992
### Flips: 410, rs: 15, checks: 205
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120572984
Train loss (w/o reg) on all data: 0.1127928
Test loss (w/o reg) on all data: 0.043237507
Train acc on all data:  0.963287138341843
Test acc on all data:   1.0
Norm of the mean of gradients: 2.438915e-06
Norm of the params: 12.474122
     Influence (LOO): fixed 186 labels. Loss 0.04324. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08361422
Train loss (w/o reg) on all data: 0.07178736
Test loss (w/o reg) on all data: 0.04128702
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.99625e-06
Norm of the params: 15.37977
                Loss: fixed 205 labels. Loss 0.04129. Accuracy 0.991.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23335488
Train loss (w/o reg) on all data: 0.22708762
Test loss (w/o reg) on all data: 0.0958112
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.48969e-06
Norm of the params: 11.1957655
              Random: fixed  21 labels. Loss 0.09581. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058236957
Train loss (w/o reg) on all data: 0.052934352
Test loss (w/o reg) on all data: 0.020029804
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4529454e-06
Norm of the params: 10.2981615
     Influence (LOO): fixed 272 labels. Loss 0.02003. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.665617e-09
Norm of the params: 6.0928183
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22389615
Train loss (w/o reg) on all data: 0.21765986
Test loss (w/o reg) on all data: 0.09071464
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9339312e-05
Norm of the params: 11.168062
              Random: fixed  38 labels. Loss 0.09071. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031334348
Train loss (w/o reg) on all data: 0.026680999
Test loss (w/o reg) on all data: 0.0105447555
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7647644e-07
Norm of the params: 9.647124
     Influence (LOO): fixed 304 labels. Loss 0.01054. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601253
Test loss (w/o reg) on all data: 0.0026560922
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7256501e-08
Norm of the params: 6.0928054
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21808015
Train loss (w/o reg) on all data: 0.211835
Test loss (w/o reg) on all data: 0.08811317
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.449747e-06
Norm of the params: 11.176002
              Random: fixed  49 labels. Loss 0.08811. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 820
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019018406
Train loss (w/o reg) on all data: 0.015538865
Test loss (w/o reg) on all data: 0.007281367
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.635723e-07
Norm of the params: 8.342113
     Influence (LOO): fixed 319 labels. Loss 0.00728. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6000294e-08
Norm of the params: 6.0928183
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20997122
Train loss (w/o reg) on all data: 0.20363784
Test loss (w/o reg) on all data: 0.08190765
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2642165e-05
Norm of the params: 11.254668
              Random: fixed  65 labels. Loss 0.08191. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01180543
Train loss (w/o reg) on all data: 0.008920355
Test loss (w/o reg) on all data: 0.0049419333
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2824353e-07
Norm of the params: 7.596151
     Influence (LOO): fixed 327 labels. Loss 0.00494. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011016
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.842749e-08
Norm of the params: 6.092831
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20000784
Train loss (w/o reg) on all data: 0.19361854
Test loss (w/o reg) on all data: 0.07690241
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.144943e-05
Norm of the params: 11.304252
              Random: fixed  84 labels. Loss 0.07690. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 1230
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052803727
Train loss (w/o reg) on all data: 0.003324088
Test loss (w/o reg) on all data: 0.0034012077
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.9171114e-08
Norm of the params: 6.2550535
     Influence (LOO): fixed 333 labels. Loss 0.00340. Accuracy 0.999.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9191483e-08
Norm of the params: 6.092804
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1890282
Train loss (w/o reg) on all data: 0.18236805
Test loss (w/o reg) on all data: 0.074181795
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0001005e-05
Norm of the params: 11.541368
              Random: fixed 100 labels. Loss 0.07418. Accuracy 0.994.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25694406
Train loss (w/o reg) on all data: 0.24885897
Test loss (w/o reg) on all data: 0.10700756
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9940273e-05
Norm of the params: 12.716206
Flipped loss: 0.10701. Accuracy: 0.994
### Flips: 410, rs: 16, checks: 205
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14264248
Train loss (w/o reg) on all data: 0.13510408
Test loss (w/o reg) on all data: 0.051400803
Train acc on all data:  0.9562363238512035
Test acc on all data:   1.0
Norm of the mean of gradients: 1.291057e-05
Norm of the params: 12.278766
     Influence (LOO): fixed 181 labels. Loss 0.05140. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09630237
Train loss (w/o reg) on all data: 0.08197375
Test loss (w/o reg) on all data: 0.056519356
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.9801902e-06
Norm of the params: 16.92845
                Loss: fixed 205 labels. Loss 0.05652. Accuracy 0.989.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663492
Train loss (w/o reg) on all data: 0.2384774
Test loss (w/o reg) on all data: 0.10180003
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0794061e-05
Norm of the params: 12.773031
              Random: fixed  20 labels. Loss 0.10180. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 410
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061258044
Train loss (w/o reg) on all data: 0.05588539
Test loss (w/o reg) on all data: 0.020003099
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4665235e-06
Norm of the params: 10.365959
     Influence (LOO): fixed 294 labels. Loss 0.02000. Accuracy 1.000.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043888255
Train loss (w/o reg) on all data: 0.0017180558
Test loss (w/o reg) on all data: 0.0030720462
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5664376e-08
Norm of the params: 7.3085837
                Loss: fixed 358 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23751959
Train loss (w/o reg) on all data: 0.22942844
Test loss (w/o reg) on all data: 0.09642658
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.1863461e-05
Norm of the params: 12.72097
              Random: fixed  40 labels. Loss 0.09643. Accuracy 0.995.
### Flips: 410, rs: 16, checks: 615
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032260105
Train loss (w/o reg) on all data: 0.028070873
Test loss (w/o reg) on all data: 0.01082571
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 7.668008e-07
Norm of the params: 9.153395
     Influence (LOO): fixed 331 labels. Loss 0.01083. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037317416
Train loss (w/o reg) on all data: 0.0013234058
Test loss (w/o reg) on all data: 0.003028588
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9595588e-07
Norm of the params: 6.9402246
                Loss: fixed 360 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22754186
Train loss (w/o reg) on all data: 0.21954712
Test loss (w/o reg) on all data: 0.08879475
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.557125e-05
Norm of the params: 12.644953
              Random: fixed  58 labels. Loss 0.08879. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018013952
Train loss (w/o reg) on all data: 0.014750923
Test loss (w/o reg) on all data: 0.0066504073
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3635044e-07
Norm of the params: 8.078403
     Influence (LOO): fixed 348 labels. Loss 0.00665. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033632657
Train loss (w/o reg) on all data: 0.0011526126
Test loss (w/o reg) on all data: 0.002782596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.479897e-08
Norm of the params: 6.64929
                Loss: fixed 361 labels. Loss 0.00278. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21594101
Train loss (w/o reg) on all data: 0.20784067
Test loss (w/o reg) on all data: 0.08459838
Train acc on all data:  0.9326525650376853
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0910398e-05
Norm of the params: 12.728197
              Random: fixed  76 labels. Loss 0.08460. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 1025
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007759409
Train loss (w/o reg) on all data: 0.0053913775
Test loss (w/o reg) on all data: 0.0035661561
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.928383e-07
Norm of the params: 6.8819056
     Influence (LOO): fixed 358 labels. Loss 0.00357. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033632652
Train loss (w/o reg) on all data: 0.0011526241
Test loss (w/o reg) on all data: 0.0027826016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0424872e-08
Norm of the params: 6.649273
                Loss: fixed 361 labels. Loss 0.00278. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20650177
Train loss (w/o reg) on all data: 0.19885686
Test loss (w/o reg) on all data: 0.078417145
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.2532857e-06
Norm of the params: 12.3652
              Random: fixed  95 labels. Loss 0.07842. Accuracy 0.997.
### Flips: 410, rs: 16, checks: 1230
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006447875
Train loss (w/o reg) on all data: 0.0041685193
Test loss (w/o reg) on all data: 0.0031612068
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.636697e-08
Norm of the params: 6.7518225
     Influence (LOO): fixed 359 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010923
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5823296e-08
Norm of the params: 6.0928326
                Loss: fixed 362 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19797464
Train loss (w/o reg) on all data: 0.19009282
Test loss (w/o reg) on all data: 0.07453119
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.992252e-06
Norm of the params: 12.555338
              Random: fixed 108 labels. Loss 0.07453. Accuracy 0.996.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25478342
Train loss (w/o reg) on all data: 0.24738257
Test loss (w/o reg) on all data: 0.1105826
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6608887e-05
Norm of the params: 12.16624
Flipped loss: 0.11058. Accuracy: 0.993
### Flips: 410, rs: 17, checks: 205
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13392216
Train loss (w/o reg) on all data: 0.12535918
Test loss (w/o reg) on all data: 0.051784266
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.8386926e-06
Norm of the params: 13.086624
     Influence (LOO): fixed 186 labels. Loss 0.05178. Accuracy 0.999.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09658618
Train loss (w/o reg) on all data: 0.0838256
Test loss (w/o reg) on all data: 0.053118665
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.8356813e-06
Norm of the params: 15.975344
                Loss: fixed 205 labels. Loss 0.05312. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24707802
Train loss (w/o reg) on all data: 0.23961768
Test loss (w/o reg) on all data: 0.106315956
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.301437e-06
Norm of the params: 12.21503
              Random: fixed  17 labels. Loss 0.10632. Accuracy 0.992.
### Flips: 410, rs: 17, checks: 410
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06777035
Train loss (w/o reg) on all data: 0.06098608
Test loss (w/o reg) on all data: 0.023453088
Train acc on all data:  0.9803063457330415
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6452329e-06
Norm of the params: 11.648405
     Influence (LOO): fixed 276 labels. Loss 0.02345. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034164607
Train loss (w/o reg) on all data: 0.001185735
Test loss (w/o reg) on all data: 0.002793187
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4835712e-08
Norm of the params: 6.6794105
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23433392
Train loss (w/o reg) on all data: 0.2268454
Test loss (w/o reg) on all data: 0.09909341
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.771476e-05
Norm of the params: 12.238076
              Random: fixed  41 labels. Loss 0.09909. Accuracy 0.993.
### Flips: 410, rs: 17, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037856795
Train loss (w/o reg) on all data: 0.033126675
Test loss (w/o reg) on all data: 0.011772476
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7553945e-06
Norm of the params: 9.726376
     Influence (LOO): fixed 314 labels. Loss 0.01177. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.0011857381
Test loss (w/o reg) on all data: 0.0027932397
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2075942e-07
Norm of the params: 6.679405
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22525139
Train loss (w/o reg) on all data: 0.21809894
Test loss (w/o reg) on all data: 0.093641914
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7791388e-05
Norm of the params: 11.960315
              Random: fixed  57 labels. Loss 0.09364. Accuracy 0.995.
### Flips: 410, rs: 17, checks: 820
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026952539
Train loss (w/o reg) on all data: 0.022983007
Test loss (w/o reg) on all data: 0.008661916
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 3.753621e-07
Norm of the params: 8.910143
     Influence (LOO): fixed 329 labels. Loss 0.00866. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.001185731
Test loss (w/o reg) on all data: 0.00279322
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.205449e-08
Norm of the params: 6.679416
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21278222
Train loss (w/o reg) on all data: 0.20593616
Test loss (w/o reg) on all data: 0.08628278
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.7528682e-05
Norm of the params: 11.701326
              Random: fixed  82 labels. Loss 0.08628. Accuracy 0.994.
### Flips: 410, rs: 17, checks: 1025
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021668179
Train loss (w/o reg) on all data: 0.017952338
Test loss (w/o reg) on all data: 0.007932504
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2434433e-06
Norm of the params: 8.620722
     Influence (LOO): fixed 335 labels. Loss 0.00793. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.0011857323
Test loss (w/o reg) on all data: 0.0027931978
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8063558e-08
Norm of the params: 6.6794143
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20401
Train loss (w/o reg) on all data: 0.1970889
Test loss (w/o reg) on all data: 0.08222325
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.53593e-06
Norm of the params: 11.765291
              Random: fixed  97 labels. Loss 0.08222. Accuracy 0.992.
### Flips: 410, rs: 17, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01840568
Train loss (w/o reg) on all data: 0.014784864
Test loss (w/o reg) on all data: 0.007239685
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.683333e-07
Norm of the params: 8.509777
     Influence (LOO): fixed 339 labels. Loss 0.00724. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034164607
Train loss (w/o reg) on all data: 0.0011857427
Test loss (w/o reg) on all data: 0.002793216
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2160923e-08
Norm of the params: 6.679399
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19838107
Train loss (w/o reg) on all data: 0.1915035
Test loss (w/o reg) on all data: 0.079794556
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3021382e-05
Norm of the params: 11.728234
              Random: fixed 108 labels. Loss 0.07979. Accuracy 0.994.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25573057
Train loss (w/o reg) on all data: 0.24779142
Test loss (w/o reg) on all data: 0.105706885
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.7693665e-06
Norm of the params: 12.600916
Flipped loss: 0.10571. Accuracy: 0.996
### Flips: 410, rs: 18, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12831704
Train loss (w/o reg) on all data: 0.120708734
Test loss (w/o reg) on all data: 0.047362585
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.823938e-06
Norm of the params: 12.335567
     Influence (LOO): fixed 192 labels. Loss 0.04736. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09983897
Train loss (w/o reg) on all data: 0.085442655
Test loss (w/o reg) on all data: 0.06227054
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.091876e-06
Norm of the params: 16.96839
                Loss: fixed 205 labels. Loss 0.06227. Accuracy 0.981.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24548505
Train loss (w/o reg) on all data: 0.23741287
Test loss (w/o reg) on all data: 0.100782305
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.135671e-05
Norm of the params: 12.706043
              Random: fixed  21 labels. Loss 0.10078. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 410
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058938023
Train loss (w/o reg) on all data: 0.05241019
Test loss (w/o reg) on all data: 0.021247793
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3567461e-06
Norm of the params: 11.426139
     Influence (LOO): fixed 288 labels. Loss 0.02125. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.47208175e-08
Norm of the params: 6.092811
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23608105
Train loss (w/o reg) on all data: 0.22796425
Test loss (w/o reg) on all data: 0.0963363
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.2945013e-06
Norm of the params: 12.741117
              Random: fixed  39 labels. Loss 0.09634. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036161788
Train loss (w/o reg) on all data: 0.03129027
Test loss (w/o reg) on all data: 0.01242114
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 7.194767e-07
Norm of the params: 9.870682
     Influence (LOO): fixed 318 labels. Loss 0.01242. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4886357e-08
Norm of the params: 6.092818
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22277647
Train loss (w/o reg) on all data: 0.21460766
Test loss (w/o reg) on all data: 0.08891321
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.4544144e-05
Norm of the params: 12.781876
              Random: fixed  61 labels. Loss 0.08891. Accuracy 0.995.
### Flips: 410, rs: 18, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022436354
Train loss (w/o reg) on all data: 0.018278737
Test loss (w/o reg) on all data: 0.009079499
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9741304e-07
Norm of the params: 9.118791
     Influence (LOO): fixed 334 labels. Loss 0.00908. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013007
Test loss (w/o reg) on all data: 0.0026561068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.387208e-08
Norm of the params: 6.0927973
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21359226
Train loss (w/o reg) on all data: 0.20541327
Test loss (w/o reg) on all data: 0.08348867
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.8340145e-06
Norm of the params: 12.789838
              Random: fixed  77 labels. Loss 0.08349. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 1025
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01619261
Train loss (w/o reg) on all data: 0.012532984
Test loss (w/o reg) on all data: 0.0072979196
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2276332e-07
Norm of the params: 8.555263
     Influence (LOO): fixed 341 labels. Loss 0.00730. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.724627e-09
Norm of the params: 6.092831
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20290026
Train loss (w/o reg) on all data: 0.19460475
Test loss (w/o reg) on all data: 0.07962734
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2332086e-05
Norm of the params: 12.880616
              Random: fixed  96 labels. Loss 0.07963. Accuracy 0.997.
### Flips: 410, rs: 18, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013928646
Train loss (w/o reg) on all data: 0.01067166
Test loss (w/o reg) on all data: 0.0054207193
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6267903e-07
Norm of the params: 8.070917
     Influence (LOO): fixed 344 labels. Loss 0.00542. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5085986e-08
Norm of the params: 6.0928116
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19400242
Train loss (w/o reg) on all data: 0.18560871
Test loss (w/o reg) on all data: 0.07618251
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.2951877e-06
Norm of the params: 12.956623
              Random: fixed 108 labels. Loss 0.07618. Accuracy 0.997.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25778025
Train loss (w/o reg) on all data: 0.24950606
Test loss (w/o reg) on all data: 0.109375484
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.0082576e-06
Norm of the params: 12.864052
Flipped loss: 0.10938. Accuracy: 0.991
### Flips: 410, rs: 19, checks: 205
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14493515
Train loss (w/o reg) on all data: 0.13556015
Test loss (w/o reg) on all data: 0.05347704
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6722823e-06
Norm of the params: 13.693057
     Influence (LOO): fixed 180 labels. Loss 0.05348. Accuracy 0.998.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10043174
Train loss (w/o reg) on all data: 0.08420347
Test loss (w/o reg) on all data: 0.053273827
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.933989e-06
Norm of the params: 18.015701
                Loss: fixed 205 labels. Loss 0.05327. Accuracy 0.988.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24992155
Train loss (w/o reg) on all data: 0.24155961
Test loss (w/o reg) on all data: 0.105673894
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.0347934e-06
Norm of the params: 12.932082
              Random: fixed  16 labels. Loss 0.10567. Accuracy 0.991.
### Flips: 410, rs: 19, checks: 410
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070937276
Train loss (w/o reg) on all data: 0.06409713
Test loss (w/o reg) on all data: 0.02423835
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8701304e-06
Norm of the params: 11.6962805
     Influence (LOO): fixed 284 labels. Loss 0.02424. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062756455
Train loss (w/o reg) on all data: 0.0025406284
Test loss (w/o reg) on all data: 0.004079333
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.07692806e-07
Norm of the params: 8.642938
                Loss: fixed 361 labels. Loss 0.00408. Accuracy 0.999.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2413469
Train loss (w/o reg) on all data: 0.23308928
Test loss (w/o reg) on all data: 0.1006007
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.353095e-05
Norm of the params: 12.851156
              Random: fixed  36 labels. Loss 0.10060. Accuracy 0.992.
### Flips: 410, rs: 19, checks: 615
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040407475
Train loss (w/o reg) on all data: 0.03571745
Test loss (w/o reg) on all data: 0.0121774925
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4225435e-06
Norm of the params: 9.685064
     Influence (LOO): fixed 324 labels. Loss 0.01218. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049146777
Train loss (w/o reg) on all data: 0.0018619669
Test loss (w/o reg) on all data: 0.0030098122
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0966807e-08
Norm of the params: 7.8137193
                Loss: fixed 363 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23681065
Train loss (w/o reg) on all data: 0.22841978
Test loss (w/o reg) on all data: 0.09724891
Train acc on all data:  0.9238998298079261
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.393487e-06
Norm of the params: 12.954443
              Random: fixed  45 labels. Loss 0.09725. Accuracy 0.992.
### Flips: 410, rs: 19, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023565471
Train loss (w/o reg) on all data: 0.019974016
Test loss (w/o reg) on all data: 0.0076889447
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 5.343001e-07
Norm of the params: 8.475206
     Influence (LOO): fixed 342 labels. Loss 0.00769. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042070076
Train loss (w/o reg) on all data: 0.0014974382
Test loss (w/o reg) on all data: 0.0029399137
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.018341e-08
Norm of the params: 7.3614798
                Loss: fixed 364 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22834271
Train loss (w/o reg) on all data: 0.21994591
Test loss (w/o reg) on all data: 0.09388252
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4466659e-05
Norm of the params: 12.95902
              Random: fixed  60 labels. Loss 0.09388. Accuracy 0.991.
### Flips: 410, rs: 19, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018428551
Train loss (w/o reg) on all data: 0.01517321
Test loss (w/o reg) on all data: 0.0060286527
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 4.94169e-07
Norm of the params: 8.06888
     Influence (LOO): fixed 349 labels. Loss 0.00603. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004207007
Train loss (w/o reg) on all data: 0.0014974519
Test loss (w/o reg) on all data: 0.0029399253
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.968601e-08
Norm of the params: 7.361461
                Loss: fixed 364 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22304478
Train loss (w/o reg) on all data: 0.21487732
Test loss (w/o reg) on all data: 0.0890047
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.510151e-06
Norm of the params: 12.780808
              Random: fixed  71 labels. Loss 0.08900. Accuracy 0.993.
### Flips: 410, rs: 19, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015542087
Train loss (w/o reg) on all data: 0.01264157
Test loss (w/o reg) on all data: 0.0051254006
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9867436e-07
Norm of the params: 7.6164527
     Influence (LOO): fixed 352 labels. Loss 0.00513. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0024045e-08
Norm of the params: 6.0928063
                Loss: fixed 365 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20670223
Train loss (w/o reg) on all data: 0.19819693
Test loss (w/o reg) on all data: 0.079579584
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.576955e-06
Norm of the params: 13.04247
              Random: fixed  99 labels. Loss 0.07958. Accuracy 0.995.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25772235
Train loss (w/o reg) on all data: 0.2510065
Test loss (w/o reg) on all data: 0.105817825
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4811283e-05
Norm of the params: 11.5895195
Flipped loss: 0.10582. Accuracy: 0.997
### Flips: 410, rs: 20, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12858395
Train loss (w/o reg) on all data: 0.12110803
Test loss (w/o reg) on all data: 0.04676972
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 3.428336e-06
Norm of the params: 12.227777
     Influence (LOO): fixed 189 labels. Loss 0.04677. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1045179
Train loss (w/o reg) on all data: 0.092356846
Test loss (w/o reg) on all data: 0.041753437
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.6486105e-06
Norm of the params: 15.595547
                Loss: fixed 205 labels. Loss 0.04175. Accuracy 0.995.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25058562
Train loss (w/o reg) on all data: 0.24376576
Test loss (w/o reg) on all data: 0.1013527
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3947829e-05
Norm of the params: 11.67891
              Random: fixed  13 labels. Loss 0.10135. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062468752
Train loss (w/o reg) on all data: 0.05610851
Test loss (w/o reg) on all data: 0.022217225
Train acc on all data:  0.9827376610746413
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1489595e-06
Norm of the params: 11.278515
     Influence (LOO): fixed 284 labels. Loss 0.02222. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.69562e-08
Norm of the params: 6.0928226
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24523962
Train loss (w/o reg) on all data: 0.23860349
Test loss (w/o reg) on all data: 0.097257495
Train acc on all data:  0.9202528567955264
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.649961e-06
Norm of the params: 11.520521
              Random: fixed  24 labels. Loss 0.09726. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 615
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042479046
Train loss (w/o reg) on all data: 0.03717001
Test loss (w/o reg) on all data: 0.013490532
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5612534e-06
Norm of the params: 10.3043995
     Influence (LOO): fixed 311 labels. Loss 0.01349. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6678793e-08
Norm of the params: 6.092823
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23798276
Train loss (w/o reg) on all data: 0.2313028
Test loss (w/o reg) on all data: 0.094274685
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6687596e-05
Norm of the params: 11.558517
              Random: fixed  37 labels. Loss 0.09427. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 820
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02788859
Train loss (w/o reg) on all data: 0.023536716
Test loss (w/o reg) on all data: 0.009508087
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3797252e-07
Norm of the params: 9.32939
     Influence (LOO): fixed 330 labels. Loss 0.00951. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560845
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.068558e-08
Norm of the params: 6.0928097
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23081371
Train loss (w/o reg) on all data: 0.22399394
Test loss (w/o reg) on all data: 0.09091023
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.5726464e-06
Norm of the params: 11.678844
              Random: fixed  50 labels. Loss 0.09091. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0171281
Train loss (w/o reg) on all data: 0.013935887
Test loss (w/o reg) on all data: 0.005405835
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1699816e-07
Norm of the params: 7.9902616
     Influence (LOO): fixed 342 labels. Loss 0.00541. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.524549e-08
Norm of the params: 6.092823
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2190699
Train loss (w/o reg) on all data: 0.21203434
Test loss (w/o reg) on all data: 0.08551419
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4805309e-05
Norm of the params: 11.862167
              Random: fixed  70 labels. Loss 0.08551. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012182314
Train loss (w/o reg) on all data: 0.009330464
Test loss (w/o reg) on all data: 0.0040464043
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 3.217562e-07
Norm of the params: 7.5522847
     Influence (LOO): fixed 346 labels. Loss 0.00405. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5043318e-08
Norm of the params: 6.0928297
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20651191
Train loss (w/o reg) on all data: 0.19940941
Test loss (w/o reg) on all data: 0.07952391
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.224841e-06
Norm of the params: 11.918476
              Random: fixed  92 labels. Loss 0.07952. Accuracy 0.998.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26383978
Train loss (w/o reg) on all data: 0.25647992
Test loss (w/o reg) on all data: 0.1163839
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5417297e-05
Norm of the params: 12.132492
Flipped loss: 0.11638. Accuracy: 0.990
### Flips: 410, rs: 21, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13458185
Train loss (w/o reg) on all data: 0.12639692
Test loss (w/o reg) on all data: 0.057171617
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.948303e-06
Norm of the params: 12.79447
     Influence (LOO): fixed 187 labels. Loss 0.05717. Accuracy 0.994.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10324415
Train loss (w/o reg) on all data: 0.08867071
Test loss (w/o reg) on all data: 0.06828751
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8484667e-06
Norm of the params: 17.072458
                Loss: fixed 205 labels. Loss 0.06829. Accuracy 0.979.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25374714
Train loss (w/o reg) on all data: 0.24659239
Test loss (w/o reg) on all data: 0.110740595
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.1677133e-06
Norm of the params: 11.962236
              Random: fixed  21 labels. Loss 0.11074. Accuracy 0.992.
### Flips: 410, rs: 21, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056809038
Train loss (w/o reg) on all data: 0.051264323
Test loss (w/o reg) on all data: 0.020313524
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.3964372e-06
Norm of the params: 10.530634
     Influence (LOO): fixed 298 labels. Loss 0.02031. Accuracy 0.997.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044784034
Train loss (w/o reg) on all data: 0.0020163876
Test loss (w/o reg) on all data: 0.0039232173
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.158586e-08
Norm of the params: 7.017144
                Loss: fixed 361 labels. Loss 0.00392. Accuracy 0.999.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24282268
Train loss (w/o reg) on all data: 0.23565273
Test loss (w/o reg) on all data: 0.10393912
Train acc on all data:  0.9214685144663263
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9792196e-05
Norm of the params: 11.974929
              Random: fixed  39 labels. Loss 0.10394. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 615
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037960924
Train loss (w/o reg) on all data: 0.033218917
Test loss (w/o reg) on all data: 0.010748369
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 2.163576e-06
Norm of the params: 9.73859
     Influence (LOO): fixed 326 labels. Loss 0.01075. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601004
Test loss (w/o reg) on all data: 0.002656036
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6285306e-08
Norm of the params: 6.092848
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23386326
Train loss (w/o reg) on all data: 0.22655745
Test loss (w/o reg) on all data: 0.098585464
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1537913e-05
Norm of the params: 12.087853
              Random: fixed  59 labels. Loss 0.09859. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02523308
Train loss (w/o reg) on all data: 0.021666814
Test loss (w/o reg) on all data: 0.008275861
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0160223e-06
Norm of the params: 8.445433
     Influence (LOO): fixed 340 labels. Loss 0.00828. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5098596e-08
Norm of the params: 6.0928173
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22477953
Train loss (w/o reg) on all data: 0.21731775
Test loss (w/o reg) on all data: 0.09410843
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.45602e-06
Norm of the params: 12.216207
              Random: fixed  73 labels. Loss 0.09411. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01951294
Train loss (w/o reg) on all data: 0.016096659
Test loss (w/o reg) on all data: 0.007108913
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4887578e-07
Norm of the params: 8.265931
     Influence (LOO): fixed 346 labels. Loss 0.00711. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601017
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5248665e-08
Norm of the params: 6.092844
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21390808
Train loss (w/o reg) on all data: 0.20643
Test loss (w/o reg) on all data: 0.08867182
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.292115e-05
Norm of the params: 12.229536
              Random: fixed  91 labels. Loss 0.08867. Accuracy 0.992.
### Flips: 410, rs: 21, checks: 1230
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012386427
Train loss (w/o reg) on all data: 0.009507898
Test loss (w/o reg) on all data: 0.0049428553
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.371723e-07
Norm of the params: 7.5875273
     Influence (LOO): fixed 353 labels. Loss 0.00494. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7294848e-08
Norm of the params: 6.0928135
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1982618
Train loss (w/o reg) on all data: 0.1909744
Test loss (w/o reg) on all data: 0.07792191
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.78709e-06
Norm of the params: 12.072605
              Random: fixed 118 labels. Loss 0.07792. Accuracy 0.995.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25118712
Train loss (w/o reg) on all data: 0.24412212
Test loss (w/o reg) on all data: 0.10185687
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9126555e-05
Norm of the params: 11.886975
Flipped loss: 0.10186. Accuracy: 0.997
### Flips: 410, rs: 22, checks: 205
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1287882
Train loss (w/o reg) on all data: 0.12145412
Test loss (w/o reg) on all data: 0.048397675
Train acc on all data:  0.9601264283977632
Test acc on all data:   1.0
Norm of the mean of gradients: 9.2905375e-06
Norm of the params: 12.111216
     Influence (LOO): fixed 183 labels. Loss 0.04840. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09297489
Train loss (w/o reg) on all data: 0.07908796
Test loss (w/o reg) on all data: 0.04918746
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.8652747e-06
Norm of the params: 16.66549
                Loss: fixed 205 labels. Loss 0.04919. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23948058
Train loss (w/o reg) on all data: 0.23224483
Test loss (w/o reg) on all data: 0.09624521
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.3501585e-06
Norm of the params: 12.029761
              Random: fixed  22 labels. Loss 0.09625. Accuracy 0.998.
### Flips: 410, rs: 22, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056695808
Train loss (w/o reg) on all data: 0.05135519
Test loss (w/o reg) on all data: 0.019305805
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2253021e-06
Norm of the params: 10.335007
     Influence (LOO): fixed 287 labels. Loss 0.01931. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031875302
Train loss (w/o reg) on all data: 0.001111962
Test loss (w/o reg) on all data: 0.0026435507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2291717e-08
Norm of the params: 6.442931
                Loss: fixed 350 labels. Loss 0.00264. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22965056
Train loss (w/o reg) on all data: 0.22204968
Test loss (w/o reg) on all data: 0.09407031
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.538406e-05
Norm of the params: 12.329541
              Random: fixed  38 labels. Loss 0.09407. Accuracy 0.996.
### Flips: 410, rs: 22, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028868033
Train loss (w/o reg) on all data: 0.024937125
Test loss (w/o reg) on all data: 0.009347178
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7311327e-07
Norm of the params: 8.866688
     Influence (LOO): fixed 324 labels. Loss 0.00935. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031875297
Train loss (w/o reg) on all data: 0.0011119392
Test loss (w/o reg) on all data: 0.0026435694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1978644e-08
Norm of the params: 6.442966
                Loss: fixed 350 labels. Loss 0.00264. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21805017
Train loss (w/o reg) on all data: 0.21075822
Test loss (w/o reg) on all data: 0.08788458
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1199043e-05
Norm of the params: 12.076375
              Random: fixed  61 labels. Loss 0.08788. Accuracy 0.997.
### Flips: 410, rs: 22, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012872776
Train loss (w/o reg) on all data: 0.010114949
Test loss (w/o reg) on all data: 0.0045827697
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5444615e-07
Norm of the params: 7.426745
     Influence (LOO): fixed 342 labels. Loss 0.00458. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601017
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.839716e-08
Norm of the params: 6.092845
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20716438
Train loss (w/o reg) on all data: 0.1996956
Test loss (w/o reg) on all data: 0.084742144
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.5850625e-05
Norm of the params: 12.221933
              Random: fixed  79 labels. Loss 0.08474. Accuracy 0.993.
### Flips: 410, rs: 22, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009980339
Train loss (w/o reg) on all data: 0.007451889
Test loss (w/o reg) on all data: 0.003782885
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9543602e-08
Norm of the params: 7.1111875
     Influence (LOO): fixed 345 labels. Loss 0.00378. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6248642e-08
Norm of the params: 6.0928206
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2018888
Train loss (w/o reg) on all data: 0.19443427
Test loss (w/o reg) on all data: 0.082766235
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0551238e-05
Norm of the params: 12.210262
              Random: fixed  89 labels. Loss 0.08277. Accuracy 0.992.
### Flips: 410, rs: 22, checks: 1230
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005209132
Train loss (w/o reg) on all data: 0.0032494012
Test loss (w/o reg) on all data: 0.0026032256
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.515309e-08
Norm of the params: 6.26056
     Influence (LOO): fixed 349 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.115791e-08
Norm of the params: 6.09281
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19317676
Train loss (w/o reg) on all data: 0.18564822
Test loss (w/o reg) on all data: 0.07935723
Train acc on all data:  0.9401896425966448
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.7433523e-06
Norm of the params: 12.270729
              Random: fixed 103 labels. Loss 0.07936. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2550112
Train loss (w/o reg) on all data: 0.24701002
Test loss (w/o reg) on all data: 0.10179943
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.0737466e-06
Norm of the params: 12.65004
Flipped loss: 0.10180. Accuracy: 0.995
### Flips: 410, rs: 23, checks: 205
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13409652
Train loss (w/o reg) on all data: 0.12554045
Test loss (w/o reg) on all data: 0.05032599
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7799887e-06
Norm of the params: 13.081337
     Influence (LOO): fixed 184 labels. Loss 0.05033. Accuracy 0.999.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09904782
Train loss (w/o reg) on all data: 0.085092746
Test loss (w/o reg) on all data: 0.050174218
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.909561e-06
Norm of the params: 16.70633
                Loss: fixed 205 labels. Loss 0.05017. Accuracy 0.988.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24693692
Train loss (w/o reg) on all data: 0.2389284
Test loss (w/o reg) on all data: 0.09654963
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0495167e-05
Norm of the params: 12.655843
              Random: fixed  17 labels. Loss 0.09655. Accuracy 0.996.
### Flips: 410, rs: 23, checks: 410
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062384505
Train loss (w/o reg) on all data: 0.05629351
Test loss (w/o reg) on all data: 0.021463431
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 2.973896e-06
Norm of the params: 11.037203
     Influence (LOO): fixed 288 labels. Loss 0.02146. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00407672
Train loss (w/o reg) on all data: 0.0015444056
Test loss (w/o reg) on all data: 0.0029258814
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.822132e-08
Norm of the params: 7.116621
                Loss: fixed 358 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23902084
Train loss (w/o reg) on all data: 0.23113617
Test loss (w/o reg) on all data: 0.09269671
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4835983e-05
Norm of the params: 12.557601
              Random: fixed  36 labels. Loss 0.09270. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 615
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04017475
Train loss (w/o reg) on all data: 0.035299413
Test loss (w/o reg) on all data: 0.013774973
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 6.302765e-07
Norm of the params: 9.874549
     Influence (LOO): fixed 317 labels. Loss 0.01377. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00328265
Train loss (w/o reg) on all data: 0.0011477231
Test loss (w/o reg) on all data: 0.0027467462
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4710492e-08
Norm of the params: 6.534412
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23105977
Train loss (w/o reg) on all data: 0.2234593
Test loss (w/o reg) on all data: 0.08750807
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2172076e-06
Norm of the params: 12.329211
              Random: fixed  52 labels. Loss 0.08751. Accuracy 0.999.
### Flips: 410, rs: 23, checks: 820
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021723818
Train loss (w/o reg) on all data: 0.018323293
Test loss (w/o reg) on all data: 0.00795198
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.229386e-07
Norm of the params: 8.246848
     Influence (LOO): fixed 340 labels. Loss 0.00795. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032826501
Train loss (w/o reg) on all data: 0.0011477079
Test loss (w/o reg) on all data: 0.0027467087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2433062e-08
Norm of the params: 6.5344353
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22071783
Train loss (w/o reg) on all data: 0.21314126
Test loss (w/o reg) on all data: 0.08276728
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6701388e-05
Norm of the params: 12.30981
              Random: fixed  72 labels. Loss 0.08277. Accuracy 0.999.
### Flips: 410, rs: 23, checks: 1025
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016098158
Train loss (w/o reg) on all data: 0.012923664
Test loss (w/o reg) on all data: 0.0060834023
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4630613e-06
Norm of the params: 7.9680533
     Influence (LOO): fixed 347 labels. Loss 0.00608. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00328265
Train loss (w/o reg) on all data: 0.0011477043
Test loss (w/o reg) on all data: 0.0027467173
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7849916e-08
Norm of the params: 6.534441
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20779309
Train loss (w/o reg) on all data: 0.1998874
Test loss (w/o reg) on all data: 0.07858887
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.190219e-06
Norm of the params: 12.574338
              Random: fixed  92 labels. Loss 0.07859. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 1230
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0107381195
Train loss (w/o reg) on all data: 0.00796477
Test loss (w/o reg) on all data: 0.004265464
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8483627e-07
Norm of the params: 7.447616
     Influence (LOO): fixed 352 labels. Loss 0.00427. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5310694e-08
Norm of the params: 6.092808
                Loss: fixed 360 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19531375
Train loss (w/o reg) on all data: 0.18752703
Test loss (w/o reg) on all data: 0.07316263
Train acc on all data:  0.9411621687332847
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0020047e-05
Norm of the params: 12.479364
              Random: fixed 112 labels. Loss 0.07316. Accuracy 0.996.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25400943
Train loss (w/o reg) on all data: 0.24632849
Test loss (w/o reg) on all data: 0.1069717
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.602898e-06
Norm of the params: 12.39431
Flipped loss: 0.10697. Accuracy: 0.989
### Flips: 410, rs: 24, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13333412
Train loss (w/o reg) on all data: 0.12550889
Test loss (w/o reg) on all data: 0.0507992
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.390357e-06
Norm of the params: 12.510173
     Influence (LOO): fixed 184 labels. Loss 0.05080. Accuracy 0.998.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09509601
Train loss (w/o reg) on all data: 0.081384145
Test loss (w/o reg) on all data: 0.06537487
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.622207e-06
Norm of the params: 16.56011
                Loss: fixed 205 labels. Loss 0.06537. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24103531
Train loss (w/o reg) on all data: 0.23331408
Test loss (w/o reg) on all data: 0.10040454
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7249246e-05
Norm of the params: 12.426775
              Random: fixed  23 labels. Loss 0.10040. Accuracy 0.991.
### Flips: 410, rs: 24, checks: 410
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06143188
Train loss (w/o reg) on all data: 0.055743065
Test loss (w/o reg) on all data: 0.020555442
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8322783e-06
Norm of the params: 10.6666
     Influence (LOO): fixed 286 labels. Loss 0.02056. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005332716
Train loss (w/o reg) on all data: 0.0021061
Test loss (w/o reg) on all data: 0.004485513
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1722238e-08
Norm of the params: 8.033201
                Loss: fixed 352 labels. Loss 0.00449. Accuracy 0.999.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23463544
Train loss (w/o reg) on all data: 0.22696447
Test loss (w/o reg) on all data: 0.096956745
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1526088e-05
Norm of the params: 12.386252
              Random: fixed  37 labels. Loss 0.09696. Accuracy 0.992.
### Flips: 410, rs: 24, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03581608
Train loss (w/o reg) on all data: 0.03125941
Test loss (w/o reg) on all data: 0.012131165
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7893404e-07
Norm of the params: 9.546383
     Influence (LOO): fixed 319 labels. Loss 0.01213. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.126004e-08
Norm of the params: 6.0928116
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22835298
Train loss (w/o reg) on all data: 0.220786
Test loss (w/o reg) on all data: 0.093321525
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.777812e-05
Norm of the params: 12.302015
              Random: fixed  50 labels. Loss 0.09332. Accuracy 0.993.
### Flips: 410, rs: 24, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023819733
Train loss (w/o reg) on all data: 0.019993916
Test loss (w/o reg) on all data: 0.008806992
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.391842e-07
Norm of the params: 8.747363
     Influence (LOO): fixed 332 labels. Loss 0.00881. Accuracy 0.998.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601107
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.62632e-08
Norm of the params: 6.09283
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21594888
Train loss (w/o reg) on all data: 0.20815913
Test loss (w/o reg) on all data: 0.08767311
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.4900697e-05
Norm of the params: 12.481785
              Random: fixed  71 labels. Loss 0.08767. Accuracy 0.993.
### Flips: 410, rs: 24, checks: 1025
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016746504
Train loss (w/o reg) on all data: 0.013519156
Test loss (w/o reg) on all data: 0.0061173444
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5579822e-07
Norm of the params: 8.034112
     Influence (LOO): fixed 341 labels. Loss 0.00612. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4772039e-08
Norm of the params: 6.0928154
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21116029
Train loss (w/o reg) on all data: 0.20355415
Test loss (w/o reg) on all data: 0.08536841
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.895285e-05
Norm of the params: 12.333807
              Random: fixed  81 labels. Loss 0.08537. Accuracy 0.992.
### Flips: 410, rs: 24, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015807712
Train loss (w/o reg) on all data: 0.012552012
Test loss (w/o reg) on all data: 0.005930728
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.148859e-07
Norm of the params: 8.069326
     Influence (LOO): fixed 342 labels. Loss 0.00593. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.650852e-09
Norm of the params: 6.0928154
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1960952
Train loss (w/o reg) on all data: 0.18832862
Test loss (w/o reg) on all data: 0.07703201
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.209227e-06
Norm of the params: 12.463207
              Random: fixed 106 labels. Loss 0.07703. Accuracy 0.993.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2601337
Train loss (w/o reg) on all data: 0.25382137
Test loss (w/o reg) on all data: 0.10134542
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.776699e-06
Norm of the params: 11.235961
Flipped loss: 0.10135. Accuracy: 0.997
### Flips: 410, rs: 25, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13343248
Train loss (w/o reg) on all data: 0.12567984
Test loss (w/o reg) on all data: 0.04548019
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.99149e-06
Norm of the params: 12.452027
     Influence (LOO): fixed 186 labels. Loss 0.04548. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10118371
Train loss (w/o reg) on all data: 0.08861588
Test loss (w/o reg) on all data: 0.046485692
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.225282e-06
Norm of the params: 15.854232
                Loss: fixed 205 labels. Loss 0.04649. Accuracy 0.991.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24917185
Train loss (w/o reg) on all data: 0.24291888
Test loss (w/o reg) on all data: 0.09526062
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3207058e-05
Norm of the params: 11.183002
              Random: fixed  21 labels. Loss 0.09526. Accuracy 0.996.
### Flips: 410, rs: 25, checks: 410
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067329615
Train loss (w/o reg) on all data: 0.061154764
Test loss (w/o reg) on all data: 0.023116605
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5293084e-06
Norm of the params: 11.112918
     Influence (LOO): fixed 279 labels. Loss 0.02312. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.684449e-08
Norm of the params: 6.0928216
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2405281
Train loss (w/o reg) on all data: 0.23407744
Test loss (w/o reg) on all data: 0.09164851
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.35219e-06
Norm of the params: 11.358406
              Random: fixed  37 labels. Loss 0.09165. Accuracy 0.995.
### Flips: 410, rs: 25, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03792333
Train loss (w/o reg) on all data: 0.0330715
Test loss (w/o reg) on all data: 0.012659164
Train acc on all data:  0.9907610017019207
Test acc on all data:   1.0
Norm of the mean of gradients: 5.345827e-07
Norm of the params: 9.850714
     Influence (LOO): fixed 317 labels. Loss 0.01266. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8246979e-08
Norm of the params: 6.092814
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2286139
Train loss (w/o reg) on all data: 0.22237143
Test loss (w/o reg) on all data: 0.08632736
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1467056e-05
Norm of the params: 11.173603
              Random: fixed  57 labels. Loss 0.08633. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 820
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02372606
Train loss (w/o reg) on all data: 0.02023196
Test loss (w/o reg) on all data: 0.0070945793
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7255687e-07
Norm of the params: 8.359544
     Influence (LOO): fixed 337 labels. Loss 0.00709. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9719026e-08
Norm of the params: 6.092821
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21727529
Train loss (w/o reg) on all data: 0.210713
Test loss (w/o reg) on all data: 0.08134975
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5293262e-06
Norm of the params: 11.4562645
              Random: fixed  76 labels. Loss 0.08135. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 1025
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012038969
Train loss (w/o reg) on all data: 0.00923639
Test loss (w/o reg) on all data: 0.0043139653
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.867133e-07
Norm of the params: 7.4867616
     Influence (LOO): fixed 347 labels. Loss 0.00431. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096014456
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4156107e-07
Norm of the params: 6.092774
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20691726
Train loss (w/o reg) on all data: 0.20054892
Test loss (w/o reg) on all data: 0.07825197
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7689619e-05
Norm of the params: 11.285689
              Random: fixed  94 labels. Loss 0.07825. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 1230
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0094948495
Train loss (w/o reg) on all data: 0.0070633874
Test loss (w/o reg) on all data: 0.0035140926
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.094279e-06
Norm of the params: 6.9734674
     Influence (LOO): fixed 350 labels. Loss 0.00351. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5497175e-08
Norm of the params: 6.092812
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20015472
Train loss (w/o reg) on all data: 0.19397084
Test loss (w/o reg) on all data: 0.07353827
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.090198e-06
Norm of the params: 11.121039
              Random: fixed 107 labels. Loss 0.07354. Accuracy 0.997.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25533602
Train loss (w/o reg) on all data: 0.247787
Test loss (w/o reg) on all data: 0.100642756
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.491213e-06
Norm of the params: 12.287414
Flipped loss: 0.10064. Accuracy: 0.997
### Flips: 410, rs: 26, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12712045
Train loss (w/o reg) on all data: 0.11958047
Test loss (w/o reg) on all data: 0.04469477
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0268286e-06
Norm of the params: 12.280051
     Influence (LOO): fixed 189 labels. Loss 0.04469. Accuracy 0.999.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09676456
Train loss (w/o reg) on all data: 0.0834667
Test loss (w/o reg) on all data: 0.050094675
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.6104243e-06
Norm of the params: 16.308191
                Loss: fixed 205 labels. Loss 0.05009. Accuracy 0.993.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2479908
Train loss (w/o reg) on all data: 0.24022056
Test loss (w/o reg) on all data: 0.09657271
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8106359e-05
Norm of the params: 12.466149
              Random: fixed  14 labels. Loss 0.09657. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 410
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061434746
Train loss (w/o reg) on all data: 0.0550353
Test loss (w/o reg) on all data: 0.019702202
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6338716e-06
Norm of the params: 11.313219
     Influence (LOO): fixed 281 labels. Loss 0.01970. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042055305
Train loss (w/o reg) on all data: 0.0017542429
Test loss (w/o reg) on all data: 0.002879897
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9264466e-08
Norm of the params: 7.0018396
                Loss: fixed 346 labels. Loss 0.00288. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23578928
Train loss (w/o reg) on all data: 0.2282693
Test loss (w/o reg) on all data: 0.08863366
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.8137243e-06
Norm of the params: 12.263761
              Random: fixed  37 labels. Loss 0.08863. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 615
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036361393
Train loss (w/o reg) on all data: 0.031501863
Test loss (w/o reg) on all data: 0.011785929
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1916275e-07
Norm of the params: 9.85853
     Influence (LOO): fixed 311 labels. Loss 0.01179. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7714336e-08
Norm of the params: 6.092828
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22055796
Train loss (w/o reg) on all data: 0.21255784
Test loss (w/o reg) on all data: 0.08140119
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.070568e-06
Norm of the params: 12.649208
              Random: fixed  62 labels. Loss 0.08140. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019989964
Train loss (w/o reg) on all data: 0.016126202
Test loss (w/o reg) on all data: 0.00877956
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.824115e-07
Norm of the params: 8.790632
     Influence (LOO): fixed 328 labels. Loss 0.00878. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.985133e-08
Norm of the params: 6.092809
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20739199
Train loss (w/o reg) on all data: 0.19924165
Test loss (w/o reg) on all data: 0.07737561
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.825883e-06
Norm of the params: 12.767411
              Random: fixed  84 labels. Loss 0.07738. Accuracy 0.998.
### Flips: 410, rs: 26, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017712127
Train loss (w/o reg) on all data: 0.014135937
Test loss (w/o reg) on all data: 0.0069380715
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5414624e-07
Norm of the params: 8.457173
     Influence (LOO): fixed 331 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601263
Test loss (w/o reg) on all data: 0.0026559408
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3594147e-07
Norm of the params: 6.0928044
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19856508
Train loss (w/o reg) on all data: 0.19031085
Test loss (w/o reg) on all data: 0.073737904
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.7740644e-06
Norm of the params: 12.848521
              Random: fixed  98 labels. Loss 0.07374. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 1230
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014886996
Train loss (w/o reg) on all data: 0.011702648
Test loss (w/o reg) on all data: 0.005891147
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.08621705e-07
Norm of the params: 7.9804106
     Influence (LOO): fixed 335 labels. Loss 0.00589. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9750575e-08
Norm of the params: 6.092814
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18618414
Train loss (w/o reg) on all data: 0.17802052
Test loss (w/o reg) on all data: 0.068934545
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.122654e-06
Norm of the params: 12.77781
              Random: fixed 119 labels. Loss 0.06893. Accuracy 0.998.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26493242
Train loss (w/o reg) on all data: 0.25823477
Test loss (w/o reg) on all data: 0.1117095
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.6984525e-06
Norm of the params: 11.573807
Flipped loss: 0.11171. Accuracy: 0.992
### Flips: 410, rs: 27, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14227295
Train loss (w/o reg) on all data: 0.13471097
Test loss (w/o reg) on all data: 0.05103013
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.201259e-06
Norm of the params: 12.297948
     Influence (LOO): fixed 186 labels. Loss 0.05103. Accuracy 0.997.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104103625
Train loss (w/o reg) on all data: 0.09097225
Test loss (w/o reg) on all data: 0.05716185
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.476503e-06
Norm of the params: 16.205784
                Loss: fixed 205 labels. Loss 0.05716. Accuracy 0.987.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25709254
Train loss (w/o reg) on all data: 0.25013742
Test loss (w/o reg) on all data: 0.10702687
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.7449629e-05
Norm of the params: 11.794172
              Random: fixed  14 labels. Loss 0.10703. Accuracy 0.994.
### Flips: 410, rs: 27, checks: 410
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06240514
Train loss (w/o reg) on all data: 0.0561729
Test loss (w/o reg) on all data: 0.020582478
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1233837e-06
Norm of the params: 11.164443
     Influence (LOO): fixed 296 labels. Loss 0.02058. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013787
Test loss (w/o reg) on all data: 0.002656092
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2088485e-08
Norm of the params: 6.092786
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24656148
Train loss (w/o reg) on all data: 0.23938434
Test loss (w/o reg) on all data: 0.102391504
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.119695e-06
Norm of the params: 11.980943
              Random: fixed  32 labels. Loss 0.10239. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 615
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03906405
Train loss (w/o reg) on all data: 0.03394801
Test loss (w/o reg) on all data: 0.012725974
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4856926e-07
Norm of the params: 10.115371
     Influence (LOO): fixed 327 labels. Loss 0.01273. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8290842e-08
Norm of the params: 6.0928106
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23811398
Train loss (w/o reg) on all data: 0.23077612
Test loss (w/o reg) on all data: 0.09766239
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.0245983e-06
Norm of the params: 12.11435
              Random: fixed  47 labels. Loss 0.09766. Accuracy 0.993.
### Flips: 410, rs: 27, checks: 820
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019382179
Train loss (w/o reg) on all data: 0.015710674
Test loss (w/o reg) on all data: 0.0072240247
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5188803e-07
Norm of the params: 8.569135
     Influence (LOO): fixed 349 labels. Loss 0.00722. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2494596e-08
Norm of the params: 6.092812
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23274796
Train loss (w/o reg) on all data: 0.22506242
Test loss (w/o reg) on all data: 0.09475014
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.4567923e-06
Norm of the params: 12.398013
              Random: fixed  57 labels. Loss 0.09475. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017704261
Train loss (w/o reg) on all data: 0.014281721
Test loss (w/o reg) on all data: 0.006856118
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7139506e-07
Norm of the params: 8.2735
     Influence (LOO): fixed 351 labels. Loss 0.00686. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7666653e-08
Norm of the params: 6.092808
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22377726
Train loss (w/o reg) on all data: 0.21575214
Test loss (w/o reg) on all data: 0.09042826
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.1489558e-05
Norm of the params: 12.668959
              Random: fixed  72 labels. Loss 0.09043. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014926717
Train loss (w/o reg) on all data: 0.011525811
Test loss (w/o reg) on all data: 0.005964403
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.744605e-08
Norm of the params: 8.24731
     Influence (LOO): fixed 354 labels. Loss 0.00596. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.002656089
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3304637e-08
Norm of the params: 6.092806
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21419635
Train loss (w/o reg) on all data: 0.20626602
Test loss (w/o reg) on all data: 0.08518139
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.6922893e-05
Norm of the params: 12.593912
              Random: fixed  90 labels. Loss 0.08518. Accuracy 0.996.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273272
Train loss (w/o reg) on all data: 0.24575529
Test loss (w/o reg) on all data: 0.09667602
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2798084e-05
Norm of the params: 11.813069
Flipped loss: 0.09668. Accuracy: 0.999
### Flips: 410, rs: 28, checks: 205
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13106634
Train loss (w/o reg) on all data: 0.12346462
Test loss (w/o reg) on all data: 0.046660468
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.29268e-05
Norm of the params: 12.330219
     Influence (LOO): fixed 184 labels. Loss 0.04666. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09305568
Train loss (w/o reg) on all data: 0.078983076
Test loss (w/o reg) on all data: 0.0473574
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7377727e-06
Norm of the params: 16.776535
                Loss: fixed 205 labels. Loss 0.04736. Accuracy 0.988.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24358766
Train loss (w/o reg) on all data: 0.23656839
Test loss (w/o reg) on all data: 0.09282847
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.428879e-05
Norm of the params: 11.848434
              Random: fixed  19 labels. Loss 0.09283. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 410
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059832983
Train loss (w/o reg) on all data: 0.053177185
Test loss (w/o reg) on all data: 0.019721355
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7316884e-06
Norm of the params: 11.537587
     Influence (LOO): fixed 285 labels. Loss 0.01972. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033914614
Train loss (w/o reg) on all data: 0.0011934079
Test loss (w/o reg) on all data: 0.0033463659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8860892e-08
Norm of the params: 6.6303144
                Loss: fixed 351 labels. Loss 0.00335. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23419854
Train loss (w/o reg) on all data: 0.22723374
Test loss (w/o reg) on all data: 0.08802067
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.1838892e-06
Norm of the params: 11.802381
              Random: fixed  38 labels. Loss 0.08802. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 615
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034894574
Train loss (w/o reg) on all data: 0.03026312
Test loss (w/o reg) on all data: 0.011361406
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 7.380184e-07
Norm of the params: 9.624401
     Influence (LOO): fixed 316 labels. Loss 0.01136. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2067952e-08
Norm of the params: 6.0928245
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2210008
Train loss (w/o reg) on all data: 0.21372089
Test loss (w/o reg) on all data: 0.08280411
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2772252e-05
Norm of the params: 12.066417
              Random: fixed  60 labels. Loss 0.08280. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 820
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01739388
Train loss (w/o reg) on all data: 0.013936342
Test loss (w/o reg) on all data: 0.006804434
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1378895e-07
Norm of the params: 8.315694
     Influence (LOO): fixed 337 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600907
Test loss (w/o reg) on all data: 0.0026560198
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6239485e-08
Norm of the params: 6.092863
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20768917
Train loss (w/o reg) on all data: 0.20024218
Test loss (w/o reg) on all data: 0.07477768
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0409426e-06
Norm of the params: 12.204093
              Random: fixed  83 labels. Loss 0.07478. Accuracy 0.998.
### Flips: 410, rs: 28, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016433343
Train loss (w/o reg) on all data: 0.013139096
Test loss (w/o reg) on all data: 0.0062374435
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 5.121959e-07
Norm of the params: 8.116955
     Influence (LOO): fixed 338 labels. Loss 0.00624. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012966
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4478436e-08
Norm of the params: 6.0927987
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20254397
Train loss (w/o reg) on all data: 0.19502208
Test loss (w/o reg) on all data: 0.07270536
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.073922e-06
Norm of the params: 12.26531
              Random: fixed  91 labels. Loss 0.07271. Accuracy 0.997.
### Flips: 410, rs: 28, checks: 1230
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012532744
Train loss (w/o reg) on all data: 0.0094568115
Test loss (w/o reg) on all data: 0.0049788174
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2401706e-07
Norm of the params: 7.8433824
     Influence (LOO): fixed 342 labels. Loss 0.00498. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560389
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.81588e-08
Norm of the params: 6.0928125
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19083975
Train loss (w/o reg) on all data: 0.18313812
Test loss (w/o reg) on all data: 0.0693932
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3827976e-06
Norm of the params: 12.410988
              Random: fixed 110 labels. Loss 0.06939. Accuracy 0.999.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2559794
Train loss (w/o reg) on all data: 0.24908748
Test loss (w/o reg) on all data: 0.104150034
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.363678e-05
Norm of the params: 11.740451
Flipped loss: 0.10415. Accuracy: 0.994
### Flips: 410, rs: 29, checks: 205
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13148803
Train loss (w/o reg) on all data: 0.124698326
Test loss (w/o reg) on all data: 0.04736683
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7677695e-06
Norm of the params: 11.653064
     Influence (LOO): fixed 188 labels. Loss 0.04737. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09293476
Train loss (w/o reg) on all data: 0.07792176
Test loss (w/o reg) on all data: 0.052289486
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.4088115e-06
Norm of the params: 17.32801
                Loss: fixed 205 labels. Loss 0.05229. Accuracy 0.989.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2498335
Train loss (w/o reg) on all data: 0.24282977
Test loss (w/o reg) on all data: 0.09987638
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.474035e-06
Norm of the params: 11.835311
              Random: fixed  13 labels. Loss 0.09988. Accuracy 0.995.
### Flips: 410, rs: 29, checks: 410
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061310485
Train loss (w/o reg) on all data: 0.055582024
Test loss (w/o reg) on all data: 0.01972594
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4926773e-06
Norm of the params: 10.7037
     Influence (LOO): fixed 285 labels. Loss 0.01973. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013234
Test loss (w/o reg) on all data: 0.0026560898
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2210744e-07
Norm of the params: 6.0927954
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24164647
Train loss (w/o reg) on all data: 0.23459834
Test loss (w/o reg) on all data: 0.094230615
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.974232e-06
Norm of the params: 11.872771
              Random: fixed  29 labels. Loss 0.09423. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033371042
Train loss (w/o reg) on all data: 0.028304871
Test loss (w/o reg) on all data: 0.011609036
Train acc on all data:  0.9917335278385606
Test acc on all data:   1.0
Norm of the mean of gradients: 2.445082e-06
Norm of the params: 10.065953
     Influence (LOO): fixed 318 labels. Loss 0.01161. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9999555e-08
Norm of the params: 6.0928035
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23128188
Train loss (w/o reg) on all data: 0.22418663
Test loss (w/o reg) on all data: 0.08635063
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8804942e-05
Norm of the params: 11.912387
              Random: fixed  50 labels. Loss 0.08635. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 820
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021442223
Train loss (w/o reg) on all data: 0.017795224
Test loss (w/o reg) on all data: 0.0070919823
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 9.055992e-07
Norm of the params: 8.54049
     Influence (LOO): fixed 333 labels. Loss 0.00709. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096013024
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9468779e-08
Norm of the params: 6.0927973
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22182669
Train loss (w/o reg) on all data: 0.21462396
Test loss (w/o reg) on all data: 0.08205969
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.412324e-06
Norm of the params: 12.002273
              Random: fixed  67 labels. Loss 0.08206. Accuracy 0.995.
### Flips: 410, rs: 29, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015618267
Train loss (w/o reg) on all data: 0.012819793
Test loss (w/o reg) on all data: 0.005092704
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1668764e-07
Norm of the params: 7.4812756
     Influence (LOO): fixed 341 labels. Loss 0.00509. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560384
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.664949e-08
Norm of the params: 6.0928216
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21284074
Train loss (w/o reg) on all data: 0.2052431
Test loss (w/o reg) on all data: 0.07835488
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.6801755e-06
Norm of the params: 12.326909
              Random: fixed  81 labels. Loss 0.07835. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01305132
Train loss (w/o reg) on all data: 0.010355762
Test loss (w/o reg) on all data: 0.004635686
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4579947e-07
Norm of the params: 7.342421
     Influence (LOO): fixed 343 labels. Loss 0.00464. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012803
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1044987e-08
Norm of the params: 6.092802
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20524856
Train loss (w/o reg) on all data: 0.1976643
Test loss (w/o reg) on all data: 0.07412881
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1365865e-05
Norm of the params: 12.31605
              Random: fixed  93 labels. Loss 0.07413. Accuracy 0.996.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2583414
Train loss (w/o reg) on all data: 0.25108784
Test loss (w/o reg) on all data: 0.11119045
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4773954e-05
Norm of the params: 12.044555
Flipped loss: 0.11119. Accuracy: 0.990
### Flips: 410, rs: 30, checks: 205
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13846765
Train loss (w/o reg) on all data: 0.13165317
Test loss (w/o reg) on all data: 0.05322803
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8894573e-06
Norm of the params: 11.674318
     Influence (LOO): fixed 187 labels. Loss 0.05323. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097239144
Train loss (w/o reg) on all data: 0.08338629
Test loss (w/o reg) on all data: 0.0539788
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.804953e-06
Norm of the params: 16.645033
                Loss: fixed 205 labels. Loss 0.05398. Accuracy 0.988.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24944985
Train loss (w/o reg) on all data: 0.24215989
Test loss (w/o reg) on all data: 0.104089126
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.0252827e-06
Norm of the params: 12.074738
              Random: fixed  16 labels. Loss 0.10409. Accuracy 0.995.
### Flips: 410, rs: 30, checks: 410
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06080538
Train loss (w/o reg) on all data: 0.055046313
Test loss (w/o reg) on all data: 0.019848023
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 3.04083e-06
Norm of the params: 10.732258
     Influence (LOO): fixed 296 labels. Loss 0.01985. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292437
Train loss (w/o reg) on all data: 0.0011683411
Test loss (w/o reg) on all data: 0.0026929867
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.461696e-08
Norm of the params: 6.420129
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24282958
Train loss (w/o reg) on all data: 0.23570158
Test loss (w/o reg) on all data: 0.09905539
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.62537e-06
Norm of the params: 11.939857
              Random: fixed  31 labels. Loss 0.09906. Accuracy 0.995.
### Flips: 410, rs: 30, checks: 615
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028853063
Train loss (w/o reg) on all data: 0.024776382
Test loss (w/o reg) on all data: 0.010919233
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.674002e-07
Norm of the params: 9.029595
     Influence (LOO): fixed 335 labels. Loss 0.01092. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292441
Train loss (w/o reg) on all data: 0.0011683536
Test loss (w/o reg) on all data: 0.00269303
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.039576e-08
Norm of the params: 6.4201097
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23335695
Train loss (w/o reg) on all data: 0.22621605
Test loss (w/o reg) on all data: 0.09384468
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0983346e-05
Norm of the params: 11.950655
              Random: fixed  49 labels. Loss 0.09384. Accuracy 0.994.
### Flips: 410, rs: 30, checks: 820
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017330762
Train loss (w/o reg) on all data: 0.013976085
Test loss (w/o reg) on all data: 0.006149701
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 4.235622e-07
Norm of the params: 8.191065
     Influence (LOO): fixed 349 labels. Loss 0.00615. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003229244
Train loss (w/o reg) on all data: 0.0011683451
Test loss (w/o reg) on all data: 0.0026930142
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5851637e-08
Norm of the params: 6.420123
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21899912
Train loss (w/o reg) on all data: 0.21162371
Test loss (w/o reg) on all data: 0.087877944
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.784356e-06
Norm of the params: 12.145297
              Random: fixed  74 labels. Loss 0.08788. Accuracy 0.994.
### Flips: 410, rs: 30, checks: 1025
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015545596
Train loss (w/o reg) on all data: 0.012398883
Test loss (w/o reg) on all data: 0.005754164
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 9.819416e-07
Norm of the params: 7.933112
     Influence (LOO): fixed 351 labels. Loss 0.00575. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292446
Train loss (w/o reg) on all data: 0.0011683492
Test loss (w/o reg) on all data: 0.002693016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8081918e-08
Norm of the params: 6.420117
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21049903
Train loss (w/o reg) on all data: 0.20320013
Test loss (w/o reg) on all data: 0.08523093
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.816042e-06
Norm of the params: 12.082135
              Random: fixed  88 labels. Loss 0.08523. Accuracy 0.992.
### Flips: 410, rs: 30, checks: 1230
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013129339
Train loss (w/o reg) on all data: 0.01019923
Test loss (w/o reg) on all data: 0.00451622
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0648179e-07
Norm of the params: 7.655205
     Influence (LOO): fixed 353 labels. Loss 0.00452. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292446
Train loss (w/o reg) on all data: 0.001168327
Test loss (w/o reg) on all data: 0.0026929914
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.364385e-08
Norm of the params: 6.4201517
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1983077
Train loss (w/o reg) on all data: 0.19056268
Test loss (w/o reg) on all data: 0.07896512
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.0613585e-06
Norm of the params: 12.445893
              Random: fixed 108 labels. Loss 0.07897. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25331464
Train loss (w/o reg) on all data: 0.2465421
Test loss (w/o reg) on all data: 0.11560204
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.0779255e-06
Norm of the params: 11.63833
Flipped loss: 0.11560. Accuracy: 0.987
### Flips: 410, rs: 31, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12784053
Train loss (w/o reg) on all data: 0.12026626
Test loss (w/o reg) on all data: 0.050786287
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2069483e-06
Norm of the params: 12.307943
     Influence (LOO): fixed 189 labels. Loss 0.05079. Accuracy 0.997.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09373373
Train loss (w/o reg) on all data: 0.080644876
Test loss (w/o reg) on all data: 0.068807036
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.039801e-06
Norm of the params: 16.179522
                Loss: fixed 205 labels. Loss 0.06881. Accuracy 0.978.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2416932
Train loss (w/o reg) on all data: 0.23446877
Test loss (w/o reg) on all data: 0.10897207
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.1245547e-06
Norm of the params: 12.020333
              Random: fixed  20 labels. Loss 0.10897. Accuracy 0.987.
### Flips: 410, rs: 31, checks: 410
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059044465
Train loss (w/o reg) on all data: 0.053376455
Test loss (w/o reg) on all data: 0.020560581
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6528836e-06
Norm of the params: 10.647078
     Influence (LOO): fixed 287 labels. Loss 0.02056. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004704952
Train loss (w/o reg) on all data: 0.0018679488
Test loss (w/o reg) on all data: 0.0028698645
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.185139e-08
Norm of the params: 7.5326004
                Loss: fixed 352 labels. Loss 0.00287. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23385364
Train loss (w/o reg) on all data: 0.22653422
Test loss (w/o reg) on all data: 0.103687204
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.554703e-06
Norm of the params: 12.099105
              Random: fixed  38 labels. Loss 0.10369. Accuracy 0.989.
### Flips: 410, rs: 31, checks: 615
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033715323
Train loss (w/o reg) on all data: 0.029489772
Test loss (w/o reg) on all data: 0.010016438
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9759125e-06
Norm of the params: 9.192985
     Influence (LOO): fixed 319 labels. Loss 0.01002. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039846366
Train loss (w/o reg) on all data: 0.0015141792
Test loss (w/o reg) on all data: 0.0027021347
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5238217e-08
Norm of the params: 7.0291643
                Loss: fixed 353 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22517678
Train loss (w/o reg) on all data: 0.2177733
Test loss (w/o reg) on all data: 0.09831144
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0396753e-05
Norm of the params: 12.1683855
              Random: fixed  56 labels. Loss 0.09831. Accuracy 0.990.
### Flips: 410, rs: 31, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022427026
Train loss (w/o reg) on all data: 0.018688777
Test loss (w/o reg) on all data: 0.007260028
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 3.166484e-07
Norm of the params: 8.646674
     Influence (LOO): fixed 332 labels. Loss 0.00726. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039846366
Train loss (w/o reg) on all data: 0.0015141795
Test loss (w/o reg) on all data: 0.0027021507
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2193168e-08
Norm of the params: 7.029164
                Loss: fixed 353 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21628839
Train loss (w/o reg) on all data: 0.20857228
Test loss (w/o reg) on all data: 0.09442868
Train acc on all data:  0.9324094335035255
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0471274e-05
Norm of the params: 12.422646
              Random: fixed  70 labels. Loss 0.09443. Accuracy 0.992.
### Flips: 410, rs: 31, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0165645
Train loss (w/o reg) on all data: 0.0129225915
Test loss (w/o reg) on all data: 0.006063933
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2271115e-06
Norm of the params: 8.534529
     Influence (LOO): fixed 338 labels. Loss 0.00606. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5379342e-08
Norm of the params: 6.0928054
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20558317
Train loss (w/o reg) on all data: 0.19775951
Test loss (w/o reg) on all data: 0.08336636
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.9885607e-06
Norm of the params: 12.508922
              Random: fixed  88 labels. Loss 0.08337. Accuracy 0.996.
### Flips: 410, rs: 31, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013637911
Train loss (w/o reg) on all data: 0.010303925
Test loss (w/o reg) on all data: 0.005485638
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8298854e-07
Norm of the params: 8.165765
     Influence (LOO): fixed 342 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601319
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7465571e-08
Norm of the params: 6.0927963
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19453818
Train loss (w/o reg) on all data: 0.18650286
Test loss (w/o reg) on all data: 0.07871983
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9351008e-05
Norm of the params: 12.677002
              Random: fixed 105 labels. Loss 0.07872. Accuracy 0.996.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2537571
Train loss (w/o reg) on all data: 0.2459469
Test loss (w/o reg) on all data: 0.10688553
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4703651e-05
Norm of the params: 12.498159
Flipped loss: 0.10689. Accuracy: 0.991
### Flips: 410, rs: 32, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1325059
Train loss (w/o reg) on all data: 0.124575295
Test loss (w/o reg) on all data: 0.05183015
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5229402e-06
Norm of the params: 12.594128
     Influence (LOO): fixed 183 labels. Loss 0.05183. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09862747
Train loss (w/o reg) on all data: 0.08403489
Test loss (w/o reg) on all data: 0.052363567
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6757285e-06
Norm of the params: 17.083666
                Loss: fixed 205 labels. Loss 0.05236. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24720791
Train loss (w/o reg) on all data: 0.23949586
Test loss (w/o reg) on all data: 0.10277758
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.913911e-06
Norm of the params: 12.419385
              Random: fixed  11 labels. Loss 0.10278. Accuracy 0.994.
### Flips: 410, rs: 32, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062813245
Train loss (w/o reg) on all data: 0.05667837
Test loss (w/o reg) on all data: 0.021556607
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6011996e-06
Norm of the params: 11.076893
     Influence (LOO): fixed 278 labels. Loss 0.02156. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043355636
Train loss (w/o reg) on all data: 0.0015491534
Test loss (w/o reg) on all data: 0.0028610437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3935177e-08
Norm of the params: 7.4651327
                Loss: fixed 351 labels. Loss 0.00286. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24057461
Train loss (w/o reg) on all data: 0.23272973
Test loss (w/o reg) on all data: 0.09676865
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1124419e-05
Norm of the params: 12.525878
              Random: fixed  25 labels. Loss 0.09677. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03344281
Train loss (w/o reg) on all data: 0.029095007
Test loss (w/o reg) on all data: 0.011727686
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4576069e-06
Norm of the params: 9.325026
     Influence (LOO): fixed 318 labels. Loss 0.01173. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004053294
Train loss (w/o reg) on all data: 0.0014179826
Test loss (w/o reg) on all data: 0.0027558748
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3375686e-08
Norm of the params: 7.259906
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2320616
Train loss (w/o reg) on all data: 0.22428863
Test loss (w/o reg) on all data: 0.091961876
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.46882885e-05
Norm of the params: 12.468329
              Random: fixed  40 labels. Loss 0.09196. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 820
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023756405
Train loss (w/o reg) on all data: 0.02004156
Test loss (w/o reg) on all data: 0.008338315
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3881446e-07
Norm of the params: 8.619565
     Influence (LOO): fixed 330 labels. Loss 0.00834. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040532937
Train loss (w/o reg) on all data: 0.0014179844
Test loss (w/o reg) on all data: 0.0027558943
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.911274e-08
Norm of the params: 7.2599025
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22279483
Train loss (w/o reg) on all data: 0.21475391
Test loss (w/o reg) on all data: 0.08715028
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8849758e-05
Norm of the params: 12.681423
              Random: fixed  56 labels. Loss 0.08715. Accuracy 0.996.
### Flips: 410, rs: 32, checks: 1025
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019394284
Train loss (w/o reg) on all data: 0.016010677
Test loss (w/o reg) on all data: 0.0063357283
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3060147e-07
Norm of the params: 8.226307
     Influence (LOO): fixed 335 labels. Loss 0.00634. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004053294
Train loss (w/o reg) on all data: 0.0014179612
Test loss (w/o reg) on all data: 0.0027558447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05413456e-07
Norm of the params: 7.2599354
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21454017
Train loss (w/o reg) on all data: 0.20668764
Test loss (w/o reg) on all data: 0.08245578
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.5787e-06
Norm of the params: 12.531975
              Random: fixed  72 labels. Loss 0.08246. Accuracy 0.996.
### Flips: 410, rs: 32, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016167764
Train loss (w/o reg) on all data: 0.013076389
Test loss (w/o reg) on all data: 0.00548775
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3500304e-07
Norm of the params: 7.8630447
     Influence (LOO): fixed 339 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040532947
Train loss (w/o reg) on all data: 0.0014179831
Test loss (w/o reg) on all data: 0.0027559102
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5020687e-08
Norm of the params: 7.2599053
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20274001
Train loss (w/o reg) on all data: 0.1944991
Test loss (w/o reg) on all data: 0.07616059
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.172763e-06
Norm of the params: 12.838158
              Random: fixed  93 labels. Loss 0.07616. Accuracy 0.996.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2517675
Train loss (w/o reg) on all data: 0.24538694
Test loss (w/o reg) on all data: 0.10024726
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4918192e-05
Norm of the params: 11.296504
Flipped loss: 0.10025. Accuracy: 0.999
### Flips: 410, rs: 33, checks: 205
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12375203
Train loss (w/o reg) on all data: 0.11626287
Test loss (w/o reg) on all data: 0.047000486
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4095948e-06
Norm of the params: 12.238598
     Influence (LOO): fixed 191 labels. Loss 0.04700. Accuracy 0.998.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09255968
Train loss (w/o reg) on all data: 0.0793952
Test loss (w/o reg) on all data: 0.046074342
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.8862144e-06
Norm of the params: 16.2262
                Loss: fixed 205 labels. Loss 0.04607. Accuracy 0.988.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24341284
Train loss (w/o reg) on all data: 0.23670524
Test loss (w/o reg) on all data: 0.096495636
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.8853403e-06
Norm of the params: 11.582398
              Random: fixed  15 labels. Loss 0.09650. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 410
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05733099
Train loss (w/o reg) on all data: 0.051149763
Test loss (w/o reg) on all data: 0.019255916
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2047138e-06
Norm of the params: 11.118655
     Influence (LOO): fixed 285 labels. Loss 0.01926. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9423098e-08
Norm of the params: 6.0928264
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23511274
Train loss (w/o reg) on all data: 0.22814128
Test loss (w/o reg) on all data: 0.092921786
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3673012e-05
Norm of the params: 11.808021
              Random: fixed  30 labels. Loss 0.09292. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 615
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034337707
Train loss (w/o reg) on all data: 0.029306395
Test loss (w/o reg) on all data: 0.01153015
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.850451e-06
Norm of the params: 10.031261
     Influence (LOO): fixed 316 labels. Loss 0.01153. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010463
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2182372e-08
Norm of the params: 6.09284
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22851522
Train loss (w/o reg) on all data: 0.22148958
Test loss (w/o reg) on all data: 0.0885493
Train acc on all data:  0.925358619012886
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1609525e-05
Norm of the params: 11.853815
              Random: fixed  43 labels. Loss 0.08855. Accuracy 1.000.
### Flips: 410, rs: 33, checks: 820
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020319644
Train loss (w/o reg) on all data: 0.016843915
Test loss (w/o reg) on all data: 0.007383787
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 3.331077e-07
Norm of the params: 8.33754
     Influence (LOO): fixed 333 labels. Loss 0.00738. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4015326e-08
Norm of the params: 6.09283
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21827163
Train loss (w/o reg) on all data: 0.21111813
Test loss (w/o reg) on all data: 0.0845515
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0200183e-05
Norm of the params: 11.961184
              Random: fixed  60 labels. Loss 0.08455. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017233264
Train loss (w/o reg) on all data: 0.013822253
Test loss (w/o reg) on all data: 0.006660218
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3036547e-07
Norm of the params: 8.259554
     Influence (LOO): fixed 336 labels. Loss 0.00666. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.002656083
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.001641e-08
Norm of the params: 6.092811
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20798238
Train loss (w/o reg) on all data: 0.20046912
Test loss (w/o reg) on all data: 0.079408444
Train acc on all data:  0.9341113542426452
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4611454e-05
Norm of the params: 12.258267
              Random: fixed  78 labels. Loss 0.07941. Accuracy 1.000.
### Flips: 410, rs: 33, checks: 1230
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010986738
Train loss (w/o reg) on all data: 0.007871921
Test loss (w/o reg) on all data: 0.004904137
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6526584e-07
Norm of the params: 7.8928027
     Influence (LOO): fixed 342 labels. Loss 0.00490. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8697548e-08
Norm of the params: 6.092824
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19209678
Train loss (w/o reg) on all data: 0.18434967
Test loss (w/o reg) on all data: 0.07111104
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.272994e-06
Norm of the params: 12.44758
              Random: fixed 105 labels. Loss 0.07111. Accuracy 0.999.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2604264
Train loss (w/o reg) on all data: 0.2536458
Test loss (w/o reg) on all data: 0.1021379
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.4645806e-06
Norm of the params: 11.645242
Flipped loss: 0.10214. Accuracy: 0.998
### Flips: 410, rs: 34, checks: 205
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13522886
Train loss (w/o reg) on all data: 0.12840044
Test loss (w/o reg) on all data: 0.04583744
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0861709e-06
Norm of the params: 11.686247
     Influence (LOO): fixed 187 labels. Loss 0.04584. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10094634
Train loss (w/o reg) on all data: 0.08817246
Test loss (w/o reg) on all data: 0.04319577
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.513721e-06
Norm of the params: 15.983667
                Loss: fixed 205 labels. Loss 0.04320. Accuracy 0.994.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2531841
Train loss (w/o reg) on all data: 0.24645878
Test loss (w/o reg) on all data: 0.0988309
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.269092e-06
Norm of the params: 11.597692
              Random: fixed  16 labels. Loss 0.09883. Accuracy 0.997.
### Flips: 410, rs: 34, checks: 410
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05971296
Train loss (w/o reg) on all data: 0.053914905
Test loss (w/o reg) on all data: 0.018287038
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1789062e-06
Norm of the params: 10.768526
     Influence (LOO): fixed 290 labels. Loss 0.01829. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002948957
Train loss (w/o reg) on all data: 0.0010428992
Test loss (w/o reg) on all data: 0.002615252
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.592763e-09
Norm of the params: 6.174233
                Loss: fixed 354 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24473308
Train loss (w/o reg) on all data: 0.23793106
Test loss (w/o reg) on all data: 0.09300198
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.749583e-06
Norm of the params: 11.663636
              Random: fixed  30 labels. Loss 0.09300. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 615
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034074932
Train loss (w/o reg) on all data: 0.029703591
Test loss (w/o reg) on all data: 0.010995328
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1072124e-07
Norm of the params: 9.35023
     Influence (LOO): fixed 324 labels. Loss 0.01100. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.532488e-08
Norm of the params: 6.0928035
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2362288
Train loss (w/o reg) on all data: 0.22978692
Test loss (w/o reg) on all data: 0.08835586
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7671382e-05
Norm of the params: 11.350657
              Random: fixed  48 labels. Loss 0.08836. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023256253
Train loss (w/o reg) on all data: 0.019553686
Test loss (w/o reg) on all data: 0.0079036495
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8904176e-07
Norm of the params: 8.6053095
     Influence (LOO): fixed 335 labels. Loss 0.00790. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.685411e-08
Norm of the params: 6.0928183
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23055992
Train loss (w/o reg) on all data: 0.22407627
Test loss (w/o reg) on all data: 0.085427254
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3822216e-06
Norm of the params: 11.387394
              Random: fixed  57 labels. Loss 0.08543. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 1025
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013852169
Train loss (w/o reg) on all data: 0.011065086
Test loss (w/o reg) on all data: 0.0052689095
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.323769e-07
Norm of the params: 7.466033
     Influence (LOO): fixed 346 labels. Loss 0.00527. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601371
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2297685e-08
Norm of the params: 6.0927863
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2207875
Train loss (w/o reg) on all data: 0.21453919
Test loss (w/o reg) on all data: 0.080947325
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8767147e-05
Norm of the params: 11.178823
              Random: fixed  74 labels. Loss 0.08095. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 1230
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008832557
Train loss (w/o reg) on all data: 0.006429615
Test loss (w/o reg) on all data: 0.0040574893
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 4.247463e-08
Norm of the params: 6.9324484
     Influence (LOO): fixed 350 labels. Loss 0.00406. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009934
Test loss (w/o reg) on all data: 0.0026560223
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4932268e-08
Norm of the params: 6.0928483
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20829259
Train loss (w/o reg) on all data: 0.20167416
Test loss (w/o reg) on all data: 0.07591304
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.866159e-05
Norm of the params: 11.5051565
              Random: fixed  94 labels. Loss 0.07591. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25422147
Train loss (w/o reg) on all data: 0.24646933
Test loss (w/o reg) on all data: 0.102671206
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.775432e-06
Norm of the params: 12.4516115
Flipped loss: 0.10267. Accuracy: 0.992
### Flips: 410, rs: 35, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13445461
Train loss (w/o reg) on all data: 0.12660873
Test loss (w/o reg) on all data: 0.046687637
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1319368e-06
Norm of the params: 12.526677
     Influence (LOO): fixed 186 labels. Loss 0.04669. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09805071
Train loss (w/o reg) on all data: 0.08476014
Test loss (w/o reg) on all data: 0.049299635
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.0847508e-06
Norm of the params: 16.30373
                Loss: fixed 205 labels. Loss 0.04930. Accuracy 0.988.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24499822
Train loss (w/o reg) on all data: 0.23735365
Test loss (w/o reg) on all data: 0.096766636
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.977465e-06
Norm of the params: 12.364927
              Random: fixed  17 labels. Loss 0.09677. Accuracy 0.993.
### Flips: 410, rs: 35, checks: 410
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06731211
Train loss (w/o reg) on all data: 0.06079167
Test loss (w/o reg) on all data: 0.022976585
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 2.334443e-06
Norm of the params: 11.419665
     Influence (LOO): fixed 278 labels. Loss 0.02298. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004261473
Train loss (w/o reg) on all data: 0.0016220029
Test loss (w/o reg) on all data: 0.0030519671
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0647104e-07
Norm of the params: 7.265632
                Loss: fixed 352 labels. Loss 0.00305. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23492403
Train loss (w/o reg) on all data: 0.22752741
Test loss (w/o reg) on all data: 0.08984869
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2139857e-05
Norm of the params: 12.1627445
              Random: fixed  37 labels. Loss 0.08985. Accuracy 0.996.
### Flips: 410, rs: 35, checks: 615
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038108397
Train loss (w/o reg) on all data: 0.032931704
Test loss (w/o reg) on all data: 0.013386318
Train acc on all data:  0.9902747386336008
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8210022e-06
Norm of the params: 10.17516
     Influence (LOO): fixed 314 labels. Loss 0.01339. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012745
Test loss (w/o reg) on all data: 0.0026560947
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.346562e-08
Norm of the params: 6.092803
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225369
Train loss (w/o reg) on all data: 0.21823904
Test loss (w/o reg) on all data: 0.08357685
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.901513e-05
Norm of the params: 11.941501
              Random: fixed  55 labels. Loss 0.08358. Accuracy 0.995.
### Flips: 410, rs: 35, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022223106
Train loss (w/o reg) on all data: 0.01892227
Test loss (w/o reg) on all data: 0.006839474
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5471896e-07
Norm of the params: 8.125067
     Influence (LOO): fixed 335 labels. Loss 0.00684. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0539412e-08
Norm of the params: 6.0928154
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21790177
Train loss (w/o reg) on all data: 0.21073459
Test loss (w/o reg) on all data: 0.079942875
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5422967e-05
Norm of the params: 11.972621
              Random: fixed  67 labels. Loss 0.07994. Accuracy 0.994.
### Flips: 410, rs: 35, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014947945
Train loss (w/o reg) on all data: 0.0119584
Test loss (w/o reg) on all data: 0.0050669643
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 4.278558e-07
Norm of the params: 7.732458
     Influence (LOO): fixed 342 labels. Loss 0.00507. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1151457e-08
Norm of the params: 6.092803
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20746702
Train loss (w/o reg) on all data: 0.20043719
Test loss (w/o reg) on all data: 0.073818
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.248134e-06
Norm of the params: 11.8573475
              Random: fixed  87 labels. Loss 0.07382. Accuracy 0.995.
### Flips: 410, rs: 35, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013043674
Train loss (w/o reg) on all data: 0.010146077
Test loss (w/o reg) on all data: 0.0046892175
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4502959e-07
Norm of the params: 7.612617
     Influence (LOO): fixed 344 labels. Loss 0.00469. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601371
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1023057e-08
Norm of the params: 6.0927863
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19612154
Train loss (w/o reg) on all data: 0.18915218
Test loss (w/o reg) on all data: 0.0691764
Train acc on all data:  0.9401896425966448
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5774112e-05
Norm of the params: 11.806238
              Random: fixed 107 labels. Loss 0.06918. Accuracy 0.997.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2528538
Train loss (w/o reg) on all data: 0.24611257
Test loss (w/o reg) on all data: 0.09927069
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.398753e-06
Norm of the params: 11.611412
Flipped loss: 0.09927. Accuracy: 0.995
### Flips: 410, rs: 36, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130347
Train loss (w/o reg) on all data: 0.12263896
Test loss (w/o reg) on all data: 0.04224347
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.225268e-06
Norm of the params: 12.416141
     Influence (LOO): fixed 188 labels. Loss 0.04224. Accuracy 0.999.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09428401
Train loss (w/o reg) on all data: 0.08150903
Test loss (w/o reg) on all data: 0.04477255
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1893106e-06
Norm of the params: 15.984355
                Loss: fixed 205 labels. Loss 0.04477. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24456257
Train loss (w/o reg) on all data: 0.23776443
Test loss (w/o reg) on all data: 0.096109845
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.696694e-06
Norm of the params: 11.660305
              Random: fixed  16 labels. Loss 0.09611. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 410
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05895624
Train loss (w/o reg) on all data: 0.0531572
Test loss (w/o reg) on all data: 0.01811825
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5524029e-06
Norm of the params: 10.769438
     Influence (LOO): fixed 284 labels. Loss 0.01812. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038724649
Train loss (w/o reg) on all data: 0.0013536076
Test loss (w/o reg) on all data: 0.0030390278
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.803137e-08
Norm of the params: 7.0976863
                Loss: fixed 346 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2345495
Train loss (w/o reg) on all data: 0.2273242
Test loss (w/o reg) on all data: 0.0922183
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.919423e-06
Norm of the params: 12.021063
              Random: fixed  31 labels. Loss 0.09222. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 615
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038628444
Train loss (w/o reg) on all data: 0.033837926
Test loss (w/o reg) on all data: 0.0122875925
Train acc on all data:  0.9907610017019207
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1254667e-07
Norm of the params: 9.788279
     Influence (LOO): fixed 310 labels. Loss 0.01229. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7907091e-08
Norm of the params: 6.092819
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220737
Train loss (w/o reg) on all data: 0.21333893
Test loss (w/o reg) on all data: 0.08739305
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.957091e-06
Norm of the params: 12.163943
              Random: fixed  55 labels. Loss 0.08739. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 820
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018615961
Train loss (w/o reg) on all data: 0.015114321
Test loss (w/o reg) on all data: 0.006770409
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8940886e-07
Norm of the params: 8.368559
     Influence (LOO): fixed 333 labels. Loss 0.00677. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.0026560773
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1177193e-08
Norm of the params: 6.0928063
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2074761
Train loss (w/o reg) on all data: 0.20011505
Test loss (w/o reg) on all data: 0.08107157
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6895077e-05
Norm of the params: 12.133463
              Random: fixed  77 labels. Loss 0.08107. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014722213
Train loss (w/o reg) on all data: 0.011599614
Test loss (w/o reg) on all data: 0.0055026445
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3302711e-07
Norm of the params: 7.902656
     Influence (LOO): fixed 337 labels. Loss 0.00550. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960093
Test loss (w/o reg) on all data: 0.0026560016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.814835e-08
Norm of the params: 6.0928593
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193442
Train loss (w/o reg) on all data: 0.18585294
Test loss (w/o reg) on all data: 0.07361551
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.686554e-06
Norm of the params: 12.31995
              Random: fixed 102 labels. Loss 0.07362. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 1230
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0105007645
Train loss (w/o reg) on all data: 0.007856365
Test loss (w/o reg) on all data: 0.0039303363
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 7.036326e-08
Norm of the params: 7.272413
     Influence (LOO): fixed 342 labels. Loss 0.00393. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010853
Test loss (w/o reg) on all data: 0.0026559895
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1115302e-07
Norm of the params: 6.092834
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18629986
Train loss (w/o reg) on all data: 0.17859878
Test loss (w/o reg) on all data: 0.07054404
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.034741e-06
Norm of the params: 12.410546
              Random: fixed 113 labels. Loss 0.07054. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25064355
Train loss (w/o reg) on all data: 0.24326424
Test loss (w/o reg) on all data: 0.105955884
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.5682137e-06
Norm of the params: 12.148512
Flipped loss: 0.10596. Accuracy: 0.996
### Flips: 410, rs: 37, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131921
Train loss (w/o reg) on all data: 0.12449084
Test loss (w/o reg) on all data: 0.04732104
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4779571e-05
Norm of the params: 12.19029
     Influence (LOO): fixed 179 labels. Loss 0.04732. Accuracy 0.999.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093429185
Train loss (w/o reg) on all data: 0.079827875
Test loss (w/o reg) on all data: 0.052323572
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.4452785e-06
Norm of the params: 16.493216
                Loss: fixed 205 labels. Loss 0.05232. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24258688
Train loss (w/o reg) on all data: 0.23525557
Test loss (w/o reg) on all data: 0.1011777
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1072236e-05
Norm of the params: 12.108936
              Random: fixed  17 labels. Loss 0.10118. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 410
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055978257
Train loss (w/o reg) on all data: 0.050540876
Test loss (w/o reg) on all data: 0.019238515
Train acc on all data:  0.9841964502796012
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7090614e-06
Norm of the params: 10.428214
     Influence (LOO): fixed 283 labels. Loss 0.01924. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.618067e-08
Norm of the params: 6.0928082
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23470905
Train loss (w/o reg) on all data: 0.22709945
Test loss (w/o reg) on all data: 0.09810214
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.175124e-06
Norm of the params: 12.336618
              Random: fixed  31 labels. Loss 0.09810. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03967921
Train loss (w/o reg) on all data: 0.035228964
Test loss (w/o reg) on all data: 0.012904937
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 5.472378e-07
Norm of the params: 9.434243
     Influence (LOO): fixed 305 labels. Loss 0.01290. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.00265608
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2755767e-08
Norm of the params: 6.092824
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2172667
Train loss (w/o reg) on all data: 0.20996958
Test loss (w/o reg) on all data: 0.08955322
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8254623e-05
Norm of the params: 12.080655
              Random: fixed  64 labels. Loss 0.08955. Accuracy 0.994.
### Flips: 410, rs: 37, checks: 820
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021773258
Train loss (w/o reg) on all data: 0.018249843
Test loss (w/o reg) on all data: 0.0070838947
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2504294e-06
Norm of the params: 8.394541
     Influence (LOO): fixed 326 labels. Loss 0.00708. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.508448e-08
Norm of the params: 6.092812
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20923333
Train loss (w/o reg) on all data: 0.20215121
Test loss (w/o reg) on all data: 0.0844192
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2304388e-05
Norm of the params: 11.901366
              Random: fixed  79 labels. Loss 0.08442. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016129227
Train loss (w/o reg) on all data: 0.012651373
Test loss (w/o reg) on all data: 0.0058332514
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9338738e-07
Norm of the params: 8.340091
     Influence (LOO): fixed 333 labels. Loss 0.00583. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601123
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1887276e-08
Norm of the params: 6.0928264
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19801573
Train loss (w/o reg) on all data: 0.19075723
Test loss (w/o reg) on all data: 0.077706076
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.633902e-06
Norm of the params: 12.04865
              Random: fixed  95 labels. Loss 0.07771. Accuracy 0.997.
### Flips: 410, rs: 37, checks: 1230
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01325745
Train loss (w/o reg) on all data: 0.010070429
Test loss (w/o reg) on all data: 0.0050037536
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5729628e-07
Norm of the params: 7.9837604
     Influence (LOO): fixed 337 labels. Loss 0.00500. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2714345e-09
Norm of the params: 6.0928164
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18607786
Train loss (w/o reg) on all data: 0.1789737
Test loss (w/o reg) on all data: 0.071837254
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4115472e-05
Norm of the params: 11.919864
              Random: fixed 113 labels. Loss 0.07184. Accuracy 0.998.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25627744
Train loss (w/o reg) on all data: 0.24816233
Test loss (w/o reg) on all data: 0.11122607
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1375681e-05
Norm of the params: 12.739794
Flipped loss: 0.11123. Accuracy: 0.992
### Flips: 410, rs: 38, checks: 205
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14042783
Train loss (w/o reg) on all data: 0.13263574
Test loss (w/o reg) on all data: 0.05049358
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.070096e-06
Norm of the params: 12.483662
     Influence (LOO): fixed 184 labels. Loss 0.05049. Accuracy 0.999.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10218264
Train loss (w/o reg) on all data: 0.08677426
Test loss (w/o reg) on all data: 0.058082342
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.723787e-06
Norm of the params: 17.5547
                Loss: fixed 205 labels. Loss 0.05808. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2436281
Train loss (w/o reg) on all data: 0.23563164
Test loss (w/o reg) on all data: 0.10304717
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.2746456e-06
Norm of the params: 12.646302
              Random: fixed  26 labels. Loss 0.10305. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 410
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061719142
Train loss (w/o reg) on all data: 0.0554241
Test loss (w/o reg) on all data: 0.021908345
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2202074e-06
Norm of the params: 11.220553
     Influence (LOO): fixed 291 labels. Loss 0.02191. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003465999
Train loss (w/o reg) on all data: 0.0012043518
Test loss (w/o reg) on all data: 0.0029205487
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19012206e-07
Norm of the params: 6.725545
                Loss: fixed 364 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23593923
Train loss (w/o reg) on all data: 0.22808333
Test loss (w/o reg) on all data: 0.09646624
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.232829e-06
Norm of the params: 12.5346775
              Random: fixed  43 labels. Loss 0.09647. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035401165
Train loss (w/o reg) on all data: 0.030557627
Test loss (w/o reg) on all data: 0.013017224
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8908858e-06
Norm of the params: 9.842295
     Influence (LOO): fixed 327 labels. Loss 0.01302. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034659994
Train loss (w/o reg) on all data: 0.0012043467
Test loss (w/o reg) on all data: 0.002920573
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2723565e-08
Norm of the params: 6.7255516
                Loss: fixed 364 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2293061
Train loss (w/o reg) on all data: 0.2214412
Test loss (w/o reg) on all data: 0.09199799
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.321754e-06
Norm of the params: 12.541862
              Random: fixed  56 labels. Loss 0.09200. Accuracy 0.996.
### Flips: 410, rs: 38, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019169226
Train loss (w/o reg) on all data: 0.015990512
Test loss (w/o reg) on all data: 0.0060893637
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8945983e-07
Norm of the params: 7.9733486
     Influence (LOO): fixed 349 labels. Loss 0.00609. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003435465
Train loss (w/o reg) on all data: 0.0012970818
Test loss (w/o reg) on all data: 0.0028204008
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.317461e-08
Norm of the params: 6.539699
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21775985
Train loss (w/o reg) on all data: 0.2101904
Test loss (w/o reg) on all data: 0.08686041
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8683104e-05
Norm of the params: 12.30402
              Random: fixed  77 labels. Loss 0.08686. Accuracy 0.996.
### Flips: 410, rs: 38, checks: 1025
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019169224
Train loss (w/o reg) on all data: 0.015990337
Test loss (w/o reg) on all data: 0.00608937
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 6.551037e-07
Norm of the params: 7.9735656
     Influence (LOO): fixed 349 labels. Loss 0.00609. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034354653
Train loss (w/o reg) on all data: 0.0012970814
Test loss (w/o reg) on all data: 0.002820409
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.126762e-08
Norm of the params: 6.5397005
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2031061
Train loss (w/o reg) on all data: 0.19558701
Test loss (w/o reg) on all data: 0.07901009
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.868727e-06
Norm of the params: 12.263026
              Random: fixed 103 labels. Loss 0.07901. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 1230
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016920315
Train loss (w/o reg) on all data: 0.013701835
Test loss (w/o reg) on all data: 0.0053914385
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.923336e-07
Norm of the params: 8.023068
     Influence (LOO): fixed 351 labels. Loss 0.00539. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034354646
Train loss (w/o reg) on all data: 0.0012970797
Test loss (w/o reg) on all data: 0.0028204261
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3235594e-08
Norm of the params: 6.539702
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19056103
Train loss (w/o reg) on all data: 0.18343459
Test loss (w/o reg) on all data: 0.07298877
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.24225e-06
Norm of the params: 11.938541
              Random: fixed 126 labels. Loss 0.07299. Accuracy 0.996.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25219053
Train loss (w/o reg) on all data: 0.24545868
Test loss (w/o reg) on all data: 0.09946252
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.16032e-06
Norm of the params: 11.603324
Flipped loss: 0.09946. Accuracy: 0.993
### Flips: 410, rs: 39, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13137996
Train loss (w/o reg) on all data: 0.12344138
Test loss (w/o reg) on all data: 0.05036672
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3888277e-06
Norm of the params: 12.600459
     Influence (LOO): fixed 183 labels. Loss 0.05037. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09506197
Train loss (w/o reg) on all data: 0.08144054
Test loss (w/o reg) on all data: 0.04708044
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.004005e-06
Norm of the params: 16.505411
                Loss: fixed 205 labels. Loss 0.04708. Accuracy 0.994.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24484113
Train loss (w/o reg) on all data: 0.23819192
Test loss (w/o reg) on all data: 0.09472803
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.5675858e-05
Norm of the params: 11.531883
              Random: fixed  16 labels. Loss 0.09473. Accuracy 0.993.
### Flips: 410, rs: 39, checks: 410
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0513806
Train loss (w/o reg) on all data: 0.04563787
Test loss (w/o reg) on all data: 0.017156105
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3651702e-06
Norm of the params: 10.717027
     Influence (LOO): fixed 295 labels. Loss 0.01716. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034810498
Train loss (w/o reg) on all data: 0.001188501
Test loss (w/o reg) on all data: 0.0026644554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4966246e-08
Norm of the params: 6.7713356
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22992034
Train loss (w/o reg) on all data: 0.22316939
Test loss (w/o reg) on all data: 0.08719745
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.34485945e-05
Norm of the params: 11.619774
              Random: fixed  41 labels. Loss 0.08720. Accuracy 0.994.
### Flips: 410, rs: 39, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03383132
Train loss (w/o reg) on all data: 0.029458394
Test loss (w/o reg) on all data: 0.010642778
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 6.482902e-07
Norm of the params: 9.351928
     Influence (LOO): fixed 317 labels. Loss 0.01064. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960128
Test loss (w/o reg) on all data: 0.0026560833
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.250213e-08
Norm of the params: 6.092801
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21889001
Train loss (w/o reg) on all data: 0.21192282
Test loss (w/o reg) on all data: 0.0812991
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4803695e-05
Norm of the params: 11.8043995
              Random: fixed  63 labels. Loss 0.08130. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01976198
Train loss (w/o reg) on all data: 0.016339814
Test loss (w/o reg) on all data: 0.006809496
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7200304e-07
Norm of the params: 8.273046
     Influence (LOO): fixed 332 labels. Loss 0.00681. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1622027e-08
Norm of the params: 6.092811
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20482203
Train loss (w/o reg) on all data: 0.1978316
Test loss (w/o reg) on all data: 0.075669914
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.7960727e-06
Norm of the params: 11.824074
              Random: fixed  87 labels. Loss 0.07567. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 1025
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015699184
Train loss (w/o reg) on all data: 0.012807527
Test loss (w/o reg) on all data: 0.0058309855
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0460726e-07
Norm of the params: 7.6048107
     Influence (LOO): fixed 337 labels. Loss 0.00583. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3420107e-08
Norm of the params: 6.0928144
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260228
Train loss (w/o reg) on all data: 0.18546574
Test loss (w/o reg) on all data: 0.069709174
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8099196e-06
Norm of the params: 11.946998
              Random: fixed 107 labels. Loss 0.06971. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 1230
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009823111
Train loss (w/o reg) on all data: 0.0075752567
Test loss (w/o reg) on all data: 0.003752506
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3989966e-08
Norm of the params: 6.705005
     Influence (LOO): fixed 343 labels. Loss 0.00375. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.000708e-08
Norm of the params: 6.09281
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153147
Train loss (w/o reg) on all data: 0.1742743
Test loss (w/o reg) on all data: 0.06500602
Train acc on all data:  0.9455385363481643
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5367712e-05
Norm of the params: 12.047554
              Random: fixed 124 labels. Loss 0.06501. Accuracy 0.997.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33491775
Train loss (w/o reg) on all data: 0.32845888
Test loss (w/o reg) on all data: 0.14969392
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3321522e-05
Norm of the params: 11.36562
Flipped loss: 0.14969. Accuracy: 0.990
### Flips: 615, rs: 0, checks: 205
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23026583
Train loss (w/o reg) on all data: 0.22162709
Test loss (w/o reg) on all data: 0.09343984
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.79997e-05
Norm of the params: 13.144382
     Influence (LOO): fixed 186 labels. Loss 0.09344. Accuracy 0.999.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18907633
Train loss (w/o reg) on all data: 0.17441344
Test loss (w/o reg) on all data: 0.10161169
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.1140179e-05
Norm of the params: 17.124777
                Loss: fixed 205 labels. Loss 0.10161. Accuracy 0.976.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32765397
Train loss (w/o reg) on all data: 0.321064
Test loss (w/o reg) on all data: 0.1456454
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7675155e-05
Norm of the params: 11.480408
              Random: fixed  17 labels. Loss 0.14565. Accuracy 0.988.
### Flips: 615, rs: 0, checks: 410
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14995094
Train loss (w/o reg) on all data: 0.14230259
Test loss (w/o reg) on all data: 0.05402923
Train acc on all data:  0.9521030877704838
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9822115e-06
Norm of the params: 12.367988
     Influence (LOO): fixed 326 labels. Loss 0.05403. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06475909
Train loss (w/o reg) on all data: 0.049815048
Test loss (w/o reg) on all data: 0.03440046
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.8864696e-06
Norm of the params: 17.288174
                Loss: fixed 410 labels. Loss 0.03440. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31388643
Train loss (w/o reg) on all data: 0.3072485
Test loss (w/o reg) on all data: 0.13717529
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.6184293e-05
Norm of the params: 11.522083
              Random: fixed  47 labels. Loss 0.13718. Accuracy 0.987.
### Flips: 615, rs: 0, checks: 615
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095694944
Train loss (w/o reg) on all data: 0.08895321
Test loss (w/o reg) on all data: 0.034324974
Train acc on all data:  0.9705810843666424
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2860366e-06
Norm of the params: 11.611831
     Influence (LOO): fixed 403 labels. Loss 0.03432. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004232459
Train loss (w/o reg) on all data: 0.0015869394
Test loss (w/o reg) on all data: 0.0030426993
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4585497e-08
Norm of the params: 7.2739525
                Loss: fixed 522 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30577654
Train loss (w/o reg) on all data: 0.29899248
Test loss (w/o reg) on all data: 0.13254113
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2300298e-05
Norm of the params: 11.648211
              Random: fixed  64 labels. Loss 0.13254. Accuracy 0.986.
### Flips: 615, rs: 0, checks: 820
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06683676
Train loss (w/o reg) on all data: 0.06115264
Test loss (w/o reg) on all data: 0.02254622
Train acc on all data:  0.9800632141988816
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5788166e-06
Norm of the params: 10.662191
     Influence (LOO): fixed 441 labels. Loss 0.02255. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003500654
Train loss (w/o reg) on all data: 0.0012505119
Test loss (w/o reg) on all data: 0.00283165
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2106212e-08
Norm of the params: 6.708416
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2922358
Train loss (w/o reg) on all data: 0.28541005
Test loss (w/o reg) on all data: 0.12293274
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8829058e-05
Norm of the params: 11.683972
              Random: fixed  96 labels. Loss 0.12293. Accuracy 0.992.
### Flips: 615, rs: 0, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048788927
Train loss (w/o reg) on all data: 0.04350733
Test loss (w/o reg) on all data: 0.015524786
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7383587e-06
Norm of the params: 10.2777405
     Influence (LOO): fixed 467 labels. Loss 0.01552. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035006544
Train loss (w/o reg) on all data: 0.0012505181
Test loss (w/o reg) on all data: 0.0028316795
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0805595e-08
Norm of the params: 6.708407
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27961165
Train loss (w/o reg) on all data: 0.2727868
Test loss (w/o reg) on all data: 0.116040125
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.120869e-06
Norm of the params: 11.6832075
              Random: fixed 121 labels. Loss 0.11604. Accuracy 0.996.
### Flips: 615, rs: 0, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03529987
Train loss (w/o reg) on all data: 0.030778738
Test loss (w/o reg) on all data: 0.01063708
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 1.113986e-06
Norm of the params: 9.509085
     Influence (LOO): fixed 487 labels. Loss 0.01064. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035006544
Train loss (w/o reg) on all data: 0.0012505207
Test loss (w/o reg) on all data: 0.0028316483
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.792256e-08
Norm of the params: 6.708403
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26684967
Train loss (w/o reg) on all data: 0.25988635
Test loss (w/o reg) on all data: 0.10798503
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.174175e-06
Norm of the params: 11.801124
              Random: fixed 149 labels. Loss 0.10799. Accuracy 0.996.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33779484
Train loss (w/o reg) on all data: 0.33175436
Test loss (w/o reg) on all data: 0.15055221
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.3948835e-06
Norm of the params: 10.991333
Flipped loss: 0.15055. Accuracy: 0.992
### Flips: 615, rs: 1, checks: 205
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23624156
Train loss (w/o reg) on all data: 0.22765568
Test loss (w/o reg) on all data: 0.09171024
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9401929e-05
Norm of the params: 13.104107
     Influence (LOO): fixed 187 labels. Loss 0.09171. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1961473
Train loss (w/o reg) on all data: 0.18377183
Test loss (w/o reg) on all data: 0.09547768
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.06559e-05
Norm of the params: 15.732423
                Loss: fixed 205 labels. Loss 0.09548. Accuracy 0.981.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32710168
Train loss (w/o reg) on all data: 0.32093954
Test loss (w/o reg) on all data: 0.14167744
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2243225e-05
Norm of the params: 11.101468
              Random: fixed  29 labels. Loss 0.14168. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 410
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15429603
Train loss (w/o reg) on all data: 0.14511403
Test loss (w/o reg) on all data: 0.058027677
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.0165182e-06
Norm of the params: 13.55138
     Influence (LOO): fixed 320 labels. Loss 0.05803. Accuracy 0.998.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07172456
Train loss (w/o reg) on all data: 0.057827704
Test loss (w/o reg) on all data: 0.036711615
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.55326e-06
Norm of the params: 16.671446
                Loss: fixed 410 labels. Loss 0.03671. Accuracy 0.994.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3154531
Train loss (w/o reg) on all data: 0.30930513
Test loss (w/o reg) on all data: 0.13228853
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.6554228e-05
Norm of the params: 11.088724
              Random: fixed  59 labels. Loss 0.13229. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102318965
Train loss (w/o reg) on all data: 0.094907455
Test loss (w/o reg) on all data: 0.035641808
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6875923e-06
Norm of the params: 12.174981
     Influence (LOO): fixed 401 labels. Loss 0.03564. Accuracy 1.000.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005629663
Train loss (w/o reg) on all data: 0.0021620085
Test loss (w/o reg) on all data: 0.0029759659
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.846861e-08
Norm of the params: 8.32785
                Loss: fixed 529 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.308523
Train loss (w/o reg) on all data: 0.30220073
Test loss (w/o reg) on all data: 0.12747045
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.2849903e-05
Norm of the params: 11.244779
              Random: fixed  75 labels. Loss 0.12747. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 820
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06569608
Train loss (w/o reg) on all data: 0.059256054
Test loss (w/o reg) on all data: 0.023960086
Train acc on all data:  0.9810357403355215
Test acc on all data:   1.0
Norm of the mean of gradients: 7.395238e-06
Norm of the params: 11.349031
     Influence (LOO): fixed 454 labels. Loss 0.02396. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043270574
Train loss (w/o reg) on all data: 0.0015154037
Test loss (w/o reg) on all data: 0.0028312742
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.939558e-08
Norm of the params: 7.498872
                Loss: fixed 531 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29677805
Train loss (w/o reg) on all data: 0.29025328
Test loss (w/o reg) on all data: 0.12102327
Train acc on all data:  0.8954534403112083
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1405769e-05
Norm of the params: 11.423461
              Random: fixed 103 labels. Loss 0.12102. Accuracy 0.992.
### Flips: 615, rs: 1, checks: 1025
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049979895
Train loss (w/o reg) on all data: 0.04438502
Test loss (w/o reg) on all data: 0.016114576
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 8.060011e-07
Norm of the params: 10.578161
     Influence (LOO): fixed 476 labels. Loss 0.01611. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7924191e-08
Norm of the params: 6.092812
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28332102
Train loss (w/o reg) on all data: 0.27655622
Test loss (w/o reg) on all data: 0.11356198
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.689818e-06
Norm of the params: 11.631674
              Random: fixed 131 labels. Loss 0.11356. Accuracy 0.994.
### Flips: 615, rs: 1, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034198876
Train loss (w/o reg) on all data: 0.029904915
Test loss (w/o reg) on all data: 0.010139569
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6277894e-06
Norm of the params: 9.267105
     Influence (LOO): fixed 499 labels. Loss 0.01014. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5095855e-08
Norm of the params: 6.0928183
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2712382
Train loss (w/o reg) on all data: 0.2647423
Test loss (w/o reg) on all data: 0.105735
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.075837e-06
Norm of the params: 11.398157
              Random: fixed 156 labels. Loss 0.10573. Accuracy 0.994.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34383342
Train loss (w/o reg) on all data: 0.33754396
Test loss (w/o reg) on all data: 0.16605145
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.073671e-05
Norm of the params: 11.215576
Flipped loss: 0.16605. Accuracy: 0.984
### Flips: 615, rs: 2, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23863725
Train loss (w/o reg) on all data: 0.22936529
Test loss (w/o reg) on all data: 0.112439476
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.9870753e-05
Norm of the params: 13.617612
     Influence (LOO): fixed 181 labels. Loss 0.11244. Accuracy 0.993.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20126079
Train loss (w/o reg) on all data: 0.18885545
Test loss (w/o reg) on all data: 0.11992889
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.1272291e-05
Norm of the params: 15.751406
                Loss: fixed 205 labels. Loss 0.11993. Accuracy 0.975.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33002758
Train loss (w/o reg) on all data: 0.3235479
Test loss (w/o reg) on all data: 0.15410388
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0379885e-05
Norm of the params: 11.383921
              Random: fixed  32 labels. Loss 0.15410. Accuracy 0.988.
### Flips: 615, rs: 2, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16200015
Train loss (w/o reg) on all data: 0.15342832
Test loss (w/o reg) on all data: 0.07169006
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.243035e-06
Norm of the params: 13.09338
     Influence (LOO): fixed 311 labels. Loss 0.07169. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07361569
Train loss (w/o reg) on all data: 0.057393648
Test loss (w/o reg) on all data: 0.061632216
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.915609e-06
Norm of the params: 18.012243
                Loss: fixed 410 labels. Loss 0.06163. Accuracy 0.980.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32001725
Train loss (w/o reg) on all data: 0.31330281
Test loss (w/o reg) on all data: 0.14894757
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1958909e-05
Norm of the params: 11.588301
              Random: fixed  56 labels. Loss 0.14895. Accuracy 0.986.
### Flips: 615, rs: 2, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1114435
Train loss (w/o reg) on all data: 0.1035028
Test loss (w/o reg) on all data: 0.046971302
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2451126e-06
Norm of the params: 12.6021385
     Influence (LOO): fixed 391 labels. Loss 0.04697. Accuracy 0.997.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008384401
Train loss (w/o reg) on all data: 0.0039466335
Test loss (w/o reg) on all data: 0.006337915
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.1906856e-07
Norm of the params: 9.421006
                Loss: fixed 529 labels. Loss 0.00634. Accuracy 0.998.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31025225
Train loss (w/o reg) on all data: 0.30351382
Test loss (w/o reg) on all data: 0.14415284
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.138615e-06
Norm of the params: 11.6089735
              Random: fixed  76 labels. Loss 0.14415. Accuracy 0.986.
### Flips: 615, rs: 2, checks: 820
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07470516
Train loss (w/o reg) on all data: 0.06675705
Test loss (w/o reg) on all data: 0.029237203
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.4533333e-06
Norm of the params: 12.608023
     Influence (LOO): fixed 446 labels. Loss 0.02924. Accuracy 0.998.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038359233
Train loss (w/o reg) on all data: 0.001430853
Test loss (w/o reg) on all data: 0.0032010612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3445003e-07
Norm of the params: 6.935518
                Loss: fixed 537 labels. Loss 0.00320. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29917777
Train loss (w/o reg) on all data: 0.29231733
Test loss (w/o reg) on all data: 0.13698414
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.6051444e-05
Norm of the params: 11.713603
              Random: fixed 101 labels. Loss 0.13698. Accuracy 0.988.
### Flips: 615, rs: 2, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055170238
Train loss (w/o reg) on all data: 0.04859708
Test loss (w/o reg) on all data: 0.021265883
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3021681e-06
Norm of the params: 11.465738
     Influence (LOO): fixed 475 labels. Loss 0.02127. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003835923
Train loss (w/o reg) on all data: 0.0014308422
Test loss (w/o reg) on all data: 0.0032010681
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1243387e-07
Norm of the params: 6.935533
                Loss: fixed 537 labels. Loss 0.00320. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2849328
Train loss (w/o reg) on all data: 0.2778343
Test loss (w/o reg) on all data: 0.1272648
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.406823e-05
Norm of the params: 11.915124
              Random: fixed 128 labels. Loss 0.12726. Accuracy 0.990.
### Flips: 615, rs: 2, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038141742
Train loss (w/o reg) on all data: 0.03284953
Test loss (w/o reg) on all data: 0.01306539
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9564184e-07
Norm of the params: 10.288062
     Influence (LOO): fixed 498 labels. Loss 0.01307. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034790626
Train loss (w/o reg) on all data: 0.0012706107
Test loss (w/o reg) on all data: 0.0029040836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0205687e-08
Norm of the params: 6.6459794
                Loss: fixed 538 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2739839
Train loss (w/o reg) on all data: 0.2668872
Test loss (w/o reg) on all data: 0.11979465
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8870827e-05
Norm of the params: 11.913602
              Random: fixed 154 labels. Loss 0.11979. Accuracy 0.993.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33489716
Train loss (w/o reg) on all data: 0.32893822
Test loss (w/o reg) on all data: 0.1529348
Train acc on all data:  0.8769754437150499
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2002228e-05
Norm of the params: 10.91692
Flipped loss: 0.15293. Accuracy: 0.993
### Flips: 615, rs: 3, checks: 205
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22515427
Train loss (w/o reg) on all data: 0.2161961
Test loss (w/o reg) on all data: 0.09443521
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.774037e-06
Norm of the params: 13.38519
     Influence (LOO): fixed 187 labels. Loss 0.09444. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19170384
Train loss (w/o reg) on all data: 0.17735437
Test loss (w/o reg) on all data: 0.103986405
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4134246e-05
Norm of the params: 16.940767
                Loss: fixed 205 labels. Loss 0.10399. Accuracy 0.980.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.324837
Train loss (w/o reg) on all data: 0.31905094
Test loss (w/o reg) on all data: 0.14605233
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1319917e-05
Norm of the params: 10.757388
              Random: fixed  25 labels. Loss 0.14605. Accuracy 0.992.
### Flips: 615, rs: 3, checks: 410
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1490506
Train loss (w/o reg) on all data: 0.14103754
Test loss (w/o reg) on all data: 0.05878041
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2957448e-05
Norm of the params: 12.659429
     Influence (LOO): fixed 318 labels. Loss 0.05878. Accuracy 0.997.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06333841
Train loss (w/o reg) on all data: 0.04729468
Test loss (w/o reg) on all data: 0.031903546
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8032615e-06
Norm of the params: 17.912971
                Loss: fixed 410 labels. Loss 0.03190. Accuracy 0.993.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31573343
Train loss (w/o reg) on all data: 0.30975562
Test loss (w/o reg) on all data: 0.13817367
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.00077295e-05
Norm of the params: 10.934169
              Random: fixed  48 labels. Loss 0.13817. Accuracy 0.995.
### Flips: 615, rs: 3, checks: 615
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098555915
Train loss (w/o reg) on all data: 0.09134774
Test loss (w/o reg) on all data: 0.03579849
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7454942e-06
Norm of the params: 12.00681
     Influence (LOO): fixed 399 labels. Loss 0.03580. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0057460926
Train loss (w/o reg) on all data: 0.0022950734
Test loss (w/o reg) on all data: 0.002870312
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9184024e-07
Norm of the params: 8.307851
                Loss: fixed 520 labels. Loss 0.00287. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304534
Train loss (w/o reg) on all data: 0.2985625
Test loss (w/o reg) on all data: 0.1303585
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.740712e-05
Norm of the params: 10.928399
              Random: fixed  74 labels. Loss 0.13036. Accuracy 0.996.
### Flips: 615, rs: 3, checks: 820
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06852581
Train loss (w/o reg) on all data: 0.062357385
Test loss (w/o reg) on all data: 0.02242863
Train acc on all data:  0.9805494772672015
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3746856e-06
Norm of the params: 11.107138
     Influence (LOO): fixed 444 labels. Loss 0.02243. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043543344
Train loss (w/o reg) on all data: 0.0016682955
Test loss (w/o reg) on all data: 0.0027570808
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4467913e-08
Norm of the params: 7.329446
                Loss: fixed 522 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28819427
Train loss (w/o reg) on all data: 0.28186414
Test loss (w/o reg) on all data: 0.118418545
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.3215012e-05
Norm of the params: 11.251787
              Random: fixed 110 labels. Loss 0.11842. Accuracy 0.998.
### Flips: 615, rs: 3, checks: 1025
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04788706
Train loss (w/o reg) on all data: 0.04289694
Test loss (w/o reg) on all data: 0.015382959
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0671791e-06
Norm of the params: 9.990113
     Influence (LOO): fixed 474 labels. Loss 0.01538. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036663134
Train loss (w/o reg) on all data: 0.001383412
Test loss (w/o reg) on all data: 0.0026188763
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7092376e-08
Norm of the params: 6.7570724
                Loss: fixed 523 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2763325
Train loss (w/o reg) on all data: 0.26992327
Test loss (w/o reg) on all data: 0.11072159
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9184467e-05
Norm of the params: 11.321863
              Random: fixed 137 labels. Loss 0.11072. Accuracy 0.997.
### Flips: 615, rs: 3, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037334755
Train loss (w/o reg) on all data: 0.032194898
Test loss (w/o reg) on all data: 0.012156464
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 5.322692e-07
Norm of the params: 10.138892
     Influence (LOO): fixed 487 labels. Loss 0.01216. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036663138
Train loss (w/o reg) on all data: 0.0013834178
Test loss (w/o reg) on all data: 0.00261888
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8824034e-08
Norm of the params: 6.7570643
                Loss: fixed 523 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26106784
Train loss (w/o reg) on all data: 0.25441253
Test loss (w/o reg) on all data: 0.10230452
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8336334e-05
Norm of the params: 11.537174
              Random: fixed 168 labels. Loss 0.10230. Accuracy 0.998.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34223163
Train loss (w/o reg) on all data: 0.33608177
Test loss (w/o reg) on all data: 0.16295554
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.42523e-05
Norm of the params: 11.090401
Flipped loss: 0.16296. Accuracy: 0.990
### Flips: 615, rs: 4, checks: 205
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23796362
Train loss (w/o reg) on all data: 0.22848172
Test loss (w/o reg) on all data: 0.10246167
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.189408e-06
Norm of the params: 13.770911
     Influence (LOO): fixed 186 labels. Loss 0.10246. Accuracy 0.996.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19999856
Train loss (w/o reg) on all data: 0.1862052
Test loss (w/o reg) on all data: 0.11116765
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.005059e-06
Norm of the params: 16.609253
                Loss: fixed 205 labels. Loss 0.11117. Accuracy 0.978.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32656452
Train loss (w/o reg) on all data: 0.32046875
Test loss (w/o reg) on all data: 0.15003131
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3424138e-05
Norm of the params: 11.041516
              Random: fixed  36 labels. Loss 0.15003. Accuracy 0.991.
### Flips: 615, rs: 4, checks: 410
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16117036
Train loss (w/o reg) on all data: 0.15314548
Test loss (w/o reg) on all data: 0.059613343
Train acc on all data:  0.9457816678823243
Test acc on all data:   1.0
Norm of the mean of gradients: 9.12319e-06
Norm of the params: 12.668767
     Influence (LOO): fixed 322 labels. Loss 0.05961. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07683524
Train loss (w/o reg) on all data: 0.06136806
Test loss (w/o reg) on all data: 0.04689543
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.243024e-06
Norm of the params: 17.588167
                Loss: fixed 410 labels. Loss 0.04690. Accuracy 0.986.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32141474
Train loss (w/o reg) on all data: 0.3154795
Test loss (w/o reg) on all data: 0.14565665
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0951772e-05
Norm of the params: 10.895192
              Random: fixed  53 labels. Loss 0.14566. Accuracy 0.991.
### Flips: 615, rs: 4, checks: 615
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104244985
Train loss (w/o reg) on all data: 0.096792206
Test loss (w/o reg) on all data: 0.035303127
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 6.105279e-06
Norm of the params: 12.208831
     Influence (LOO): fixed 408 labels. Loss 0.03530. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007518418
Train loss (w/o reg) on all data: 0.003321599
Test loss (w/o reg) on all data: 0.0042445627
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1113193e-07
Norm of the params: 9.161681
                Loss: fixed 535 labels. Loss 0.00424. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30959988
Train loss (w/o reg) on all data: 0.3038691
Test loss (w/o reg) on all data: 0.1358982
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.299898e-06
Norm of the params: 10.705876
              Random: fixed  83 labels. Loss 0.13590. Accuracy 0.997.
### Flips: 615, rs: 4, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07496712
Train loss (w/o reg) on all data: 0.06824819
Test loss (w/o reg) on all data: 0.024407636
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5079563e-06
Norm of the params: 11.59218
     Influence (LOO): fixed 450 labels. Loss 0.02441. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058502564
Train loss (w/o reg) on all data: 0.002491742
Test loss (w/o reg) on all data: 0.0043949173
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5029346e-08
Norm of the params: 8.195748
                Loss: fixed 539 labels. Loss 0.00439. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29674613
Train loss (w/o reg) on all data: 0.29068992
Test loss (w/o reg) on all data: 0.13075805
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.945162e-06
Norm of the params: 11.005659
              Random: fixed 108 labels. Loss 0.13076. Accuracy 0.994.
### Flips: 615, rs: 4, checks: 1025
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055745218
Train loss (w/o reg) on all data: 0.049850576
Test loss (w/o reg) on all data: 0.018009275
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 4.425678e-06
Norm of the params: 10.857847
     Influence (LOO): fixed 479 labels. Loss 0.01801. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036378535
Train loss (w/o reg) on all data: 0.0013172977
Test loss (w/o reg) on all data: 0.0026034757
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6394048e-08
Norm of the params: 6.812571
                Loss: fixed 542 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28400415
Train loss (w/o reg) on all data: 0.27751032
Test loss (w/o reg) on all data: 0.12289199
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3606957e-05
Norm of the params: 11.3963585
              Random: fixed 135 labels. Loss 0.12289. Accuracy 0.993.
### Flips: 615, rs: 4, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04048094
Train loss (w/o reg) on all data: 0.035543893
Test loss (w/o reg) on all data: 0.012408923
Train acc on all data:  0.9897884755652808
Test acc on all data:   1.0
Norm of the mean of gradients: 9.228561e-07
Norm of the params: 9.93685
     Influence (LOO): fixed 501 labels. Loss 0.01241. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026561038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.044586e-08
Norm of the params: 6.0928044
                Loss: fixed 543 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27263352
Train loss (w/o reg) on all data: 0.26631323
Test loss (w/o reg) on all data: 0.11524175
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.37664265e-05
Norm of the params: 11.243047
              Random: fixed 160 labels. Loss 0.11524. Accuracy 0.995.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34328127
Train loss (w/o reg) on all data: 0.3368946
Test loss (w/o reg) on all data: 0.15875483
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.547122e-06
Norm of the params: 11.301922
Flipped loss: 0.15875. Accuracy: 0.990
### Flips: 615, rs: 5, checks: 205
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24129716
Train loss (w/o reg) on all data: 0.23338965
Test loss (w/o reg) on all data: 0.10367965
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.980827e-06
Norm of the params: 12.57578
     Influence (LOO): fixed 179 labels. Loss 0.10368. Accuracy 0.995.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20435524
Train loss (w/o reg) on all data: 0.19248593
Test loss (w/o reg) on all data: 0.10943523
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.7392158e-05
Norm of the params: 15.407339
                Loss: fixed 205 labels. Loss 0.10944. Accuracy 0.978.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3311748
Train loss (w/o reg) on all data: 0.32479763
Test loss (w/o reg) on all data: 0.14637378
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4014706e-05
Norm of the params: 11.29351
              Random: fixed  30 labels. Loss 0.14637. Accuracy 0.993.
### Flips: 615, rs: 5, checks: 410
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15711989
Train loss (w/o reg) on all data: 0.14867303
Test loss (w/o reg) on all data: 0.0650885
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6763444e-06
Norm of the params: 12.997585
     Influence (LOO): fixed 314 labels. Loss 0.06509. Accuracy 0.998.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075067975
Train loss (w/o reg) on all data: 0.06087951
Test loss (w/o reg) on all data: 0.049556855
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.2293523e-06
Norm of the params: 16.845453
                Loss: fixed 410 labels. Loss 0.04956. Accuracy 0.987.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31790882
Train loss (w/o reg) on all data: 0.311736
Test loss (w/o reg) on all data: 0.1366065
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3846274e-05
Norm of the params: 11.111109
              Random: fixed  63 labels. Loss 0.13661. Accuracy 0.992.
### Flips: 615, rs: 5, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1068005
Train loss (w/o reg) on all data: 0.098735385
Test loss (w/o reg) on all data: 0.042379357
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1872163e-06
Norm of the params: 12.70048
     Influence (LOO): fixed 396 labels. Loss 0.04238. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042359037
Train loss (w/o reg) on all data: 0.0015771972
Test loss (w/o reg) on all data: 0.0031910567
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2077679e-07
Norm of the params: 7.2920594
                Loss: fixed 534 labels. Loss 0.00319. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30324617
Train loss (w/o reg) on all data: 0.29682654
Test loss (w/o reg) on all data: 0.12936178
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.1612844e-06
Norm of the params: 11.331035
              Random: fixed  95 labels. Loss 0.12936. Accuracy 0.992.
### Flips: 615, rs: 5, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07217111
Train loss (w/o reg) on all data: 0.06549305
Test loss (w/o reg) on all data: 0.026307143
Train acc on all data:  0.9783612934597617
Test acc on all data:   1.0
Norm of the mean of gradients: 7.382897e-06
Norm of the params: 11.556865
     Influence (LOO): fixed 447 labels. Loss 0.02631. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2305944e-08
Norm of the params: 6.092819
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28821886
Train loss (w/o reg) on all data: 0.2820473
Test loss (w/o reg) on all data: 0.11930741
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.4774781e-05
Norm of the params: 11.109952
              Random: fixed 126 labels. Loss 0.11931. Accuracy 0.993.
### Flips: 615, rs: 5, checks: 1025
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05407033
Train loss (w/o reg) on all data: 0.048088733
Test loss (w/o reg) on all data: 0.019420205
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1001488e-06
Norm of the params: 10.93764
     Influence (LOO): fixed 474 labels. Loss 0.01942. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.379369e-09
Norm of the params: 6.092819
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2726025
Train loss (w/o reg) on all data: 0.26599246
Test loss (w/o reg) on all data: 0.111865744
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1348676e-05
Norm of the params: 11.497865
              Random: fixed 156 labels. Loss 0.11187. Accuracy 0.996.
### Flips: 615, rs: 5, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044199664
Train loss (w/o reg) on all data: 0.03902321
Test loss (w/o reg) on all data: 0.015500589
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3357175e-06
Norm of the params: 10.174926
     Influence (LOO): fixed 488 labels. Loss 0.01550. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6576993e-08
Norm of the params: 6.0928307
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26445597
Train loss (w/o reg) on all data: 0.25730187
Test loss (w/o reg) on all data: 0.107214995
Train acc on all data:  0.9100413323608072
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2987606e-05
Norm of the params: 11.9616995
              Random: fixed 172 labels. Loss 0.10721. Accuracy 0.997.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33447415
Train loss (w/o reg) on all data: 0.3283726
Test loss (w/o reg) on all data: 0.15692225
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0088007e-05
Norm of the params: 11.04675
Flipped loss: 0.15692. Accuracy: 0.990
### Flips: 615, rs: 6, checks: 205
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22690855
Train loss (w/o reg) on all data: 0.21786338
Test loss (w/o reg) on all data: 0.10383255
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1576674e-05
Norm of the params: 13.450037
     Influence (LOO): fixed 187 labels. Loss 0.10383. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19240904
Train loss (w/o reg) on all data: 0.17878592
Test loss (w/o reg) on all data: 0.111384295
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.6323223e-05
Norm of the params: 16.506432
                Loss: fixed 205 labels. Loss 0.11138. Accuracy 0.970.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32380056
Train loss (w/o reg) on all data: 0.3174357
Test loss (w/o reg) on all data: 0.15087488
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.14966515e-05
Norm of the params: 11.2825985
              Random: fixed  26 labels. Loss 0.15087. Accuracy 0.989.
### Flips: 615, rs: 6, checks: 410
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15230246
Train loss (w/o reg) on all data: 0.14400843
Test loss (w/o reg) on all data: 0.06223128
Train acc on all data:  0.949914903963044
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9835513e-06
Norm of the params: 12.879469
     Influence (LOO): fixed 321 labels. Loss 0.06223. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0694585
Train loss (w/o reg) on all data: 0.053703908
Test loss (w/o reg) on all data: 0.04660283
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.7390495e-06
Norm of the params: 17.750828
                Loss: fixed 409 labels. Loss 0.04660. Accuracy 0.987.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31380144
Train loss (w/o reg) on all data: 0.30719924
Test loss (w/o reg) on all data: 0.14521669
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.4576262e-05
Norm of the params: 11.491033
              Random: fixed  48 labels. Loss 0.14522. Accuracy 0.991.
### Flips: 615, rs: 6, checks: 615
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10731936
Train loss (w/o reg) on all data: 0.09969661
Test loss (w/o reg) on all data: 0.04253197
Train acc on all data:  0.9659615852176027
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3217965e-05
Norm of the params: 12.347272
     Influence (LOO): fixed 391 labels. Loss 0.04253. Accuracy 1.000.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007021242
Train loss (w/o reg) on all data: 0.0031650781
Test loss (w/o reg) on all data: 0.0060908897
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7384764e-07
Norm of the params: 8.781986
                Loss: fixed 524 labels. Loss 0.00609. Accuracy 0.999.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30329275
Train loss (w/o reg) on all data: 0.29662195
Test loss (w/o reg) on all data: 0.13701041
Train acc on all data:  0.8901045465596887
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9966736e-05
Norm of the params: 11.550589
              Random: fixed  74 labels. Loss 0.13701. Accuracy 0.992.
### Flips: 615, rs: 6, checks: 820
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07273143
Train loss (w/o reg) on all data: 0.066261515
Test loss (w/o reg) on all data: 0.026726035
Train acc on all data:  0.9786044249939218
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6814043e-06
Norm of the params: 11.375338
     Influence (LOO): fixed 444 labels. Loss 0.02673. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003154451
Train loss (w/o reg) on all data: 0.0010668448
Test loss (w/o reg) on all data: 0.0025823025
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.539927e-08
Norm of the params: 6.461589
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29505143
Train loss (w/o reg) on all data: 0.28824303
Test loss (w/o reg) on all data: 0.13137908
Train acc on all data:  0.8944809141745684
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.731528e-05
Norm of the params: 11.669102
              Random: fixed  91 labels. Loss 0.13138. Accuracy 0.993.
### Flips: 615, rs: 6, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050894577
Train loss (w/o reg) on all data: 0.045142777
Test loss (w/o reg) on all data: 0.018754302
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1752096e-06
Norm of the params: 10.725484
     Influence (LOO): fixed 473 labels. Loss 0.01875. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668425
Test loss (w/o reg) on all data: 0.0025823326
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.255158e-08
Norm of the params: 6.4615917
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28402695
Train loss (w/o reg) on all data: 0.27743775
Test loss (w/o reg) on all data: 0.12749988
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.244803e-06
Norm of the params: 11.479725
              Random: fixed 114 labels. Loss 0.12750. Accuracy 0.990.
### Flips: 615, rs: 6, checks: 1230
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037529323
Train loss (w/o reg) on all data: 0.032470122
Test loss (w/o reg) on all data: 0.013567091
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0877222e-06
Norm of the params: 10.059026
     Influence (LOO): fixed 491 labels. Loss 0.01357. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544515
Train loss (w/o reg) on all data: 0.0010668356
Test loss (w/o reg) on all data: 0.0025823
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0715751e-08
Norm of the params: 6.461603
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2717511
Train loss (w/o reg) on all data: 0.2651425
Test loss (w/o reg) on all data: 0.1202672
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5539397e-05
Norm of the params: 11.496608
              Random: fixed 142 labels. Loss 0.12027. Accuracy 0.990.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34101433
Train loss (w/o reg) on all data: 0.33395815
Test loss (w/o reg) on all data: 0.16277082
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.5482703e-06
Norm of the params: 11.879552
Flipped loss: 0.16277. Accuracy: 0.984
### Flips: 615, rs: 7, checks: 205
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23931043
Train loss (w/o reg) on all data: 0.23024881
Test loss (w/o reg) on all data: 0.10701552
Train acc on all data:  0.912229516168247
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4016269e-05
Norm of the params: 13.462259
     Influence (LOO): fixed 181 labels. Loss 0.10702. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20469093
Train loss (w/o reg) on all data: 0.19115552
Test loss (w/o reg) on all data: 0.112433665
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7298722e-05
Norm of the params: 16.453215
                Loss: fixed 205 labels. Loss 0.11243. Accuracy 0.974.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32826465
Train loss (w/o reg) on all data: 0.32111385
Test loss (w/o reg) on all data: 0.14948942
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7118966e-05
Norm of the params: 11.95894
              Random: fixed  31 labels. Loss 0.14949. Accuracy 0.990.
### Flips: 615, rs: 7, checks: 410
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1651482
Train loss (w/o reg) on all data: 0.15652089
Test loss (w/o reg) on all data: 0.06879225
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.853387e-06
Norm of the params: 13.135684
     Influence (LOO): fixed 312 labels. Loss 0.06879. Accuracy 0.998.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08153202
Train loss (w/o reg) on all data: 0.06590396
Test loss (w/o reg) on all data: 0.051708326
Train acc on all data:  0.973984925844882
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4097282e-06
Norm of the params: 17.6794
                Loss: fixed 410 labels. Loss 0.05171. Accuracy 0.989.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31398162
Train loss (w/o reg) on all data: 0.30709884
Test loss (w/o reg) on all data: 0.14089887
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.2296592e-05
Norm of the params: 11.732684
              Random: fixed  67 labels. Loss 0.14090. Accuracy 0.990.
### Flips: 615, rs: 7, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12183614
Train loss (w/o reg) on all data: 0.11319653
Test loss (w/o reg) on all data: 0.050882075
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.837237e-06
Norm of the params: 13.145046
     Influence (LOO): fixed 379 labels. Loss 0.05088. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007933978
Train loss (w/o reg) on all data: 0.0036394557
Test loss (w/o reg) on all data: 0.004757282
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3405237e-07
Norm of the params: 9.267711
                Loss: fixed 542 labels. Loss 0.00476. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3026889
Train loss (w/o reg) on all data: 0.29571235
Test loss (w/o reg) on all data: 0.13493761
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.08484e-06
Norm of the params: 11.812313
              Random: fixed  93 labels. Loss 0.13494. Accuracy 0.991.
### Flips: 615, rs: 7, checks: 820
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081994735
Train loss (w/o reg) on all data: 0.07422335
Test loss (w/o reg) on all data: 0.031623535
Train acc on all data:  0.973984925844882
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5349741e-06
Norm of the params: 12.467067
     Influence (LOO): fixed 442 labels. Loss 0.03162. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004919571
Train loss (w/o reg) on all data: 0.0020054753
Test loss (w/o reg) on all data: 0.0038157054
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4802796e-07
Norm of the params: 7.6342597
                Loss: fixed 546 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2925633
Train loss (w/o reg) on all data: 0.28568
Test loss (w/o reg) on all data: 0.12773244
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.7143767e-06
Norm of the params: 11.733123
              Random: fixed 115 labels. Loss 0.12773. Accuracy 0.991.
### Flips: 615, rs: 7, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06219444
Train loss (w/o reg) on all data: 0.05518613
Test loss (w/o reg) on all data: 0.024378406
Train acc on all data:  0.9807926088013615
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8710087e-06
Norm of the params: 11.839183
     Influence (LOO): fixed 470 labels. Loss 0.02438. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034880317
Train loss (w/o reg) on all data: 0.0012594495
Test loss (w/o reg) on all data: 0.0030900189
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8978616e-08
Norm of the params: 6.6762
                Loss: fixed 548 labels. Loss 0.00309. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27838698
Train loss (w/o reg) on all data: 0.2713498
Test loss (w/o reg) on all data: 0.12009712
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0333648e-05
Norm of the params: 11.863557
              Random: fixed 145 labels. Loss 0.12010. Accuracy 0.993.
### Flips: 615, rs: 7, checks: 1230
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045068245
Train loss (w/o reg) on all data: 0.039155945
Test loss (w/o reg) on all data: 0.01663598
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 7.388899e-07
Norm of the params: 10.874099
     Influence (LOO): fixed 498 labels. Loss 0.01664. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4041215e-08
Norm of the params: 6.092816
                Loss: fixed 549 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26885986
Train loss (w/o reg) on all data: 0.2617855
Test loss (w/o reg) on all data: 0.11269669
Train acc on all data:  0.9093119377583273
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.9420083e-05
Norm of the params: 11.89484
              Random: fixed 165 labels. Loss 0.11270. Accuracy 0.996.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429344
Train loss (w/o reg) on all data: 0.33689013
Test loss (w/o reg) on all data: 0.15165615
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.495094e-05
Norm of the params: 10.994786
Flipped loss: 0.15166. Accuracy: 0.990
### Flips: 615, rs: 8, checks: 205
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24131513
Train loss (w/o reg) on all data: 0.23281527
Test loss (w/o reg) on all data: 0.09981807
Train acc on all data:  0.911986384634087
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3545557e-05
Norm of the params: 13.038301
     Influence (LOO): fixed 183 labels. Loss 0.09982. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20720322
Train loss (w/o reg) on all data: 0.19390602
Test loss (w/o reg) on all data: 0.09731652
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.1474e-06
Norm of the params: 16.30779
                Loss: fixed 205 labels. Loss 0.09732. Accuracy 0.983.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33048087
Train loss (w/o reg) on all data: 0.32423604
Test loss (w/o reg) on all data: 0.14352652
Train acc on all data:  0.8794067590566497
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.5228179e-05
Norm of the params: 11.175731
              Random: fixed  31 labels. Loss 0.14353. Accuracy 0.991.
### Flips: 615, rs: 8, checks: 410
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1633477
Train loss (w/o reg) on all data: 0.15482119
Test loss (w/o reg) on all data: 0.06333619
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7443885e-05
Norm of the params: 13.058732
     Influence (LOO): fixed 314 labels. Loss 0.06334. Accuracy 0.999.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07915159
Train loss (w/o reg) on all data: 0.063797824
Test loss (w/o reg) on all data: 0.037827466
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.876637e-06
Norm of the params: 17.523567
                Loss: fixed 410 labels. Loss 0.03783. Accuracy 0.993.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31940693
Train loss (w/o reg) on all data: 0.31329352
Test loss (w/o reg) on all data: 0.13560383
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1906862e-05
Norm of the params: 11.057496
              Random: fixed  55 labels. Loss 0.13560. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 615
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107862376
Train loss (w/o reg) on all data: 0.09968971
Test loss (w/o reg) on all data: 0.039873738
Train acc on all data:  0.9642596644784829
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3408805e-06
Norm of the params: 12.784889
     Influence (LOO): fixed 399 labels. Loss 0.03987. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060732057
Train loss (w/o reg) on all data: 0.0026994636
Test loss (w/o reg) on all data: 0.004156945
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7930765e-08
Norm of the params: 8.214308
                Loss: fixed 540 labels. Loss 0.00416. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30791476
Train loss (w/o reg) on all data: 0.3016696
Test loss (w/o reg) on all data: 0.12944888
Train acc on all data:  0.8888888888888888
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.8609807e-05
Norm of the params: 11.176012
              Random: fixed  79 labels. Loss 0.12945. Accuracy 0.993.
### Flips: 615, rs: 8, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0785246
Train loss (w/o reg) on all data: 0.071214214
Test loss (w/o reg) on all data: 0.027437948
Train acc on all data:  0.975443715049842
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1455849e-06
Norm of the params: 12.091637
     Influence (LOO): fixed 445 labels. Loss 0.02744. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034138595
Train loss (w/o reg) on all data: 0.0012052646
Test loss (w/o reg) on all data: 0.0026837494
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.032035e-08
Norm of the params: 6.646194
                Loss: fixed 545 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2976827
Train loss (w/o reg) on all data: 0.2915752
Test loss (w/o reg) on all data: 0.12314282
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.40597e-05
Norm of the params: 11.052156
              Random: fixed 102 labels. Loss 0.12314. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062050793
Train loss (w/o reg) on all data: 0.055550985
Test loss (w/o reg) on all data: 0.020519616
Train acc on all data:  0.9820082664721614
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7324206e-06
Norm of the params: 11.401587
     Influence (LOO): fixed 472 labels. Loss 0.02052. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034138598
Train loss (w/o reg) on all data: 0.001205258
Test loss (w/o reg) on all data: 0.0026837634
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.627949e-08
Norm of the params: 6.6462045
                Loss: fixed 545 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28218013
Train loss (w/o reg) on all data: 0.27579394
Test loss (w/o reg) on all data: 0.11471307
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0646105e-05
Norm of the params: 11.301503
              Random: fixed 136 labels. Loss 0.11471. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044938922
Train loss (w/o reg) on all data: 0.03963234
Test loss (w/o reg) on all data: 0.014521819
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7733847e-06
Norm of the params: 10.302021
     Influence (LOO): fixed 495 labels. Loss 0.01452. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096014637
Test loss (w/o reg) on all data: 0.0026561043
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0661328e-08
Norm of the params: 6.092772
                Loss: fixed 546 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26967987
Train loss (w/o reg) on all data: 0.2633853
Test loss (w/o reg) on all data: 0.106730774
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.286567e-06
Norm of the params: 11.22014
              Random: fixed 162 labels. Loss 0.10673. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33740157
Train loss (w/o reg) on all data: 0.3308191
Test loss (w/o reg) on all data: 0.1615944
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.6202388e-06
Norm of the params: 11.473851
Flipped loss: 0.16159. Accuracy: 0.986
### Flips: 615, rs: 9, checks: 205
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23501389
Train loss (w/o reg) on all data: 0.22586681
Test loss (w/o reg) on all data: 0.10538794
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.9334646e-06
Norm of the params: 13.525587
     Influence (LOO): fixed 185 labels. Loss 0.10539. Accuracy 0.994.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19994026
Train loss (w/o reg) on all data: 0.18707229
Test loss (w/o reg) on all data: 0.10827941
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.49579e-06
Norm of the params: 16.042427
                Loss: fixed 205 labels. Loss 0.10828. Accuracy 0.980.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3241165
Train loss (w/o reg) on all data: 0.31767675
Test loss (w/o reg) on all data: 0.15299566
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.386769e-06
Norm of the params: 11.348787
              Random: fixed  33 labels. Loss 0.15300. Accuracy 0.986.
### Flips: 615, rs: 9, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16345523
Train loss (w/o reg) on all data: 0.15484715
Test loss (w/o reg) on all data: 0.06365666
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.513778e-06
Norm of the params: 13.121048
     Influence (LOO): fixed 310 labels. Loss 0.06366. Accuracy 0.999.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07323243
Train loss (w/o reg) on all data: 0.05885264
Test loss (w/o reg) on all data: 0.04756346
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7662866e-06
Norm of the params: 16.958649
                Loss: fixed 409 labels. Loss 0.04756. Accuracy 0.988.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31309694
Train loss (w/o reg) on all data: 0.30677673
Test loss (w/o reg) on all data: 0.143411
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2228867e-05
Norm of the params: 11.242973
              Random: fixed  63 labels. Loss 0.14341. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10974635
Train loss (w/o reg) on all data: 0.1023246
Test loss (w/o reg) on all data: 0.038767308
Train acc on all data:  0.9662047167517627
Test acc on all data:   1.0
Norm of the mean of gradients: 2.773878e-06
Norm of the params: 12.183393
     Influence (LOO): fixed 396 labels. Loss 0.03877. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.0014175078
Test loss (w/o reg) on all data: 0.002808931
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6258316e-07
Norm of the params: 7.087825
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30135506
Train loss (w/o reg) on all data: 0.29508474
Test loss (w/o reg) on all data: 0.13680075
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9149922e-05
Norm of the params: 11.198509
              Random: fixed  89 labels. Loss 0.13680. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 820
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066076204
Train loss (w/o reg) on all data: 0.05987195
Test loss (w/o reg) on all data: 0.02124793
Train acc on all data:  0.9815220034038414
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1709083e-06
Norm of the params: 11.139347
     Influence (LOO): fixed 459 labels. Loss 0.02125. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.001417512
Test loss (w/o reg) on all data: 0.002808909
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8328684e-08
Norm of the params: 7.087819
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29093847
Train loss (w/o reg) on all data: 0.28450298
Test loss (w/o reg) on all data: 0.1305278
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.3712145e-06
Norm of the params: 11.345019
              Random: fixed 111 labels. Loss 0.13053. Accuracy 0.990.
### Flips: 615, rs: 9, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04971477
Train loss (w/o reg) on all data: 0.044257507
Test loss (w/o reg) on all data: 0.016225511
Train acc on all data:  0.987114028689521
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7538063e-06
Norm of the params: 10.447261
     Influence (LOO): fixed 482 labels. Loss 0.01623. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.0014175329
Test loss (w/o reg) on all data: 0.0028089269
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.757004e-07
Norm of the params: 7.0877895
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27697882
Train loss (w/o reg) on all data: 0.2705613
Test loss (w/o reg) on all data: 0.121636674
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.036602e-06
Norm of the params: 11.329173
              Random: fixed 142 labels. Loss 0.12164. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032916274
Train loss (w/o reg) on all data: 0.028354097
Test loss (w/o reg) on all data: 0.010706902
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 5.823562e-07
Norm of the params: 9.552148
     Influence (LOO): fixed 504 labels. Loss 0.01071. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039293715
Train loss (w/o reg) on all data: 0.0014175077
Test loss (w/o reg) on all data: 0.0028089308
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9934074e-08
Norm of the params: 7.0878263
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26461124
Train loss (w/o reg) on all data: 0.25806713
Test loss (w/o reg) on all data: 0.11546035
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.334003e-06
Norm of the params: 11.440388
              Random: fixed 166 labels. Loss 0.11546. Accuracy 0.991.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33747423
Train loss (w/o reg) on all data: 0.33033323
Test loss (w/o reg) on all data: 0.16476052
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.619424e-05
Norm of the params: 11.950719
Flipped loss: 0.16476. Accuracy: 0.988
### Flips: 615, rs: 10, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23304169
Train loss (w/o reg) on all data: 0.2233409
Test loss (w/o reg) on all data: 0.10336481
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3895398e-05
Norm of the params: 13.92896
     Influence (LOO): fixed 182 labels. Loss 0.10336. Accuracy 0.993.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19912578
Train loss (w/o reg) on all data: 0.185086
Test loss (w/o reg) on all data: 0.11177492
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.9347374e-06
Norm of the params: 16.75696
                Loss: fixed 205 labels. Loss 0.11177. Accuracy 0.975.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33000213
Train loss (w/o reg) on all data: 0.3227922
Test loss (w/o reg) on all data: 0.15842743
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3393626e-05
Norm of the params: 12.008265
              Random: fixed  20 labels. Loss 0.15843. Accuracy 0.987.
### Flips: 615, rs: 10, checks: 410
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16622515
Train loss (w/o reg) on all data: 0.15712999
Test loss (w/o reg) on all data: 0.06605921
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9348796e-06
Norm of the params: 13.487151
     Influence (LOO): fixed 308 labels. Loss 0.06606. Accuracy 0.999.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07984384
Train loss (w/o reg) on all data: 0.064516105
Test loss (w/o reg) on all data: 0.045853056
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.7719884e-06
Norm of the params: 17.508703
                Loss: fixed 410 labels. Loss 0.04585. Accuracy 0.990.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773818
Train loss (w/o reg) on all data: 0.31044716
Test loss (w/o reg) on all data: 0.15019618
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3172218e-05
Norm of the params: 12.0756035
              Random: fixed  50 labels. Loss 0.15020. Accuracy 0.986.
### Flips: 615, rs: 10, checks: 615
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11826869
Train loss (w/o reg) on all data: 0.110671155
Test loss (w/o reg) on all data: 0.045806296
Train acc on all data:  0.9613420860685631
Test acc on all data:   1.0
Norm of the mean of gradients: 7.770311e-06
Norm of the params: 12.32683
     Influence (LOO): fixed 388 labels. Loss 0.04581. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007558399
Train loss (w/o reg) on all data: 0.0033245042
Test loss (w/o reg) on all data: 0.0050483258
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.370461e-07
Norm of the params: 9.20206
                Loss: fixed 540 labels. Loss 0.00505. Accuracy 0.999.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30756018
Train loss (w/o reg) on all data: 0.29999128
Test loss (w/o reg) on all data: 0.14164647
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.5089893e-05
Norm of the params: 12.303564
              Random: fixed  75 labels. Loss 0.14165. Accuracy 0.988.
### Flips: 615, rs: 10, checks: 820
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081024565
Train loss (w/o reg) on all data: 0.07426411
Test loss (w/o reg) on all data: 0.027951015
Train acc on all data:  0.975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 3.365413e-06
Norm of the params: 11.627945
     Influence (LOO): fixed 448 labels. Loss 0.02795. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042529134
Train loss (w/o reg) on all data: 0.0015241543
Test loss (w/o reg) on all data: 0.0033314694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5620884e-08
Norm of the params: 7.3875012
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29855153
Train loss (w/o reg) on all data: 0.2909677
Test loss (w/o reg) on all data: 0.13489684
Train acc on all data:  0.8915633357646486
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.6056565e-05
Norm of the params: 12.315705
              Random: fixed  98 labels. Loss 0.13490. Accuracy 0.988.
### Flips: 615, rs: 10, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062426295
Train loss (w/o reg) on all data: 0.05589888
Test loss (w/o reg) on all data: 0.021498417
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 7.311947e-07
Norm of the params: 11.425776
     Influence (LOO): fixed 475 labels. Loss 0.02150. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004252912
Train loss (w/o reg) on all data: 0.0015241919
Test loss (w/o reg) on all data: 0.0033314896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3702349e-07
Norm of the params: 7.3874493
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2879135
Train loss (w/o reg) on all data: 0.28034687
Test loss (w/o reg) on all data: 0.12799105
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.743604e-06
Norm of the params: 12.301744
              Random: fixed 121 labels. Loss 0.12799. Accuracy 0.992.
### Flips: 615, rs: 10, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045580726
Train loss (w/o reg) on all data: 0.040113926
Test loss (w/o reg) on all data: 0.014228905
Train acc on all data:  0.9880865548261609
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1088715e-06
Norm of the params: 10.456387
     Influence (LOO): fixed 499 labels. Loss 0.01423. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004252913
Train loss (w/o reg) on all data: 0.0015241758
Test loss (w/o reg) on all data: 0.003331543
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6184397e-08
Norm of the params: 7.387472
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27407956
Train loss (w/o reg) on all data: 0.2664422
Test loss (w/o reg) on all data: 0.1193324
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2980977e-05
Norm of the params: 12.359081
              Random: fixed 152 labels. Loss 0.11933. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33602
Train loss (w/o reg) on all data: 0.32930556
Test loss (w/o reg) on all data: 0.16213647
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.781599e-05
Norm of the params: 11.588304
Flipped loss: 0.16214. Accuracy: 0.990
### Flips: 615, rs: 11, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23517461
Train loss (w/o reg) on all data: 0.22524333
Test loss (w/o reg) on all data: 0.107420675
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.8792754e-05
Norm of the params: 14.093456
     Influence (LOO): fixed 183 labels. Loss 0.10742. Accuracy 0.991.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19456926
Train loss (w/o reg) on all data: 0.17928629
Test loss (w/o reg) on all data: 0.11970432
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0106332e-05
Norm of the params: 17.483124
                Loss: fixed 205 labels. Loss 0.11970. Accuracy 0.980.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32856688
Train loss (w/o reg) on all data: 0.32213315
Test loss (w/o reg) on all data: 0.15728614
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.3980525e-06
Norm of the params: 11.343489
              Random: fixed  21 labels. Loss 0.15729. Accuracy 0.990.
### Flips: 615, rs: 11, checks: 410
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15761232
Train loss (w/o reg) on all data: 0.14950304
Test loss (w/o reg) on all data: 0.062773496
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6285065e-05
Norm of the params: 12.735217
     Influence (LOO): fixed 317 labels. Loss 0.06277. Accuracy 0.998.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07738518
Train loss (w/o reg) on all data: 0.061112314
Test loss (w/o reg) on all data: 0.04631072
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.2877737e-06
Norm of the params: 18.040434
                Loss: fixed 410 labels. Loss 0.04631. Accuracy 0.986.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32051104
Train loss (w/o reg) on all data: 0.31424394
Test loss (w/o reg) on all data: 0.15014438
Train acc on all data:  0.8830537320690494
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.168182e-06
Norm of the params: 11.195618
              Random: fixed  44 labels. Loss 0.15014. Accuracy 0.991.
### Flips: 615, rs: 11, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102725014
Train loss (w/o reg) on all data: 0.09536502
Test loss (w/o reg) on all data: 0.04037488
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4264487e-05
Norm of the params: 12.1326
     Influence (LOO): fixed 398 labels. Loss 0.04037. Accuracy 0.997.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006421295
Train loss (w/o reg) on all data: 0.0026698906
Test loss (w/o reg) on all data: 0.0037899665
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3327886e-07
Norm of the params: 8.661876
                Loss: fixed 528 labels. Loss 0.00379. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3121613
Train loss (w/o reg) on all data: 0.30570605
Test loss (w/o reg) on all data: 0.1439274
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9361745e-05
Norm of the params: 11.36243
              Random: fixed  64 labels. Loss 0.14393. Accuracy 0.991.
### Flips: 615, rs: 11, checks: 820
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07247735
Train loss (w/o reg) on all data: 0.06588779
Test loss (w/o reg) on all data: 0.026108567
Train acc on all data:  0.9778750303914417
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2003702e-06
Norm of the params: 11.480034
     Influence (LOO): fixed 443 labels. Loss 0.02611. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035537411
Train loss (w/o reg) on all data: 0.0013711051
Test loss (w/o reg) on all data: 0.0027915405
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.619278e-08
Norm of the params: 6.6070204
                Loss: fixed 533 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30157486
Train loss (w/o reg) on all data: 0.29525617
Test loss (w/o reg) on all data: 0.13680148
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.679104e-06
Norm of the params: 11.241619
              Random: fixed  89 labels. Loss 0.13680. Accuracy 0.988.
### Flips: 615, rs: 11, checks: 1025
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05300026
Train loss (w/o reg) on all data: 0.04721578
Test loss (w/o reg) on all data: 0.017531296
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9750275e-06
Norm of the params: 10.75591
     Influence (LOO): fixed 470 labels. Loss 0.01753. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5814564e-08
Norm of the params: 6.0928164
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28842866
Train loss (w/o reg) on all data: 0.28237382
Test loss (w/o reg) on all data: 0.12936419
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.8460196e-05
Norm of the params: 11.0044155
              Random: fixed 119 labels. Loss 0.12936. Accuracy 0.988.
### Flips: 615, rs: 11, checks: 1230
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03718611
Train loss (w/o reg) on all data: 0.031989273
Test loss (w/o reg) on all data: 0.0128347315
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6013178e-06
Norm of the params: 10.194937
     Influence (LOO): fixed 490 labels. Loss 0.01283. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9341009e-08
Norm of the params: 6.0928154
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26807418
Train loss (w/o reg) on all data: 0.26151428
Test loss (w/o reg) on all data: 0.11869936
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.422166e-06
Norm of the params: 11.454169
              Random: fixed 158 labels. Loss 0.11870. Accuracy 0.985.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33764744
Train loss (w/o reg) on all data: 0.3309865
Test loss (w/o reg) on all data: 0.15733463
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3786952e-05
Norm of the params: 11.542056
Flipped loss: 0.15733. Accuracy: 0.990
### Flips: 615, rs: 12, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23384628
Train loss (w/o reg) on all data: 0.22448976
Test loss (w/o reg) on all data: 0.102763064
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.5176275e-05
Norm of the params: 13.679558
     Influence (LOO): fixed 184 labels. Loss 0.10276. Accuracy 0.996.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19840373
Train loss (w/o reg) on all data: 0.18482576
Test loss (w/o reg) on all data: 0.10849266
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3840347e-05
Norm of the params: 16.479057
                Loss: fixed 205 labels. Loss 0.10849. Accuracy 0.975.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32816014
Train loss (w/o reg) on all data: 0.32142478
Test loss (w/o reg) on all data: 0.1531395
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2830679e-05
Norm of the params: 11.606345
              Random: fixed  24 labels. Loss 0.15314. Accuracy 0.987.
### Flips: 615, rs: 12, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1589643
Train loss (w/o reg) on all data: 0.15081458
Test loss (w/o reg) on all data: 0.06710129
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.0799102e-06
Norm of the params: 12.766933
     Influence (LOO): fixed 313 labels. Loss 0.06710. Accuracy 0.997.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070981674
Train loss (w/o reg) on all data: 0.05554677
Test loss (w/o reg) on all data: 0.0533823
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.93763e-07
Norm of the params: 17.569807
                Loss: fixed 410 labels. Loss 0.05338. Accuracy 0.987.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31358033
Train loss (w/o reg) on all data: 0.30696607
Test loss (w/o reg) on all data: 0.14360094
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.8038357e-06
Norm of the params: 11.50153
              Random: fixed  58 labels. Loss 0.14360. Accuracy 0.989.
### Flips: 615, rs: 12, checks: 615
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10186631
Train loss (w/o reg) on all data: 0.09457442
Test loss (w/o reg) on all data: 0.041372065
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.0026557e-06
Norm of the params: 12.076335
     Influence (LOO): fixed 402 labels. Loss 0.04137. Accuracy 0.999.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006988569
Train loss (w/o reg) on all data: 0.0028981694
Test loss (w/o reg) on all data: 0.0047643282
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.179355e-07
Norm of the params: 9.044777
                Loss: fixed 526 labels. Loss 0.00476. Accuracy 0.998.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30318323
Train loss (w/o reg) on all data: 0.29663742
Test loss (w/o reg) on all data: 0.13522732
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.305301e-06
Norm of the params: 11.441869
              Random: fixed  84 labels. Loss 0.13523. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 820
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06749539
Train loss (w/o reg) on all data: 0.061264694
Test loss (w/o reg) on all data: 0.026820375
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5282736e-06
Norm of the params: 11.16306
     Influence (LOO): fixed 449 labels. Loss 0.02682. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004666795
Train loss (w/o reg) on all data: 0.0017106821
Test loss (w/o reg) on all data: 0.0038554717
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.8520577e-08
Norm of the params: 7.6891003
                Loss: fixed 531 labels. Loss 0.00386. Accuracy 0.998.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29374596
Train loss (w/o reg) on all data: 0.28709352
Test loss (w/o reg) on all data: 0.12754185
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8052506e-05
Norm of the params: 11.534679
              Random: fixed 107 labels. Loss 0.12754. Accuracy 0.993.
### Flips: 615, rs: 12, checks: 1025
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053672872
Train loss (w/o reg) on all data: 0.0479186
Test loss (w/o reg) on all data: 0.02143104
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 6.716256e-07
Norm of the params: 10.727791
     Influence (LOO): fixed 469 labels. Loss 0.02143. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003996824
Train loss (w/o reg) on all data: 0.001390273
Test loss (w/o reg) on all data: 0.0035521523
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.9618854e-08
Norm of the params: 7.2201815
                Loss: fixed 532 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27977067
Train loss (w/o reg) on all data: 0.27290675
Test loss (w/o reg) on all data: 0.119243726
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3480245e-05
Norm of the params: 11.716579
              Random: fixed 137 labels. Loss 0.11924. Accuracy 0.994.
### Flips: 615, rs: 12, checks: 1230
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034621477
Train loss (w/o reg) on all data: 0.029400378
Test loss (w/o reg) on all data: 0.015622249
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.804935e-07
Norm of the params: 10.218707
     Influence (LOO): fixed 494 labels. Loss 0.01562. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039968244
Train loss (w/o reg) on all data: 0.0013902573
Test loss (w/o reg) on all data: 0.0035521109
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.374731e-08
Norm of the params: 7.2202034
                Loss: fixed 532 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26052397
Train loss (w/o reg) on all data: 0.25342807
Test loss (w/o reg) on all data: 0.10919017
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.46695e-06
Norm of the params: 11.912945
              Random: fixed 174 labels. Loss 0.10919. Accuracy 0.995.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33703798
Train loss (w/o reg) on all data: 0.33065543
Test loss (w/o reg) on all data: 0.15362696
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.157371e-05
Norm of the params: 11.298283
Flipped loss: 0.15363. Accuracy: 0.988
### Flips: 615, rs: 13, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23171887
Train loss (w/o reg) on all data: 0.22281282
Test loss (w/o reg) on all data: 0.100726135
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.495483e-06
Norm of the params: 13.346199
     Influence (LOO): fixed 188 labels. Loss 0.10073. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19851242
Train loss (w/o reg) on all data: 0.18513484
Test loss (w/o reg) on all data: 0.097457446
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.8934243e-06
Norm of the params: 16.356998
                Loss: fixed 205 labels. Loss 0.09746. Accuracy 0.984.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33004013
Train loss (w/o reg) on all data: 0.32381582
Test loss (w/o reg) on all data: 0.14760171
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.1151012e-05
Norm of the params: 11.157337
              Random: fixed  20 labels. Loss 0.14760. Accuracy 0.991.
### Flips: 615, rs: 13, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16479282
Train loss (w/o reg) on all data: 0.15547653
Test loss (w/o reg) on all data: 0.06469002
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1293074e-05
Norm of the params: 13.650124
     Influence (LOO): fixed 305 labels. Loss 0.06469. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073333964
Train loss (w/o reg) on all data: 0.0584549
Test loss (w/o reg) on all data: 0.04158291
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.9643523e-06
Norm of the params: 17.250542
                Loss: fixed 409 labels. Loss 0.04158. Accuracy 0.988.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32027397
Train loss (w/o reg) on all data: 0.31413358
Test loss (w/o reg) on all data: 0.13852265
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.1936011e-05
Norm of the params: 11.081854
              Random: fixed  45 labels. Loss 0.13852. Accuracy 0.993.
### Flips: 615, rs: 13, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10804022
Train loss (w/o reg) on all data: 0.09988662
Test loss (w/o reg) on all data: 0.04046205
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.167356e-06
Norm of the params: 12.769969
     Influence (LOO): fixed 394 labels. Loss 0.04046. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072600264
Train loss (w/o reg) on all data: 0.003297832
Test loss (w/o reg) on all data: 0.004537911
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.336496e-07
Norm of the params: 8.901904
                Loss: fixed 534 labels. Loss 0.00454. Accuracy 0.999.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31295934
Train loss (w/o reg) on all data: 0.30662596
Test loss (w/o reg) on all data: 0.13291644
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.4696102e-05
Norm of the params: 11.254679
              Random: fixed  64 labels. Loss 0.13292. Accuracy 0.992.
### Flips: 615, rs: 13, checks: 820
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07819278
Train loss (w/o reg) on all data: 0.070718184
Test loss (w/o reg) on all data: 0.027779756
Train acc on all data:  0.975929978118162
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8088026e-06
Norm of the params: 12.226689
     Influence (LOO): fixed 442 labels. Loss 0.02778. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003511866
Train loss (w/o reg) on all data: 0.0012997665
Test loss (w/o reg) on all data: 0.0039346856
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3607392e-08
Norm of the params: 6.6514654
                Loss: fixed 539 labels. Loss 0.00393. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30012465
Train loss (w/o reg) on all data: 0.29375145
Test loss (w/o reg) on all data: 0.12834126
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.858791e-06
Norm of the params: 11.289981
              Random: fixed  92 labels. Loss 0.12834. Accuracy 0.991.
### Flips: 615, rs: 13, checks: 1025
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057019383
Train loss (w/o reg) on all data: 0.05076141
Test loss (w/o reg) on all data: 0.019070037
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6886931e-06
Norm of the params: 11.1874695
     Influence (LOO): fixed 473 labels. Loss 0.01907. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215597
Train loss (w/o reg) on all data: 0.001109751
Test loss (w/o reg) on all data: 0.0036361793
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.4872237e-08
Norm of the params: 6.183541
                Loss: fixed 540 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29107487
Train loss (w/o reg) on all data: 0.28464162
Test loss (w/o reg) on all data: 0.12194792
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.7663183e-06
Norm of the params: 11.343053
              Random: fixed 113 labels. Loss 0.12195. Accuracy 0.992.
### Flips: 615, rs: 13, checks: 1230
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04604314
Train loss (w/o reg) on all data: 0.04047502
Test loss (w/o reg) on all data: 0.014671665
Train acc on all data:  0.987357160223681
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5778846e-07
Norm of the params: 10.55284
     Influence (LOO): fixed 489 labels. Loss 0.01467. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601296
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2356279e-08
Norm of the params: 6.0927997
                Loss: fixed 541 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27669597
Train loss (w/o reg) on all data: 0.2701841
Test loss (w/o reg) on all data: 0.11262903
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8759762e-05
Norm of the params: 11.412145
              Random: fixed 144 labels. Loss 0.11263. Accuracy 0.995.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33768785
Train loss (w/o reg) on all data: 0.33079618
Test loss (w/o reg) on all data: 0.1632368
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.937084e-06
Norm of the params: 11.740246
Flipped loss: 0.16324. Accuracy: 0.987
### Flips: 615, rs: 14, checks: 205
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23496689
Train loss (w/o reg) on all data: 0.22568665
Test loss (w/o reg) on all data: 0.10445234
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.030714e-05
Norm of the params: 13.623684
     Influence (LOO): fixed 180 labels. Loss 0.10445. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19864033
Train loss (w/o reg) on all data: 0.18468784
Test loss (w/o reg) on all data: 0.11491594
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.6892654e-05
Norm of the params: 16.704786
                Loss: fixed 205 labels. Loss 0.11492. Accuracy 0.969.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3263919
Train loss (w/o reg) on all data: 0.31944168
Test loss (w/o reg) on all data: 0.15398547
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.5279027e-05
Norm of the params: 11.790016
              Random: fixed  26 labels. Loss 0.15399. Accuracy 0.989.
### Flips: 615, rs: 14, checks: 410
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16490752
Train loss (w/o reg) on all data: 0.15695468
Test loss (w/o reg) on all data: 0.06786306
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7972677e-05
Norm of the params: 12.611767
     Influence (LOO): fixed 307 labels. Loss 0.06786. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07661757
Train loss (w/o reg) on all data: 0.06118278
Test loss (w/o reg) on all data: 0.04125119
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5460995e-06
Norm of the params: 17.569742
                Loss: fixed 410 labels. Loss 0.04125. Accuracy 0.987.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31877118
Train loss (w/o reg) on all data: 0.3117244
Test loss (w/o reg) on all data: 0.1500281
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.9702045e-05
Norm of the params: 11.87164
              Random: fixed  45 labels. Loss 0.15003. Accuracy 0.988.
### Flips: 615, rs: 14, checks: 615
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111607075
Train loss (w/o reg) on all data: 0.10454148
Test loss (w/o reg) on all data: 0.044830102
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.041829e-06
Norm of the params: 11.887469
     Influence (LOO): fixed 390 labels. Loss 0.04483. Accuracy 0.996.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075130844
Train loss (w/o reg) on all data: 0.0034559944
Test loss (w/o reg) on all data: 0.0038887858
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2879254e-07
Norm of the params: 9.0078745
                Loss: fixed 530 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30722526
Train loss (w/o reg) on all data: 0.30026403
Test loss (w/o reg) on all data: 0.14206181
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.821843e-06
Norm of the params: 11.799351
              Random: fixed  73 labels. Loss 0.14206. Accuracy 0.989.
### Flips: 615, rs: 14, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07295436
Train loss (w/o reg) on all data: 0.06709942
Test loss (w/o reg) on all data: 0.02596923
Train acc on all data:  0.9776318988572817
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3901007e-06
Norm of the params: 10.821216
     Influence (LOO): fixed 449 labels. Loss 0.02597. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004610222
Train loss (w/o reg) on all data: 0.0017119988
Test loss (w/o reg) on all data: 0.003161586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8363746e-08
Norm of the params: 7.61344
                Loss: fixed 537 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299744
Train loss (w/o reg) on all data: 0.2930439
Test loss (w/o reg) on all data: 0.13566147
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.5807786e-05
Norm of the params: 11.57593
              Random: fixed  93 labels. Loss 0.13566. Accuracy 0.991.
### Flips: 615, rs: 14, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049576145
Train loss (w/o reg) on all data: 0.044578128
Test loss (w/o reg) on all data: 0.01697398
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 7.600601e-07
Norm of the params: 9.998015
     Influence (LOO): fixed 484 labels. Loss 0.01697. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960136
Test loss (w/o reg) on all data: 0.0026560922
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1061505e-08
Norm of the params: 6.092789
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2882123
Train loss (w/o reg) on all data: 0.28148308
Test loss (w/o reg) on all data: 0.12619701
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.526487e-06
Norm of the params: 11.6010475
              Random: fixed 119 labels. Loss 0.12620. Accuracy 0.988.
### Flips: 615, rs: 14, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037672527
Train loss (w/o reg) on all data: 0.033378318
Test loss (w/o reg) on all data: 0.012335105
Train acc on all data:  0.9902747386336008
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5337665e-06
Norm of the params: 9.26737
     Influence (LOO): fixed 501 labels. Loss 0.01234. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.332963e-09
Norm of the params: 6.0928173
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27610973
Train loss (w/o reg) on all data: 0.2694531
Test loss (w/o reg) on all data: 0.11940187
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.846718e-05
Norm of the params: 11.538303
              Random: fixed 144 labels. Loss 0.11940. Accuracy 0.989.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33303282
Train loss (w/o reg) on all data: 0.3262061
Test loss (w/o reg) on all data: 0.17194563
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3128691e-05
Norm of the params: 11.684788
Flipped loss: 0.17195. Accuracy: 0.983
### Flips: 615, rs: 15, checks: 205
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23158617
Train loss (w/o reg) on all data: 0.22247419
Test loss (w/o reg) on all data: 0.11106449
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.859771e-06
Norm of the params: 13.499615
     Influence (LOO): fixed 183 labels. Loss 0.11106. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1942893
Train loss (w/o reg) on all data: 0.18047835
Test loss (w/o reg) on all data: 0.122700945
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 5.021179e-06
Norm of the params: 16.61984
                Loss: fixed 205 labels. Loss 0.12270. Accuracy 0.973.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3190406
Train loss (w/o reg) on all data: 0.31212664
Test loss (w/o reg) on all data: 0.16004333
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.320637e-06
Norm of the params: 11.759227
              Random: fixed  34 labels. Loss 0.16004. Accuracy 0.986.
### Flips: 615, rs: 15, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1608911
Train loss (w/o reg) on all data: 0.15192126
Test loss (w/o reg) on all data: 0.072631516
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.151665e-06
Norm of the params: 13.393911
     Influence (LOO): fixed 307 labels. Loss 0.07263. Accuracy 0.995.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07213664
Train loss (w/o reg) on all data: 0.057067018
Test loss (w/o reg) on all data: 0.06338015
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.5394638e-06
Norm of the params: 17.360657
                Loss: fixed 408 labels. Loss 0.06338. Accuracy 0.982.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30555782
Train loss (w/o reg) on all data: 0.29843232
Test loss (w/o reg) on all data: 0.15294088
Train acc on all data:  0.8901045465596887
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.8669825e-06
Norm of the params: 11.937755
              Random: fixed  64 labels. Loss 0.15294. Accuracy 0.985.
### Flips: 615, rs: 15, checks: 615
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10314862
Train loss (w/o reg) on all data: 0.095165394
Test loss (w/o reg) on all data: 0.043404456
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.217125e-06
Norm of the params: 12.635838
     Influence (LOO): fixed 397 labels. Loss 0.04340. Accuracy 0.997.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00924946
Train loss (w/o reg) on all data: 0.0045381472
Test loss (w/o reg) on all data: 0.007514388
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3218628e-07
Norm of the params: 9.707022
                Loss: fixed 521 labels. Loss 0.00751. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29069284
Train loss (w/o reg) on all data: 0.28324932
Test loss (w/o reg) on all data: 0.1445922
Train acc on all data:  0.8966690979820082
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3653266e-05
Norm of the params: 12.201245
              Random: fixed  97 labels. Loss 0.14459. Accuracy 0.986.
### Flips: 615, rs: 15, checks: 820
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07104369
Train loss (w/o reg) on all data: 0.064408064
Test loss (w/o reg) on all data: 0.028208338
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9153863e-06
Norm of the params: 11.520092
     Influence (LOO): fixed 445 labels. Loss 0.02821. Accuracy 0.998.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00734567
Train loss (w/o reg) on all data: 0.0032882649
Test loss (w/o reg) on all data: 0.006636322
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2304258e-07
Norm of the params: 9.008224
                Loss: fixed 524 labels. Loss 0.00664. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28077945
Train loss (w/o reg) on all data: 0.2733745
Test loss (w/o reg) on all data: 0.13017789
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4160134e-05
Norm of the params: 12.1696005
              Random: fixed 122 labels. Loss 0.13018. Accuracy 0.990.
### Flips: 615, rs: 15, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055733062
Train loss (w/o reg) on all data: 0.050058
Test loss (w/o reg) on all data: 0.023106586
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1713836e-06
Norm of the params: 10.653695
     Influence (LOO): fixed 468 labels. Loss 0.02311. Accuracy 0.998.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065517477
Train loss (w/o reg) on all data: 0.0028484508
Test loss (w/o reg) on all data: 0.005822948
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5745862e-07
Norm of the params: 8.606157
                Loss: fixed 526 labels. Loss 0.00582. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26775393
Train loss (w/o reg) on all data: 0.26018158
Test loss (w/o reg) on all data: 0.1249536
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.626674e-06
Norm of the params: 12.306373
              Random: fixed 149 labels. Loss 0.12495. Accuracy 0.989.
### Flips: 615, rs: 15, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035287082
Train loss (w/o reg) on all data: 0.030619495
Test loss (w/o reg) on all data: 0.012412467
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4930298e-06
Norm of the params: 9.66187
     Influence (LOO): fixed 497 labels. Loss 0.01241. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005127207
Train loss (w/o reg) on all data: 0.0020589768
Test loss (w/o reg) on all data: 0.004966825
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.404236e-08
Norm of the params: 7.8335557
                Loss: fixed 529 labels. Loss 0.00497. Accuracy 0.999.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25403413
Train loss (w/o reg) on all data: 0.24645522
Test loss (w/o reg) on all data: 0.11869514
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1298447e-05
Norm of the params: 12.311711
              Random: fixed 176 labels. Loss 0.11870. Accuracy 0.987.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33379048
Train loss (w/o reg) on all data: 0.327521
Test loss (w/o reg) on all data: 0.1620687
Train acc on all data:  0.8735716022368101
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.3395679e-05
Norm of the params: 11.197754
Flipped loss: 0.16207. Accuracy: 0.983
### Flips: 615, rs: 16, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2265385
Train loss (w/o reg) on all data: 0.21731474
Test loss (w/o reg) on all data: 0.10056039
Train acc on all data:  0.9168490153172867
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.2666737e-06
Norm of the params: 13.582163
     Influence (LOO): fixed 184 labels. Loss 0.10056. Accuracy 0.993.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19096643
Train loss (w/o reg) on all data: 0.17701094
Test loss (w/o reg) on all data: 0.105903275
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.0263612e-06
Norm of the params: 16.706581
                Loss: fixed 205 labels. Loss 0.10590. Accuracy 0.977.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3203196
Train loss (w/o reg) on all data: 0.31396458
Test loss (w/o reg) on all data: 0.15590416
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.3975279e-05
Norm of the params: 11.27387
              Random: fixed  32 labels. Loss 0.15590. Accuracy 0.983.
### Flips: 615, rs: 16, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15033814
Train loss (w/o reg) on all data: 0.14187424
Test loss (w/o reg) on all data: 0.059691984
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2675351e-05
Norm of the params: 13.010693
     Influence (LOO): fixed 319 labels. Loss 0.05969. Accuracy 0.999.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071177475
Train loss (w/o reg) on all data: 0.05597136
Test loss (w/o reg) on all data: 0.04040988
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9574664e-06
Norm of the params: 17.4391
                Loss: fixed 408 labels. Loss 0.04041. Accuracy 0.991.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3090745
Train loss (w/o reg) on all data: 0.30232018
Test loss (w/o reg) on all data: 0.1479294
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.02325685e-05
Norm of the params: 11.622659
              Random: fixed  55 labels. Loss 0.14793. Accuracy 0.983.
### Flips: 615, rs: 16, checks: 615
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10342184
Train loss (w/o reg) on all data: 0.09533429
Test loss (w/o reg) on all data: 0.039411932
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.765873e-05
Norm of the params: 12.718134
     Influence (LOO): fixed 392 labels. Loss 0.03941. Accuracy 0.999.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004425027
Train loss (w/o reg) on all data: 0.0019103814
Test loss (w/o reg) on all data: 0.0035283754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.987114e-08
Norm of the params: 7.0917497
                Loss: fixed 527 labels. Loss 0.00353. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29168895
Train loss (w/o reg) on all data: 0.28431603
Test loss (w/o reg) on all data: 0.13664319
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.173649e-05
Norm of the params: 12.143236
              Random: fixed  89 labels. Loss 0.13664. Accuracy 0.986.
### Flips: 615, rs: 16, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07203813
Train loss (w/o reg) on all data: 0.06524244
Test loss (w/o reg) on all data: 0.02550885
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8778181e-06
Norm of the params: 11.65821
     Influence (LOO): fixed 439 labels. Loss 0.02551. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032012095
Train loss (w/o reg) on all data: 0.001221371
Test loss (w/o reg) on all data: 0.0028971236
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5252055e-08
Norm of the params: 6.2925963
                Loss: fixed 531 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2801157
Train loss (w/o reg) on all data: 0.2726145
Test loss (w/o reg) on all data: 0.12887733
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.7569276e-05
Norm of the params: 12.248417
              Random: fixed 116 labels. Loss 0.12888. Accuracy 0.988.
### Flips: 615, rs: 16, checks: 1025
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056244288
Train loss (w/o reg) on all data: 0.04994909
Test loss (w/o reg) on all data: 0.020310257
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8664001e-06
Norm of the params: 11.220692
     Influence (LOO): fixed 463 labels. Loss 0.02031. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00320121
Train loss (w/o reg) on all data: 0.0012213698
Test loss (w/o reg) on all data: 0.0028971103
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5959507e-08
Norm of the params: 6.2925987
                Loss: fixed 531 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26809254
Train loss (w/o reg) on all data: 0.2605253
Test loss (w/o reg) on all data: 0.12066508
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.127087e-06
Norm of the params: 12.302233
              Random: fixed 141 labels. Loss 0.12067. Accuracy 0.987.
### Flips: 615, rs: 16, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039434772
Train loss (w/o reg) on all data: 0.033689264
Test loss (w/o reg) on all data: 0.013098655
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0347752e-06
Norm of the params: 10.719615
     Influence (LOO): fixed 487 labels. Loss 0.01310. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4997984e-08
Norm of the params: 6.0928054
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273895
Train loss (w/o reg) on all data: 0.24500026
Test loss (w/o reg) on all data: 0.11095574
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2555518e-05
Norm of the params: 12.440816
              Random: fixed 170 labels. Loss 0.11096. Accuracy 0.990.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33850026
Train loss (w/o reg) on all data: 0.3323153
Test loss (w/o reg) on all data: 0.15787145
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.3828742e-05
Norm of the params: 11.122024
Flipped loss: 0.15787. Accuracy: 0.990
### Flips: 615, rs: 17, checks: 205
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23184252
Train loss (w/o reg) on all data: 0.2231594
Test loss (w/o reg) on all data: 0.09515411
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.4269744e-06
Norm of the params: 13.178103
     Influence (LOO): fixed 186 labels. Loss 0.09515. Accuracy 0.999.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19894369
Train loss (w/o reg) on all data: 0.18567225
Test loss (w/o reg) on all data: 0.10754737
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.6904961e-05
Norm of the params: 16.291983
                Loss: fixed 205 labels. Loss 0.10755. Accuracy 0.982.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32893878
Train loss (w/o reg) on all data: 0.32264072
Test loss (w/o reg) on all data: 0.1501017
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.334263e-06
Norm of the params: 11.223253
              Random: fixed  25 labels. Loss 0.15010. Accuracy 0.991.
### Flips: 615, rs: 17, checks: 410
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15008165
Train loss (w/o reg) on all data: 0.14116906
Test loss (w/o reg) on all data: 0.05542405
Train acc on all data:  0.9506442985655239
Test acc on all data:   1.0
Norm of the mean of gradients: 1.560064e-05
Norm of the params: 13.351097
     Influence (LOO): fixed 323 labels. Loss 0.05542. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06915875
Train loss (w/o reg) on all data: 0.053279612
Test loss (w/o reg) on all data: 0.04277585
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4827729e-06
Norm of the params: 17.820852
                Loss: fixed 408 labels. Loss 0.04278. Accuracy 0.990.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3156176
Train loss (w/o reg) on all data: 0.3092732
Test loss (w/o reg) on all data: 0.14026362
Train acc on all data:  0.8862144420131292
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2096815e-05
Norm of the params: 11.264436
              Random: fixed  55 labels. Loss 0.14026. Accuracy 0.989.
### Flips: 615, rs: 17, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10109784
Train loss (w/o reg) on all data: 0.09318349
Test loss (w/o reg) on all data: 0.036329124
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 5.679918e-06
Norm of the params: 12.581216
     Influence (LOO): fixed 398 labels. Loss 0.03633. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003544506
Train loss (w/o reg) on all data: 0.0013002635
Test loss (w/o reg) on all data: 0.0036189742
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9307256e-08
Norm of the params: 6.699616
                Loss: fixed 524 labels. Loss 0.00362. Accuracy 0.999.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3045128
Train loss (w/o reg) on all data: 0.29803002
Test loss (w/o reg) on all data: 0.13315937
Train acc on all data:  0.8910770726963287
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.72108e-06
Norm of the params: 11.386643
              Random: fixed  79 labels. Loss 0.13316. Accuracy 0.990.
### Flips: 615, rs: 17, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06993364
Train loss (w/o reg) on all data: 0.06359604
Test loss (w/o reg) on all data: 0.023197219
Train acc on all data:  0.9803063457330415
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5146888e-06
Norm of the params: 11.25842
     Influence (LOO): fixed 445 labels. Loss 0.02320. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600708
Test loss (w/o reg) on all data: 0.0026560354
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.2932424e-08
Norm of the params: 6.092897
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29459712
Train loss (w/o reg) on all data: 0.28809708
Test loss (w/o reg) on all data: 0.12836885
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.628023e-06
Norm of the params: 11.401791
              Random: fixed  98 labels. Loss 0.12837. Accuracy 0.988.
### Flips: 615, rs: 17, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050272085
Train loss (w/o reg) on all data: 0.04464968
Test loss (w/o reg) on all data: 0.017095827
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 1.185375e-06
Norm of the params: 10.6041565
     Influence (LOO): fixed 469 labels. Loss 0.01710. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0510473e-08
Norm of the params: 6.0928097
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27962744
Train loss (w/o reg) on all data: 0.27268052
Test loss (w/o reg) on all data: 0.121135876
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.4607642e-05
Norm of the params: 11.787204
              Random: fixed 127 labels. Loss 0.12114. Accuracy 0.988.
### Flips: 615, rs: 17, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037682302
Train loss (w/o reg) on all data: 0.03262742
Test loss (w/o reg) on all data: 0.012275431
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 8.549938e-07
Norm of the params: 10.054733
     Influence (LOO): fixed 487 labels. Loss 0.01228. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4435426e-08
Norm of the params: 6.0928144
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27207914
Train loss (w/o reg) on all data: 0.26509464
Test loss (w/o reg) on all data: 0.11520998
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.318893e-06
Norm of the params: 11.81906
              Random: fixed 144 labels. Loss 0.11521. Accuracy 0.991.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33699888
Train loss (w/o reg) on all data: 0.33086208
Test loss (w/o reg) on all data: 0.16531323
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7585204e-05
Norm of the params: 11.078641
Flipped loss: 0.16531. Accuracy: 0.983
### Flips: 615, rs: 18, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23177528
Train loss (w/o reg) on all data: 0.22197114
Test loss (w/o reg) on all data: 0.10788709
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3026875e-05
Norm of the params: 14.002956
     Influence (LOO): fixed 184 labels. Loss 0.10789. Accuracy 0.988.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19540927
Train loss (w/o reg) on all data: 0.18231763
Test loss (w/o reg) on all data: 0.115252785
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.9677314e-05
Norm of the params: 16.18125
                Loss: fixed 205 labels. Loss 0.11525. Accuracy 0.971.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32644677
Train loss (w/o reg) on all data: 0.31995124
Test loss (w/o reg) on all data: 0.15865195
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.6941285e-06
Norm of the params: 11.397845
              Random: fixed  25 labels. Loss 0.15865. Accuracy 0.985.
### Flips: 615, rs: 18, checks: 410
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15930244
Train loss (w/o reg) on all data: 0.1508453
Test loss (w/o reg) on all data: 0.05798429
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.167615e-06
Norm of the params: 13.005495
     Influence (LOO): fixed 315 labels. Loss 0.05798. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072587565
Train loss (w/o reg) on all data: 0.056881245
Test loss (w/o reg) on all data: 0.05339148
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.6122806e-06
Norm of the params: 17.723612
                Loss: fixed 410 labels. Loss 0.05339. Accuracy 0.983.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31732208
Train loss (w/o reg) on all data: 0.3111863
Test loss (w/o reg) on all data: 0.15129551
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.3614728e-05
Norm of the params: 11.077681
              Random: fixed  51 labels. Loss 0.15130. Accuracy 0.984.
### Flips: 615, rs: 18, checks: 615
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10416862
Train loss (w/o reg) on all data: 0.09642792
Test loss (w/o reg) on all data: 0.03680615
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6508009e-06
Norm of the params: 12.442435
     Influence (LOO): fixed 402 labels. Loss 0.03681. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006302654
Train loss (w/o reg) on all data: 0.0025934363
Test loss (w/o reg) on all data: 0.006288752
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.468144e-08
Norm of the params: 8.613033
                Loss: fixed 530 labels. Loss 0.00629. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3044608
Train loss (w/o reg) on all data: 0.29826918
Test loss (w/o reg) on all data: 0.14034511
Train acc on all data:  0.8910770726963287
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.136659e-05
Norm of the params: 11.127993
              Random: fixed  80 labels. Loss 0.14035. Accuracy 0.988.
### Flips: 615, rs: 18, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06841693
Train loss (w/o reg) on all data: 0.06118841
Test loss (w/o reg) on all data: 0.025248712
Train acc on all data:  0.9790906880622416
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6613009e-06
Norm of the params: 12.023744
     Influence (LOO): fixed 451 labels. Loss 0.02525. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039045964
Train loss (w/o reg) on all data: 0.0014013131
Test loss (w/o reg) on all data: 0.0029887948
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.624075e-08
Norm of the params: 7.0757093
                Loss: fixed 535 labels. Loss 0.00299. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2901485
Train loss (w/o reg) on all data: 0.28365067
Test loss (w/o reg) on all data: 0.1307359
Train acc on all data:  0.8966690979820082
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7834012e-05
Norm of the params: 11.399841
              Random: fixed 109 labels. Loss 0.13074. Accuracy 0.990.
### Flips: 615, rs: 18, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053217515
Train loss (w/o reg) on all data: 0.04713569
Test loss (w/o reg) on all data: 0.020082314
Train acc on all data:  0.9849258448820812
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0251804e-06
Norm of the params: 11.028894
     Influence (LOO): fixed 475 labels. Loss 0.02008. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717895
Train loss (w/o reg) on all data: 0.0010944224
Test loss (w/o reg) on all data: 0.0030825362
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8351442e-08
Norm of the params: 6.4457226
                Loss: fixed 536 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2775714
Train loss (w/o reg) on all data: 0.27091026
Test loss (w/o reg) on all data: 0.121526495
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9164303e-05
Norm of the params: 11.542229
              Random: fixed 133 labels. Loss 0.12153. Accuracy 0.991.
### Flips: 615, rs: 18, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035158604
Train loss (w/o reg) on all data: 0.029829757
Test loss (w/o reg) on all data: 0.012489094
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6001853e-06
Norm of the params: 10.323609
     Influence (LOO): fixed 498 labels. Loss 0.01249. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717885
Train loss (w/o reg) on all data: 0.0010944181
Test loss (w/o reg) on all data: 0.0030825175
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3772214e-08
Norm of the params: 6.445728
                Loss: fixed 536 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26766658
Train loss (w/o reg) on all data: 0.2610074
Test loss (w/o reg) on all data: 0.11423761
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.1416765e-06
Norm of the params: 11.540513
              Random: fixed 155 labels. Loss 0.11424. Accuracy 0.989.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33789074
Train loss (w/o reg) on all data: 0.3310374
Test loss (w/o reg) on all data: 0.16374926
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.0395076e-05
Norm of the params: 11.707567
Flipped loss: 0.16375. Accuracy: 0.986
### Flips: 615, rs: 19, checks: 205
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24009375
Train loss (w/o reg) on all data: 0.23097122
Test loss (w/o reg) on all data: 0.10152286
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6511621e-05
Norm of the params: 13.507435
     Influence (LOO): fixed 182 labels. Loss 0.10152. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19851954
Train loss (w/o reg) on all data: 0.18546797
Test loss (w/o reg) on all data: 0.108049296
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6087675e-05
Norm of the params: 16.15647
                Loss: fixed 205 labels. Loss 0.10805. Accuracy 0.974.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32709348
Train loss (w/o reg) on all data: 0.32050538
Test loss (w/o reg) on all data: 0.15349159
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1913681e-05
Norm of the params: 11.478752
              Random: fixed  30 labels. Loss 0.15349. Accuracy 0.989.
### Flips: 615, rs: 19, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165521
Train loss (w/o reg) on all data: 0.15649965
Test loss (w/o reg) on all data: 0.066020705
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.8548236e-06
Norm of the params: 13.432309
     Influence (LOO): fixed 311 labels. Loss 0.06602. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07692864
Train loss (w/o reg) on all data: 0.06067373
Test loss (w/o reg) on all data: 0.042467482
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9289564e-06
Norm of the params: 18.03048
                Loss: fixed 410 labels. Loss 0.04247. Accuracy 0.993.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3130329
Train loss (w/o reg) on all data: 0.30622718
Test loss (w/o reg) on all data: 0.14521937
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.1876955e-05
Norm of the params: 11.666799
              Random: fixed  61 labels. Loss 0.14522. Accuracy 0.988.
### Flips: 615, rs: 19, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1094842
Train loss (w/o reg) on all data: 0.101213574
Test loss (w/o reg) on all data: 0.04047939
Train acc on all data:  0.9657184536834428
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7518973e-06
Norm of the params: 12.861282
     Influence (LOO): fixed 400 labels. Loss 0.04048. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004550893
Train loss (w/o reg) on all data: 0.0017087144
Test loss (w/o reg) on all data: 0.0030844656
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.018748e-07
Norm of the params: 7.539468
                Loss: fixed 540 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2990414
Train loss (w/o reg) on all data: 0.29228336
Test loss (w/o reg) on all data: 0.13465007
Train acc on all data:  0.8913202042304887
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6205409e-05
Norm of the params: 11.625859
              Random: fixed  93 labels. Loss 0.13465. Accuracy 0.993.
### Flips: 615, rs: 19, checks: 820
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07200116
Train loss (w/o reg) on all data: 0.06462319
Test loss (w/o reg) on all data: 0.023639983
Train acc on all data:  0.9793338195964016
Test acc on all data:   1.0
Norm of the mean of gradients: 1.766647e-06
Norm of the params: 12.147403
     Influence (LOO): fixed 456 labels. Loss 0.02364. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004550892
Train loss (w/o reg) on all data: 0.0017086567
Test loss (w/o reg) on all data: 0.0030843525
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6990177e-07
Norm of the params: 7.5395436
                Loss: fixed 540 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2836757
Train loss (w/o reg) on all data: 0.2768698
Test loss (w/o reg) on all data: 0.12524322
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8755469e-05
Norm of the params: 11.66696
              Random: fixed 128 labels. Loss 0.12524. Accuracy 0.994.
### Flips: 615, rs: 19, checks: 1025
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05287848
Train loss (w/o reg) on all data: 0.04635181
Test loss (w/o reg) on all data: 0.016395375
Train acc on all data:  0.9858983710187211
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0521982e-06
Norm of the params: 11.425123
     Influence (LOO): fixed 484 labels. Loss 0.01640. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003966168
Train loss (w/o reg) on all data: 0.0014478205
Test loss (w/o reg) on all data: 0.0030069938
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4099352e-08
Norm of the params: 7.096968
                Loss: fixed 541 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27163777
Train loss (w/o reg) on all data: 0.26465213
Test loss (w/o reg) on all data: 0.11822631
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.830137e-05
Norm of the params: 11.820008
              Random: fixed 151 labels. Loss 0.11823. Accuracy 0.993.
### Flips: 615, rs: 19, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040361747
Train loss (w/o reg) on all data: 0.03454447
Test loss (w/o reg) on all data: 0.012427992
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5551733e-06
Norm of the params: 10.786357
     Influence (LOO): fixed 499 labels. Loss 0.01243. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039661676
Train loss (w/o reg) on all data: 0.0014478319
Test loss (w/o reg) on all data: 0.0030069903
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1010742e-08
Norm of the params: 7.0969515
                Loss: fixed 541 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2598736
Train loss (w/o reg) on all data: 0.25272503
Test loss (w/o reg) on all data: 0.11227276
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.057868e-05
Norm of the params: 11.957056
              Random: fixed 173 labels. Loss 0.11227. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33736348
Train loss (w/o reg) on all data: 0.331085
Test loss (w/o reg) on all data: 0.15878484
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.028436e-05
Norm of the params: 11.205788
Flipped loss: 0.15878. Accuracy: 0.984
### Flips: 615, rs: 20, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23663756
Train loss (w/o reg) on all data: 0.22819246
Test loss (w/o reg) on all data: 0.102005556
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.5954654e-05
Norm of the params: 12.996226
     Influence (LOO): fixed 181 labels. Loss 0.10201. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19822785
Train loss (w/o reg) on all data: 0.18526284
Test loss (w/o reg) on all data: 0.10832438
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.862095e-06
Norm of the params: 16.102802
                Loss: fixed 205 labels. Loss 0.10832. Accuracy 0.974.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.324962
Train loss (w/o reg) on all data: 0.3185108
Test loss (w/o reg) on all data: 0.15047814
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3161711e-05
Norm of the params: 11.358871
              Random: fixed  28 labels. Loss 0.15048. Accuracy 0.988.
### Flips: 615, rs: 20, checks: 410
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15643792
Train loss (w/o reg) on all data: 0.14814459
Test loss (w/o reg) on all data: 0.05950923
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4709216e-05
Norm of the params: 12.878917
     Influence (LOO): fixed 317 labels. Loss 0.05951. Accuracy 0.999.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06992033
Train loss (w/o reg) on all data: 0.054677006
Test loss (w/o reg) on all data: 0.046462096
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.4455385e-06
Norm of the params: 17.460428
                Loss: fixed 410 labels. Loss 0.04646. Accuracy 0.984.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31310079
Train loss (w/o reg) on all data: 0.30665803
Test loss (w/o reg) on all data: 0.14336403
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2735583e-05
Norm of the params: 11.351433
              Random: fixed  57 labels. Loss 0.14336. Accuracy 0.991.
### Flips: 615, rs: 20, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10099279
Train loss (w/o reg) on all data: 0.093909234
Test loss (w/o reg) on all data: 0.03478339
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7148213e-06
Norm of the params: 11.902572
     Influence (LOO): fixed 404 labels. Loss 0.03478. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061530555
Train loss (w/o reg) on all data: 0.0024377867
Test loss (w/o reg) on all data: 0.0056000203
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.091973e-07
Norm of the params: 8.620057
                Loss: fixed 527 labels. Loss 0.00560. Accuracy 0.997.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3010496
Train loss (w/o reg) on all data: 0.2941693
Test loss (w/o reg) on all data: 0.13548614
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.158535e-05
Norm of the params: 11.730547
              Random: fixed  82 labels. Loss 0.13549. Accuracy 0.990.
### Flips: 615, rs: 20, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06822671
Train loss (w/o reg) on all data: 0.06200017
Test loss (w/o reg) on all data: 0.022290364
Train acc on all data:  0.9795769511305616
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4748998e-06
Norm of the params: 11.159336
     Influence (LOO): fixed 449 labels. Loss 0.02229. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036231081
Train loss (w/o reg) on all data: 0.0013771856
Test loss (w/o reg) on all data: 0.004197418
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.980948e-08
Norm of the params: 6.7021227
                Loss: fixed 531 labels. Loss 0.00420. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28659454
Train loss (w/o reg) on all data: 0.2797249
Test loss (w/o reg) on all data: 0.12711479
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.007399e-05
Norm of the params: 11.721468
              Random: fixed 113 labels. Loss 0.12711. Accuracy 0.986.
### Flips: 615, rs: 20, checks: 1025
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05600694
Train loss (w/o reg) on all data: 0.050180577
Test loss (w/o reg) on all data: 0.018284965
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 8.765503e-07
Norm of the params: 10.794779
     Influence (LOO): fixed 466 labels. Loss 0.01828. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036231086
Train loss (w/o reg) on all data: 0.0013772056
Test loss (w/o reg) on all data: 0.0041974285
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.188417e-08
Norm of the params: 6.7020936
                Loss: fixed 531 labels. Loss 0.00420. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27099475
Train loss (w/o reg) on all data: 0.26393586
Test loss (w/o reg) on all data: 0.12052148
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.826878e-06
Norm of the params: 11.88183
              Random: fixed 145 labels. Loss 0.12052. Accuracy 0.986.
### Flips: 615, rs: 20, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038868073
Train loss (w/o reg) on all data: 0.03403443
Test loss (w/o reg) on all data: 0.012677416
Train acc on all data:  0.9890590809628009
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1011575e-07
Norm of the params: 9.832236
     Influence (LOO): fixed 488 labels. Loss 0.01268. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6161922e-08
Norm of the params: 6.0928173
                Loss: fixed 533 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25865862
Train loss (w/o reg) on all data: 0.25140905
Test loss (w/o reg) on all data: 0.114179894
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.505503e-06
Norm of the params: 12.041233
              Random: fixed 169 labels. Loss 0.11418. Accuracy 0.987.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32813367
Train loss (w/o reg) on all data: 0.32199347
Test loss (w/o reg) on all data: 0.15288842
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.761989e-06
Norm of the params: 11.081709
Flipped loss: 0.15289. Accuracy: 0.988
### Flips: 615, rs: 21, checks: 205
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22570615
Train loss (w/o reg) on all data: 0.2170507
Test loss (w/o reg) on all data: 0.100564405
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.4581712e-06
Norm of the params: 13.157089
     Influence (LOO): fixed 178 labels. Loss 0.10056. Accuracy 0.995.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1887469
Train loss (w/o reg) on all data: 0.1758182
Test loss (w/o reg) on all data: 0.106436625
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.423188e-06
Norm of the params: 16.080229
                Loss: fixed 205 labels. Loss 0.10644. Accuracy 0.972.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3152674
Train loss (w/o reg) on all data: 0.3090233
Test loss (w/o reg) on all data: 0.14434122
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5120339e-05
Norm of the params: 11.17507
              Random: fixed  31 labels. Loss 0.14434. Accuracy 0.992.
### Flips: 615, rs: 21, checks: 410
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15090969
Train loss (w/o reg) on all data: 0.14265186
Test loss (w/o reg) on all data: 0.061053157
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.226579e-06
Norm of the params: 12.851326
     Influence (LOO): fixed 308 labels. Loss 0.06105. Accuracy 0.999.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063055076
Train loss (w/o reg) on all data: 0.049941093
Test loss (w/o reg) on all data: 0.039272577
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3407662e-06
Norm of the params: 16.19505
                Loss: fixed 410 labels. Loss 0.03927. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3055839
Train loss (w/o reg) on all data: 0.29942465
Test loss (w/o reg) on all data: 0.13627239
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.268701e-05
Norm of the params: 11.098884
              Random: fixed  54 labels. Loss 0.13627. Accuracy 0.994.
### Flips: 615, rs: 21, checks: 615
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09621219
Train loss (w/o reg) on all data: 0.089266144
Test loss (w/o reg) on all data: 0.038663283
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.830602e-06
Norm of the params: 11.786473
     Influence (LOO): fixed 392 labels. Loss 0.03866. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003171789
Train loss (w/o reg) on all data: 0.0010943833
Test loss (w/o reg) on all data: 0.0030823783
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.054721e-08
Norm of the params: 6.445782
                Loss: fixed 518 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29230854
Train loss (w/o reg) on all data: 0.28555724
Test loss (w/o reg) on all data: 0.13041347
Train acc on all data:  0.8920495988329686
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.8970637e-06
Norm of the params: 11.620065
              Random: fixed  78 labels. Loss 0.13041. Accuracy 0.991.
### Flips: 615, rs: 21, checks: 820
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07249729
Train loss (w/o reg) on all data: 0.06634846
Test loss (w/o reg) on all data: 0.029141875
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8148287e-06
Norm of the params: 11.089485
     Influence (LOO): fixed 425 labels. Loss 0.02914. Accuracy 0.998.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717883
Train loss (w/o reg) on all data: 0.0010943837
Test loss (w/o reg) on all data: 0.003082516
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9807975e-08
Norm of the params: 6.4457817
                Loss: fixed 518 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28374213
Train loss (w/o reg) on all data: 0.2768506
Test loss (w/o reg) on all data: 0.12387146
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.5248845e-06
Norm of the params: 11.740107
              Random: fixed  97 labels. Loss 0.12387. Accuracy 0.994.
### Flips: 615, rs: 21, checks: 1025
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049814068
Train loss (w/o reg) on all data: 0.043791194
Test loss (w/o reg) on all data: 0.021501344
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.496544e-07
Norm of the params: 10.975312
     Influence (LOO): fixed 456 labels. Loss 0.02150. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.332572e-09
Norm of the params: 6.09282
                Loss: fixed 519 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27124912
Train loss (w/o reg) on all data: 0.26451385
Test loss (w/o reg) on all data: 0.11582897
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1338326e-05
Norm of the params: 11.606265
              Random: fixed 127 labels. Loss 0.11583. Accuracy 0.996.
### Flips: 615, rs: 21, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04036959
Train loss (w/o reg) on all data: 0.03504178
Test loss (w/o reg) on all data: 0.015571613
Train acc on all data:  0.9880865548261609
Test acc on all data:   1.0
Norm of the mean of gradients: 4.174674e-06
Norm of the params: 10.322605
     Influence (LOO): fixed 470 labels. Loss 0.01557. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601278
Test loss (w/o reg) on all data: 0.0026561068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1281534e-08
Norm of the params: 6.0928025
                Loss: fixed 519 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2612093
Train loss (w/o reg) on all data: 0.25419593
Test loss (w/o reg) on all data: 0.111396074
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.16109e-06
Norm of the params: 11.843459
              Random: fixed 146 labels. Loss 0.11140. Accuracy 0.995.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34830007
Train loss (w/o reg) on all data: 0.34258455
Test loss (w/o reg) on all data: 0.16307335
Train acc on all data:  0.8672501823486506
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.3957027e-06
Norm of the params: 10.691597
Flipped loss: 0.16307. Accuracy: 0.992
### Flips: 615, rs: 22, checks: 205
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24472338
Train loss (w/o reg) on all data: 0.23563115
Test loss (w/o reg) on all data: 0.11251522
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3929e-05
Norm of the params: 13.484977
     Influence (LOO): fixed 179 labels. Loss 0.11252. Accuracy 0.994.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20659833
Train loss (w/o reg) on all data: 0.19396701
Test loss (w/o reg) on all data: 0.110562496
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 5.359534e-06
Norm of the params: 15.894219
                Loss: fixed 205 labels. Loss 0.11056. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34042588
Train loss (w/o reg) on all data: 0.33472916
Test loss (w/o reg) on all data: 0.15669559
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.145279e-05
Norm of the params: 10.67399
              Random: fixed  20 labels. Loss 0.15670. Accuracy 0.992.
### Flips: 615, rs: 22, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16973019
Train loss (w/o reg) on all data: 0.16087063
Test loss (w/o reg) on all data: 0.074634545
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5754513e-05
Norm of the params: 13.311316
     Influence (LOO): fixed 309 labels. Loss 0.07463. Accuracy 0.997.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08499778
Train loss (w/o reg) on all data: 0.07008058
Test loss (w/o reg) on all data: 0.053962115
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.595313e-06
Norm of the params: 17.27264
                Loss: fixed 410 labels. Loss 0.05396. Accuracy 0.983.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32464787
Train loss (w/o reg) on all data: 0.31856647
Test loss (w/o reg) on all data: 0.14599022
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4906544e-05
Norm of the params: 11.02851
              Random: fixed  52 labels. Loss 0.14599. Accuracy 0.991.
### Flips: 615, rs: 22, checks: 615
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1162182
Train loss (w/o reg) on all data: 0.10786311
Test loss (w/o reg) on all data: 0.0484676
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6576057e-06
Norm of the params: 12.9267845
     Influence (LOO): fixed 391 labels. Loss 0.04847. Accuracy 0.996.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004285237
Train loss (w/o reg) on all data: 0.0017259556
Test loss (w/o reg) on all data: 0.0029036985
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.309402e-07
Norm of the params: 7.154413
                Loss: fixed 548 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31290615
Train loss (w/o reg) on all data: 0.30682418
Test loss (w/o reg) on all data: 0.13729654
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.671747e-06
Norm of the params: 11.029022
              Random: fixed  81 labels. Loss 0.13730. Accuracy 0.994.
### Flips: 615, rs: 22, checks: 820
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08076888
Train loss (w/o reg) on all data: 0.07224257
Test loss (w/o reg) on all data: 0.03219718
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.194788e-06
Norm of the params: 13.058568
     Influence (LOO): fixed 443 labels. Loss 0.03220. Accuracy 0.998.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668467
Test loss (w/o reg) on all data: 0.002582321
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3871086e-08
Norm of the params: 6.461585
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29985943
Train loss (w/o reg) on all data: 0.2935113
Test loss (w/o reg) on all data: 0.12710999
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.007323e-05
Norm of the params: 11.267782
              Random: fixed 111 labels. Loss 0.12711. Accuracy 0.995.
### Flips: 615, rs: 22, checks: 1025
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059002876
Train loss (w/o reg) on all data: 0.051846217
Test loss (w/o reg) on all data: 0.02209233
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 5.793596e-06
Norm of the params: 11.963828
     Influence (LOO): fixed 478 labels. Loss 0.02209. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544515
Train loss (w/o reg) on all data: 0.0010668571
Test loss (w/o reg) on all data: 0.0025823426
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6544514e-08
Norm of the params: 6.46157
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2860242
Train loss (w/o reg) on all data: 0.27941144
Test loss (w/o reg) on all data: 0.12049603
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8126573e-05
Norm of the params: 11.500245
              Random: fixed 133 labels. Loss 0.12050. Accuracy 0.993.
### Flips: 615, rs: 22, checks: 1230
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042441443
Train loss (w/o reg) on all data: 0.03654678
Test loss (w/o reg) on all data: 0.015866883
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2460292e-07
Norm of the params: 10.857864
     Influence (LOO): fixed 501 labels. Loss 0.01587. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668562
Test loss (w/o reg) on all data: 0.0025823696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.723729e-08
Norm of the params: 6.4615703
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27359876
Train loss (w/o reg) on all data: 0.2667887
Test loss (w/o reg) on all data: 0.116318636
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.308702e-06
Norm of the params: 11.670542
              Random: fixed 157 labels. Loss 0.11632. Accuracy 0.991.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33449236
Train loss (w/o reg) on all data: 0.32767954
Test loss (w/o reg) on all data: 0.16622442
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.8816936e-05
Norm of the params: 11.672879
Flipped loss: 0.16622. Accuracy: 0.984
### Flips: 615, rs: 23, checks: 205
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23304361
Train loss (w/o reg) on all data: 0.22432026
Test loss (w/o reg) on all data: 0.10543284
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5957075e-05
Norm of the params: 13.208596
     Influence (LOO): fixed 184 labels. Loss 0.10543. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19705974
Train loss (w/o reg) on all data: 0.18254352
Test loss (w/o reg) on all data: 0.11919922
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.007745e-06
Norm of the params: 17.038908
                Loss: fixed 205 labels. Loss 0.11920. Accuracy 0.971.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3248893
Train loss (w/o reg) on all data: 0.31804237
Test loss (w/o reg) on all data: 0.15832733
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.9201514e-05
Norm of the params: 11.702082
              Random: fixed  22 labels. Loss 0.15833. Accuracy 0.983.
### Flips: 615, rs: 23, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15805618
Train loss (w/o reg) on all data: 0.14934757
Test loss (w/o reg) on all data: 0.070215665
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2082028e-06
Norm of the params: 13.197431
     Influence (LOO): fixed 313 labels. Loss 0.07022. Accuracy 0.997.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0718504
Train loss (w/o reg) on all data: 0.057234082
Test loss (w/o reg) on all data: 0.054548286
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.5751702e-06
Norm of the params: 17.097551
                Loss: fixed 410 labels. Loss 0.05455. Accuracy 0.986.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3133005
Train loss (w/o reg) on all data: 0.3061373
Test loss (w/o reg) on all data: 0.15138859
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.497787e-05
Norm of the params: 11.969283
              Random: fixed  48 labels. Loss 0.15139. Accuracy 0.984.
### Flips: 615, rs: 23, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10597261
Train loss (w/o reg) on all data: 0.09845821
Test loss (w/o reg) on all data: 0.042539626
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.815984e-06
Norm of the params: 12.259202
     Influence (LOO): fixed 396 labels. Loss 0.04254. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007562927
Train loss (w/o reg) on all data: 0.003350122
Test loss (w/o reg) on all data: 0.0066681644
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.450428e-08
Norm of the params: 9.179112
                Loss: fixed 527 labels. Loss 0.00667. Accuracy 0.999.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30089608
Train loss (w/o reg) on all data: 0.29339868
Test loss (w/o reg) on all data: 0.14356759
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.8976428e-05
Norm of the params: 12.245333
              Random: fixed  75 labels. Loss 0.14357. Accuracy 0.985.
### Flips: 615, rs: 23, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074959345
Train loss (w/o reg) on all data: 0.068097085
Test loss (w/o reg) on all data: 0.029230705
Train acc on all data:  0.9771456357889619
Test acc on all data:   1.0
Norm of the mean of gradients: 1.034209e-06
Norm of the params: 11.715173
     Influence (LOO): fixed 442 labels. Loss 0.02923. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0050335885
Train loss (w/o reg) on all data: 0.0020179162
Test loss (w/o reg) on all data: 0.005045139
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.9267316e-08
Norm of the params: 7.766174
                Loss: fixed 531 labels. Loss 0.00505. Accuracy 0.999.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28923166
Train loss (w/o reg) on all data: 0.28161436
Test loss (w/o reg) on all data: 0.13393787
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.1806714e-05
Norm of the params: 12.3428545
              Random: fixed 100 labels. Loss 0.13394. Accuracy 0.987.
### Flips: 615, rs: 23, checks: 1025
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056557126
Train loss (w/o reg) on all data: 0.050288692
Test loss (w/o reg) on all data: 0.021369496
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 4.026845e-06
Norm of the params: 11.196816
     Influence (LOO): fixed 467 labels. Loss 0.02137. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037321304
Train loss (w/o reg) on all data: 0.0014723375
Test loss (w/o reg) on all data: 0.0043936367
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.523526e-08
Norm of the params: 6.722786
                Loss: fixed 534 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27805912
Train loss (w/o reg) on all data: 0.2701811
Test loss (w/o reg) on all data: 0.124162346
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.5663685e-05
Norm of the params: 12.552307
              Random: fixed 123 labels. Loss 0.12416. Accuracy 0.990.
### Flips: 615, rs: 23, checks: 1230
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044374656
Train loss (w/o reg) on all data: 0.03879361
Test loss (w/o reg) on all data: 0.017203324
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 8.5483816e-07
Norm of the params: 10.565082
     Influence (LOO): fixed 484 labels. Loss 0.01720. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00373213
Train loss (w/o reg) on all data: 0.001472328
Test loss (w/o reg) on all data: 0.004393555
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3221926e-08
Norm of the params: 6.7228
                Loss: fixed 534 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26799458
Train loss (w/o reg) on all data: 0.25990906
Test loss (w/o reg) on all data: 0.1198547
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.915979e-06
Norm of the params: 12.716544
              Random: fixed 144 labels. Loss 0.11985. Accuracy 0.990.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34350508
Train loss (w/o reg) on all data: 0.33756185
Test loss (w/o reg) on all data: 0.15485395
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.310422e-06
Norm of the params: 10.902502
Flipped loss: 0.15485. Accuracy: 0.993
### Flips: 615, rs: 24, checks: 205
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23964088
Train loss (w/o reg) on all data: 0.23112594
Test loss (w/o reg) on all data: 0.09928719
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.180483e-05
Norm of the params: 13.049862
     Influence (LOO): fixed 184 labels. Loss 0.09929. Accuracy 0.994.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2050611
Train loss (w/o reg) on all data: 0.19233419
Test loss (w/o reg) on all data: 0.099109285
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.7672223e-06
Norm of the params: 15.9542465
                Loss: fixed 205 labels. Loss 0.09911. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33183908
Train loss (w/o reg) on all data: 0.3258968
Test loss (w/o reg) on all data: 0.1474474
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6049392e-06
Norm of the params: 10.901646
              Random: fixed  28 labels. Loss 0.14745. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 410
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15936464
Train loss (w/o reg) on all data: 0.15116024
Test loss (w/o reg) on all data: 0.05920245
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0936612e-06
Norm of the params: 12.809689
     Influence (LOO): fixed 319 labels. Loss 0.05920. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07844353
Train loss (w/o reg) on all data: 0.063706115
Test loss (w/o reg) on all data: 0.046230152
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1543938e-06
Norm of the params: 17.168234
                Loss: fixed 410 labels. Loss 0.04623. Accuracy 0.987.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31712553
Train loss (w/o reg) on all data: 0.31109792
Test loss (w/o reg) on all data: 0.13754219
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.128784e-05
Norm of the params: 10.979612
              Random: fixed  62 labels. Loss 0.13754. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11013162
Train loss (w/o reg) on all data: 0.102582835
Test loss (w/o reg) on all data: 0.03804397
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9402196e-06
Norm of the params: 12.287217
     Influence (LOO): fixed 397 labels. Loss 0.03804. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004512337
Train loss (w/o reg) on all data: 0.001929713
Test loss (w/o reg) on all data: 0.0026895409
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9059416e-08
Norm of the params: 7.186966
                Loss: fixed 537 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30763632
Train loss (w/o reg) on all data: 0.3015217
Test loss (w/o reg) on all data: 0.13112651
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0663959e-05
Norm of the params: 11.058597
              Random: fixed  82 labels. Loss 0.13113. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07678442
Train loss (w/o reg) on all data: 0.07025465
Test loss (w/o reg) on all data: 0.02546889
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5790833e-06
Norm of the params: 11.427831
     Influence (LOO): fixed 446 labels. Loss 0.02547. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.0026560419
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.214615e-08
Norm of the params: 6.092834
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29782924
Train loss (w/o reg) on all data: 0.29160425
Test loss (w/o reg) on all data: 0.12570229
Train acc on all data:  0.8956965718453683
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.7944406e-05
Norm of the params: 11.15795
              Random: fixed 104 labels. Loss 0.12570. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 1025
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057124723
Train loss (w/o reg) on all data: 0.051236942
Test loss (w/o reg) on all data: 0.018321974
Train acc on all data:  0.9841964502796012
Test acc on all data:   1.0
Norm of the mean of gradients: 2.543382e-06
Norm of the params: 10.851525
     Influence (LOO): fixed 474 labels. Loss 0.01832. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601403
Test loss (w/o reg) on all data: 0.0026560964
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.315938e-08
Norm of the params: 6.0927815
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28699797
Train loss (w/o reg) on all data: 0.28091654
Test loss (w/o reg) on all data: 0.11695548
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8726692e-05
Norm of the params: 11.028551
              Random: fixed 128 labels. Loss 0.11696. Accuracy 0.993.
### Flips: 615, rs: 24, checks: 1230
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040982693
Train loss (w/o reg) on all data: 0.035949677
Test loss (w/o reg) on all data: 0.012365654
Train acc on all data:  0.9890590809628009
Test acc on all data:   1.0
Norm of the mean of gradients: 8.140554e-07
Norm of the params: 10.03296
     Influence (LOO): fixed 494 labels. Loss 0.01237. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4696654e-08
Norm of the params: 6.092817
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26809508
Train loss (w/o reg) on all data: 0.26175076
Test loss (w/o reg) on all data: 0.10410239
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.696736e-06
Norm of the params: 11.26438
              Random: fixed 165 labels. Loss 0.10410. Accuracy 0.995.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3374462
Train loss (w/o reg) on all data: 0.33024743
Test loss (w/o reg) on all data: 0.1726599
Train acc on all data:  0.8704108922927304
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 8.826423e-06
Norm of the params: 11.9989805
Flipped loss: 0.17266. Accuracy: 0.980
### Flips: 615, rs: 25, checks: 205
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24410853
Train loss (w/o reg) on all data: 0.2348575
Test loss (w/o reg) on all data: 0.11151279
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.7738803e-05
Norm of the params: 13.602225
     Influence (LOO): fixed 173 labels. Loss 0.11151. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19791517
Train loss (w/o reg) on all data: 0.18336777
Test loss (w/o reg) on all data: 0.12238329
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.5888456e-06
Norm of the params: 17.057192
                Loss: fixed 205 labels. Loss 0.12238. Accuracy 0.973.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32722765
Train loss (w/o reg) on all data: 0.31982347
Test loss (w/o reg) on all data: 0.163581
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.990115e-05
Norm of the params: 12.168956
              Random: fixed  24 labels. Loss 0.16358. Accuracy 0.979.
### Flips: 615, rs: 25, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16610572
Train loss (w/o reg) on all data: 0.15722147
Test loss (w/o reg) on all data: 0.06498446
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.019918e-05
Norm of the params: 13.329854
     Influence (LOO): fixed 318 labels. Loss 0.06498. Accuracy 0.996.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08113088
Train loss (w/o reg) on all data: 0.066183984
Test loss (w/o reg) on all data: 0.053410023
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.5841446e-06
Norm of the params: 17.289822
                Loss: fixed 409 labels. Loss 0.05341. Accuracy 0.985.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773043
Train loss (w/o reg) on all data: 0.3103808
Test loss (w/o reg) on all data: 0.15838447
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.497414e-06
Norm of the params: 12.124052
              Random: fixed  48 labels. Loss 0.15838. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10694414
Train loss (w/o reg) on all data: 0.098321676
Test loss (w/o reg) on all data: 0.040912136
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9650506e-06
Norm of the params: 13.131992
     Influence (LOO): fixed 410 labels. Loss 0.04091. Accuracy 0.997.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010279266
Train loss (w/o reg) on all data: 0.0053503932
Test loss (w/o reg) on all data: 0.006815066
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8057216e-07
Norm of the params: 9.928618
                Loss: fixed 536 labels. Loss 0.00682. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30574247
Train loss (w/o reg) on all data: 0.298355
Test loss (w/o reg) on all data: 0.15067512
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.833638e-05
Norm of the params: 12.155201
              Random: fixed  76 labels. Loss 0.15068. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 820
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0746945
Train loss (w/o reg) on all data: 0.06775248
Test loss (w/o reg) on all data: 0.024804158
Train acc on all data:  0.9778750303914417
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6392272e-06
Norm of the params: 11.783054
     Influence (LOO): fixed 459 labels. Loss 0.02480. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054062456
Train loss (w/o reg) on all data: 0.0023650185
Test loss (w/o reg) on all data: 0.0044897567
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5708786e-07
Norm of the params: 7.799009
                Loss: fixed 546 labels. Loss 0.00449. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2930343
Train loss (w/o reg) on all data: 0.2855317
Test loss (w/o reg) on all data: 0.14191075
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.7243261e-05
Norm of the params: 12.249566
              Random: fixed 105 labels. Loss 0.14191. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 1025
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05758189
Train loss (w/o reg) on all data: 0.051548224
Test loss (w/o reg) on all data: 0.019154498
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3200664e-06
Norm of the params: 10.985143
     Influence (LOO): fixed 482 labels. Loss 0.01915. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005072645
Train loss (w/o reg) on all data: 0.0021577294
Test loss (w/o reg) on all data: 0.0037462148
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.727201e-08
Norm of the params: 7.635333
                Loss: fixed 547 labels. Loss 0.00375. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28180227
Train loss (w/o reg) on all data: 0.27465186
Test loss (w/o reg) on all data: 0.1334773
Train acc on all data:  0.9008023340627279
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.778902e-05
Norm of the params: 11.958592
              Random: fixed 132 labels. Loss 0.13348. Accuracy 0.985.
### Flips: 615, rs: 25, checks: 1230
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04401715
Train loss (w/o reg) on all data: 0.03898533
Test loss (w/o reg) on all data: 0.014802405
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8587707e-06
Norm of the params: 10.03177
     Influence (LOO): fixed 500 labels. Loss 0.01480. Accuracy 0.999.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042684786
Train loss (w/o reg) on all data: 0.0016213274
Test loss (w/o reg) on all data: 0.0033914463
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4327173e-08
Norm of the params: 7.2761955
                Loss: fixed 548 labels. Loss 0.00339. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2654189
Train loss (w/o reg) on all data: 0.25821024
Test loss (w/o reg) on all data: 0.12419733
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.534405e-06
Norm of the params: 12.007196
              Random: fixed 168 labels. Loss 0.12420. Accuracy 0.984.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3420739
Train loss (w/o reg) on all data: 0.33652934
Test loss (w/o reg) on all data: 0.16394147
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7776343e-05
Norm of the params: 10.530475
Flipped loss: 0.16394. Accuracy: 0.992
### Flips: 615, rs: 26, checks: 205
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230582
Train loss (w/o reg) on all data: 0.22132571
Test loss (w/o reg) on all data: 0.10262725
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.395923e-06
Norm of the params: 13.6060915
     Influence (LOO): fixed 184 labels. Loss 0.10263. Accuracy 0.996.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2028164
Train loss (w/o reg) on all data: 0.19004263
Test loss (w/o reg) on all data: 0.10991554
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.747256e-06
Norm of the params: 15.983594
                Loss: fixed 205 labels. Loss 0.10992. Accuracy 0.982.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3289035
Train loss (w/o reg) on all data: 0.32321098
Test loss (w/o reg) on all data: 0.15637563
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.865065e-06
Norm of the params: 10.670052
              Random: fixed  31 labels. Loss 0.15638. Accuracy 0.990.
### Flips: 615, rs: 26, checks: 410
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15682615
Train loss (w/o reg) on all data: 0.14817248
Test loss (w/o reg) on all data: 0.06773063
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.565291e-06
Norm of the params: 13.155735
     Influence (LOO): fixed 309 labels. Loss 0.06773. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07630946
Train loss (w/o reg) on all data: 0.061634794
Test loss (w/o reg) on all data: 0.04144365
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.183965e-06
Norm of the params: 17.131647
                Loss: fixed 410 labels. Loss 0.04144. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31668663
Train loss (w/o reg) on all data: 0.31078956
Test loss (w/o reg) on all data: 0.14957517
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4667332e-05
Norm of the params: 10.860095
              Random: fixed  57 labels. Loss 0.14958. Accuracy 0.990.
### Flips: 615, rs: 26, checks: 615
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10867928
Train loss (w/o reg) on all data: 0.10149782
Test loss (w/o reg) on all data: 0.041675188
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1077205e-06
Norm of the params: 11.984535
     Influence (LOO): fixed 388 labels. Loss 0.04168. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044889892
Train loss (w/o reg) on all data: 0.0018073801
Test loss (w/o reg) on all data: 0.003168288
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.242713e-08
Norm of the params: 7.3234
                Loss: fixed 531 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30701855
Train loss (w/o reg) on all data: 0.30096075
Test loss (w/o reg) on all data: 0.13996048
Train acc on all data:  0.8908339411621687
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4121226e-05
Norm of the params: 11.007094
              Random: fixed  80 labels. Loss 0.13996. Accuracy 0.991.
### Flips: 615, rs: 26, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07856505
Train loss (w/o reg) on all data: 0.07198203
Test loss (w/o reg) on all data: 0.027088258
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7414445e-06
Norm of the params: 11.474338
     Influence (LOO): fixed 433 labels. Loss 0.02709. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003915035
Train loss (w/o reg) on all data: 0.0014900179
Test loss (w/o reg) on all data: 0.0026008862
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.890076e-08
Norm of the params: 6.9642186
                Loss: fixed 533 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29423222
Train loss (w/o reg) on all data: 0.2880535
Test loss (w/o reg) on all data: 0.13062143
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.028102e-06
Norm of the params: 11.116381
              Random: fixed 109 labels. Loss 0.13062. Accuracy 0.993.
### Flips: 615, rs: 26, checks: 1025
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05813677
Train loss (w/o reg) on all data: 0.05215794
Test loss (w/o reg) on all data: 0.018404575
Train acc on all data:  0.9827376610746413
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0393638e-05
Norm of the params: 10.935107
     Influence (LOO): fixed 464 labels. Loss 0.01840. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039150342
Train loss (w/o reg) on all data: 0.0014900722
Test loss (w/o reg) on all data: 0.00260092
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2692027e-07
Norm of the params: 6.9641395
                Loss: fixed 533 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27654946
Train loss (w/o reg) on all data: 0.2702394
Test loss (w/o reg) on all data: 0.11959025
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.061335e-06
Norm of the params: 11.233931
              Random: fixed 146 labels. Loss 0.11959. Accuracy 0.995.
### Flips: 615, rs: 26, checks: 1230
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042827565
Train loss (w/o reg) on all data: 0.037374597
Test loss (w/o reg) on all data: 0.013507748
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3483507e-07
Norm of the params: 10.443148
     Influence (LOO): fixed 484 labels. Loss 0.01351. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560386
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.837901e-08
Norm of the params: 6.092812
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26361465
Train loss (w/o reg) on all data: 0.25720263
Test loss (w/o reg) on all data: 0.110975094
Train acc on all data:  0.912472647702407
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.292315e-05
Norm of the params: 11.324339
              Random: fixed 173 labels. Loss 0.11098. Accuracy 0.997.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33904606
Train loss (w/o reg) on all data: 0.33314422
Test loss (w/o reg) on all data: 0.1523206
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6084265e-05
Norm of the params: 10.864465
Flipped loss: 0.15232. Accuracy: 0.996
### Flips: 615, rs: 27, checks: 205
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23309882
Train loss (w/o reg) on all data: 0.22545068
Test loss (w/o reg) on all data: 0.09895419
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.556562e-05
Norm of the params: 12.367809
     Influence (LOO): fixed 186 labels. Loss 0.09895. Accuracy 0.998.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19669339
Train loss (w/o reg) on all data: 0.18520807
Test loss (w/o reg) on all data: 0.09285875
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2228354e-05
Norm of the params: 15.156073
                Loss: fixed 205 labels. Loss 0.09286. Accuracy 0.984.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33236364
Train loss (w/o reg) on all data: 0.3265715
Test loss (w/o reg) on all data: 0.14742965
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.986047e-05
Norm of the params: 10.763023
              Random: fixed  17 labels. Loss 0.14743. Accuracy 0.995.
### Flips: 615, rs: 27, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16188107
Train loss (w/o reg) on all data: 0.15398614
Test loss (w/o reg) on all data: 0.06538111
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.7918162e-06
Norm of the params: 12.565769
     Influence (LOO): fixed 305 labels. Loss 0.06538. Accuracy 0.997.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07183351
Train loss (w/o reg) on all data: 0.05814444
Test loss (w/o reg) on all data: 0.03848881
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.620238e-06
Norm of the params: 16.546345
                Loss: fixed 410 labels. Loss 0.03849. Accuracy 0.990.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32042918
Train loss (w/o reg) on all data: 0.31479135
Test loss (w/o reg) on all data: 0.13973927
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.579158e-05
Norm of the params: 10.618698
              Random: fixed  45 labels. Loss 0.13974. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 615
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11544999
Train loss (w/o reg) on all data: 0.108468086
Test loss (w/o reg) on all data: 0.04372365
Train acc on all data:  0.9645027960126429
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0761177e-06
Norm of the params: 11.816852
     Influence (LOO): fixed 385 labels. Loss 0.04372. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004655451
Train loss (w/o reg) on all data: 0.0018194286
Test loss (w/o reg) on all data: 0.004318791
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2744547e-07
Norm of the params: 7.531298
                Loss: fixed 527 labels. Loss 0.00432. Accuracy 0.999.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30767837
Train loss (w/o reg) on all data: 0.30197006
Test loss (w/o reg) on all data: 0.13255456
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8127954e-05
Norm of the params: 10.684869
              Random: fixed  72 labels. Loss 0.13255. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 820
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07466452
Train loss (w/o reg) on all data: 0.06862524
Test loss (w/o reg) on all data: 0.025274456
Train acc on all data:  0.9786044249939218
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1202883e-06
Norm of the params: 10.990248
     Influence (LOO): fixed 443 labels. Loss 0.02527. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.0026560414
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220807e-08
Norm of the params: 6.092824
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29992244
Train loss (w/o reg) on all data: 0.29405773
Test loss (w/o reg) on all data: 0.12595084
Train acc on all data:  0.8939946511062484
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.932541e-06
Norm of the params: 10.830236
              Random: fixed  93 labels. Loss 0.12595. Accuracy 0.995.
### Flips: 615, rs: 27, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052711032
Train loss (w/o reg) on all data: 0.04736392
Test loss (w/o reg) on all data: 0.016569728
Train acc on all data:  0.9854121079504011
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2754382e-06
Norm of the params: 10.341291
     Influence (LOO): fixed 472 labels. Loss 0.01657. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012774
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7531064e-08
Norm of the params: 6.092803
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2853513
Train loss (w/o reg) on all data: 0.2796994
Test loss (w/o reg) on all data: 0.11817575
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5637964e-05
Norm of the params: 10.631955
              Random: fixed 125 labels. Loss 0.11818. Accuracy 0.997.
### Flips: 615, rs: 27, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037454188
Train loss (w/o reg) on all data: 0.03282105
Test loss (w/o reg) on all data: 0.011748282
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1176867e-06
Norm of the params: 9.626146
     Influence (LOO): fixed 491 labels. Loss 0.01175. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013374
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6601452e-08
Norm of the params: 6.092792
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26942316
Train loss (w/o reg) on all data: 0.26360798
Test loss (w/o reg) on all data: 0.10843262
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1579136e-05
Norm of the params: 10.784424
              Random: fixed 155 labels. Loss 0.10843. Accuracy 0.997.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34065416
Train loss (w/o reg) on all data: 0.33475122
Test loss (w/o reg) on all data: 0.16300923
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.5204276e-05
Norm of the params: 10.865504
Flipped loss: 0.16301. Accuracy: 0.992
### Flips: 615, rs: 28, checks: 205
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23170124
Train loss (w/o reg) on all data: 0.22234137
Test loss (w/o reg) on all data: 0.10124367
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.4433e-06
Norm of the params: 13.682012
     Influence (LOO): fixed 187 labels. Loss 0.10124. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20273668
Train loss (w/o reg) on all data: 0.19045678
Test loss (w/o reg) on all data: 0.109407574
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.588072e-06
Norm of the params: 15.671569
                Loss: fixed 205 labels. Loss 0.10941. Accuracy 0.980.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3314765
Train loss (w/o reg) on all data: 0.32556224
Test loss (w/o reg) on all data: 0.15584344
Train acc on all data:  0.8764891806467299
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.3126755e-06
Norm of the params: 10.875907
              Random: fixed  23 labels. Loss 0.15584. Accuracy 0.993.
### Flips: 615, rs: 28, checks: 410
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1596428
Train loss (w/o reg) on all data: 0.15064175
Test loss (w/o reg) on all data: 0.0665607
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.4205626e-06
Norm of the params: 13.417184
     Influence (LOO): fixed 312 labels. Loss 0.06656. Accuracy 0.997.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07001768
Train loss (w/o reg) on all data: 0.055660732
Test loss (w/o reg) on all data: 0.036086097
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.009093e-06
Norm of the params: 16.945173
                Loss: fixed 410 labels. Loss 0.03609. Accuracy 0.994.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31698895
Train loss (w/o reg) on all data: 0.31055325
Test loss (w/o reg) on all data: 0.1495413
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.7024958e-05
Norm of the params: 11.3452015
              Random: fixed  54 labels. Loss 0.14954. Accuracy 0.992.
### Flips: 615, rs: 28, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107840136
Train loss (w/o reg) on all data: 0.100036964
Test loss (w/o reg) on all data: 0.039928354
Train acc on all data:  0.9652321906151228
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5572658e-05
Norm of the params: 12.492535
     Influence (LOO): fixed 394 labels. Loss 0.03993. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048884796
Train loss (w/o reg) on all data: 0.0018235355
Test loss (w/o reg) on all data: 0.002892901
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8354125e-08
Norm of the params: 7.82936
                Loss: fixed 534 labels. Loss 0.00289. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30495495
Train loss (w/o reg) on all data: 0.29827103
Test loss (w/o reg) on all data: 0.14025728
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.0807143e-05
Norm of the params: 11.56194
              Random: fixed  79 labels. Loss 0.14026. Accuracy 0.991.
### Flips: 615, rs: 28, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076688804
Train loss (w/o reg) on all data: 0.069791675
Test loss (w/o reg) on all data: 0.027796058
Train acc on all data:  0.9769025042548019
Test acc on all data:   1.0
Norm of the mean of gradients: 9.346197e-06
Norm of the params: 11.744897
     Influence (LOO): fixed 442 labels. Loss 0.02780. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048884796
Train loss (w/o reg) on all data: 0.0018235357
Test loss (w/o reg) on all data: 0.0028928842
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.309701e-08
Norm of the params: 7.82936
                Loss: fixed 534 labels. Loss 0.00289. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2891905
Train loss (w/o reg) on all data: 0.28218696
Test loss (w/o reg) on all data: 0.13045426
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.372448e-05
Norm of the params: 11.835147
              Random: fixed 111 labels. Loss 0.13045. Accuracy 0.993.
### Flips: 615, rs: 28, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053657927
Train loss (w/o reg) on all data: 0.047711764
Test loss (w/o reg) on all data: 0.019024275
Train acc on all data:  0.9851689764162412
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5363886e-06
Norm of the params: 10.905194
     Influence (LOO): fixed 476 labels. Loss 0.01902. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004186409
Train loss (w/o reg) on all data: 0.0015017479
Test loss (w/o reg) on all data: 0.0025759176
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.83909e-08
Norm of the params: 7.3275657
                Loss: fixed 535 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27825755
Train loss (w/o reg) on all data: 0.27151677
Test loss (w/o reg) on all data: 0.12306196
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.936349e-06
Norm of the params: 11.611018
              Random: fixed 136 labels. Loss 0.12306. Accuracy 0.995.
### Flips: 615, rs: 28, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038151804
Train loss (w/o reg) on all data: 0.03282166
Test loss (w/o reg) on all data: 0.013176966
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8800723e-06
Norm of the params: 10.324867
     Influence (LOO): fixed 496 labels. Loss 0.01318. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3665953e-08
Norm of the params: 6.092808
                Loss: fixed 537 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26174888
Train loss (w/o reg) on all data: 0.25460336
Test loss (w/o reg) on all data: 0.114005916
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.2988134e-06
Norm of the params: 11.954506
              Random: fixed 170 labels. Loss 0.11401. Accuracy 0.994.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33137086
Train loss (w/o reg) on all data: 0.32503012
Test loss (w/o reg) on all data: 0.17008096
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.4484767e-05
Norm of the params: 11.261206
Flipped loss: 0.17008. Accuracy: 0.977
### Flips: 615, rs: 29, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22648358
Train loss (w/o reg) on all data: 0.21702822
Test loss (w/o reg) on all data: 0.10705842
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.156893e-05
Norm of the params: 13.751637
     Influence (LOO): fixed 180 labels. Loss 0.10706. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18777129
Train loss (w/o reg) on all data: 0.17404923
Test loss (w/o reg) on all data: 0.12973794
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.6272661e-05
Norm of the params: 16.566269
                Loss: fixed 205 labels. Loss 0.12974. Accuracy 0.967.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32194254
Train loss (w/o reg) on all data: 0.3157973
Test loss (w/o reg) on all data: 0.16019844
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.2850195e-05
Norm of the params: 11.086236
              Random: fixed  27 labels. Loss 0.16020. Accuracy 0.981.
### Flips: 615, rs: 29, checks: 410
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15303966
Train loss (w/o reg) on all data: 0.1445914
Test loss (w/o reg) on all data: 0.06212258
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.660586e-06
Norm of the params: 12.998661
     Influence (LOO): fixed 316 labels. Loss 0.06212. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06548906
Train loss (w/o reg) on all data: 0.0502714
Test loss (w/o reg) on all data: 0.057722807
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5080363e-06
Norm of the params: 17.445723
                Loss: fixed 406 labels. Loss 0.05772. Accuracy 0.986.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30521852
Train loss (w/o reg) on all data: 0.29903087
Test loss (w/o reg) on all data: 0.14700377
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.430321e-06
Norm of the params: 11.124433
              Random: fixed  65 labels. Loss 0.14700. Accuracy 0.984.
### Flips: 615, rs: 29, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10186991
Train loss (w/o reg) on all data: 0.09413775
Test loss (w/o reg) on all data: 0.037653115
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9594156e-06
Norm of the params: 12.435561
     Influence (LOO): fixed 397 labels. Loss 0.03765. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007974518
Train loss (w/o reg) on all data: 0.0035049596
Test loss (w/o reg) on all data: 0.00668143
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8592523e-07
Norm of the params: 9.454691
                Loss: fixed 518 labels. Loss 0.00668. Accuracy 0.999.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2936456
Train loss (w/o reg) on all data: 0.28738064
Test loss (w/o reg) on all data: 0.1396198
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.9845787e-06
Norm of the params: 11.193714
              Random: fixed  90 labels. Loss 0.13962. Accuracy 0.983.
### Flips: 615, rs: 29, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068439744
Train loss (w/o reg) on all data: 0.06100138
Test loss (w/o reg) on all data: 0.026129335
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.196069e-06
Norm of the params: 12.197023
     Influence (LOO): fixed 442 labels. Loss 0.02613. Accuracy 0.999.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006737929
Train loss (w/o reg) on all data: 0.0027958604
Test loss (w/o reg) on all data: 0.003910338
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5424703e-07
Norm of the params: 8.879267
                Loss: fixed 521 labels. Loss 0.00391. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27872813
Train loss (w/o reg) on all data: 0.27201116
Test loss (w/o reg) on all data: 0.13129434
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.0914878e-05
Norm of the params: 11.590482
              Random: fixed 119 labels. Loss 0.13129. Accuracy 0.984.
### Flips: 615, rs: 29, checks: 1025
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048157547
Train loss (w/o reg) on all data: 0.041697614
Test loss (w/o reg) on all data: 0.019397298
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3255609e-06
Norm of the params: 11.36656
     Influence (LOO): fixed 468 labels. Loss 0.01940. Accuracy 0.999.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0067379284
Train loss (w/o reg) on all data: 0.0027959174
Test loss (w/o reg) on all data: 0.0039101923
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0862203e-07
Norm of the params: 8.879202
                Loss: fixed 521 labels. Loss 0.00391. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26699057
Train loss (w/o reg) on all data: 0.26045513
Test loss (w/o reg) on all data: 0.124101356
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.426042e-05
Norm of the params: 11.432784
              Random: fixed 146 labels. Loss 0.12410. Accuracy 0.987.
### Flips: 615, rs: 29, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036014013
Train loss (w/o reg) on all data: 0.030300695
Test loss (w/o reg) on all data: 0.012889677
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5670015e-06
Norm of the params: 10.689543
     Influence (LOO): fixed 485 labels. Loss 0.01289. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006166627
Train loss (w/o reg) on all data: 0.0025381192
Test loss (w/o reg) on all data: 0.0033893264
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7577476e-07
Norm of the params: 8.518812
                Loss: fixed 523 labels. Loss 0.00339. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25796074
Train loss (w/o reg) on all data: 0.25169644
Test loss (w/o reg) on all data: 0.11552467
Train acc on all data:  0.913202042304887
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.7451265e-06
Norm of the params: 11.1931305
              Random: fixed 168 labels. Loss 0.11552. Accuracy 0.986.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3341535
Train loss (w/o reg) on all data: 0.32748958
Test loss (w/o reg) on all data: 0.14738075
Train acc on all data:  0.8735716022368101
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.9775498e-05
Norm of the params: 11.54463
Flipped loss: 0.14738. Accuracy: 0.992
### Flips: 615, rs: 30, checks: 205
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22871265
Train loss (w/o reg) on all data: 0.2200012
Test loss (w/o reg) on all data: 0.098149635
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.742248e-06
Norm of the params: 13.199579
     Influence (LOO): fixed 187 labels. Loss 0.09815. Accuracy 0.998.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19240992
Train loss (w/o reg) on all data: 0.17868839
Test loss (w/o reg) on all data: 0.0960358
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6471784e-05
Norm of the params: 16.565945
                Loss: fixed 205 labels. Loss 0.09604. Accuracy 0.984.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32158995
Train loss (w/o reg) on all data: 0.31520224
Test loss (w/o reg) on all data: 0.13953799
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.9575083e-05
Norm of the params: 11.302837
              Random: fixed  29 labels. Loss 0.13954. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14872989
Train loss (w/o reg) on all data: 0.1400593
Test loss (w/o reg) on all data: 0.062226217
Train acc on all data:  0.949671772428884
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6291639e-06
Norm of the params: 13.168583
     Influence (LOO): fixed 318 labels. Loss 0.06223. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06395259
Train loss (w/o reg) on all data: 0.049548108
Test loss (w/o reg) on all data: 0.031849757
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9509318e-06
Norm of the params: 16.9732
                Loss: fixed 410 labels. Loss 0.03185. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3122914
Train loss (w/o reg) on all data: 0.30587342
Test loss (w/o reg) on all data: 0.13440548
Train acc on all data:  0.8862144420131292
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9545309e-05
Norm of the params: 11.329592
              Random: fixed  52 labels. Loss 0.13441. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 615
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1016195
Train loss (w/o reg) on all data: 0.093853615
Test loss (w/o reg) on all data: 0.039224617
Train acc on all data:  0.9676635059567226
Test acc on all data:   1.0
Norm of the mean of gradients: 7.648312e-06
Norm of the params: 12.462648
     Influence (LOO): fixed 397 labels. Loss 0.03922. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054401653
Train loss (w/o reg) on all data: 0.002207693
Test loss (w/o reg) on all data: 0.003296337
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4647668e-08
Norm of the params: 8.040488
                Loss: fixed 525 labels. Loss 0.00330. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30279467
Train loss (w/o reg) on all data: 0.29627174
Test loss (w/o reg) on all data: 0.12734051
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.540942e-06
Norm of the params: 11.42183
              Random: fixed  74 labels. Loss 0.12734. Accuracy 0.994.
### Flips: 615, rs: 30, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07342417
Train loss (w/o reg) on all data: 0.06692782
Test loss (w/o reg) on all data: 0.025299514
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9073195e-06
Norm of the params: 11.398553
     Influence (LOO): fixed 438 labels. Loss 0.02530. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023472
Train loss (w/o reg) on all data: 0.0014920566
Test loss (w/o reg) on all data: 0.0029244528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9707264e-08
Norm of the params: 7.115357
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2910179
Train loss (w/o reg) on all data: 0.2846675
Test loss (w/o reg) on all data: 0.11942661
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1466659e-05
Norm of the params: 11.269778
              Random: fixed 102 labels. Loss 0.11943. Accuracy 0.996.
### Flips: 615, rs: 30, checks: 1025
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05358411
Train loss (w/o reg) on all data: 0.047569342
Test loss (w/o reg) on all data: 0.019113561
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4861604e-06
Norm of the params: 10.967925
     Influence (LOO): fixed 465 labels. Loss 0.01911. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023473
Train loss (w/o reg) on all data: 0.0014920764
Test loss (w/o reg) on all data: 0.0029244632
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4710714e-08
Norm of the params: 7.11533
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2764175
Train loss (w/o reg) on all data: 0.27008432
Test loss (w/o reg) on all data: 0.110867545
Train acc on all data:  0.9049355701434476
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.786596e-06
Norm of the params: 11.254475
              Random: fixed 131 labels. Loss 0.11087. Accuracy 0.996.
### Flips: 615, rs: 30, checks: 1230
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039958403
Train loss (w/o reg) on all data: 0.03516559
Test loss (w/o reg) on all data: 0.013129422
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7666738e-06
Norm of the params: 9.790621
     Influence (LOO): fixed 485 labels. Loss 0.01313. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023473
Train loss (w/o reg) on all data: 0.0014920873
Test loss (w/o reg) on all data: 0.002924485
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.799324e-08
Norm of the params: 7.1153154
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26488477
Train loss (w/o reg) on all data: 0.25864527
Test loss (w/o reg) on all data: 0.10418423
Train acc on all data:  0.9100413323608072
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.769684e-06
Norm of the params: 11.170938
              Random: fixed 155 labels. Loss 0.10418. Accuracy 0.997.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33346385
Train loss (w/o reg) on all data: 0.32700348
Test loss (w/o reg) on all data: 0.15941758
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.983167e-05
Norm of the params: 11.366955
Flipped loss: 0.15942. Accuracy: 0.986
### Flips: 615, rs: 31, checks: 205
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22763075
Train loss (w/o reg) on all data: 0.21885602
Test loss (w/o reg) on all data: 0.10026579
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.774634e-06
Norm of the params: 13.247441
     Influence (LOO): fixed 187 labels. Loss 0.10027. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19070303
Train loss (w/o reg) on all data: 0.1778714
Test loss (w/o reg) on all data: 0.11134828
Train acc on all data:  0.9273036712861659
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.2237575e-06
Norm of the params: 16.019753
                Loss: fixed 205 labels. Loss 0.11135. Accuracy 0.976.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31693023
Train loss (w/o reg) on all data: 0.30995157
Test loss (w/o reg) on all data: 0.15000544
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.9090796e-05
Norm of the params: 11.8141165
              Random: fixed  35 labels. Loss 0.15001. Accuracy 0.989.
### Flips: 615, rs: 31, checks: 410
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15593675
Train loss (w/o reg) on all data: 0.14784306
Test loss (w/o reg) on all data: 0.060595915
Train acc on all data:  0.9469973255531242
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4284032e-05
Norm of the params: 12.722955
     Influence (LOO): fixed 312 labels. Loss 0.06060. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06630491
Train loss (w/o reg) on all data: 0.05125526
Test loss (w/o reg) on all data: 0.049519908
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.0293628e-05
Norm of the params: 17.349146
                Loss: fixed 410 labels. Loss 0.04952. Accuracy 0.985.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30804017
Train loss (w/o reg) on all data: 0.30113685
Test loss (w/o reg) on all data: 0.14248312
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.3121307e-05
Norm of the params: 11.750172
              Random: fixed  56 labels. Loss 0.14248. Accuracy 0.990.
### Flips: 615, rs: 31, checks: 615
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10340326
Train loss (w/o reg) on all data: 0.09614614
Test loss (w/o reg) on all data: 0.03590283
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5936063e-06
Norm of the params: 12.047512
     Influence (LOO): fixed 396 labels. Loss 0.03590. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043785744
Train loss (w/o reg) on all data: 0.0018253187
Test loss (w/o reg) on all data: 0.0033195189
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.455238e-08
Norm of the params: 7.1459856
                Loss: fixed 527 labels. Loss 0.00332. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29875833
Train loss (w/o reg) on all data: 0.291731
Test loss (w/o reg) on all data: 0.1350679
Train acc on all data:  0.8935083880379285
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.624344e-06
Norm of the params: 11.855225
              Random: fixed  77 labels. Loss 0.13507. Accuracy 0.991.
### Flips: 615, rs: 31, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0697444
Train loss (w/o reg) on all data: 0.06365159
Test loss (w/o reg) on all data: 0.02349662
Train acc on all data:  0.9800632141988816
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2895516e-06
Norm of the params: 11.038847
     Influence (LOO): fixed 448 labels. Loss 0.02350. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034206016
Train loss (w/o reg) on all data: 0.0011897179
Test loss (w/o reg) on all data: 0.0025928807
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8479048e-08
Norm of the params: 6.6796465
                Loss: fixed 529 labels. Loss 0.00259. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28612047
Train loss (w/o reg) on all data: 0.27902263
Test loss (w/o reg) on all data: 0.1275911
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6442891e-05
Norm of the params: 11.914562
              Random: fixed 105 labels. Loss 0.12759. Accuracy 0.988.
### Flips: 615, rs: 31, checks: 1025
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051282324
Train loss (w/o reg) on all data: 0.04594933
Test loss (w/o reg) on all data: 0.017693454
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3186533e-06
Norm of the params: 10.327627
     Influence (LOO): fixed 471 labels. Loss 0.01769. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601321
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7009401e-08
Norm of the params: 6.0927944
                Loss: fixed 530 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27439567
Train loss (w/o reg) on all data: 0.2672381
Test loss (w/o reg) on all data: 0.12029329
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.536312e-05
Norm of the params: 11.964584
              Random: fixed 132 labels. Loss 0.12029. Accuracy 0.990.
### Flips: 615, rs: 31, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034539204
Train loss (w/o reg) on all data: 0.029831475
Test loss (w/o reg) on all data: 0.012435629
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0891246e-06
Norm of the params: 9.703327
     Influence (LOO): fixed 493 labels. Loss 0.01244. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601388
Test loss (w/o reg) on all data: 0.0026561052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.8997636e-08
Norm of the params: 6.0927844
                Loss: fixed 530 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26038182
Train loss (w/o reg) on all data: 0.25339264
Test loss (w/o reg) on all data: 0.10799842
Train acc on all data:  0.911986384634087
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3213195e-05
Norm of the params: 11.823023
              Random: fixed 162 labels. Loss 0.10800. Accuracy 0.993.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34022105
Train loss (w/o reg) on all data: 0.33326095
Test loss (w/o reg) on all data: 0.16066901
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5698128e-05
Norm of the params: 11.798395
Flipped loss: 0.16067. Accuracy: 0.986
### Flips: 615, rs: 32, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23067464
Train loss (w/o reg) on all data: 0.2213696
Test loss (w/o reg) on all data: 0.09881987
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.114046e-05
Norm of the params: 13.641885
     Influence (LOO): fixed 185 labels. Loss 0.09882. Accuracy 0.993.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19932576
Train loss (w/o reg) on all data: 0.1857332
Test loss (w/o reg) on all data: 0.10950132
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.008644e-06
Norm of the params: 16.487906
                Loss: fixed 205 labels. Loss 0.10950. Accuracy 0.978.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317062
Train loss (w/o reg) on all data: 0.3249858
Test loss (w/o reg) on all data: 0.15347359
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.204085e-05
Norm of the params: 11.593445
              Random: fixed  25 labels. Loss 0.15347. Accuracy 0.988.
### Flips: 615, rs: 32, checks: 410
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15209916
Train loss (w/o reg) on all data: 0.14309175
Test loss (w/o reg) on all data: 0.062557824
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4786312e-06
Norm of the params: 13.421928
     Influence (LOO): fixed 316 labels. Loss 0.06256. Accuracy 0.997.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069976136
Train loss (w/o reg) on all data: 0.054323684
Test loss (w/o reg) on all data: 0.05538289
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7338614e-06
Norm of the params: 17.693195
                Loss: fixed 410 labels. Loss 0.05538. Accuracy 0.983.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3219869
Train loss (w/o reg) on all data: 0.31494677
Test loss (w/o reg) on all data: 0.14313318
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4975792e-05
Norm of the params: 11.866035
              Random: fixed  47 labels. Loss 0.14313. Accuracy 0.990.
### Flips: 615, rs: 32, checks: 615
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10247628
Train loss (w/o reg) on all data: 0.09469337
Test loss (w/o reg) on all data: 0.039233148
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2597205e-06
Norm of the params: 12.476302
     Influence (LOO): fixed 395 labels. Loss 0.03923. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059745936
Train loss (w/o reg) on all data: 0.0026378846
Test loss (w/o reg) on all data: 0.0050558923
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8932099e-07
Norm of the params: 8.169099
                Loss: fixed 524 labels. Loss 0.00506. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30787855
Train loss (w/o reg) on all data: 0.3006042
Test loss (w/o reg) on all data: 0.13376516
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.495743e-05
Norm of the params: 12.061798
              Random: fixed  81 labels. Loss 0.13377. Accuracy 0.988.
### Flips: 615, rs: 32, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072511636
Train loss (w/o reg) on all data: 0.06517815
Test loss (w/o reg) on all data: 0.026876051
Train acc on all data:  0.9781181619256017
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5733275e-06
Norm of the params: 12.110728
     Influence (LOO): fixed 441 labels. Loss 0.02688. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035330732
Train loss (w/o reg) on all data: 0.001273004
Test loss (w/o reg) on all data: 0.0034286093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.206071e-08
Norm of the params: 6.723198
                Loss: fixed 529 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29909405
Train loss (w/o reg) on all data: 0.29187325
Test loss (w/o reg) on all data: 0.12785384
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2840813e-05
Norm of the params: 12.017319
              Random: fixed 101 labels. Loss 0.12785. Accuracy 0.992.
### Flips: 615, rs: 32, checks: 1025
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056311987
Train loss (w/o reg) on all data: 0.050122414
Test loss (w/o reg) on all data: 0.020198248
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1183073e-06
Norm of the params: 11.126161
     Influence (LOO): fixed 464 labels. Loss 0.02020. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035330732
Train loss (w/o reg) on all data: 0.0012730471
Test loss (w/o reg) on all data: 0.0034286643
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4369539e-07
Norm of the params: 6.723134
                Loss: fixed 529 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28272462
Train loss (w/o reg) on all data: 0.27514946
Test loss (w/o reg) on all data: 0.11790861
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.5937475e-06
Norm of the params: 12.308656
              Random: fixed 133 labels. Loss 0.11791. Accuracy 0.991.
### Flips: 615, rs: 32, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04433215
Train loss (w/o reg) on all data: 0.0389276
Test loss (w/o reg) on all data: 0.016705386
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2907657e-06
Norm of the params: 10.396684
     Influence (LOO): fixed 480 labels. Loss 0.01671. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362247
Train loss (w/o reg) on all data: 0.0011011972
Test loss (w/o reg) on all data: 0.002976784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.147066e-08
Norm of the params: 6.2209764
                Loss: fixed 530 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2729268
Train loss (w/o reg) on all data: 0.26534382
Test loss (w/o reg) on all data: 0.11282508
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.0251314e-06
Norm of the params: 12.315028
              Random: fixed 151 labels. Loss 0.11283. Accuracy 0.988.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34124428
Train loss (w/o reg) on all data: 0.33573514
Test loss (w/o reg) on all data: 0.15919712
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1785592e-05
Norm of the params: 10.496809
Flipped loss: 0.15920. Accuracy: 0.988
### Flips: 615, rs: 33, checks: 205
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23077345
Train loss (w/o reg) on all data: 0.22190227
Test loss (w/o reg) on all data: 0.10005572
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3801713e-05
Norm of the params: 13.320048
     Influence (LOO): fixed 186 labels. Loss 0.10006. Accuracy 0.998.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2029578
Train loss (w/o reg) on all data: 0.1908476
Test loss (w/o reg) on all data: 0.10440285
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.7796352e-05
Norm of the params: 15.5629
                Loss: fixed 205 labels. Loss 0.10440. Accuracy 0.975.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3321297
Train loss (w/o reg) on all data: 0.32654056
Test loss (w/o reg) on all data: 0.15214941
Train acc on all data:  0.8769754437150499
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3363275e-05
Norm of the params: 10.572732
              Random: fixed  22 labels. Loss 0.15215. Accuracy 0.993.
### Flips: 615, rs: 33, checks: 410
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15896045
Train loss (w/o reg) on all data: 0.15100569
Test loss (w/o reg) on all data: 0.06276689
Train acc on all data:  0.9469973255531242
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5178409e-05
Norm of the params: 12.613298
     Influence (LOO): fixed 315 labels. Loss 0.06277. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07596011
Train loss (w/o reg) on all data: 0.062150933
Test loss (w/o reg) on all data: 0.04800365
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6540702e-06
Norm of the params: 16.618769
                Loss: fixed 410 labels. Loss 0.04800. Accuracy 0.988.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32307366
Train loss (w/o reg) on all data: 0.31741846
Test loss (w/o reg) on all data: 0.14572956
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.2559016e-06
Norm of the params: 10.635038
              Random: fixed  42 labels. Loss 0.14573. Accuracy 0.991.
### Flips: 615, rs: 33, checks: 615
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10918095
Train loss (w/o reg) on all data: 0.1020771
Test loss (w/o reg) on all data: 0.039988518
Train acc on all data:  0.9664478482859227
Test acc on all data:   1.0
Norm of the mean of gradients: 7.217681e-06
Norm of the params: 11.919609
     Influence (LOO): fixed 396 labels. Loss 0.03999. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060113138
Train loss (w/o reg) on all data: 0.0023209166
Test loss (w/o reg) on all data: 0.0032206296
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6323548e-07
Norm of the params: 8.591155
                Loss: fixed 531 labels. Loss 0.00322. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3120011
Train loss (w/o reg) on all data: 0.30617243
Test loss (w/o reg) on all data: 0.139163
Train acc on all data:  0.888402625820569
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4025549e-05
Norm of the params: 10.796926
              Random: fixed  67 labels. Loss 0.13916. Accuracy 0.990.
### Flips: 615, rs: 33, checks: 820
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0707934
Train loss (w/o reg) on all data: 0.06437952
Test loss (w/o reg) on all data: 0.025248332
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8636675e-06
Norm of the params: 11.325969
     Influence (LOO): fixed 450 labels. Loss 0.02525. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091934
Train loss (w/o reg) on all data: 0.001179342
Test loss (w/o reg) on all data: 0.0029090338
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.544357e-08
Norm of the params: 6.6781
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3012923
Train loss (w/o reg) on all data: 0.2954728
Test loss (w/o reg) on all data: 0.1327475
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0426905e-05
Norm of the params: 10.78842
              Random: fixed  89 labels. Loss 0.13275. Accuracy 0.991.
### Flips: 615, rs: 33, checks: 1025
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048325393
Train loss (w/o reg) on all data: 0.042723868
Test loss (w/o reg) on all data: 0.017945362
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8589917e-07
Norm of the params: 10.584446
     Influence (LOO): fixed 479 labels. Loss 0.01795. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091924
Train loss (w/o reg) on all data: 0.0011793463
Test loss (w/o reg) on all data: 0.0029090699
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.810395e-08
Norm of the params: 6.678093
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28911313
Train loss (w/o reg) on all data: 0.28300172
Test loss (w/o reg) on all data: 0.12482204
Train acc on all data:  0.9008023340627279
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.196231e-06
Norm of the params: 11.055682
              Random: fixed 116 labels. Loss 0.12482. Accuracy 0.993.
### Flips: 615, rs: 33, checks: 1230
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036664575
Train loss (w/o reg) on all data: 0.03183069
Test loss (w/o reg) on all data: 0.014186179
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1333592e-06
Norm of the params: 9.832482
     Influence (LOO): fixed 492 labels. Loss 0.01419. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091931
Train loss (w/o reg) on all data: 0.0011793356
Test loss (w/o reg) on all data: 0.0029090606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.908396e-08
Norm of the params: 6.67811
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277223
Train loss (w/o reg) on all data: 0.27105448
Test loss (w/o reg) on all data: 0.11796891
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6824013e-05
Norm of the params: 11.107223
              Random: fixed 141 labels. Loss 0.11797. Accuracy 0.995.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33680874
Train loss (w/o reg) on all data: 0.3299984
Test loss (w/o reg) on all data: 0.15702745
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.429987e-05
Norm of the params: 11.670769
Flipped loss: 0.15703. Accuracy: 0.987
### Flips: 615, rs: 34, checks: 205
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2363902
Train loss (w/o reg) on all data: 0.22613141
Test loss (w/o reg) on all data: 0.10203187
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.62474e-05
Norm of the params: 14.323958
     Influence (LOO): fixed 182 labels. Loss 0.10203. Accuracy 0.996.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19699025
Train loss (w/o reg) on all data: 0.1837189
Test loss (w/o reg) on all data: 0.11081733
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.188554e-06
Norm of the params: 16.29193
                Loss: fixed 204 labels. Loss 0.11082. Accuracy 0.971.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32257366
Train loss (w/o reg) on all data: 0.3155659
Test loss (w/o reg) on all data: 0.14688602
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.857647e-05
Norm of the params: 11.8387165
              Random: fixed  32 labels. Loss 0.14689. Accuracy 0.990.
### Flips: 615, rs: 34, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16484042
Train loss (w/o reg) on all data: 0.1563394
Test loss (w/o reg) on all data: 0.06442793
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2562513e-05
Norm of the params: 13.039182
     Influence (LOO): fixed 309 labels. Loss 0.06443. Accuracy 0.998.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06945739
Train loss (w/o reg) on all data: 0.053482
Test loss (w/o reg) on all data: 0.046314154
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.8925936e-06
Norm of the params: 17.874783
                Loss: fixed 408 labels. Loss 0.04631. Accuracy 0.982.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31721294
Train loss (w/o reg) on all data: 0.31024247
Test loss (w/o reg) on all data: 0.14166856
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.2466387e-05
Norm of the params: 11.80718
              Random: fixed  47 labels. Loss 0.14167. Accuracy 0.993.
### Flips: 615, rs: 34, checks: 615
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10384309
Train loss (w/o reg) on all data: 0.095983826
Test loss (w/o reg) on all data: 0.03840869
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7882502e-06
Norm of the params: 12.53736
     Influence (LOO): fixed 398 labels. Loss 0.03841. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00800826
Train loss (w/o reg) on all data: 0.003643776
Test loss (w/o reg) on all data: 0.004313317
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.252483e-08
Norm of the params: 9.342895
                Loss: fixed 523 labels. Loss 0.00431. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3040853
Train loss (w/o reg) on all data: 0.2974529
Test loss (w/o reg) on all data: 0.1326876
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.756528e-06
Norm of the params: 11.517294
              Random: fixed  79 labels. Loss 0.13269. Accuracy 0.991.
### Flips: 615, rs: 34, checks: 820
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07454037
Train loss (w/o reg) on all data: 0.06774673
Test loss (w/o reg) on all data: 0.027940381
Train acc on all data:  0.9766593727206418
Test acc on all data:   1.0
Norm of the mean of gradients: 2.107581e-06
Norm of the params: 11.656447
     Influence (LOO): fixed 440 labels. Loss 0.02794. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046856613
Train loss (w/o reg) on all data: 0.0019460734
Test loss (w/o reg) on all data: 0.0036926472
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0415361e-07
Norm of the params: 7.402146
                Loss: fixed 531 labels. Loss 0.00369. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29379395
Train loss (w/o reg) on all data: 0.28692728
Test loss (w/o reg) on all data: 0.12699941
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.007104e-06
Norm of the params: 11.718917
              Random: fixed 101 labels. Loss 0.12700. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046472844
Train loss (w/o reg) on all data: 0.04041531
Test loss (w/o reg) on all data: 0.018526992
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2896085e-06
Norm of the params: 11.006848
     Influence (LOO): fixed 479 labels. Loss 0.01853. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.272939e-08
Norm of the params: 6.092821
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2812355
Train loss (w/o reg) on all data: 0.27426597
Test loss (w/o reg) on all data: 0.11834966
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.725781e-06
Norm of the params: 11.806362
              Random: fixed 130 labels. Loss 0.11835. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1230
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03794877
Train loss (w/o reg) on all data: 0.03272954
Test loss (w/o reg) on all data: 0.014474973
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7612654e-06
Norm of the params: 10.216876
     Influence (LOO): fixed 491 labels. Loss 0.01447. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010725
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.85647e-08
Norm of the params: 6.092836
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269514
Train loss (w/o reg) on all data: 0.2625712
Test loss (w/o reg) on all data: 0.109781355
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.6721996e-06
Norm of the params: 11.783732
              Random: fixed 155 labels. Loss 0.10978. Accuracy 0.994.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33802563
Train loss (w/o reg) on all data: 0.33137766
Test loss (w/o reg) on all data: 0.16098398
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.547989e-06
Norm of the params: 11.530817
Flipped loss: 0.16098. Accuracy: 0.988
### Flips: 615, rs: 35, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22866301
Train loss (w/o reg) on all data: 0.21967553
Test loss (w/o reg) on all data: 0.10107044
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.8364208e-06
Norm of the params: 13.407073
     Influence (LOO): fixed 191 labels. Loss 0.10107. Accuracy 0.998.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2018789
Train loss (w/o reg) on all data: 0.1896433
Test loss (w/o reg) on all data: 0.10729536
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0793939e-05
Norm of the params: 15.643277
                Loss: fixed 205 labels. Loss 0.10730. Accuracy 0.975.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32753205
Train loss (w/o reg) on all data: 0.32083187
Test loss (w/o reg) on all data: 0.15246461
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.8364719e-05
Norm of the params: 11.576006
              Random: fixed  26 labels. Loss 0.15246. Accuracy 0.990.
### Flips: 615, rs: 35, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16367073
Train loss (w/o reg) on all data: 0.15490967
Test loss (w/o reg) on all data: 0.06542116
Train acc on all data:  0.9452954048140044
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.51255e-06
Norm of the params: 13.237115
     Influence (LOO): fixed 307 labels. Loss 0.06542. Accuracy 0.998.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07269869
Train loss (w/o reg) on all data: 0.05871153
Test loss (w/o reg) on all data: 0.04274699
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.8886935e-07
Norm of the params: 16.725529
                Loss: fixed 409 labels. Loss 0.04275. Accuracy 0.984.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3146333
Train loss (w/o reg) on all data: 0.3074358
Test loss (w/o reg) on all data: 0.1432246
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.8566355e-05
Norm of the params: 11.997914
              Random: fixed  54 labels. Loss 0.14322. Accuracy 0.992.
### Flips: 615, rs: 35, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11295895
Train loss (w/o reg) on all data: 0.10449494
Test loss (w/o reg) on all data: 0.042155366
Train acc on all data:  0.9640165329443229
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6607327e-06
Norm of the params: 13.010775
     Influence (LOO): fixed 392 labels. Loss 0.04216. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007421744
Train loss (w/o reg) on all data: 0.0033416764
Test loss (w/o reg) on all data: 0.00568283
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7504335e-07
Norm of the params: 9.033347
                Loss: fixed 531 labels. Loss 0.00568. Accuracy 0.998.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3050007
Train loss (w/o reg) on all data: 0.29773173
Test loss (w/o reg) on all data: 0.13706034
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0154898e-05
Norm of the params: 12.057338
              Random: fixed  76 labels. Loss 0.13706. Accuracy 0.992.
### Flips: 615, rs: 35, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08219425
Train loss (w/o reg) on all data: 0.0750938
Test loss (w/o reg) on all data: 0.029278075
Train acc on all data:  0.975443715049842
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4929706e-06
Norm of the params: 11.916751
     Influence (LOO): fixed 439 labels. Loss 0.02928. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601019
Test loss (w/o reg) on all data: 0.002656029
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2583293e-08
Norm of the params: 6.092845
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2896842
Train loss (w/o reg) on all data: 0.2824227
Test loss (w/o reg) on all data: 0.12708035
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.016216e-05
Norm of the params: 12.051144
              Random: fixed 110 labels. Loss 0.12708. Accuracy 0.995.
### Flips: 615, rs: 35, checks: 1025
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059749566
Train loss (w/o reg) on all data: 0.053347856
Test loss (w/o reg) on all data: 0.020441012
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4927055e-06
Norm of the params: 11.31522
     Influence (LOO): fixed 472 labels. Loss 0.02044. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8166594e-08
Norm of the params: 6.092814
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27730495
Train loss (w/o reg) on all data: 0.27014208
Test loss (w/o reg) on all data: 0.11956835
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.679631e-06
Norm of the params: 11.96901
              Random: fixed 137 labels. Loss 0.11957. Accuracy 0.995.
### Flips: 615, rs: 35, checks: 1230
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045337215
Train loss (w/o reg) on all data: 0.039256062
Test loss (w/o reg) on all data: 0.014544065
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3933061e-06
Norm of the params: 11.028284
     Influence (LOO): fixed 489 labels. Loss 0.01454. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5299795e-08
Norm of the params: 6.0928206
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26384264
Train loss (w/o reg) on all data: 0.2564896
Test loss (w/o reg) on all data: 0.111659735
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4936997e-05
Norm of the params: 12.12686
              Random: fixed 166 labels. Loss 0.11166. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3310782
Train loss (w/o reg) on all data: 0.32421505
Test loss (w/o reg) on all data: 0.1552266
Train acc on all data:  0.8764891806467299
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.711864e-06
Norm of the params: 11.715935
Flipped loss: 0.15523. Accuracy: 0.986
### Flips: 615, rs: 36, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23037685
Train loss (w/o reg) on all data: 0.22171396
Test loss (w/o reg) on all data: 0.101366304
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.559296e-05
Norm of the params: 13.162751
     Influence (LOO): fixed 181 labels. Loss 0.10137. Accuracy 0.994.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19060647
Train loss (w/o reg) on all data: 0.17662482
Test loss (w/o reg) on all data: 0.101166986
Train acc on all data:  0.9285193289569658
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.85042e-06
Norm of the params: 16.72223
                Loss: fixed 205 labels. Loss 0.10117. Accuracy 0.983.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32124603
Train loss (w/o reg) on all data: 0.31400254
Test loss (w/o reg) on all data: 0.14943415
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1687985e-05
Norm of the params: 12.03617
              Random: fixed  19 labels. Loss 0.14943. Accuracy 0.985.
### Flips: 615, rs: 36, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14971237
Train loss (w/o reg) on all data: 0.14137508
Test loss (w/o reg) on all data: 0.059115004
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.868923e-06
Norm of the params: 12.913008
     Influence (LOO): fixed 324 labels. Loss 0.05912. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06204537
Train loss (w/o reg) on all data: 0.047318958
Test loss (w/o reg) on all data: 0.038229827
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1665967e-06
Norm of the params: 17.161825
                Loss: fixed 410 labels. Loss 0.03823. Accuracy 0.989.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3099796
Train loss (w/o reg) on all data: 0.30275232
Test loss (w/o reg) on all data: 0.1429506
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9233514e-05
Norm of the params: 12.022717
              Random: fixed  44 labels. Loss 0.14295. Accuracy 0.984.
### Flips: 615, rs: 36, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09678482
Train loss (w/o reg) on all data: 0.0888459
Test loss (w/o reg) on all data: 0.036313184
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4853687e-06
Norm of the params: 12.600733
     Influence (LOO): fixed 400 labels. Loss 0.03631. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005756858
Train loss (w/o reg) on all data: 0.0023774419
Test loss (w/o reg) on all data: 0.006291648
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.4291734e-08
Norm of the params: 8.221211
                Loss: fixed 521 labels. Loss 0.00629. Accuracy 0.998.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298131
Train loss (w/o reg) on all data: 0.29100963
Test loss (w/o reg) on all data: 0.13621691
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.790357e-06
Norm of the params: 11.934284
              Random: fixed  71 labels. Loss 0.13622. Accuracy 0.987.
### Flips: 615, rs: 36, checks: 820
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06719948
Train loss (w/o reg) on all data: 0.06030535
Test loss (w/o reg) on all data: 0.02526421
Train acc on all data:  0.9793338195964016
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7195127e-06
Norm of the params: 11.742348
     Influence (LOO): fixed 441 labels. Loss 0.02526. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003936438
Train loss (w/o reg) on all data: 0.0015328514
Test loss (w/o reg) on all data: 0.004602556
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.5487484e-08
Norm of the params: 6.933378
                Loss: fixed 525 labels. Loss 0.00460. Accuracy 0.999.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2780371
Train loss (w/o reg) on all data: 0.27036703
Test loss (w/o reg) on all data: 0.1253601
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.186017e-06
Norm of the params: 12.385542
              Random: fixed 110 labels. Loss 0.12536. Accuracy 0.985.
### Flips: 615, rs: 36, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05176158
Train loss (w/o reg) on all data: 0.046017785
Test loss (w/o reg) on all data: 0.018440364
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 1.465654e-06
Norm of the params: 10.718016
     Influence (LOO): fixed 468 labels. Loss 0.01844. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215606
Train loss (w/o reg) on all data: 0.0011097605
Test loss (w/o reg) on all data: 0.0036361483
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4355729e-08
Norm of the params: 6.1835265
                Loss: fixed 526 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2683644
Train loss (w/o reg) on all data: 0.2605983
Test loss (w/o reg) on all data: 0.11830799
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.5009694e-06
Norm of the params: 12.462832
              Random: fixed 131 labels. Loss 0.11831. Accuracy 0.987.
### Flips: 615, rs: 36, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037248917
Train loss (w/o reg) on all data: 0.032069225
Test loss (w/o reg) on all data: 0.014857658
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.0952464e-07
Norm of the params: 10.178105
     Influence (LOO): fixed 487 labels. Loss 0.01486. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011267
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5637454e-08
Norm of the params: 6.092826
                Loss: fixed 527 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2555217
Train loss (w/o reg) on all data: 0.24794476
Test loss (w/o reg) on all data: 0.10623782
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.917766e-06
Norm of the params: 12.310116
              Random: fixed 165 labels. Loss 0.10624. Accuracy 0.991.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3315459
Train loss (w/o reg) on all data: 0.32488835
Test loss (w/o reg) on all data: 0.15236342
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.16748115e-05
Norm of the params: 11.539096
Flipped loss: 0.15236. Accuracy: 0.990
### Flips: 615, rs: 37, checks: 205
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22440653
Train loss (w/o reg) on all data: 0.21525165
Test loss (w/o reg) on all data: 0.09514582
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.41997725e-05
Norm of the params: 13.53135
     Influence (LOO): fixed 185 labels. Loss 0.09515. Accuracy 0.994.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18960133
Train loss (w/o reg) on all data: 0.17547862
Test loss (w/o reg) on all data: 0.093106374
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.1186116e-06
Norm of the params: 16.806374
                Loss: fixed 204 labels. Loss 0.09311. Accuracy 0.987.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3198232
Train loss (w/o reg) on all data: 0.3133331
Test loss (w/o reg) on all data: 0.14257082
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5275882e-05
Norm of the params: 11.393074
              Random: fixed  31 labels. Loss 0.14257. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 410
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641666
Train loss (w/o reg) on all data: 0.13724805
Test loss (w/o reg) on all data: 0.05871808
Train acc on all data:  0.949428640894724
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9336846e-06
Norm of the params: 13.541503
     Influence (LOO): fixed 316 labels. Loss 0.05872. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06963726
Train loss (w/o reg) on all data: 0.05518586
Test loss (w/o reg) on all data: 0.035425294
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7392291e-06
Norm of the params: 17.000828
                Loss: fixed 409 labels. Loss 0.03543. Accuracy 0.990.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30821475
Train loss (w/o reg) on all data: 0.30197498
Test loss (w/o reg) on all data: 0.13412493
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.369216e-06
Norm of the params: 11.171177
              Random: fixed  58 labels. Loss 0.13412. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09791408
Train loss (w/o reg) on all data: 0.08989611
Test loss (w/o reg) on all data: 0.036044933
Train acc on all data:  0.9664478482859227
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6591254e-06
Norm of the params: 12.663308
     Influence (LOO): fixed 388 labels. Loss 0.03604. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071039693
Train loss (w/o reg) on all data: 0.0031159767
Test loss (w/o reg) on all data: 0.005011896
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4665069e-07
Norm of the params: 8.930838
                Loss: fixed 520 labels. Loss 0.00501. Accuracy 0.998.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29980767
Train loss (w/o reg) on all data: 0.29362983
Test loss (w/o reg) on all data: 0.12933938
Train acc on all data:  0.8935083880379285
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.335945e-06
Norm of the params: 11.115608
              Random: fixed  81 labels. Loss 0.12934. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0752281
Train loss (w/o reg) on all data: 0.06771588
Test loss (w/o reg) on all data: 0.031038348
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2595427e-06
Norm of the params: 12.257421
     Influence (LOO): fixed 423 labels. Loss 0.03104. Accuracy 0.999.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047900416
Train loss (w/o reg) on all data: 0.0018329821
Test loss (w/o reg) on all data: 0.004735605
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.293879e-08
Norm of the params: 7.6903305
                Loss: fixed 524 labels. Loss 0.00474. Accuracy 0.999.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28732002
Train loss (w/o reg) on all data: 0.28071752
Test loss (w/o reg) on all data: 0.12298041
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.376573e-05
Norm of the params: 11.491302
              Random: fixed 107 labels. Loss 0.12298. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05662559
Train loss (w/o reg) on all data: 0.050332133
Test loss (w/o reg) on all data: 0.021711662
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6108053e-06
Norm of the params: 11.219143
     Influence (LOO): fixed 454 labels. Loss 0.02171. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039342325
Train loss (w/o reg) on all data: 0.0013878134
Test loss (w/o reg) on all data: 0.003309214
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.014524e-08
Norm of the params: 7.136412
                Loss: fixed 525 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27147925
Train loss (w/o reg) on all data: 0.26475054
Test loss (w/o reg) on all data: 0.11409855
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0563235e-05
Norm of the params: 11.60062
              Random: fixed 139 labels. Loss 0.11410. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436895
Train loss (w/o reg) on all data: 0.03802124
Test loss (w/o reg) on all data: 0.016841643
Train acc on all data:  0.986627765621201
Test acc on all data:   1.0
Norm of the mean of gradients: 8.886676e-07
Norm of the params: 10.647308
     Influence (LOO): fixed 472 labels. Loss 0.01684. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003934232
Train loss (w/o reg) on all data: 0.0013878149
Test loss (w/o reg) on all data: 0.0033092317
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.036173e-08
Norm of the params: 7.13641
                Loss: fixed 525 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26021677
Train loss (w/o reg) on all data: 0.25328043
Test loss (w/o reg) on all data: 0.10876453
Train acc on all data:  0.913202042304887
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.6703866e-05
Norm of the params: 11.778247
              Random: fixed 161 labels. Loss 0.10876. Accuracy 0.996.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33792454
Train loss (w/o reg) on all data: 0.33187932
Test loss (w/o reg) on all data: 0.15768756
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2113606e-05
Norm of the params: 10.995649
Flipped loss: 0.15769. Accuracy: 0.987
### Flips: 615, rs: 38, checks: 205
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22731505
Train loss (w/o reg) on all data: 0.21809201
Test loss (w/o reg) on all data: 0.098463416
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7593109e-05
Norm of the params: 13.581635
     Influence (LOO): fixed 190 labels. Loss 0.09846. Accuracy 0.995.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19766153
Train loss (w/o reg) on all data: 0.18446875
Test loss (w/o reg) on all data: 0.105159104
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.0643894e-06
Norm of the params: 16.243635
                Loss: fixed 205 labels. Loss 0.10516. Accuracy 0.981.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3261277
Train loss (w/o reg) on all data: 0.31988913
Test loss (w/o reg) on all data: 0.15041065
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4545386e-05
Norm of the params: 11.170107
              Random: fixed  28 labels. Loss 0.15041. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 410
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1557804
Train loss (w/o reg) on all data: 0.1473676
Test loss (w/o reg) on all data: 0.05975484
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.016158e-05
Norm of the params: 12.971362
     Influence (LOO): fixed 316 labels. Loss 0.05975. Accuracy 0.999.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07042335
Train loss (w/o reg) on all data: 0.055282474
Test loss (w/o reg) on all data: 0.042794876
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.219924e-06
Norm of the params: 17.401653
                Loss: fixed 410 labels. Loss 0.04279. Accuracy 0.990.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31355077
Train loss (w/o reg) on all data: 0.30716184
Test loss (w/o reg) on all data: 0.1433201
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.654687e-06
Norm of the params: 11.303914
              Random: fixed  57 labels. Loss 0.14332. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 615
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10710622
Train loss (w/o reg) on all data: 0.09992296
Test loss (w/o reg) on all data: 0.03875171
Train acc on all data:  0.9662047167517627
Test acc on all data:   1.0
Norm of the mean of gradients: 8.699108e-06
Norm of the params: 11.986045
     Influence (LOO): fixed 394 labels. Loss 0.03875. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004774265
Train loss (w/o reg) on all data: 0.0021003091
Test loss (w/o reg) on all data: 0.0036115774
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.7402214e-08
Norm of the params: 7.312942
                Loss: fixed 531 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30451182
Train loss (w/o reg) on all data: 0.29796883
Test loss (w/o reg) on all data: 0.136986
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5971238e-05
Norm of the params: 11.439383
              Random: fixed  79 labels. Loss 0.13699. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0705756
Train loss (w/o reg) on all data: 0.064221494
Test loss (w/o reg) on all data: 0.023174709
Train acc on all data:  0.9798200826647216
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3589655e-06
Norm of the params: 11.27307
     Influence (LOO): fixed 447 labels. Loss 0.02317. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096009194
Test loss (w/o reg) on all data: 0.0026560093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.250145e-08
Norm of the params: 6.09286
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29248247
Train loss (w/o reg) on all data: 0.28596985
Test loss (w/o reg) on all data: 0.12668909
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.580476e-05
Norm of the params: 11.412799
              Random: fixed 108 labels. Loss 0.12669. Accuracy 0.991.
### Flips: 615, rs: 38, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049706187
Train loss (w/o reg) on all data: 0.044131745
Test loss (w/o reg) on all data: 0.015750732
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7881354e-06
Norm of the params: 10.558826
     Influence (LOO): fixed 477 labels. Loss 0.01575. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012565
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.25368835e-08
Norm of the params: 6.092806
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28300563
Train loss (w/o reg) on all data: 0.27651718
Test loss (w/o reg) on all data: 0.11998736
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.8557492e-06
Norm of the params: 11.391625
              Random: fixed 127 labels. Loss 0.11999. Accuracy 0.994.
### Flips: 615, rs: 38, checks: 1230
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040227324
Train loss (w/o reg) on all data: 0.035340555
Test loss (w/o reg) on all data: 0.013029549
Train acc on all data:  0.9888159494286409
Test acc on all data:   1.0
Norm of the mean of gradients: 6.987989e-07
Norm of the params: 9.88612
     Influence (LOO): fixed 488 labels. Loss 0.01303. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3399855e-08
Norm of the params: 6.0928073
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27260602
Train loss (w/o reg) on all data: 0.26589218
Test loss (w/o reg) on all data: 0.11540272
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4319936e-05
Norm of the params: 11.587777
              Random: fixed 146 labels. Loss 0.11540. Accuracy 0.994.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3382498
Train loss (w/o reg) on all data: 0.33193418
Test loss (w/o reg) on all data: 0.15439202
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.935133e-05
Norm of the params: 11.238867
Flipped loss: 0.15439. Accuracy: 0.991
### Flips: 615, rs: 39, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23289034
Train loss (w/o reg) on all data: 0.22339012
Test loss (w/o reg) on all data: 0.10124865
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.3219622e-06
Norm of the params: 13.784206
     Influence (LOO): fixed 188 labels. Loss 0.10125. Accuracy 0.996.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20079133
Train loss (w/o reg) on all data: 0.18836714
Test loss (w/o reg) on all data: 0.09782306
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.518764e-06
Norm of the params: 15.763364
                Loss: fixed 205 labels. Loss 0.09782. Accuracy 0.983.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32549948
Train loss (w/o reg) on all data: 0.3188265
Test loss (w/o reg) on all data: 0.1487796
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8918741e-05
Norm of the params: 11.552483
              Random: fixed  28 labels. Loss 0.14878. Accuracy 0.993.
### Flips: 615, rs: 39, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161173
Train loss (w/o reg) on all data: 0.1522802
Test loss (w/o reg) on all data: 0.06289141
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.376806e-05
Norm of the params: 13.336266
     Influence (LOO): fixed 313 labels. Loss 0.06289. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07047078
Train loss (w/o reg) on all data: 0.055104848
Test loss (w/o reg) on all data: 0.037714433
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3703556e-06
Norm of the params: 17.530506
                Loss: fixed 410 labels. Loss 0.03771. Accuracy 0.994.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31587112
Train loss (w/o reg) on all data: 0.3092113
Test loss (w/o reg) on all data: 0.14348611
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.869868e-06
Norm of the params: 11.541062
              Random: fixed  49 labels. Loss 0.14349. Accuracy 0.994.
### Flips: 615, rs: 39, checks: 615
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10874131
Train loss (w/o reg) on all data: 0.10057397
Test loss (w/o reg) on all data: 0.04044075
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9467922e-06
Norm of the params: 12.78072
     Influence (LOO): fixed 392 labels. Loss 0.04044. Accuracy 0.999.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006084604
Train loss (w/o reg) on all data: 0.0025540963
Test loss (w/o reg) on all data: 0.0037928496
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.13881136e-07
Norm of the params: 8.402986
                Loss: fixed 530 labels. Loss 0.00379. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30291355
Train loss (w/o reg) on all data: 0.2959126
Test loss (w/o reg) on all data: 0.13516098
Train acc on all data:  0.8915633357646486
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6579565e-05
Norm of the params: 11.832968
              Random: fixed  78 labels. Loss 0.13516. Accuracy 0.995.
### Flips: 615, rs: 39, checks: 820
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07467677
Train loss (w/o reg) on all data: 0.067811824
Test loss (w/o reg) on all data: 0.027678354
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6479909e-06
Norm of the params: 11.71746
     Influence (LOO): fixed 441 labels. Loss 0.02768. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040627676
Train loss (w/o reg) on all data: 0.0015014237
Test loss (w/o reg) on all data: 0.0027393624
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.877678e-08
Norm of the params: 7.157295
                Loss: fixed 533 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2891919
Train loss (w/o reg) on all data: 0.28198475
Test loss (w/o reg) on all data: 0.12626575
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.959596e-06
Norm of the params: 12.005949
              Random: fixed 109 labels. Loss 0.12627. Accuracy 0.996.
### Flips: 615, rs: 39, checks: 1025
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050050143
Train loss (w/o reg) on all data: 0.04390363
Test loss (w/o reg) on all data: 0.018773478
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9037053e-06
Norm of the params: 11.087392
     Influence (LOO): fixed 475 labels. Loss 0.01877. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040627685
Train loss (w/o reg) on all data: 0.0015014269
Test loss (w/o reg) on all data: 0.0027393734
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.845095e-08
Norm of the params: 7.157292
                Loss: fixed 533 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27715674
Train loss (w/o reg) on all data: 0.27017125
Test loss (w/o reg) on all data: 0.11716279
Train acc on all data:  0.9044493070751276
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0487896e-05
Norm of the params: 11.819888
              Random: fixed 134 labels. Loss 0.11716. Accuracy 0.995.
### Flips: 615, rs: 39, checks: 1230
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04101617
Train loss (w/o reg) on all data: 0.035490092
Test loss (w/o reg) on all data: 0.015831323
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1034676e-06
Norm of the params: 10.512922
     Influence (LOO): fixed 488 labels. Loss 0.01583. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9384998e-08
Norm of the params: 6.092809
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26409692
Train loss (w/o reg) on all data: 0.25648677
Test loss (w/o reg) on all data: 0.110725515
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5205918e-05
Norm of the params: 12.337043
              Random: fixed 159 labels. Loss 0.11073. Accuracy 0.994.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40600786
Train loss (w/o reg) on all data: 0.40027133
Test loss (w/o reg) on all data: 0.21944065
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.232459e-05
Norm of the params: 10.711226
Flipped loss: 0.21944. Accuracy: 0.980
### Flips: 820, rs: 0, checks: 205
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3160299
Train loss (w/o reg) on all data: 0.30736127
Test loss (w/o reg) on all data: 0.16193926
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4591597e-05
Norm of the params: 13.167092
     Influence (LOO): fixed 174 labels. Loss 0.16194. Accuracy 0.989.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27687058
Train loss (w/o reg) on all data: 0.26477826
Test loss (w/o reg) on all data: 0.16600016
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 3.0650248e-05
Norm of the params: 15.551409
                Loss: fixed 205 labels. Loss 0.16600. Accuracy 0.955.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39528322
Train loss (w/o reg) on all data: 0.3894024
Test loss (w/o reg) on all data: 0.2077941
Train acc on all data:  0.837588135181133
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1549339e-05
Norm of the params: 10.845129
              Random: fixed  32 labels. Loss 0.20779. Accuracy 0.985.
### Flips: 820, rs: 0, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25355983
Train loss (w/o reg) on all data: 0.24501087
Test loss (w/o reg) on all data: 0.1208446
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.597312e-06
Norm of the params: 13.075894
     Influence (LOO): fixed 306 labels. Loss 0.12084. Accuracy 0.994.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16387196
Train loss (w/o reg) on all data: 0.14778036
Test loss (w/o reg) on all data: 0.104439944
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.4456862e-05
Norm of the params: 17.939674
                Loss: fixed 410 labels. Loss 0.10444. Accuracy 0.970.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38112554
Train loss (w/o reg) on all data: 0.37484932
Test loss (w/o reg) on all data: 0.19735385
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4105664e-05
Norm of the params: 11.203761
              Random: fixed  72 labels. Loss 0.19735. Accuracy 0.990.
### Flips: 820, rs: 0, checks: 615
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20244044
Train loss (w/o reg) on all data: 0.19424385
Test loss (w/o reg) on all data: 0.088955864
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.5533125e-06
Norm of the params: 12.803589
     Influence (LOO): fixed 404 labels. Loss 0.08896. Accuracy 0.997.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05764088
Train loss (w/o reg) on all data: 0.044018794
Test loss (w/o reg) on all data: 0.033691727
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.375677e-06
Norm of the params: 16.50581
                Loss: fixed 612 labels. Loss 0.03369. Accuracy 0.991.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3667985
Train loss (w/o reg) on all data: 0.36061427
Test loss (w/o reg) on all data: 0.184655
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2489503e-05
Norm of the params: 11.121362
              Random: fixed 109 labels. Loss 0.18465. Accuracy 0.989.
### Flips: 820, rs: 0, checks: 820
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15035342
Train loss (w/o reg) on all data: 0.14202462
Test loss (w/o reg) on all data: 0.062639214
Train acc on all data:  0.9472404570872842
Test acc on all data:   1.0
Norm of the mean of gradients: 1.013804e-05
Norm of the params: 12.906425
     Influence (LOO): fixed 494 labels. Loss 0.06264. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009755777
Train loss (w/o reg) on all data: 0.0046707024
Test loss (w/o reg) on all data: 0.0050170436
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0925494e-07
Norm of the params: 10.084716
                Loss: fixed 701 labels. Loss 0.00502. Accuracy 0.999.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35267326
Train loss (w/o reg) on all data: 0.34640518
Test loss (w/o reg) on all data: 0.17657709
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.237754e-06
Norm of the params: 11.196495
              Random: fixed 145 labels. Loss 0.17658. Accuracy 0.988.
### Flips: 820, rs: 0, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11514884
Train loss (w/o reg) on all data: 0.107607625
Test loss (w/o reg) on all data: 0.043171693
Train acc on all data:  0.9613420860685631
Test acc on all data:   1.0
Norm of the mean of gradients: 2.791561e-06
Norm of the params: 12.2810545
     Influence (LOO): fixed 555 labels. Loss 0.04317. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053525446
Train loss (w/o reg) on all data: 0.0020424896
Test loss (w/o reg) on all data: 0.0030898203
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7789376e-07
Norm of the params: 8.136405
                Loss: fixed 709 labels. Loss 0.00309. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34018812
Train loss (w/o reg) on all data: 0.33378178
Test loss (w/o reg) on all data: 0.16650438
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.233939e-05
Norm of the params: 11.31931
              Random: fixed 176 labels. Loss 0.16650. Accuracy 0.988.
### Flips: 820, rs: 0, checks: 1230
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08618472
Train loss (w/o reg) on all data: 0.07898814
Test loss (w/o reg) on all data: 0.030672658
Train acc on all data:  0.9717967420374423
Test acc on all data:   1.0
Norm of the mean of gradients: 1.52059e-06
Norm of the params: 11.997145
     Influence (LOO): fixed 600 labels. Loss 0.03067. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005013216
Train loss (w/o reg) on all data: 0.0018859539
Test loss (w/o reg) on all data: 0.0030347372
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0521402e-07
Norm of the params: 7.9085546
                Loss: fixed 710 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32770294
Train loss (w/o reg) on all data: 0.32137987
Test loss (w/o reg) on all data: 0.15730357
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.907243e-06
Norm of the params: 11.245494
              Random: fixed 207 labels. Loss 0.15730. Accuracy 0.988.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4068962
Train loss (w/o reg) on all data: 0.40183434
Test loss (w/o reg) on all data: 0.20702499
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.8737002e-05
Norm of the params: 10.0616665
Flipped loss: 0.20702. Accuracy: 0.984
### Flips: 820, rs: 1, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31298578
Train loss (w/o reg) on all data: 0.30351385
Test loss (w/o reg) on all data: 0.1479477
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2447207e-05
Norm of the params: 13.763663
     Influence (LOO): fixed 180 labels. Loss 0.14795. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27821118
Train loss (w/o reg) on all data: 0.26640788
Test loss (w/o reg) on all data: 0.16039073
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 6.5894023e-06
Norm of the params: 15.364443
                Loss: fixed 205 labels. Loss 0.16039. Accuracy 0.961.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39587194
Train loss (w/o reg) on all data: 0.3906438
Test loss (w/o reg) on all data: 0.19558837
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.137473e-05
Norm of the params: 10.225599
              Random: fixed  33 labels. Loss 0.19559. Accuracy 0.986.
### Flips: 820, rs: 1, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24522725
Train loss (w/o reg) on all data: 0.23563907
Test loss (w/o reg) on all data: 0.112351894
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2204289e-05
Norm of the params: 13.8478775
     Influence (LOO): fixed 313 labels. Loss 0.11235. Accuracy 0.995.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16488369
Train loss (w/o reg) on all data: 0.14828016
Test loss (w/o reg) on all data: 0.10253033
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.253811e-06
Norm of the params: 18.222805
                Loss: fixed 410 labels. Loss 0.10253. Accuracy 0.971.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837116
Train loss (w/o reg) on all data: 0.3783271
Test loss (w/o reg) on all data: 0.18545075
Train acc on all data:  0.849015317286652
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6023974e-05
Norm of the params: 10.377384
              Random: fixed  67 labels. Loss 0.18545. Accuracy 0.988.
### Flips: 820, rs: 1, checks: 615
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19406822
Train loss (w/o reg) on all data: 0.1848102
Test loss (w/o reg) on all data: 0.08447777
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.346965e-06
Norm of the params: 13.60736
     Influence (LOO): fixed 404 labels. Loss 0.08448. Accuracy 0.999.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05389574
Train loss (w/o reg) on all data: 0.04065775
Test loss (w/o reg) on all data: 0.026326116
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.3518679e-06
Norm of the params: 16.271442
                Loss: fixed 611 labels. Loss 0.02633. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37271476
Train loss (w/o reg) on all data: 0.36730108
Test loss (w/o reg) on all data: 0.17571537
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1197976e-05
Norm of the params: 10.405459
              Random: fixed  95 labels. Loss 0.17572. Accuracy 0.987.
### Flips: 820, rs: 1, checks: 820
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15596004
Train loss (w/o reg) on all data: 0.14749262
Test loss (w/o reg) on all data: 0.06571739
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.357662e-06
Norm of the params: 13.013393
     Influence (LOO): fixed 471 labels. Loss 0.06572. Accuracy 0.998.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059883306
Train loss (w/o reg) on all data: 0.0024990812
Test loss (w/o reg) on all data: 0.0032114615
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9321855e-08
Norm of the params: 8.353741
                Loss: fixed 697 labels. Loss 0.00321. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.359559
Train loss (w/o reg) on all data: 0.35416228
Test loss (w/o reg) on all data: 0.16334307
Train acc on all data:  0.862387551665451
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4204658e-05
Norm of the params: 10.389145
              Random: fixed 132 labels. Loss 0.16334. Accuracy 0.989.
### Flips: 820, rs: 1, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11733628
Train loss (w/o reg) on all data: 0.109087974
Test loss (w/o reg) on all data: 0.046609547
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1231386e-05
Norm of the params: 12.843913
     Influence (LOO): fixed 535 labels. Loss 0.04661. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.065159e-09
Norm of the params: 6.092825
                Loss: fixed 702 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429243
Train loss (w/o reg) on all data: 0.33730564
Test loss (w/o reg) on all data: 0.1519371
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.122864e-06
Norm of the params: 10.600625
              Random: fixed 170 labels. Loss 0.15194. Accuracy 0.993.
### Flips: 820, rs: 1, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091706626
Train loss (w/o reg) on all data: 0.08361958
Test loss (w/o reg) on all data: 0.035983216
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 6.258441e-06
Norm of the params: 12.717738
     Influence (LOO): fixed 575 labels. Loss 0.03598. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.137097e-09
Norm of the params: 6.09282
                Loss: fixed 702 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32701093
Train loss (w/o reg) on all data: 0.32112107
Test loss (w/o reg) on all data: 0.142996
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.1239113e-05
Norm of the params: 10.853454
              Random: fixed 203 labels. Loss 0.14300. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40377468
Train loss (w/o reg) on all data: 0.39771327
Test loss (w/o reg) on all data: 0.2123148
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.479648e-06
Norm of the params: 11.010372
Flipped loss: 0.21231. Accuracy: 0.974
### Flips: 820, rs: 2, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32084596
Train loss (w/o reg) on all data: 0.31246978
Test loss (w/o reg) on all data: 0.16011336
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.050765e-06
Norm of the params: 12.943096
     Influence (LOO): fixed 174 labels. Loss 0.16011. Accuracy 0.988.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27892807
Train loss (w/o reg) on all data: 0.2661743
Test loss (w/o reg) on all data: 0.16111298
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.8943423e-05
Norm of the params: 15.971086
                Loss: fixed 205 labels. Loss 0.16111. Accuracy 0.960.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39090094
Train loss (w/o reg) on all data: 0.384745
Test loss (w/o reg) on all data: 0.19707137
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.1731402e-05
Norm of the params: 11.09589
              Random: fixed  38 labels. Loss 0.19707. Accuracy 0.983.
### Flips: 820, rs: 2, checks: 410
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24845451
Train loss (w/o reg) on all data: 0.23856767
Test loss (w/o reg) on all data: 0.114765204
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.548345e-06
Norm of the params: 14.061894
     Influence (LOO): fixed 313 labels. Loss 0.11477. Accuracy 0.990.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1655327
Train loss (w/o reg) on all data: 0.14791615
Test loss (w/o reg) on all data: 0.10330393
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.6631783e-06
Norm of the params: 18.770475
                Loss: fixed 409 labels. Loss 0.10330. Accuracy 0.973.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37915784
Train loss (w/o reg) on all data: 0.37282506
Test loss (w/o reg) on all data: 0.18705358
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.0925141e-05
Norm of the params: 11.254141
              Random: fixed  69 labels. Loss 0.18705. Accuracy 0.982.
### Flips: 820, rs: 2, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19052514
Train loss (w/o reg) on all data: 0.18113646
Test loss (w/o reg) on all data: 0.08255093
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5896192e-05
Norm of the params: 13.703059
     Influence (LOO): fixed 424 labels. Loss 0.08255. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056671433
Train loss (w/o reg) on all data: 0.04169573
Test loss (w/o reg) on all data: 0.04595096
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5187763e-06
Norm of the params: 17.306477
                Loss: fixed 610 labels. Loss 0.04595. Accuracy 0.987.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3690058
Train loss (w/o reg) on all data: 0.36259666
Test loss (w/o reg) on all data: 0.17714344
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.139526e-06
Norm of the params: 11.32178
              Random: fixed  95 labels. Loss 0.17714. Accuracy 0.984.
### Flips: 820, rs: 2, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15143937
Train loss (w/o reg) on all data: 0.14213532
Test loss (w/o reg) on all data: 0.06167197
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2313517e-06
Norm of the params: 13.6411495
     Influence (LOO): fixed 494 labels. Loss 0.06167. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010922138
Train loss (w/o reg) on all data: 0.0055148415
Test loss (w/o reg) on all data: 0.0055578067
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5370178e-07
Norm of the params: 10.399323
                Loss: fixed 703 labels. Loss 0.00556. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35786238
Train loss (w/o reg) on all data: 0.35137588
Test loss (w/o reg) on all data: 0.16614023
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.071875e-06
Norm of the params: 11.389923
              Random: fixed 126 labels. Loss 0.16614. Accuracy 0.985.
### Flips: 820, rs: 2, checks: 1025
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11889634
Train loss (w/o reg) on all data: 0.11094549
Test loss (w/o reg) on all data: 0.047054745
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1388669e-05
Norm of the params: 12.610195
     Influence (LOO): fixed 546 labels. Loss 0.04705. Accuracy 0.999.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007561993
Train loss (w/o reg) on all data: 0.0035282576
Test loss (w/o reg) on all data: 0.0053211837
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8354207e-07
Norm of the params: 8.98191
                Loss: fixed 708 labels. Loss 0.00532. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3377408
Train loss (w/o reg) on all data: 0.33116516
Test loss (w/o reg) on all data: 0.15330425
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.80863e-06
Norm of the params: 11.467902
              Random: fixed 176 labels. Loss 0.15330. Accuracy 0.986.
### Flips: 820, rs: 2, checks: 1230
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08624299
Train loss (w/o reg) on all data: 0.07865367
Test loss (w/o reg) on all data: 0.032884993
Train acc on all data:  0.9710673474349624
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4465415e-06
Norm of the params: 12.320161
     Influence (LOO): fixed 597 labels. Loss 0.03288. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004987283
Train loss (w/o reg) on all data: 0.0019895006
Test loss (w/o reg) on all data: 0.0044290177
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.516147e-08
Norm of the params: 7.7431035
                Loss: fixed 712 labels. Loss 0.00443. Accuracy 0.999.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3226419
Train loss (w/o reg) on all data: 0.31611526
Test loss (w/o reg) on all data: 0.14109322
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2703539e-05
Norm of the params: 11.425097
              Random: fixed 216 labels. Loss 0.14109. Accuracy 0.989.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39547467
Train loss (w/o reg) on all data: 0.3894768
Test loss (w/o reg) on all data: 0.21622339
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.5179677e-05
Norm of the params: 10.952496
Flipped loss: 0.21622. Accuracy: 0.967
### Flips: 820, rs: 3, checks: 205
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30666322
Train loss (w/o reg) on all data: 0.29662225
Test loss (w/o reg) on all data: 0.15994549
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.968826e-06
Norm of the params: 14.171081
     Influence (LOO): fixed 172 labels. Loss 0.15995. Accuracy 0.982.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26980907
Train loss (w/o reg) on all data: 0.25644565
Test loss (w/o reg) on all data: 0.17433058
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 2.0900783e-05
Norm of the params: 16.348352
                Loss: fixed 205 labels. Loss 0.17433. Accuracy 0.948.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3826764
Train loss (w/o reg) on all data: 0.37673834
Test loss (w/o reg) on all data: 0.20230147
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3279998e-05
Norm of the params: 10.8977585
              Random: fixed  39 labels. Loss 0.20230. Accuracy 0.972.
### Flips: 820, rs: 3, checks: 410
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24151605
Train loss (w/o reg) on all data: 0.23158479
Test loss (w/o reg) on all data: 0.11149426
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.21971e-06
Norm of the params: 14.093448
     Influence (LOO): fixed 306 labels. Loss 0.11149. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15651119
Train loss (w/o reg) on all data: 0.13938096
Test loss (w/o reg) on all data: 0.11798046
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.7905625e-06
Norm of the params: 18.50958
                Loss: fixed 408 labels. Loss 0.11798. Accuracy 0.962.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37135822
Train loss (w/o reg) on all data: 0.36535335
Test loss (w/o reg) on all data: 0.19211352
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.4244545e-06
Norm of the params: 10.958888
              Random: fixed  72 labels. Loss 0.19211. Accuracy 0.973.
### Flips: 820, rs: 3, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19542456
Train loss (w/o reg) on all data: 0.18581374
Test loss (w/o reg) on all data: 0.08585613
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.097253e-06
Norm of the params: 13.864215
     Influence (LOO): fixed 401 labels. Loss 0.08586. Accuracy 0.993.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05505872
Train loss (w/o reg) on all data: 0.04062756
Test loss (w/o reg) on all data: 0.038122594
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4362633e-06
Norm of the params: 16.988914
                Loss: fixed 604 labels. Loss 0.03812. Accuracy 0.990.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35973257
Train loss (w/o reg) on all data: 0.3536493
Test loss (w/o reg) on all data: 0.17818664
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.691164e-06
Norm of the params: 11.030212
              Random: fixed 108 labels. Loss 0.17819. Accuracy 0.979.
### Flips: 820, rs: 3, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14589286
Train loss (w/o reg) on all data: 0.13708809
Test loss (w/o reg) on all data: 0.0581295
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9190974e-06
Norm of the params: 13.270099
     Influence (LOO): fixed 492 labels. Loss 0.05813. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011796025
Train loss (w/o reg) on all data: 0.0060103904
Test loss (w/o reg) on all data: 0.0055677462
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7969043e-07
Norm of the params: 10.756983
                Loss: fixed 688 labels. Loss 0.00557. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34866604
Train loss (w/o reg) on all data: 0.3424888
Test loss (w/o reg) on all data: 0.16850263
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.3248379e-05
Norm of the params: 11.115079
              Random: fixed 139 labels. Loss 0.16850. Accuracy 0.982.
### Flips: 820, rs: 3, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101985745
Train loss (w/o reg) on all data: 0.09394647
Test loss (w/o reg) on all data: 0.03961206
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 1.590579e-06
Norm of the params: 12.6801195
     Influence (LOO): fixed 563 labels. Loss 0.03961. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006424996
Train loss (w/o reg) on all data: 0.0029633406
Test loss (w/o reg) on all data: 0.0037591283
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.12445484e-07
Norm of the params: 8.320643
                Loss: fixed 697 labels. Loss 0.00376. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3329884
Train loss (w/o reg) on all data: 0.3269058
Test loss (w/o reg) on all data: 0.15668449
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.7215747e-05
Norm of the params: 11.029622
              Random: fixed 179 labels. Loss 0.15668. Accuracy 0.983.
### Flips: 820, rs: 3, checks: 1230
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07677103
Train loss (w/o reg) on all data: 0.06919082
Test loss (w/o reg) on all data: 0.031843647
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.186706e-07
Norm of the params: 12.312764
     Influence (LOO): fixed 600 labels. Loss 0.03184. Accuracy 0.996.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037254915
Train loss (w/o reg) on all data: 0.0015213402
Test loss (w/o reg) on all data: 0.0031758223
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7457535e-08
Norm of the params: 6.639505
                Loss: fixed 701 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31693068
Train loss (w/o reg) on all data: 0.31098118
Test loss (w/o reg) on all data: 0.14113791
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.4919294e-06
Norm of the params: 10.908241
              Random: fixed 217 labels. Loss 0.14114. Accuracy 0.991.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40062317
Train loss (w/o reg) on all data: 0.39492163
Test loss (w/o reg) on all data: 0.20725346
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.4718596e-05
Norm of the params: 10.678511
Flipped loss: 0.20725. Accuracy: 0.983
### Flips: 820, rs: 4, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3071152
Train loss (w/o reg) on all data: 0.2976237
Test loss (w/o reg) on all data: 0.15022217
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4190743e-05
Norm of the params: 13.777879
     Influence (LOO): fixed 180 labels. Loss 0.15022. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26853335
Train loss (w/o reg) on all data: 0.25516537
Test loss (w/o reg) on all data: 0.1597979
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 3.7819234e-05
Norm of the params: 16.351135
                Loss: fixed 205 labels. Loss 0.15980. Accuracy 0.963.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38600054
Train loss (w/o reg) on all data: 0.3799893
Test loss (w/o reg) on all data: 0.1960122
Train acc on all data:  0.8407488451252128
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.45431e-05
Norm of the params: 10.9647255
              Random: fixed  38 labels. Loss 0.19601. Accuracy 0.985.
### Flips: 820, rs: 4, checks: 410
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24402542
Train loss (w/o reg) on all data: 0.2338842
Test loss (w/o reg) on all data: 0.10942137
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.0296105e-06
Norm of the params: 14.241643
     Influence (LOO): fixed 310 labels. Loss 0.10942. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15477799
Train loss (w/o reg) on all data: 0.13704821
Test loss (w/o reg) on all data: 0.10703342
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 8.9547275e-06
Norm of the params: 18.830708
                Loss: fixed 410 labels. Loss 0.10703. Accuracy 0.964.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3722072
Train loss (w/o reg) on all data: 0.36593258
Test loss (w/o reg) on all data: 0.18359178
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3615543e-05
Norm of the params: 11.202328
              Random: fixed  74 labels. Loss 0.18359. Accuracy 0.985.
### Flips: 820, rs: 4, checks: 615
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18384677
Train loss (w/o reg) on all data: 0.17355405
Test loss (w/o reg) on all data: 0.07645049
Train acc on all data:  0.9319231704352055
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0206184e-05
Norm of the params: 14.347626
     Influence (LOO): fixed 425 labels. Loss 0.07645. Accuracy 0.997.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053422004
Train loss (w/o reg) on all data: 0.0396661
Test loss (w/o reg) on all data: 0.03471791
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.9363304e-06
Norm of the params: 16.586681
                Loss: fixed 611 labels. Loss 0.03472. Accuracy 0.990.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35845098
Train loss (w/o reg) on all data: 0.35184315
Test loss (w/o reg) on all data: 0.17640181
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.490547e-05
Norm of the params: 11.495932
              Random: fixed 107 labels. Loss 0.17640. Accuracy 0.986.
### Flips: 820, rs: 4, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14597164
Train loss (w/o reg) on all data: 0.13716027
Test loss (w/o reg) on all data: 0.059062444
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.814286e-06
Norm of the params: 13.275064
     Influence (LOO): fixed 491 labels. Loss 0.05906. Accuracy 0.997.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00803331
Train loss (w/o reg) on all data: 0.003714861
Test loss (w/o reg) on all data: 0.0044467235
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.733622e-07
Norm of the params: 9.293491
                Loss: fixed 695 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34912062
Train loss (w/o reg) on all data: 0.3427177
Test loss (w/o reg) on all data: 0.16817431
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1596535e-05
Norm of the params: 11.316272
              Random: fixed 136 labels. Loss 0.16817. Accuracy 0.984.
### Flips: 820, rs: 4, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1180354
Train loss (w/o reg) on all data: 0.10969751
Test loss (w/o reg) on all data: 0.04622851
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2414553e-06
Norm of the params: 12.913468
     Influence (LOO): fixed 537 labels. Loss 0.04623. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005765235
Train loss (w/o reg) on all data: 0.002331878
Test loss (w/o reg) on all data: 0.0033435381
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2052342e-07
Norm of the params: 8.286564
                Loss: fixed 699 labels. Loss 0.00334. Accuracy 1.000.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33746216
Train loss (w/o reg) on all data: 0.33098623
Test loss (w/o reg) on all data: 0.15866399
Train acc on all data:  0.8711402868952103
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8893297e-05
Norm of the params: 11.3806095
              Random: fixed 164 labels. Loss 0.15866. Accuracy 0.990.
### Flips: 820, rs: 4, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09210799
Train loss (w/o reg) on all data: 0.084265135
Test loss (w/o reg) on all data: 0.034830432
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6395547e-06
Norm of the params: 12.524258
     Influence (LOO): fixed 578 labels. Loss 0.03483. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044076974
Train loss (w/o reg) on all data: 0.0016621319
Test loss (w/o reg) on all data: 0.0027852869
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2445445e-08
Norm of the params: 7.410217
                Loss: fixed 702 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3234703
Train loss (w/o reg) on all data: 0.31697482
Test loss (w/o reg) on all data: 0.1485822
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5220208e-05
Norm of the params: 11.397779
              Random: fixed 198 labels. Loss 0.14858. Accuracy 0.987.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40283075
Train loss (w/o reg) on all data: 0.3973336
Test loss (w/o reg) on all data: 0.21768238
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4874573e-05
Norm of the params: 10.485377
Flipped loss: 0.21768. Accuracy: 0.982
### Flips: 820, rs: 5, checks: 205
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31298128
Train loss (w/o reg) on all data: 0.30361295
Test loss (w/o reg) on all data: 0.165068
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1602189e-05
Norm of the params: 13.688186
     Influence (LOO): fixed 177 labels. Loss 0.16507. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27250773
Train loss (w/o reg) on all data: 0.26083994
Test loss (w/o reg) on all data: 0.16695766
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 8.9541e-06
Norm of the params: 15.275987
                Loss: fixed 205 labels. Loss 0.16696. Accuracy 0.958.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39274737
Train loss (w/o reg) on all data: 0.38721737
Test loss (w/o reg) on all data: 0.208296
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.3291504e-05
Norm of the params: 10.51666
              Random: fixed  29 labels. Loss 0.20830. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24366759
Train loss (w/o reg) on all data: 0.23472592
Test loss (w/o reg) on all data: 0.12283702
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.9369705e-06
Norm of the params: 13.372854
     Influence (LOO): fixed 312 labels. Loss 0.12284. Accuracy 0.990.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16022988
Train loss (w/o reg) on all data: 0.14448778
Test loss (w/o reg) on all data: 0.11097278
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.511495e-06
Norm of the params: 17.743782
                Loss: fixed 410 labels. Loss 0.11097. Accuracy 0.965.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3816245
Train loss (w/o reg) on all data: 0.37601015
Test loss (w/o reg) on all data: 0.19786528
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.5058317e-05
Norm of the params: 10.596542
              Random: fixed  63 labels. Loss 0.19787. Accuracy 0.984.
### Flips: 820, rs: 5, checks: 615
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19672033
Train loss (w/o reg) on all data: 0.18736489
Test loss (w/o reg) on all data: 0.093048565
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0612655e-05
Norm of the params: 13.678772
     Influence (LOO): fixed 401 labels. Loss 0.09305. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058917485
Train loss (w/o reg) on all data: 0.045118466
Test loss (w/o reg) on all data: 0.043258756
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.142894e-06
Norm of the params: 16.61266
                Loss: fixed 611 labels. Loss 0.04326. Accuracy 0.985.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36670145
Train loss (w/o reg) on all data: 0.3611922
Test loss (w/o reg) on all data: 0.1857218
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.844872e-06
Norm of the params: 10.496922
              Random: fixed 103 labels. Loss 0.18572. Accuracy 0.987.
### Flips: 820, rs: 5, checks: 820
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15179071
Train loss (w/o reg) on all data: 0.1430861
Test loss (w/o reg) on all data: 0.06899704
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.143958e-06
Norm of the params: 13.194399
     Influence (LOO): fixed 479 labels. Loss 0.06900. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01137839
Train loss (w/o reg) on all data: 0.0057227234
Test loss (w/o reg) on all data: 0.0073380945
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8588355e-07
Norm of the params: 10.635475
                Loss: fixed 703 labels. Loss 0.00734. Accuracy 0.997.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35023072
Train loss (w/o reg) on all data: 0.34447363
Test loss (w/o reg) on all data: 0.1716865
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.399902e-06
Norm of the params: 10.730405
              Random: fixed 144 labels. Loss 0.17169. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 1025
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1171061
Train loss (w/o reg) on all data: 0.10833089
Test loss (w/o reg) on all data: 0.05393817
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.144965e-06
Norm of the params: 13.247805
     Influence (LOO): fixed 535 labels. Loss 0.05394. Accuracy 0.994.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007552716
Train loss (w/o reg) on all data: 0.0034258973
Test loss (w/o reg) on all data: 0.0040151994
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7041074e-07
Norm of the params: 9.084953
                Loss: fixed 712 labels. Loss 0.00402. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33346853
Train loss (w/o reg) on all data: 0.32792068
Test loss (w/o reg) on all data: 0.1600009
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.462499e-06
Norm of the params: 10.533607
              Random: fixed 188 labels. Loss 0.16000. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 1230
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09890899
Train loss (w/o reg) on all data: 0.090729326
Test loss (w/o reg) on all data: 0.04152264
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.098639e-06
Norm of the params: 12.790362
     Influence (LOO): fixed 571 labels. Loss 0.04152. Accuracy 0.997.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006218318
Train loss (w/o reg) on all data: 0.0027032557
Test loss (w/o reg) on all data: 0.0035239568
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 9.492201e-08
Norm of the params: 8.384583
                Loss: fixed 714 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31938043
Train loss (w/o reg) on all data: 0.3138508
Test loss (w/o reg) on all data: 0.14978549
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.62224e-06
Norm of the params: 10.516327
              Random: fixed 224 labels. Loss 0.14979. Accuracy 0.988.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40066463
Train loss (w/o reg) on all data: 0.39573327
Test loss (w/o reg) on all data: 0.20891973
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.9194866e-06
Norm of the params: 9.93112
Flipped loss: 0.20892. Accuracy: 0.977
### Flips: 820, rs: 6, checks: 205
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31082827
Train loss (w/o reg) on all data: 0.30209342
Test loss (w/o reg) on all data: 0.15191998
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.0465188e-05
Norm of the params: 13.21731
     Influence (LOO): fixed 176 labels. Loss 0.15192. Accuracy 0.990.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27045095
Train loss (w/o reg) on all data: 0.2588534
Test loss (w/o reg) on all data: 0.15505315
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.3902463e-05
Norm of the params: 15.229942
                Loss: fixed 205 labels. Loss 0.15505. Accuracy 0.957.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39303714
Train loss (w/o reg) on all data: 0.38802767
Test loss (w/o reg) on all data: 0.20002481
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1415402e-05
Norm of the params: 10.009472
              Random: fixed  24 labels. Loss 0.20002. Accuracy 0.979.
### Flips: 820, rs: 6, checks: 410
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24380346
Train loss (w/o reg) on all data: 0.23435257
Test loss (w/o reg) on all data: 0.112794675
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7056453e-05
Norm of the params: 13.748368
     Influence (LOO): fixed 303 labels. Loss 0.11279. Accuracy 0.991.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15589684
Train loss (w/o reg) on all data: 0.14068532
Test loss (w/o reg) on all data: 0.09569844
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.1826321e-05
Norm of the params: 17.442202
                Loss: fixed 410 labels. Loss 0.09570. Accuracy 0.963.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3772526
Train loss (w/o reg) on all data: 0.3717451
Test loss (w/o reg) on all data: 0.18888666
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4274529e-05
Norm of the params: 10.495226
              Random: fixed  65 labels. Loss 0.18889. Accuracy 0.980.
### Flips: 820, rs: 6, checks: 615
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19461046
Train loss (w/o reg) on all data: 0.18505271
Test loss (w/o reg) on all data: 0.08552018
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.090438e-06
Norm of the params: 13.825887
     Influence (LOO): fixed 396 labels. Loss 0.08552. Accuracy 0.993.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052614607
Train loss (w/o reg) on all data: 0.03841206
Test loss (w/o reg) on all data: 0.03824488
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7777994e-06
Norm of the params: 16.853813
                Loss: fixed 609 labels. Loss 0.03824. Accuracy 0.990.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3625948
Train loss (w/o reg) on all data: 0.35699674
Test loss (w/o reg) on all data: 0.17569025
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.180845e-05
Norm of the params: 10.581173
              Random: fixed 102 labels. Loss 0.17569. Accuracy 0.982.
### Flips: 820, rs: 6, checks: 820
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14737524
Train loss (w/o reg) on all data: 0.13840972
Test loss (w/o reg) on all data: 0.061770763
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.933084e-06
Norm of the params: 13.390684
     Influence (LOO): fixed 477 labels. Loss 0.06177. Accuracy 0.993.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0126221385
Train loss (w/o reg) on all data: 0.0063436953
Test loss (w/o reg) on all data: 0.007410857
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1685448e-06
Norm of the params: 11.205751
                Loss: fixed 684 labels. Loss 0.00741. Accuracy 0.999.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34988168
Train loss (w/o reg) on all data: 0.34424424
Test loss (w/o reg) on all data: 0.16605113
Train acc on all data:  0.8653051300753708
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.8904906e-06
Norm of the params: 10.618333
              Random: fixed 135 labels. Loss 0.16605. Accuracy 0.981.
### Flips: 820, rs: 6, checks: 1025
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11177419
Train loss (w/o reg) on all data: 0.10384595
Test loss (w/o reg) on all data: 0.044159945
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.284358e-06
Norm of the params: 12.5922575
     Influence (LOO): fixed 539 labels. Loss 0.04416. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062779365
Train loss (w/o reg) on all data: 0.0027718486
Test loss (w/o reg) on all data: 0.0043557244
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.760104e-08
Norm of the params: 8.373874
                Loss: fixed 695 labels. Loss 0.00436. Accuracy 0.999.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33124712
Train loss (w/o reg) on all data: 0.32541576
Test loss (w/o reg) on all data: 0.15373977
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6678638e-05
Norm of the params: 10.799412
              Random: fixed 181 labels. Loss 0.15374. Accuracy 0.982.
### Flips: 820, rs: 6, checks: 1230
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08907828
Train loss (w/o reg) on all data: 0.08136062
Test loss (w/o reg) on all data: 0.033070218
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6074424e-06
Norm of the params: 12.423889
     Influence (LOO): fixed 577 labels. Loss 0.03307. Accuracy 0.998.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035644767
Train loss (w/o reg) on all data: 0.0012908224
Test loss (w/o reg) on all data: 0.0030733468
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.294879e-08
Norm of the params: 6.7433734
                Loss: fixed 699 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31853455
Train loss (w/o reg) on all data: 0.31279954
Test loss (w/o reg) on all data: 0.14319892
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.543816e-06
Norm of the params: 10.709819
              Random: fixed 215 labels. Loss 0.14320. Accuracy 0.984.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4057077
Train loss (w/o reg) on all data: 0.40012714
Test loss (w/o reg) on all data: 0.21718258
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.1911193e-05
Norm of the params: 10.564601
Flipped loss: 0.21718. Accuracy: 0.983
### Flips: 820, rs: 7, checks: 205
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31922662
Train loss (w/o reg) on all data: 0.31030393
Test loss (w/o reg) on all data: 0.16014625
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.254879e-05
Norm of the params: 13.358669
     Influence (LOO): fixed 175 labels. Loss 0.16015. Accuracy 0.991.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28111792
Train loss (w/o reg) on all data: 0.2699791
Test loss (w/o reg) on all data: 0.17310768
Train acc on all data:  0.8830537320690494
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.1522698e-05
Norm of the params: 14.925707
                Loss: fixed 205 labels. Loss 0.17311. Accuracy 0.948.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39205942
Train loss (w/o reg) on all data: 0.38622722
Test loss (w/o reg) on all data: 0.20610523
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6895427e-05
Norm of the params: 10.80017
              Random: fixed  39 labels. Loss 0.20611. Accuracy 0.982.
### Flips: 820, rs: 7, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24802917
Train loss (w/o reg) on all data: 0.2383144
Test loss (w/o reg) on all data: 0.11506785
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7789867e-05
Norm of the params: 13.938986
     Influence (LOO): fixed 315 labels. Loss 0.11507. Accuracy 0.996.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16224931
Train loss (w/o reg) on all data: 0.14730117
Test loss (w/o reg) on all data: 0.10719899
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.6398222e-06
Norm of the params: 17.29054
                Loss: fixed 410 labels. Loss 0.10720. Accuracy 0.966.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37799385
Train loss (w/o reg) on all data: 0.37205288
Test loss (w/o reg) on all data: 0.1930658
Train acc on all data:  0.8509603695599319
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.566893e-06
Norm of the params: 10.900429
              Random: fixed  79 labels. Loss 0.19307. Accuracy 0.987.
### Flips: 820, rs: 7, checks: 615
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19542293
Train loss (w/o reg) on all data: 0.18541934
Test loss (w/o reg) on all data: 0.08580918
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.064979e-06
Norm of the params: 14.144677
     Influence (LOO): fixed 409 labels. Loss 0.08581. Accuracy 0.996.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051732577
Train loss (w/o reg) on all data: 0.037747335
Test loss (w/o reg) on all data: 0.032145426
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.489738e-06
Norm of the params: 16.724379
                Loss: fixed 612 labels. Loss 0.03215. Accuracy 0.988.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36389238
Train loss (w/o reg) on all data: 0.35763636
Test loss (w/o reg) on all data: 0.18020335
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.9280922e-05
Norm of the params: 11.185726
              Random: fixed 119 labels. Loss 0.18020. Accuracy 0.982.
### Flips: 820, rs: 7, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15553553
Train loss (w/o reg) on all data: 0.1469419
Test loss (w/o reg) on all data: 0.06280113
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.71583e-06
Norm of the params: 13.110018
     Influence (LOO): fixed 482 labels. Loss 0.06280. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071083764
Train loss (w/o reg) on all data: 0.0031571395
Test loss (w/o reg) on all data: 0.0058017196
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9860404e-07
Norm of the params: 8.889586
                Loss: fixed 694 labels. Loss 0.00580. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35331076
Train loss (w/o reg) on all data: 0.34694552
Test loss (w/o reg) on all data: 0.17010939
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8661692e-05
Norm of the params: 11.282952
              Random: fixed 145 labels. Loss 0.17011. Accuracy 0.984.
### Flips: 820, rs: 7, checks: 1025
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12103907
Train loss (w/o reg) on all data: 0.11328291
Test loss (w/o reg) on all data: 0.048815444
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6823666e-06
Norm of the params: 12.454846
     Influence (LOO): fixed 535 labels. Loss 0.04882. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060831597
Train loss (w/o reg) on all data: 0.0026623493
Test loss (w/o reg) on all data: 0.0043506045
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6936214e-07
Norm of the params: 8.271409
                Loss: fixed 697 labels. Loss 0.00435. Accuracy 0.999.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34142983
Train loss (w/o reg) on all data: 0.33500275
Test loss (w/o reg) on all data: 0.1600448
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.3524068e-05
Norm of the params: 11.337617
              Random: fixed 175 labels. Loss 0.16004. Accuracy 0.984.
### Flips: 820, rs: 7, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091631934
Train loss (w/o reg) on all data: 0.08411045
Test loss (w/o reg) on all data: 0.037731823
Train acc on all data:  0.9698516897641624
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4767635e-06
Norm of the params: 12.264972
     Influence (LOO): fixed 580 labels. Loss 0.03773. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042159455
Train loss (w/o reg) on all data: 0.0017915873
Test loss (w/o reg) on all data: 0.0041373884
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.719793e-08
Norm of the params: 6.9632735
                Loss: fixed 701 labels. Loss 0.00414. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3225378
Train loss (w/o reg) on all data: 0.31578913
Test loss (w/o reg) on all data: 0.14779674
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1705004e-05
Norm of the params: 11.617799
              Random: fixed 214 labels. Loss 0.14780. Accuracy 0.990.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40358672
Train loss (w/o reg) on all data: 0.39744598
Test loss (w/o reg) on all data: 0.22059558
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.502441e-06
Norm of the params: 11.082173
Flipped loss: 0.22060. Accuracy: 0.980
### Flips: 820, rs: 8, checks: 205
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3151097
Train loss (w/o reg) on all data: 0.30542403
Test loss (w/o reg) on all data: 0.16335246
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.799215e-06
Norm of the params: 13.918087
     Influence (LOO): fixed 176 labels. Loss 0.16335. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2802477
Train loss (w/o reg) on all data: 0.26717013
Test loss (w/o reg) on all data: 0.17344317
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.8258663e-05
Norm of the params: 16.172546
                Loss: fixed 204 labels. Loss 0.17344. Accuracy 0.956.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39221576
Train loss (w/o reg) on all data: 0.38630378
Test loss (w/o reg) on all data: 0.20799251
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.8171333e-05
Norm of the params: 10.873788
              Random: fixed  37 labels. Loss 0.20799. Accuracy 0.981.
### Flips: 820, rs: 8, checks: 410
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25146347
Train loss (w/o reg) on all data: 0.24136893
Test loss (w/o reg) on all data: 0.11760668
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.064654e-05
Norm of the params: 14.208818
     Influence (LOO): fixed 311 labels. Loss 0.11761. Accuracy 0.994.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16803831
Train loss (w/o reg) on all data: 0.15112503
Test loss (w/o reg) on all data: 0.11346898
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.9333767e-06
Norm of the params: 18.391994
                Loss: fixed 407 labels. Loss 0.11347. Accuracy 0.966.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3805587
Train loss (w/o reg) on all data: 0.37477592
Test loss (w/o reg) on all data: 0.19720781
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.4244065e-05
Norm of the params: 10.754319
              Random: fixed  73 labels. Loss 0.19721. Accuracy 0.981.
### Flips: 820, rs: 8, checks: 615
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19920146
Train loss (w/o reg) on all data: 0.19043271
Test loss (w/o reg) on all data: 0.08685453
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.889092e-06
Norm of the params: 13.242926
     Influence (LOO): fixed 411 labels. Loss 0.08685. Accuracy 0.995.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054959223
Train loss (w/o reg) on all data: 0.039468415
Test loss (w/o reg) on all data: 0.04258871
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3497057e-06
Norm of the params: 17.601595
                Loss: fixed 609 labels. Loss 0.04259. Accuracy 0.985.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36588857
Train loss (w/o reg) on all data: 0.35996175
Test loss (w/o reg) on all data: 0.18507689
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.2790555e-06
Norm of the params: 10.887435
              Random: fixed 110 labels. Loss 0.18508. Accuracy 0.983.
### Flips: 820, rs: 8, checks: 820
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14961837
Train loss (w/o reg) on all data: 0.14086097
Test loss (w/o reg) on all data: 0.062308203
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.436126e-06
Norm of the params: 13.234345
     Influence (LOO): fixed 497 labels. Loss 0.06231. Accuracy 0.998.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008994998
Train loss (w/o reg) on all data: 0.0043590795
Test loss (w/o reg) on all data: 0.0057333405
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3604674e-07
Norm of the params: 9.62904
                Loss: fixed 698 labels. Loss 0.00573. Accuracy 0.998.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3572019
Train loss (w/o reg) on all data: 0.35127193
Test loss (w/o reg) on all data: 0.17825113
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0698149e-05
Norm of the params: 10.8903475
              Random: fixed 132 labels. Loss 0.17825. Accuracy 0.983.
### Flips: 820, rs: 8, checks: 1025
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113952115
Train loss (w/o reg) on all data: 0.10565576
Test loss (w/o reg) on all data: 0.047429476
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.894593e-06
Norm of the params: 12.881269
     Influence (LOO): fixed 551 labels. Loss 0.04743. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047376426
Train loss (w/o reg) on all data: 0.0017720186
Test loss (w/o reg) on all data: 0.0035018178
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.39433e-08
Norm of the params: 7.70146
                Loss: fixed 708 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34337
Train loss (w/o reg) on all data: 0.33697027
Test loss (w/o reg) on all data: 0.16952336
Train acc on all data:  0.8694383661560905
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.579765e-06
Norm of the params: 11.313472
              Random: fixed 162 labels. Loss 0.16952. Accuracy 0.987.
### Flips: 820, rs: 8, checks: 1230
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089364015
Train loss (w/o reg) on all data: 0.0817556
Test loss (w/o reg) on all data: 0.035154745
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.3839636e-06
Norm of the params: 12.3356495
     Influence (LOO): fixed 589 labels. Loss 0.03515. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003925835
Train loss (w/o reg) on all data: 0.0013753938
Test loss (w/o reg) on all data: 0.0029414597
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1506823e-08
Norm of the params: 7.142046
                Loss: fixed 710 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317929
Train loss (w/o reg) on all data: 0.325514
Test loss (w/o reg) on all data: 0.15701437
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.20471e-06
Norm of the params: 11.206167
              Random: fixed 195 labels. Loss 0.15701. Accuracy 0.988.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40194875
Train loss (w/o reg) on all data: 0.3957744
Test loss (w/o reg) on all data: 0.22473592
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7319042e-05
Norm of the params: 11.112474
Flipped loss: 0.22474. Accuracy: 0.964
### Flips: 820, rs: 9, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.314321
Train loss (w/o reg) on all data: 0.3042322
Test loss (w/o reg) on all data: 0.16057771
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.9899776e-06
Norm of the params: 14.204789
     Influence (LOO): fixed 179 labels. Loss 0.16058. Accuracy 0.986.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27513847
Train loss (w/o reg) on all data: 0.2613194
Test loss (w/o reg) on all data: 0.18559788
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.3960925e-05
Norm of the params: 16.624722
                Loss: fixed 204 labels. Loss 0.18560. Accuracy 0.947.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3906826
Train loss (w/o reg) on all data: 0.38425606
Test loss (w/o reg) on all data: 0.21552362
Train acc on all data:  0.8417213712618526
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.156169e-06
Norm of the params: 11.337137
              Random: fixed  31 labels. Loss 0.21552. Accuracy 0.966.
### Flips: 820, rs: 9, checks: 410
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2500288
Train loss (w/o reg) on all data: 0.2397409
Test loss (w/o reg) on all data: 0.12101247
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2935194e-05
Norm of the params: 14.344269
     Influence (LOO): fixed 307 labels. Loss 0.12101. Accuracy 0.990.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15969454
Train loss (w/o reg) on all data: 0.14168361
Test loss (w/o reg) on all data: 0.1412848
Train acc on all data:  0.9380014587892049
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.5348762e-05
Norm of the params: 18.979427
                Loss: fixed 408 labels. Loss 0.14128. Accuracy 0.949.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38114372
Train loss (w/o reg) on all data: 0.37469083
Test loss (w/o reg) on all data: 0.2059156
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.9704506e-05
Norm of the params: 11.360352
              Random: fixed  60 labels. Loss 0.20592. Accuracy 0.972.
### Flips: 820, rs: 9, checks: 615
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18628712
Train loss (w/o reg) on all data: 0.17749181
Test loss (w/o reg) on all data: 0.08373767
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.4176993e-06
Norm of the params: 13.262961
     Influence (LOO): fixed 426 labels. Loss 0.08374. Accuracy 0.995.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056962196
Train loss (w/o reg) on all data: 0.04223141
Test loss (w/o reg) on all data: 0.052030265
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.7807444e-06
Norm of the params: 17.164373
                Loss: fixed 608 labels. Loss 0.05203. Accuracy 0.981.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3683066
Train loss (w/o reg) on all data: 0.36140275
Test loss (w/o reg) on all data: 0.19625403
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.3946463e-05
Norm of the params: 11.750614
              Random: fixed  92 labels. Loss 0.19625. Accuracy 0.971.
### Flips: 820, rs: 9, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14259668
Train loss (w/o reg) on all data: 0.13433944
Test loss (w/o reg) on all data: 0.059153218
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.263793e-06
Norm of the params: 12.850869
     Influence (LOO): fixed 497 labels. Loss 0.05915. Accuracy 0.999.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013429468
Train loss (w/o reg) on all data: 0.0069728987
Test loss (w/o reg) on all data: 0.0073900335
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.931056e-07
Norm of the params: 11.363599
                Loss: fixed 688 labels. Loss 0.00739. Accuracy 0.997.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35526595
Train loss (w/o reg) on all data: 0.34845605
Test loss (w/o reg) on all data: 0.18455312
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.3833367e-05
Norm of the params: 11.670369
              Random: fixed 128 labels. Loss 0.18455. Accuracy 0.975.
### Flips: 820, rs: 9, checks: 1025
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10955067
Train loss (w/o reg) on all data: 0.10115839
Test loss (w/o reg) on all data: 0.045001373
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.23412e-06
Norm of the params: 12.955526
     Influence (LOO): fixed 547 labels. Loss 0.04500. Accuracy 0.997.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006392462
Train loss (w/o reg) on all data: 0.0026117235
Test loss (w/o reg) on all data: 0.0041308207
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.07228985e-07
Norm of the params: 8.695675
                Loss: fixed 701 labels. Loss 0.00413. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34455538
Train loss (w/o reg) on all data: 0.3375512
Test loss (w/o reg) on all data: 0.17465724
Train acc on all data:  0.8691952346219305
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.123155e-05
Norm of the params: 11.835677
              Random: fixed 156 labels. Loss 0.17466. Accuracy 0.979.
### Flips: 820, rs: 9, checks: 1230
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08458304
Train loss (w/o reg) on all data: 0.076620296
Test loss (w/o reg) on all data: 0.033433396
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.468992e-06
Norm of the params: 12.619621
     Influence (LOO): fixed 587 labels. Loss 0.03343. Accuracy 0.999.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051352354
Train loss (w/o reg) on all data: 0.0020358835
Test loss (w/o reg) on all data: 0.0039673233
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3189456e-08
Norm of the params: 7.8731856
                Loss: fixed 703 labels. Loss 0.00397. Accuracy 0.999.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32675463
Train loss (w/o reg) on all data: 0.31922117
Test loss (w/o reg) on all data: 0.16208364
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.6149228e-05
Norm of the params: 12.274739
              Random: fixed 196 labels. Loss 0.16208. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40327826
Train loss (w/o reg) on all data: 0.39841482
Test loss (w/o reg) on all data: 0.2099033
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.95379e-05
Norm of the params: 9.862493
Flipped loss: 0.20990. Accuracy: 0.984
### Flips: 820, rs: 10, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3089944
Train loss (w/o reg) on all data: 0.29937997
Test loss (w/o reg) on all data: 0.15079077
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1060129e-05
Norm of the params: 13.866817
     Influence (LOO): fixed 181 labels. Loss 0.15079. Accuracy 0.991.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27979904
Train loss (w/o reg) on all data: 0.2691553
Test loss (w/o reg) on all data: 0.15244983
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 9.270462e-06
Norm of the params: 14.590226
                Loss: fixed 205 labels. Loss 0.15245. Accuracy 0.969.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38854736
Train loss (w/o reg) on all data: 0.38344845
Test loss (w/o reg) on all data: 0.19880939
Train acc on all data:  0.8424507658643327
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.8284023e-05
Norm of the params: 10.098433
              Random: fixed  40 labels. Loss 0.19881. Accuracy 0.986.
### Flips: 820, rs: 10, checks: 410
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24366407
Train loss (w/o reg) on all data: 0.23463443
Test loss (w/o reg) on all data: 0.11205339
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2876375e-05
Norm of the params: 13.438483
     Influence (LOO): fixed 310 labels. Loss 0.11205. Accuracy 0.996.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1609138
Train loss (w/o reg) on all data: 0.14579809
Test loss (w/o reg) on all data: 0.09460941
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2271367e-05
Norm of the params: 17.387186
                Loss: fixed 409 labels. Loss 0.09461. Accuracy 0.978.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3784065
Train loss (w/o reg) on all data: 0.37354997
Test loss (w/o reg) on all data: 0.18998711
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.697397e-05
Norm of the params: 9.855482
              Random: fixed  72 labels. Loss 0.18999. Accuracy 0.987.
### Flips: 820, rs: 10, checks: 615
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19193417
Train loss (w/o reg) on all data: 0.18311502
Test loss (w/o reg) on all data: 0.084417075
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5622872e-06
Norm of the params: 13.280929
     Influence (LOO): fixed 408 labels. Loss 0.08442. Accuracy 0.997.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055613913
Train loss (w/o reg) on all data: 0.041677494
Test loss (w/o reg) on all data: 0.027188722
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.187571e-06
Norm of the params: 16.695162
                Loss: fixed 608 labels. Loss 0.02719. Accuracy 0.994.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36233425
Train loss (w/o reg) on all data: 0.35719693
Test loss (w/o reg) on all data: 0.17577565
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.1371313e-05
Norm of the params: 10.136397
              Random: fixed 115 labels. Loss 0.17578. Accuracy 0.988.
### Flips: 820, rs: 10, checks: 820
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14648157
Train loss (w/o reg) on all data: 0.13729225
Test loss (w/o reg) on all data: 0.06385453
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2886155e-05
Norm of the params: 13.556788
     Influence (LOO): fixed 487 labels. Loss 0.06385. Accuracy 0.998.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009901248
Train loss (w/o reg) on all data: 0.004786962
Test loss (w/o reg) on all data: 0.0058881524
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 2.419992e-07
Norm of the params: 10.11364
                Loss: fixed 689 labels. Loss 0.00589. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34763193
Train loss (w/o reg) on all data: 0.34257764
Test loss (w/o reg) on all data: 0.16416171
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2566427e-05
Norm of the params: 10.054162
              Random: fixed 153 labels. Loss 0.16416. Accuracy 0.991.
### Flips: 820, rs: 10, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11118457
Train loss (w/o reg) on all data: 0.10311911
Test loss (w/o reg) on all data: 0.04702886
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1980998e-06
Norm of the params: 12.700751
     Influence (LOO): fixed 548 labels. Loss 0.04703. Accuracy 0.998.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042363405
Train loss (w/o reg) on all data: 0.0015978612
Test loss (w/o reg) on all data: 0.0032735553
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0518281e-07
Norm of the params: 7.2642684
                Loss: fixed 697 labels. Loss 0.00327. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33045426
Train loss (w/o reg) on all data: 0.32530123
Test loss (w/o reg) on all data: 0.15203181
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6869447e-06
Norm of the params: 10.1518755
              Random: fixed 195 labels. Loss 0.15203. Accuracy 0.992.
### Flips: 820, rs: 10, checks: 1230
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08595286
Train loss (w/o reg) on all data: 0.07901958
Test loss (w/o reg) on all data: 0.034780394
Train acc on all data:  0.9732555312424022
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3726885e-06
Norm of the params: 11.775642
     Influence (LOO): fixed 589 labels. Loss 0.03478. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.719952e-08
Norm of the params: 6.092822
                Loss: fixed 699 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31663153
Train loss (w/o reg) on all data: 0.31170037
Test loss (w/o reg) on all data: 0.14236355
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.244501e-06
Norm of the params: 9.930924
              Random: fixed 229 labels. Loss 0.14236. Accuracy 0.991.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39581588
Train loss (w/o reg) on all data: 0.3902057
Test loss (w/o reg) on all data: 0.20401286
Train acc on all data:  0.8334548991004134
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.9101425e-05
Norm of the params: 10.592606
Flipped loss: 0.20401. Accuracy: 0.979
### Flips: 820, rs: 11, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30653277
Train loss (w/o reg) on all data: 0.2970353
Test loss (w/o reg) on all data: 0.15193345
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1980075e-05
Norm of the params: 13.782218
     Influence (LOO): fixed 176 labels. Loss 0.15193. Accuracy 0.985.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27308515
Train loss (w/o reg) on all data: 0.26069963
Test loss (w/o reg) on all data: 0.15752651
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8418653e-05
Norm of the params: 15.73882
                Loss: fixed 205 labels. Loss 0.15753. Accuracy 0.958.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3794957
Train loss (w/o reg) on all data: 0.37394032
Test loss (w/o reg) on all data: 0.19141747
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.944945e-06
Norm of the params: 10.540762
              Random: fixed  47 labels. Loss 0.19142. Accuracy 0.978.
### Flips: 820, rs: 11, checks: 410
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24303164
Train loss (w/o reg) on all data: 0.23370521
Test loss (w/o reg) on all data: 0.11453313
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.693346e-06
Norm of the params: 13.657545
     Influence (LOO): fixed 304 labels. Loss 0.11453. Accuracy 0.989.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15311474
Train loss (w/o reg) on all data: 0.13639505
Test loss (w/o reg) on all data: 0.10024234
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.850584e-06
Norm of the params: 18.28643
                Loss: fixed 410 labels. Loss 0.10024. Accuracy 0.970.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36391255
Train loss (w/o reg) on all data: 0.35854936
Test loss (w/o reg) on all data: 0.1733361
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.80106e-05
Norm of the params: 10.356824
              Random: fixed  97 labels. Loss 0.17334. Accuracy 0.981.
### Flips: 820, rs: 11, checks: 615
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139817
Train loss (w/o reg) on all data: 0.181575
Test loss (w/o reg) on all data: 0.08469519
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7407124e-06
Norm of the params: 14.0165415
     Influence (LOO): fixed 404 labels. Loss 0.08470. Accuracy 0.994.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049843647
Train loss (w/o reg) on all data: 0.03587167
Test loss (w/o reg) on all data: 0.03201806
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.0814915e-06
Norm of the params: 16.716444
                Loss: fixed 608 labels. Loss 0.03202. Accuracy 0.990.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35308763
Train loss (w/o reg) on all data: 0.34772706
Test loss (w/o reg) on all data: 0.16462642
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.9180415e-06
Norm of the params: 10.354289
              Random: fixed 125 labels. Loss 0.16463. Accuracy 0.987.
### Flips: 820, rs: 11, checks: 820
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14118321
Train loss (w/o reg) on all data: 0.13187845
Test loss (w/o reg) on all data: 0.05978727
Train acc on all data:  0.949914903963044
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5902744e-06
Norm of the params: 13.641668
     Influence (LOO): fixed 491 labels. Loss 0.05979. Accuracy 0.997.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010316942
Train loss (w/o reg) on all data: 0.0051793056
Test loss (w/o reg) on all data: 0.00964232
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3317022e-07
Norm of the params: 10.136702
                Loss: fixed 690 labels. Loss 0.00964. Accuracy 0.997.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34032932
Train loss (w/o reg) on all data: 0.3346264
Test loss (w/o reg) on all data: 0.15734345
Train acc on all data:  0.8694383661560905
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.311695e-05
Norm of the params: 10.679802
              Random: fixed 155 labels. Loss 0.15734. Accuracy 0.988.
### Flips: 820, rs: 11, checks: 1025
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10711108
Train loss (w/o reg) on all data: 0.09839075
Test loss (w/o reg) on all data: 0.045403782
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.909336e-06
Norm of the params: 13.20631
     Influence (LOO): fixed 550 labels. Loss 0.04540. Accuracy 0.996.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006481714
Train loss (w/o reg) on all data: 0.0029573233
Test loss (w/o reg) on all data: 0.0053311135
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.205901e-08
Norm of the params: 8.395702
                Loss: fixed 697 labels. Loss 0.00533. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32582793
Train loss (w/o reg) on all data: 0.32003412
Test loss (w/o reg) on all data: 0.14772493
Train acc on all data:  0.8784342329200098
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2573234e-05
Norm of the params: 10.764594
              Random: fixed 193 labels. Loss 0.14772. Accuracy 0.987.
### Flips: 820, rs: 11, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08646194
Train loss (w/o reg) on all data: 0.07875227
Test loss (w/o reg) on all data: 0.03551251
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.7503003e-06
Norm of the params: 12.417463
     Influence (LOO): fixed 584 labels. Loss 0.03551. Accuracy 0.998.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005520237
Train loss (w/o reg) on all data: 0.0025212727
Test loss (w/o reg) on all data: 0.005076599
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.363872e-07
Norm of the params: 7.7446294
                Loss: fixed 699 labels. Loss 0.00508. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3151799
Train loss (w/o reg) on all data: 0.30921158
Test loss (w/o reg) on all data: 0.14026706
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.957205e-05
Norm of the params: 10.925499
              Random: fixed 218 labels. Loss 0.14027. Accuracy 0.987.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39839792
Train loss (w/o reg) on all data: 0.39282867
Test loss (w/o reg) on all data: 0.21276613
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.031935e-05
Norm of the params: 10.553899
Flipped loss: 0.21277. Accuracy: 0.972
### Flips: 820, rs: 12, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30769378
Train loss (w/o reg) on all data: 0.2985249
Test loss (w/o reg) on all data: 0.1518517
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4142736e-05
Norm of the params: 13.54171
     Influence (LOO): fixed 182 labels. Loss 0.15185. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26911724
Train loss (w/o reg) on all data: 0.2562726
Test loss (w/o reg) on all data: 0.1664324
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 9.502076e-06
Norm of the params: 16.027866
                Loss: fixed 205 labels. Loss 0.16643. Accuracy 0.957.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37981725
Train loss (w/o reg) on all data: 0.3738523
Test loss (w/o reg) on all data: 0.19552742
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.9303245e-05
Norm of the params: 10.92238
              Random: fixed  48 labels. Loss 0.19553. Accuracy 0.977.
### Flips: 820, rs: 12, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23988882
Train loss (w/o reg) on all data: 0.23056513
Test loss (w/o reg) on all data: 0.10659062
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.566684e-06
Norm of the params: 13.655542
     Influence (LOO): fixed 323 labels. Loss 0.10659. Accuracy 0.996.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15116565
Train loss (w/o reg) on all data: 0.13399076
Test loss (w/o reg) on all data: 0.114914276
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.4096367e-05
Norm of the params: 18.533691
                Loss: fixed 410 labels. Loss 0.11491. Accuracy 0.959.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36563453
Train loss (w/o reg) on all data: 0.35950413
Test loss (w/o reg) on all data: 0.18312895
Train acc on all data:  0.8533916849015317
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.7256094e-05
Norm of the params: 11.07285
              Random: fixed  86 labels. Loss 0.18313. Accuracy 0.979.
### Flips: 820, rs: 12, checks: 615
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19033824
Train loss (w/o reg) on all data: 0.18116589
Test loss (w/o reg) on all data: 0.08342897
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.7542676e-06
Norm of the params: 13.544257
     Influence (LOO): fixed 410 labels. Loss 0.08343. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056055725
Train loss (w/o reg) on all data: 0.041776083
Test loss (w/o reg) on all data: 0.032218296
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6449595e-06
Norm of the params: 16.899492
                Loss: fixed 608 labels. Loss 0.03222. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35351202
Train loss (w/o reg) on all data: 0.34738642
Test loss (w/o reg) on all data: 0.17190753
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.18878925e-05
Norm of the params: 11.068514
              Random: fixed 122 labels. Loss 0.17191. Accuracy 0.984.
### Flips: 820, rs: 12, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1461666
Train loss (w/o reg) on all data: 0.13743094
Test loss (w/o reg) on all data: 0.0591241
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2411946e-06
Norm of the params: 13.217914
     Influence (LOO): fixed 485 labels. Loss 0.05912. Accuracy 0.999.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009301083
Train loss (w/o reg) on all data: 0.0042178323
Test loss (w/o reg) on all data: 0.0037023292
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.693346e-08
Norm of the params: 10.082907
                Loss: fixed 689 labels. Loss 0.00370. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34348026
Train loss (w/o reg) on all data: 0.3376855
Test loss (w/o reg) on all data: 0.16142552
Train acc on all data:  0.8687089715536105
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.914502e-05
Norm of the params: 10.76547
              Random: fixed 153 labels. Loss 0.16143. Accuracy 0.988.
### Flips: 820, rs: 12, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11290177
Train loss (w/o reg) on all data: 0.10488237
Test loss (w/o reg) on all data: 0.04371785
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.7981e-06
Norm of the params: 12.66444
     Influence (LOO): fixed 542 labels. Loss 0.04372. Accuracy 0.999.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00575954
Train loss (w/o reg) on all data: 0.0022757319
Test loss (w/o reg) on all data: 0.002969289
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.305587e-07
Norm of the params: 8.347224
                Loss: fixed 694 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32903123
Train loss (w/o reg) on all data: 0.3231969
Test loss (w/o reg) on all data: 0.14985187
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3269606e-05
Norm of the params: 10.802165
              Random: fixed 190 labels. Loss 0.14985. Accuracy 0.991.
### Flips: 820, rs: 12, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08695716
Train loss (w/o reg) on all data: 0.079971366
Test loss (w/o reg) on all data: 0.030613327
Train acc on all data:  0.9720398735716023
Test acc on all data:   1.0
Norm of the mean of gradients: 1.454951e-06
Norm of the params: 11.820146
     Influence (LOO): fixed 586 labels. Loss 0.03061. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00575954
Train loss (w/o reg) on all data: 0.0022757365
Test loss (w/o reg) on all data: 0.0029693365
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.7595736e-08
Norm of the params: 8.347219
                Loss: fixed 694 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31672582
Train loss (w/o reg) on all data: 0.3109467
Test loss (w/o reg) on all data: 0.13955481
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.59895e-05
Norm of the params: 10.750929
              Random: fixed 220 labels. Loss 0.13955. Accuracy 0.991.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39620453
Train loss (w/o reg) on all data: 0.38987115
Test loss (w/o reg) on all data: 0.21348038
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2966498e-05
Norm of the params: 11.254675
Flipped loss: 0.21348. Accuracy: 0.983
### Flips: 820, rs: 13, checks: 205
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3037733
Train loss (w/o reg) on all data: 0.2947479
Test loss (w/o reg) on all data: 0.14948228
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.657806e-06
Norm of the params: 13.43535
     Influence (LOO): fixed 182 labels. Loss 0.14948. Accuracy 0.994.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271639
Train loss (w/o reg) on all data: 0.25702044
Test loss (w/o reg) on all data: 0.15495987
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.75322e-05
Norm of the params: 17.098864
                Loss: fixed 205 labels. Loss 0.15496. Accuracy 0.974.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3892532
Train loss (w/o reg) on all data: 0.38297486
Test loss (w/o reg) on all data: 0.20466247
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.970028e-06
Norm of the params: 11.20566
              Random: fixed  26 labels. Loss 0.20466. Accuracy 0.986.
### Flips: 820, rs: 13, checks: 410
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24688366
Train loss (w/o reg) on all data: 0.23669982
Test loss (w/o reg) on all data: 0.114412196
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6295516e-05
Norm of the params: 14.271537
     Influence (LOO): fixed 297 labels. Loss 0.11441. Accuracy 0.993.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15602274
Train loss (w/o reg) on all data: 0.13701868
Test loss (w/o reg) on all data: 0.09292133
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 6.3205994e-06
Norm of the params: 19.495668
                Loss: fixed 410 labels. Loss 0.09292. Accuracy 0.975.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38006178
Train loss (w/o reg) on all data: 0.37348765
Test loss (w/o reg) on all data: 0.19543436
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.7888977e-05
Norm of the params: 11.46659
              Random: fixed  51 labels. Loss 0.19543. Accuracy 0.987.
### Flips: 820, rs: 13, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1909256
Train loss (w/o reg) on all data: 0.1803523
Test loss (w/o reg) on all data: 0.0830293
Train acc on all data:  0.9285193289569658
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7221268e-05
Norm of the params: 14.541865
     Influence (LOO): fixed 404 labels. Loss 0.08303. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053244084
Train loss (w/o reg) on all data: 0.03887173
Test loss (w/o reg) on all data: 0.0251789
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8413134e-06
Norm of the params: 16.954264
                Loss: fixed 610 labels. Loss 0.02518. Accuracy 0.995.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36461115
Train loss (w/o reg) on all data: 0.35791814
Test loss (w/o reg) on all data: 0.18064402
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.431373e-06
Norm of the params: 11.569788
              Random: fixed  93 labels. Loss 0.18064. Accuracy 0.991.
### Flips: 820, rs: 13, checks: 820
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14607169
Train loss (w/o reg) on all data: 0.13627537
Test loss (w/o reg) on all data: 0.06081617
Train acc on all data:  0.9467541940189642
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7485372e-05
Norm of the params: 13.9973755
     Influence (LOO): fixed 482 labels. Loss 0.06082. Accuracy 1.000.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009924587
Train loss (w/o reg) on all data: 0.004449251
Test loss (w/o reg) on all data: 0.0049971086
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.04609086e-07
Norm of the params: 10.464545
                Loss: fixed 685 labels. Loss 0.00500. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3508461
Train loss (w/o reg) on all data: 0.34365833
Test loss (w/o reg) on all data: 0.17077959
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.4851814e-05
Norm of the params: 11.989828
              Random: fixed 126 labels. Loss 0.17078. Accuracy 0.994.
### Flips: 820, rs: 13, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111130595
Train loss (w/o reg) on all data: 0.10203248
Test loss (w/o reg) on all data: 0.044517733
Train acc on all data:  0.9618283491368831
Test acc on all data:   1.0
Norm of the mean of gradients: 5.192778e-06
Norm of the params: 13.489337
     Influence (LOO): fixed 540 labels. Loss 0.04452. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008299515
Train loss (w/o reg) on all data: 0.003527726
Test loss (w/o reg) on all data: 0.004300069
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2507403e-07
Norm of the params: 9.769124
                Loss: fixed 688 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33994585
Train loss (w/o reg) on all data: 0.3328933
Test loss (w/o reg) on all data: 0.15830824
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.085516e-05
Norm of the params: 11.876487
              Random: fixed 157 labels. Loss 0.15831. Accuracy 0.995.
### Flips: 820, rs: 13, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08634068
Train loss (w/o reg) on all data: 0.078489766
Test loss (w/o reg) on all data: 0.03391133
Train acc on all data:  0.9710673474349624
Test acc on all data:   1.0
Norm of the mean of gradients: 9.096534e-06
Norm of the params: 12.530692
     Influence (LOO): fixed 579 labels. Loss 0.03391. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007068956
Train loss (w/o reg) on all data: 0.0029162858
Test loss (w/o reg) on all data: 0.0041153124
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7562425e-08
Norm of the params: 9.113363
                Loss: fixed 690 labels. Loss 0.00412. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32517257
Train loss (w/o reg) on all data: 0.31815964
Test loss (w/o reg) on all data: 0.14532539
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.5376914e-05
Norm of the params: 11.843076
              Random: fixed 196 labels. Loss 0.14533. Accuracy 0.996.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38489366
Train loss (w/o reg) on all data: 0.37824976
Test loss (w/o reg) on all data: 0.2014847
Train acc on all data:  0.8431801604668125
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.0966144e-05
Norm of the params: 11.527256
Flipped loss: 0.20148. Accuracy: 0.978
### Flips: 820, rs: 14, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29520467
Train loss (w/o reg) on all data: 0.28513277
Test loss (w/o reg) on all data: 0.14583282
Train acc on all data:  0.8813518113299295
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3370298e-05
Norm of the params: 14.192883
     Influence (LOO): fixed 175 labels. Loss 0.14583. Accuracy 0.983.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25231227
Train loss (w/o reg) on all data: 0.23786736
Test loss (w/o reg) on all data: 0.15373261
Train acc on all data:  0.8956965718453683
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.4881007e-05
Norm of the params: 16.997004
                Loss: fixed 205 labels. Loss 0.15373. Accuracy 0.964.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37407202
Train loss (w/o reg) on all data: 0.36714786
Test loss (w/o reg) on all data: 0.18832248
Train acc on all data:  0.849744711889132
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0995754e-05
Norm of the params: 11.767879
              Random: fixed  33 labels. Loss 0.18832. Accuracy 0.980.
### Flips: 820, rs: 14, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22696038
Train loss (w/o reg) on all data: 0.21564603
Test loss (w/o reg) on all data: 0.104522325
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5493808e-05
Norm of the params: 15.042836
     Influence (LOO): fixed 314 labels. Loss 0.10452. Accuracy 0.990.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13952388
Train loss (w/o reg) on all data: 0.12065108
Test loss (w/o reg) on all data: 0.10488976
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.3545048e-06
Norm of the params: 19.428228
                Loss: fixed 409 labels. Loss 0.10489. Accuracy 0.970.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3635575
Train loss (w/o reg) on all data: 0.35674837
Test loss (w/o reg) on all data: 0.17928648
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1283334e-05
Norm of the params: 11.669721
              Random: fixed  65 labels. Loss 0.17929. Accuracy 0.983.
### Flips: 820, rs: 14, checks: 615
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17556466
Train loss (w/o reg) on all data: 0.16562726
Test loss (w/o reg) on all data: 0.07447321
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.7935453e-06
Norm of the params: 14.097808
     Influence (LOO): fixed 418 labels. Loss 0.07447. Accuracy 0.998.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04896045
Train loss (w/o reg) on all data: 0.03416935
Test loss (w/o reg) on all data: 0.03673592
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3732913e-06
Norm of the params: 17.199476
                Loss: fixed 594 labels. Loss 0.03674. Accuracy 0.990.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35233724
Train loss (w/o reg) on all data: 0.34548336
Test loss (w/o reg) on all data: 0.16688848
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4100962e-05
Norm of the params: 11.708023
              Random: fixed  97 labels. Loss 0.16689. Accuracy 0.987.
### Flips: 820, rs: 14, checks: 820
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13920476
Train loss (w/o reg) on all data: 0.13000998
Test loss (w/o reg) on all data: 0.054438006
Train acc on all data:  0.9516168247021639
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.023718e-06
Norm of the params: 13.560807
     Influence (LOO): fixed 483 labels. Loss 0.05444. Accuracy 0.999.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014919021
Train loss (w/o reg) on all data: 0.008046971
Test loss (w/o reg) on all data: 0.0072621224
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.695585e-07
Norm of the params: 11.723523
                Loss: fixed 664 labels. Loss 0.00726. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3397808
Train loss (w/o reg) on all data: 0.33288142
Test loss (w/o reg) on all data: 0.15924712
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.51311e-05
Norm of the params: 11.746831
              Random: fixed 125 labels. Loss 0.15925. Accuracy 0.985.
### Flips: 820, rs: 14, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109740615
Train loss (w/o reg) on all data: 0.1013946
Test loss (w/o reg) on all data: 0.041269153
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6333329e-06
Norm of the params: 12.919761
     Influence (LOO): fixed 530 labels. Loss 0.04127. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210889
Train loss (w/o reg) on all data: 0.0041923467
Test loss (w/o reg) on all data: 0.004328898
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5472236e-07
Norm of the params: 10.018526
                Loss: fixed 675 labels. Loss 0.00433. Accuracy 1.000.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32938743
Train loss (w/o reg) on all data: 0.32264525
Test loss (w/o reg) on all data: 0.15003702
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8529688e-05
Norm of the params: 11.612224
              Random: fixed 158 labels. Loss 0.15004. Accuracy 0.984.
### Flips: 820, rs: 14, checks: 1230
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08419254
Train loss (w/o reg) on all data: 0.076971345
Test loss (w/o reg) on all data: 0.029721316
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 3.798708e-06
Norm of the params: 12.017645
     Influence (LOO): fixed 573 labels. Loss 0.02972. Accuracy 1.000.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008543421
Train loss (w/o reg) on all data: 0.0038215616
Test loss (w/o reg) on all data: 0.0042636604
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 5.102838e-07
Norm of the params: 9.717879
                Loss: fixed 676 labels. Loss 0.00426. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.315972
Train loss (w/o reg) on all data: 0.3089824
Test loss (w/o reg) on all data: 0.1413605
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.052047e-05
Norm of the params: 11.82336
              Random: fixed 189 labels. Loss 0.14136. Accuracy 0.985.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4086317
Train loss (w/o reg) on all data: 0.40299487
Test loss (w/o reg) on all data: 0.21800274
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.8629129e-05
Norm of the params: 10.617758
Flipped loss: 0.21800. Accuracy: 0.972
### Flips: 820, rs: 15, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3166966
Train loss (w/o reg) on all data: 0.3076983
Test loss (w/o reg) on all data: 0.15859921
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.6642742e-05
Norm of the params: 13.415148
     Influence (LOO): fixed 181 labels. Loss 0.15860. Accuracy 0.983.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28489572
Train loss (w/o reg) on all data: 0.2730727
Test loss (w/o reg) on all data: 0.17492707
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 8.411095e-06
Norm of the params: 15.37727
                Loss: fixed 205 labels. Loss 0.17493. Accuracy 0.952.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39570478
Train loss (w/o reg) on all data: 0.3895575
Test loss (w/o reg) on all data: 0.20935696
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.1193011e-05
Norm of the params: 11.088082
              Random: fixed  33 labels. Loss 0.20936. Accuracy 0.972.
### Flips: 820, rs: 15, checks: 410
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2538557
Train loss (w/o reg) on all data: 0.2437093
Test loss (w/o reg) on all data: 0.12282362
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.566862e-06
Norm of the params: 14.245296
     Influence (LOO): fixed 306 labels. Loss 0.12282. Accuracy 0.988.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17000234
Train loss (w/o reg) on all data: 0.15217371
Test loss (w/o reg) on all data: 0.121921785
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 5.889243e-06
Norm of the params: 18.883131
                Loss: fixed 409 labels. Loss 0.12192. Accuracy 0.959.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3831382
Train loss (w/o reg) on all data: 0.37688637
Test loss (w/o reg) on all data: 0.19618234
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.1068266e-05
Norm of the params: 11.181998
              Random: fixed  70 labels. Loss 0.19618. Accuracy 0.977.
### Flips: 820, rs: 15, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1997238
Train loss (w/o reg) on all data: 0.18952309
Test loss (w/o reg) on all data: 0.09276759
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.301906e-06
Norm of the params: 14.283349
     Influence (LOO): fixed 407 labels. Loss 0.09277. Accuracy 0.990.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06413019
Train loss (w/o reg) on all data: 0.047536876
Test loss (w/o reg) on all data: 0.051169902
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2088148e-06
Norm of the params: 18.217197
                Loss: fixed 612 labels. Loss 0.05117. Accuracy 0.986.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36794966
Train loss (w/o reg) on all data: 0.36168164
Test loss (w/o reg) on all data: 0.184653
Train acc on all data:  0.8563092633114515
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.739472e-06
Norm of the params: 11.1964445
              Random: fixed 111 labels. Loss 0.18465. Accuracy 0.979.
### Flips: 820, rs: 15, checks: 820
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15929842
Train loss (w/o reg) on all data: 0.1496104
Test loss (w/o reg) on all data: 0.07246247
Train acc on all data:  0.9411621687332847
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.113113e-06
Norm of the params: 13.919779
     Influence (LOO): fixed 481 labels. Loss 0.07246. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014033744
Train loss (w/o reg) on all data: 0.0072819623
Test loss (w/o reg) on all data: 0.0077537764
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.97506e-07
Norm of the params: 11.620482
                Loss: fixed 713 labels. Loss 0.00775. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35167426
Train loss (w/o reg) on all data: 0.34507084
Test loss (w/o reg) on all data: 0.1748453
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 5.7257097e-05
Norm of the params: 11.4921
              Random: fixed 149 labels. Loss 0.17485. Accuracy 0.974.
### Flips: 820, rs: 15, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124143384
Train loss (w/o reg) on all data: 0.11499177
Test loss (w/o reg) on all data: 0.056740623
Train acc on all data:  0.9547775346462436
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1280842e-06
Norm of the params: 13.528944
     Influence (LOO): fixed 540 labels. Loss 0.05674. Accuracy 0.995.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058773886
Train loss (w/o reg) on all data: 0.0024470356
Test loss (w/o reg) on all data: 0.0040706126
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.3432786e-08
Norm of the params: 8.282938
                Loss: fixed 724 labels. Loss 0.00407. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3376129
Train loss (w/o reg) on all data: 0.3308974
Test loss (w/o reg) on all data: 0.1640193
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2476027e-05
Norm of the params: 11.589236
              Random: fixed 188 labels. Loss 0.16402. Accuracy 0.978.
### Flips: 820, rs: 15, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10106511
Train loss (w/o reg) on all data: 0.09287681
Test loss (w/o reg) on all data: 0.04253073
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9785732e-06
Norm of the params: 12.797111
     Influence (LOO): fixed 586 labels. Loss 0.04253. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049343193
Train loss (w/o reg) on all data: 0.0020112402
Test loss (w/o reg) on all data: 0.003815509
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.471277e-08
Norm of the params: 7.646018
                Loss: fixed 727 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3232857
Train loss (w/o reg) on all data: 0.31618384
Test loss (w/o reg) on all data: 0.15475431
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.2966338e-05
Norm of the params: 11.917931
              Random: fixed 223 labels. Loss 0.15475. Accuracy 0.981.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40370485
Train loss (w/o reg) on all data: 0.39753932
Test loss (w/o reg) on all data: 0.21818413
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0744721e-05
Norm of the params: 11.104532
Flipped loss: 0.21818. Accuracy: 0.977
### Flips: 820, rs: 16, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31096232
Train loss (w/o reg) on all data: 0.3015457
Test loss (w/o reg) on all data: 0.1549431
Train acc on all data:  0.8721128130318502
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2538488e-05
Norm of the params: 13.723428
     Influence (LOO): fixed 179 labels. Loss 0.15494. Accuracy 0.984.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2810119
Train loss (w/o reg) on all data: 0.26837188
Test loss (w/o reg) on all data: 0.16929302
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 6.0640414e-06
Norm of the params: 15.899697
                Loss: fixed 205 labels. Loss 0.16929. Accuracy 0.960.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39100355
Train loss (w/o reg) on all data: 0.38481745
Test loss (w/o reg) on all data: 0.20359096
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.8841137e-05
Norm of the params: 11.123033
              Random: fixed  41 labels. Loss 0.20359. Accuracy 0.980.
### Flips: 820, rs: 16, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25174055
Train loss (w/o reg) on all data: 0.24177961
Test loss (w/o reg) on all data: 0.11793797
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8827453e-05
Norm of the params: 14.114476
     Influence (LOO): fixed 306 labels. Loss 0.11794. Accuracy 0.990.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16939515
Train loss (w/o reg) on all data: 0.15130617
Test loss (w/o reg) on all data: 0.122081414
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.07692795e-05
Norm of the params: 19.020502
                Loss: fixed 410 labels. Loss 0.12208. Accuracy 0.962.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37769446
Train loss (w/o reg) on all data: 0.37126386
Test loss (w/o reg) on all data: 0.19457227
Train acc on all data:  0.8473133965475322
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.4042343e-05
Norm of the params: 11.340711
              Random: fixed  76 labels. Loss 0.19457. Accuracy 0.981.
### Flips: 820, rs: 16, checks: 615
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1977697
Train loss (w/o reg) on all data: 0.18809378
Test loss (w/o reg) on all data: 0.082883984
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.780339e-06
Norm of the params: 13.911086
     Influence (LOO): fixed 413 labels. Loss 0.08288. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062436916
Train loss (w/o reg) on all data: 0.04632123
Test loss (w/o reg) on all data: 0.046391968
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.8031819e-06
Norm of the params: 17.9531
                Loss: fixed 611 labels. Loss 0.04639. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36501053
Train loss (w/o reg) on all data: 0.35844162
Test loss (w/o reg) on all data: 0.18589783
Train acc on all data:  0.8541210795040116
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7815752e-05
Norm of the params: 11.462036
              Random: fixed 108 labels. Loss 0.18590. Accuracy 0.983.
### Flips: 820, rs: 16, checks: 820
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15341401
Train loss (w/o reg) on all data: 0.14422835
Test loss (w/o reg) on all data: 0.061733313
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4083928e-05
Norm of the params: 13.554079
     Influence (LOO): fixed 487 labels. Loss 0.06173. Accuracy 0.998.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013755372
Train loss (w/o reg) on all data: 0.0073871776
Test loss (w/o reg) on all data: 0.010088389
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.708563e-07
Norm of the params: 11.285561
                Loss: fixed 703 labels. Loss 0.01009. Accuracy 0.997.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35188258
Train loss (w/o reg) on all data: 0.34530923
Test loss (w/o reg) on all data: 0.1754073
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.969725e-06
Norm of the params: 11.465899
              Random: fixed 144 labels. Loss 0.17541. Accuracy 0.984.
### Flips: 820, rs: 16, checks: 1025
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12358876
Train loss (w/o reg) on all data: 0.11538543
Test loss (w/o reg) on all data: 0.04786191
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2752703e-06
Norm of the params: 12.808851
     Influence (LOO): fixed 541 labels. Loss 0.04786. Accuracy 1.000.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009828361
Train loss (w/o reg) on all data: 0.0047478606
Test loss (w/o reg) on all data: 0.009353004
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7931517e-07
Norm of the params: 10.080178
                Loss: fixed 711 labels. Loss 0.00935. Accuracy 0.997.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34046385
Train loss (w/o reg) on all data: 0.3334524
Test loss (w/o reg) on all data: 0.16756004
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.2220602e-05
Norm of the params: 11.841824
              Random: fixed 171 labels. Loss 0.16756. Accuracy 0.985.
### Flips: 820, rs: 16, checks: 1230
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09859353
Train loss (w/o reg) on all data: 0.09101435
Test loss (w/o reg) on all data: 0.039227087
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4782681e-05
Norm of the params: 12.311934
     Influence (LOO): fixed 581 labels. Loss 0.03923. Accuracy 0.999.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007964648
Train loss (w/o reg) on all data: 0.0036103677
Test loss (w/o reg) on all data: 0.007916094
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5992082e-07
Norm of the params: 9.331967
                Loss: fixed 715 labels. Loss 0.00792. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32518297
Train loss (w/o reg) on all data: 0.31812903
Test loss (w/o reg) on all data: 0.15481225
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.693214e-06
Norm of the params: 11.877672
              Random: fixed 210 labels. Loss 0.15481. Accuracy 0.983.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4003284
Train loss (w/o reg) on all data: 0.3933856
Test loss (w/o reg) on all data: 0.21347332
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.78025e-05
Norm of the params: 11.783719
Flipped loss: 0.21347. Accuracy: 0.983
### Flips: 820, rs: 17, checks: 205
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3140061
Train loss (w/o reg) on all data: 0.30461958
Test loss (w/o reg) on all data: 0.15585688
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0308263e-05
Norm of the params: 13.701477
     Influence (LOO): fixed 174 labels. Loss 0.15586. Accuracy 0.988.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2724942
Train loss (w/o reg) on all data: 0.2594823
Test loss (w/o reg) on all data: 0.1669848
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.88128e-05
Norm of the params: 16.131903
                Loss: fixed 205 labels. Loss 0.16698. Accuracy 0.957.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39094353
Train loss (w/o reg) on all data: 0.3838932
Test loss (w/o reg) on all data: 0.2042321
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.6223248e-05
Norm of the params: 11.874623
              Random: fixed  28 labels. Loss 0.20423. Accuracy 0.985.
### Flips: 820, rs: 17, checks: 410
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244139
Train loss (w/o reg) on all data: 0.23425469
Test loss (w/o reg) on all data: 0.110555045
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2367902e-05
Norm of the params: 14.060097
     Influence (LOO): fixed 313 labels. Loss 0.11056. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16047199
Train loss (w/o reg) on all data: 0.14201176
Test loss (w/o reg) on all data: 0.121132575
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.083738e-06
Norm of the params: 19.214697
                Loss: fixed 410 labels. Loss 0.12113. Accuracy 0.963.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3773895
Train loss (w/o reg) on all data: 0.37023228
Test loss (w/o reg) on all data: 0.19411635
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.712558e-05
Norm of the params: 11.964278
              Random: fixed  70 labels. Loss 0.19412. Accuracy 0.984.
### Flips: 820, rs: 17, checks: 615
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18821758
Train loss (w/o reg) on all data: 0.17838566
Test loss (w/o reg) on all data: 0.08167932
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8361964e-06
Norm of the params: 14.022783
     Influence (LOO): fixed 412 labels. Loss 0.08168. Accuracy 0.999.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056861755
Train loss (w/o reg) on all data: 0.042305738
Test loss (w/o reg) on all data: 0.030788299
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6504152e-06
Norm of the params: 17.062248
                Loss: fixed 612 labels. Loss 0.03079. Accuracy 0.993.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3656819
Train loss (w/o reg) on all data: 0.35859513
Test loss (w/o reg) on all data: 0.18164372
Train acc on all data:  0.8553367371748116
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.01831965e-05
Norm of the params: 11.905259
              Random: fixed 103 labels. Loss 0.18164. Accuracy 0.988.
### Flips: 820, rs: 17, checks: 820
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14507186
Train loss (w/o reg) on all data: 0.13544163
Test loss (w/o reg) on all data: 0.061509497
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2814553e-06
Norm of the params: 13.878209
     Influence (LOO): fixed 485 labels. Loss 0.06151. Accuracy 0.999.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009373032
Train loss (w/o reg) on all data: 0.004163579
Test loss (w/o reg) on all data: 0.0071607865
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.4662468e-07
Norm of the params: 10.207304
                Loss: fixed 696 labels. Loss 0.00716. Accuracy 0.998.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35487685
Train loss (w/o reg) on all data: 0.34808937
Test loss (w/o reg) on all data: 0.17458887
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.411807e-06
Norm of the params: 11.651153
              Random: fixed 138 labels. Loss 0.17459. Accuracy 0.986.
### Flips: 820, rs: 17, checks: 1025
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11862477
Train loss (w/o reg) on all data: 0.10927238
Test loss (w/o reg) on all data: 0.044484723
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 3.395782e-06
Norm of the params: 13.67654
     Influence (LOO): fixed 534 labels. Loss 0.04448. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046436796
Train loss (w/o reg) on all data: 0.001658912
Test loss (w/o reg) on all data: 0.0036220425
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.0054954e-08
Norm of the params: 7.7262764
                Loss: fixed 702 labels. Loss 0.00362. Accuracy 0.999.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34361112
Train loss (w/o reg) on all data: 0.33652586
Test loss (w/o reg) on all data: 0.16523543
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0138626e-05
Norm of the params: 11.903993
              Random: fixed 163 labels. Loss 0.16524. Accuracy 0.987.
### Flips: 820, rs: 17, checks: 1230
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09457334
Train loss (w/o reg) on all data: 0.08654329
Test loss (w/o reg) on all data: 0.032671038
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8680724e-06
Norm of the params: 12.672844
     Influence (LOO): fixed 573 labels. Loss 0.03267. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039968244
Train loss (w/o reg) on all data: 0.0013902466
Test loss (w/o reg) on all data: 0.0035520387
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1981398e-07
Norm of the params: 7.220218
                Loss: fixed 703 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33111286
Train loss (w/o reg) on all data: 0.32389
Test loss (w/o reg) on all data: 0.15920462
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.2091393e-06
Norm of the params: 12.019027
              Random: fixed 191 labels. Loss 0.15920. Accuracy 0.987.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40709037
Train loss (w/o reg) on all data: 0.4021575
Test loss (w/o reg) on all data: 0.20983988
Train acc on all data:  0.8312667152929735
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.7627002e-05
Norm of the params: 9.932651
Flipped loss: 0.20984. Accuracy: 0.978
### Flips: 820, rs: 18, checks: 205
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31851783
Train loss (w/o reg) on all data: 0.3100884
Test loss (w/o reg) on all data: 0.15152231
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.4439195e-05
Norm of the params: 12.984166
     Influence (LOO): fixed 180 labels. Loss 0.15152. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28406474
Train loss (w/o reg) on all data: 0.2723182
Test loss (w/o reg) on all data: 0.15308516
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 6.566428e-06
Norm of the params: 15.32744
                Loss: fixed 205 labels. Loss 0.15309. Accuracy 0.964.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3957457
Train loss (w/o reg) on all data: 0.39091766
Test loss (w/o reg) on all data: 0.1981284
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.85851e-06
Norm of the params: 9.826517
              Random: fixed  35 labels. Loss 0.19813. Accuracy 0.987.
### Flips: 820, rs: 18, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24612553
Train loss (w/o reg) on all data: 0.23752144
Test loss (w/o reg) on all data: 0.10668474
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3987789e-05
Norm of the params: 13.117994
     Influence (LOO): fixed 324 labels. Loss 0.10668. Accuracy 0.997.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16276881
Train loss (w/o reg) on all data: 0.14616871
Test loss (w/o reg) on all data: 0.09642361
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.0369585e-06
Norm of the params: 18.220919
                Loss: fixed 410 labels. Loss 0.09642. Accuracy 0.966.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37947187
Train loss (w/o reg) on all data: 0.37407318
Test loss (w/o reg) on all data: 0.1873726
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.6937447e-05
Norm of the params: 10.391052
              Random: fixed  76 labels. Loss 0.18737. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 615
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19068262
Train loss (w/o reg) on all data: 0.18287617
Test loss (w/o reg) on all data: 0.0771142
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1075939e-05
Norm of the params: 12.495161
     Influence (LOO): fixed 430 labels. Loss 0.07711. Accuracy 0.997.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04858551
Train loss (w/o reg) on all data: 0.033948276
Test loss (w/o reg) on all data: 0.028417286
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4728739e-06
Norm of the params: 17.109783
                Loss: fixed 614 labels. Loss 0.02842. Accuracy 0.990.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36323223
Train loss (w/o reg) on all data: 0.3575245
Test loss (w/o reg) on all data: 0.1751098
Train acc on all data:  0.8570386579139314
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.10907e-06
Norm of the params: 10.684309
              Random: fixed 117 labels. Loss 0.17511. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13840455
Train loss (w/o reg) on all data: 0.13042991
Test loss (w/o reg) on all data: 0.055892583
Train acc on all data:  0.9518599562363238
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8094213e-05
Norm of the params: 12.629052
     Influence (LOO): fixed 511 labels. Loss 0.05589. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076252194
Train loss (w/o reg) on all data: 0.0035293798
Test loss (w/o reg) on all data: 0.004946348
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7825076e-07
Norm of the params: 9.050789
                Loss: fixed 698 labels. Loss 0.00495. Accuracy 0.999.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35262343
Train loss (w/o reg) on all data: 0.34677872
Test loss (w/o reg) on all data: 0.16768155
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.212843e-05
Norm of the params: 10.811773
              Random: fixed 146 labels. Loss 0.16768. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 1025
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10583195
Train loss (w/o reg) on all data: 0.09801071
Test loss (w/o reg) on all data: 0.041535947
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5023189e-06
Norm of the params: 12.506987
     Influence (LOO): fixed 562 labels. Loss 0.04154. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040249005
Train loss (w/o reg) on all data: 0.0015036239
Test loss (w/o reg) on all data: 0.0028595545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.267171e-08
Norm of the params: 7.1010933
                Loss: fixed 705 labels. Loss 0.00286. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33839244
Train loss (w/o reg) on all data: 0.33236724
Test loss (w/o reg) on all data: 0.15681157
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.949262e-05
Norm of the params: 10.977423
              Random: fixed 180 labels. Loss 0.15681. Accuracy 0.990.
### Flips: 820, rs: 18, checks: 1230
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08516659
Train loss (w/o reg) on all data: 0.07804776
Test loss (w/o reg) on all data: 0.033906568
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4979198e-06
Norm of the params: 11.932164
     Influence (LOO): fixed 593 labels. Loss 0.03391. Accuracy 0.998.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031704062
Train loss (w/o reg) on all data: 0.0011096192
Test loss (w/o reg) on all data: 0.0029596975
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6374468e-07
Norm of the params: 6.4199486
                Loss: fixed 707 labels. Loss 0.00296. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32615113
Train loss (w/o reg) on all data: 0.3197925
Test loss (w/o reg) on all data: 0.14430253
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0114969e-05
Norm of the params: 11.277088
              Random: fixed 208 labels. Loss 0.14430. Accuracy 0.995.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3987244
Train loss (w/o reg) on all data: 0.3931953
Test loss (w/o reg) on all data: 0.198781
Train acc on all data:  0.8315098468271335
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2394856e-05
Norm of the params: 10.515813
Flipped loss: 0.19878. Accuracy: 0.984
### Flips: 820, rs: 19, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30697417
Train loss (w/o reg) on all data: 0.29826954
Test loss (w/o reg) on all data: 0.14127968
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.77799e-05
Norm of the params: 13.194413
     Influence (LOO): fixed 182 labels. Loss 0.14128. Accuracy 0.993.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27225837
Train loss (w/o reg) on all data: 0.26037034
Test loss (w/o reg) on all data: 0.14142732
Train acc on all data:  0.8813518113299295
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.18027e-06
Norm of the params: 15.419489
                Loss: fixed 205 labels. Loss 0.14143. Accuracy 0.974.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3863889
Train loss (w/o reg) on all data: 0.3809333
Test loss (w/o reg) on all data: 0.18620764
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0055368e-05
Norm of the params: 10.445644
              Random: fixed  39 labels. Loss 0.18621. Accuracy 0.989.
### Flips: 820, rs: 19, checks: 410
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23926887
Train loss (w/o reg) on all data: 0.2302318
Test loss (w/o reg) on all data: 0.104216
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.264667e-06
Norm of the params: 13.444008
     Influence (LOO): fixed 313 labels. Loss 0.10422. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15880395
Train loss (w/o reg) on all data: 0.1424218
Test loss (w/o reg) on all data: 0.07743271
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3207136e-05
Norm of the params: 18.100916
                Loss: fixed 409 labels. Loss 0.07743. Accuracy 0.986.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37424237
Train loss (w/o reg) on all data: 0.36863136
Test loss (w/o reg) on all data: 0.17723921
Train acc on all data:  0.849744711889132
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1079e-05
Norm of the params: 10.5934
              Random: fixed  76 labels. Loss 0.17724. Accuracy 0.990.
### Flips: 820, rs: 19, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153083
Train loss (w/o reg) on all data: 0.1721623
Test loss (w/o reg) on all data: 0.08082975
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8864785e-05
Norm of the params: 13.688346
     Influence (LOO): fixed 410 labels. Loss 0.08083. Accuracy 0.997.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05351682
Train loss (w/o reg) on all data: 0.039064776
Test loss (w/o reg) on all data: 0.022542404
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6072213e-06
Norm of the params: 17.001205
                Loss: fixed 608 labels. Loss 0.02254. Accuracy 0.997.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35963497
Train loss (w/o reg) on all data: 0.35377106
Test loss (w/o reg) on all data: 0.16747703
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.028198e-05
Norm of the params: 10.829505
              Random: fixed 112 labels. Loss 0.16748. Accuracy 0.988.
### Flips: 820, rs: 19, checks: 820
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14199889
Train loss (w/o reg) on all data: 0.13319057
Test loss (w/o reg) on all data: 0.06090121
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6232944e-06
Norm of the params: 13.27277
     Influence (LOO): fixed 483 labels. Loss 0.06090. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009597626
Train loss (w/o reg) on all data: 0.004801608
Test loss (w/o reg) on all data: 0.0051226374
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 8.866196e-07
Norm of the params: 9.793893
                Loss: fixed 689 labels. Loss 0.00512. Accuracy 1.000.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34784237
Train loss (w/o reg) on all data: 0.3416063
Test loss (w/o reg) on all data: 0.15646689
Train acc on all data:  0.8679795769511306
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3373186e-05
Norm of the params: 11.167872
              Random: fixed 146 labels. Loss 0.15647. Accuracy 0.992.
### Flips: 820, rs: 19, checks: 1025
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11080356
Train loss (w/o reg) on all data: 0.10218636
Test loss (w/o reg) on all data: 0.046913538
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1655252e-06
Norm of the params: 13.127986
     Influence (LOO): fixed 531 labels. Loss 0.04691. Accuracy 0.998.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044300994
Train loss (w/o reg) on all data: 0.0016926451
Test loss (w/o reg) on all data: 0.0029435281
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.438946e-07
Norm of the params: 7.3992624
                Loss: fixed 698 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33190995
Train loss (w/o reg) on all data: 0.32581717
Test loss (w/o reg) on all data: 0.14506352
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6436946e-06
Norm of the params: 11.038836
              Random: fixed 186 labels. Loss 0.14506. Accuracy 0.992.
### Flips: 820, rs: 19, checks: 1230
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08893815
Train loss (w/o reg) on all data: 0.08080271
Test loss (w/o reg) on all data: 0.03693852
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6150786e-06
Norm of the params: 12.755734
     Influence (LOO): fixed 570 labels. Loss 0.03694. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044300985
Train loss (w/o reg) on all data: 0.0016926894
Test loss (w/o reg) on all data: 0.002943509
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.697274e-08
Norm of the params: 7.399201
                Loss: fixed 698 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3170183
Train loss (w/o reg) on all data: 0.31108856
Test loss (w/o reg) on all data: 0.13595918
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0594694e-05
Norm of the params: 10.890115
              Random: fixed 223 labels. Loss 0.13596. Accuracy 0.996.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3960762
Train loss (w/o reg) on all data: 0.38936645
Test loss (w/o reg) on all data: 0.21868789
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.237827e-05
Norm of the params: 11.584258
Flipped loss: 0.21869. Accuracy: 0.978
### Flips: 820, rs: 20, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304693
Train loss (w/o reg) on all data: 0.29413554
Test loss (w/o reg) on all data: 0.15899144
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.42816825e-05
Norm of the params: 14.530991
     Influence (LOO): fixed 179 labels. Loss 0.15899. Accuracy 0.989.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26954278
Train loss (w/o reg) on all data: 0.257026
Test loss (w/o reg) on all data: 0.15925196
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.6009577e-05
Norm of the params: 15.822008
                Loss: fixed 205 labels. Loss 0.15925. Accuracy 0.962.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38291818
Train loss (w/o reg) on all data: 0.37628832
Test loss (w/o reg) on all data: 0.2053163
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4355973e-05
Norm of the params: 11.515089
              Random: fixed  37 labels. Loss 0.20532. Accuracy 0.980.
### Flips: 820, rs: 20, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2391315
Train loss (w/o reg) on all data: 0.2275008
Test loss (w/o reg) on all data: 0.117848806
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.080605e-06
Norm of the params: 15.251686
     Influence (LOO): fixed 306 labels. Loss 0.11785. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15554647
Train loss (w/o reg) on all data: 0.1386938
Test loss (w/o reg) on all data: 0.091385506
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.7259941e-06
Norm of the params: 18.35902
                Loss: fixed 408 labels. Loss 0.09139. Accuracy 0.981.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36463824
Train loss (w/o reg) on all data: 0.35742176
Test loss (w/o reg) on all data: 0.18905605
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0896441e-05
Norm of the params: 12.013719
              Random: fixed  85 labels. Loss 0.18906. Accuracy 0.983.
### Flips: 820, rs: 20, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19249097
Train loss (w/o reg) on all data: 0.18153521
Test loss (w/o reg) on all data: 0.08795564
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.1498765e-06
Norm of the params: 14.80253
     Influence (LOO): fixed 403 labels. Loss 0.08796. Accuracy 0.993.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052923024
Train loss (w/o reg) on all data: 0.03754389
Test loss (w/o reg) on all data: 0.034912657
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4705182e-06
Norm of the params: 17.538038
                Loss: fixed 598 labels. Loss 0.03491. Accuracy 0.990.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3508947
Train loss (w/o reg) on all data: 0.3434673
Test loss (w/o reg) on all data: 0.17811266
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.0758943e-05
Norm of the params: 12.188021
              Random: fixed 119 labels. Loss 0.17811. Accuracy 0.984.
### Flips: 820, rs: 20, checks: 820
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14469771
Train loss (w/o reg) on all data: 0.13490975
Test loss (w/o reg) on all data: 0.059475757
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5764077e-06
Norm of the params: 13.991395
     Influence (LOO): fixed 491 labels. Loss 0.05948. Accuracy 0.999.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015073219
Train loss (w/o reg) on all data: 0.007826192
Test loss (w/o reg) on all data: 0.0074850256
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3420285e-07
Norm of the params: 12.0391245
                Loss: fixed 681 labels. Loss 0.00749. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33519292
Train loss (w/o reg) on all data: 0.327704
Test loss (w/o reg) on all data: 0.16518381
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.194401e-06
Norm of the params: 12.23838
              Random: fixed 162 labels. Loss 0.16518. Accuracy 0.987.
### Flips: 820, rs: 20, checks: 1025
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10918409
Train loss (w/o reg) on all data: 0.100209564
Test loss (w/o reg) on all data: 0.044223882
Train acc on all data:  0.962314612205203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0057819e-06
Norm of the params: 13.397404
     Influence (LOO): fixed 550 labels. Loss 0.04422. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009005335
Train loss (w/o reg) on all data: 0.004173178
Test loss (w/o reg) on all data: 0.0043025473
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6274682e-07
Norm of the params: 9.830725
                Loss: fixed 694 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31851667
Train loss (w/o reg) on all data: 0.3106415
Test loss (w/o reg) on all data: 0.15372442
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.8675699e-05
Norm of the params: 12.550028
              Random: fixed 200 labels. Loss 0.15372. Accuracy 0.989.
### Flips: 820, rs: 20, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08585262
Train loss (w/o reg) on all data: 0.07787539
Test loss (w/o reg) on all data: 0.032687128
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3533768e-06
Norm of the params: 12.631099
     Influence (LOO): fixed 590 labels. Loss 0.03269. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007165039
Train loss (w/o reg) on all data: 0.0033274035
Test loss (w/o reg) on all data: 0.0038524552
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3243742e-07
Norm of the params: 8.760862
                Loss: fixed 699 labels. Loss 0.00385. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30167592
Train loss (w/o reg) on all data: 0.29388058
Test loss (w/o reg) on all data: 0.14382912
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.0434192e-05
Norm of the params: 12.486251
              Random: fixed 237 labels. Loss 0.14383. Accuracy 0.991.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40806612
Train loss (w/o reg) on all data: 0.40293348
Test loss (w/o reg) on all data: 0.21502207
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.1463572e-05
Norm of the params: 10.131776
Flipped loss: 0.21502. Accuracy: 0.983
### Flips: 820, rs: 21, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3161656
Train loss (w/o reg) on all data: 0.30682838
Test loss (w/o reg) on all data: 0.1637345
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2788572e-05
Norm of the params: 13.665452
     Influence (LOO): fixed 181 labels. Loss 0.16373. Accuracy 0.990.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28455442
Train loss (w/o reg) on all data: 0.27350977
Test loss (w/o reg) on all data: 0.16237472
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.455272e-05
Norm of the params: 14.862461
                Loss: fixed 205 labels. Loss 0.16237. Accuracy 0.966.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39692885
Train loss (w/o reg) on all data: 0.39164615
Test loss (w/o reg) on all data: 0.20597443
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4937257e-05
Norm of the params: 10.278816
              Random: fixed  34 labels. Loss 0.20597. Accuracy 0.982.
### Flips: 820, rs: 21, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25121504
Train loss (w/o reg) on all data: 0.24243507
Test loss (w/o reg) on all data: 0.12280471
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.118132e-05
Norm of the params: 13.2514
     Influence (LOO): fixed 312 labels. Loss 0.12280. Accuracy 0.989.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16967145
Train loss (w/o reg) on all data: 0.15452999
Test loss (w/o reg) on all data: 0.10442414
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.30816e-06
Norm of the params: 17.401983
                Loss: fixed 409 labels. Loss 0.10442. Accuracy 0.977.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38440183
Train loss (w/o reg) on all data: 0.37907732
Test loss (w/o reg) on all data: 0.19446969
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9358877e-05
Norm of the params: 10.319421
              Random: fixed  70 labels. Loss 0.19447. Accuracy 0.984.
### Flips: 820, rs: 21, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19601037
Train loss (w/o reg) on all data: 0.18690312
Test loss (w/o reg) on all data: 0.086469606
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.00069e-06
Norm of the params: 13.496104
     Influence (LOO): fixed 413 labels. Loss 0.08647. Accuracy 0.997.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055923093
Train loss (w/o reg) on all data: 0.040837657
Test loss (w/o reg) on all data: 0.03769253
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.6040892e-06
Norm of the params: 17.369764
                Loss: fixed 614 labels. Loss 0.03769. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3731064
Train loss (w/o reg) on all data: 0.3678078
Test loss (w/o reg) on all data: 0.1842077
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.7122813e-05
Norm of the params: 10.294265
              Random: fixed 103 labels. Loss 0.18421. Accuracy 0.988.
### Flips: 820, rs: 21, checks: 820
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15262173
Train loss (w/o reg) on all data: 0.14339375
Test loss (w/o reg) on all data: 0.06459592
Train acc on all data:  0.9448091417456844
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0625472e-05
Norm of the params: 13.585265
     Influence (LOO): fixed 489 labels. Loss 0.06460. Accuracy 1.000.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009746602
Train loss (w/o reg) on all data: 0.0046751387
Test loss (w/o reg) on all data: 0.0071503026
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.892681e-07
Norm of the params: 10.071209
                Loss: fixed 705 labels. Loss 0.00715. Accuracy 0.998.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35918656
Train loss (w/o reg) on all data: 0.3535941
Test loss (w/o reg) on all data: 0.1741392
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.702476e-05
Norm of the params: 10.575877
              Random: fixed 138 labels. Loss 0.17414. Accuracy 0.992.
### Flips: 820, rs: 21, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11596914
Train loss (w/o reg) on all data: 0.107854314
Test loss (w/o reg) on all data: 0.04661283
Train acc on all data:  0.9589107707269633
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4069816e-06
Norm of the params: 12.739562
     Influence (LOO): fixed 549 labels. Loss 0.04661. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071888017
Train loss (w/o reg) on all data: 0.0029360652
Test loss (w/o reg) on all data: 0.0064634546
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4719267e-07
Norm of the params: 9.222512
                Loss: fixed 709 labels. Loss 0.00646. Accuracy 0.998.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34712288
Train loss (w/o reg) on all data: 0.34132767
Test loss (w/o reg) on all data: 0.1641331
Train acc on all data:  0.8704108922927304
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4940677e-05
Norm of the params: 10.765893
              Random: fixed 171 labels. Loss 0.16413. Accuracy 0.990.
### Flips: 820, rs: 21, checks: 1230
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089412205
Train loss (w/o reg) on all data: 0.08217169
Test loss (w/o reg) on all data: 0.033316404
Train acc on all data:  0.9688791636275225
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4449354e-06
Norm of the params: 12.033715
     Influence (LOO): fixed 591 labels. Loss 0.03332. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042282767
Train loss (w/o reg) on all data: 0.001648526
Test loss (w/o reg) on all data: 0.0060863458
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.068129e-08
Norm of the params: 7.1829667
                Loss: fixed 712 labels. Loss 0.00609. Accuracy 0.998.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317332
Train loss (w/o reg) on all data: 0.3257602
Test loss (w/o reg) on all data: 0.15483682
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.5822237e-06
Norm of the params: 10.929777
              Random: fixed 206 labels. Loss 0.15484. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40882102
Train loss (w/o reg) on all data: 0.40371263
Test loss (w/o reg) on all data: 0.21515015
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.028748e-05
Norm of the params: 10.107817
Flipped loss: 0.21515. Accuracy: 0.988
### Flips: 820, rs: 22, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31848028
Train loss (w/o reg) on all data: 0.30892673
Test loss (w/o reg) on all data: 0.15393114
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7179855e-05
Norm of the params: 13.822844
     Influence (LOO): fixed 179 labels. Loss 0.15393. Accuracy 0.995.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28595802
Train loss (w/o reg) on all data: 0.27324995
Test loss (w/o reg) on all data: 0.15792552
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6317948e-05
Norm of the params: 15.942433
                Loss: fixed 205 labels. Loss 0.15793. Accuracy 0.974.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39438882
Train loss (w/o reg) on all data: 0.38899064
Test loss (w/o reg) on all data: 0.20314558
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.6952373e-05
Norm of the params: 10.390554
              Random: fixed  39 labels. Loss 0.20315. Accuracy 0.988.
### Flips: 820, rs: 22, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25061753
Train loss (w/o reg) on all data: 0.24023767
Test loss (w/o reg) on all data: 0.11378243
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9404888e-05
Norm of the params: 14.408241
     Influence (LOO): fixed 310 labels. Loss 0.11378. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17374195
Train loss (w/o reg) on all data: 0.15740424
Test loss (w/o reg) on all data: 0.094919704
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.6003718e-06
Norm of the params: 18.07634
                Loss: fixed 409 labels. Loss 0.09492. Accuracy 0.983.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3823142
Train loss (w/o reg) on all data: 0.37698448
Test loss (w/o reg) on all data: 0.19235563
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.302038e-06
Norm of the params: 10.324462
              Random: fixed  77 labels. Loss 0.19236. Accuracy 0.988.
### Flips: 820, rs: 22, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2056405
Train loss (w/o reg) on all data: 0.19613725
Test loss (w/o reg) on all data: 0.088695355
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.922412e-06
Norm of the params: 13.786408
     Influence (LOO): fixed 398 labels. Loss 0.08870. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05620432
Train loss (w/o reg) on all data: 0.041212734
Test loss (w/o reg) on all data: 0.026379695
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2981532e-06
Norm of the params: 17.31565
                Loss: fixed 614 labels. Loss 0.02638. Accuracy 0.997.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3690309
Train loss (w/o reg) on all data: 0.36367846
Test loss (w/o reg) on all data: 0.17987455
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4228303e-05
Norm of the params: 10.346428
              Random: fixed 115 labels. Loss 0.17987. Accuracy 0.993.
### Flips: 820, rs: 22, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16162123
Train loss (w/o reg) on all data: 0.15287033
Test loss (w/o reg) on all data: 0.064601004
Train acc on all data:  0.9433503525407245
Test acc on all data:   1.0
Norm of the mean of gradients: 3.593942e-06
Norm of the params: 13.229435
     Influence (LOO): fixed 481 labels. Loss 0.06460. Accuracy 1.000.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008078334
Train loss (w/o reg) on all data: 0.003479631
Test loss (w/o reg) on all data: 0.004082309
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9869854e-07
Norm of the params: 9.59031
                Loss: fixed 702 labels. Loss 0.00408. Accuracy 1.000.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35752746
Train loss (w/o reg) on all data: 0.3518238
Test loss (w/o reg) on all data: 0.1729713
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1777818e-05
Norm of the params: 10.680499
              Random: fixed 142 labels. Loss 0.17297. Accuracy 0.992.
### Flips: 820, rs: 22, checks: 1025
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12010915
Train loss (w/o reg) on all data: 0.11192329
Test loss (w/o reg) on all data: 0.04477156
Train acc on all data:  0.9610989545344031
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0027118e-06
Norm of the params: 12.795199
     Influence (LOO): fixed 550 labels. Loss 0.04477. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073363446
Train loss (w/o reg) on all data: 0.0031567249
Test loss (w/o reg) on all data: 0.004159177
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4289952e-07
Norm of the params: 9.142888
                Loss: fixed 705 labels. Loss 0.00416. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34582207
Train loss (w/o reg) on all data: 0.34015822
Test loss (w/o reg) on all data: 0.1619849
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.779286e-06
Norm of the params: 10.643156
              Random: fixed 172 labels. Loss 0.16198. Accuracy 0.990.
### Flips: 820, rs: 22, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08944832
Train loss (w/o reg) on all data: 0.08195691
Test loss (w/o reg) on all data: 0.032528054
Train acc on all data:  0.9717967420374423
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4421214e-06
Norm of the params: 12.240434
     Influence (LOO): fixed 596 labels. Loss 0.03253. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060321176
Train loss (w/o reg) on all data: 0.002503368
Test loss (w/o reg) on all data: 0.003496953
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1307717e-07
Norm of the params: 8.400892
                Loss: fixed 707 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33094794
Train loss (w/o reg) on all data: 0.32513234
Test loss (w/o reg) on all data: 0.15391326
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.44191e-06
Norm of the params: 10.784809
              Random: fixed 204 labels. Loss 0.15391. Accuracy 0.993.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.404835
Train loss (w/o reg) on all data: 0.399268
Test loss (w/o reg) on all data: 0.23043117
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.442941e-05
Norm of the params: 10.55176
Flipped loss: 0.23043. Accuracy: 0.967
### Flips: 820, rs: 23, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30941674
Train loss (w/o reg) on all data: 0.2996175
Test loss (w/o reg) on all data: 0.16409509
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.643552e-05
Norm of the params: 13.999463
     Influence (LOO): fixed 179 labels. Loss 0.16410. Accuracy 0.980.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28020388
Train loss (w/o reg) on all data: 0.26824403
Test loss (w/o reg) on all data: 0.18467982
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 3.6486457e-05
Norm of the params: 15.466003
                Loss: fixed 205 labels. Loss 0.18468. Accuracy 0.945.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39851224
Train loss (w/o reg) on all data: 0.39301595
Test loss (w/o reg) on all data: 0.2240893
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.7320686e-05
Norm of the params: 10.484559
              Random: fixed  22 labels. Loss 0.22409. Accuracy 0.975.
### Flips: 820, rs: 23, checks: 410
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24668188
Train loss (w/o reg) on all data: 0.23721108
Test loss (w/o reg) on all data: 0.12042475
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.067874e-06
Norm of the params: 13.762849
     Influence (LOO): fixed 308 labels. Loss 0.12042. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16535074
Train loss (w/o reg) on all data: 0.14863425
Test loss (w/o reg) on all data: 0.12637584
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 8.414921e-06
Norm of the params: 18.284685
                Loss: fixed 409 labels. Loss 0.12638. Accuracy 0.959.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3876579
Train loss (w/o reg) on all data: 0.38207352
Test loss (w/o reg) on all data: 0.21485211
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.1362458e-05
Norm of the params: 10.56825
              Random: fixed  54 labels. Loss 0.21485. Accuracy 0.972.
### Flips: 820, rs: 23, checks: 615
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19354099
Train loss (w/o reg) on all data: 0.18478066
Test loss (w/o reg) on all data: 0.084806375
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.7878736e-06
Norm of the params: 13.236565
     Influence (LOO): fixed 415 labels. Loss 0.08481. Accuracy 0.997.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059843507
Train loss (w/o reg) on all data: 0.044817403
Test loss (w/o reg) on all data: 0.04391458
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4048684e-06
Norm of the params: 17.335571
                Loss: fixed 612 labels. Loss 0.04391. Accuracy 0.985.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37670383
Train loss (w/o reg) on all data: 0.37116972
Test loss (w/o reg) on all data: 0.1990628
Train acc on all data:  0.8509603695599319
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.670062e-05
Norm of the params: 10.520567
              Random: fixed  86 labels. Loss 0.19906. Accuracy 0.980.
### Flips: 820, rs: 23, checks: 820
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1469966
Train loss (w/o reg) on all data: 0.13889697
Test loss (w/o reg) on all data: 0.06214892
Train acc on all data:  0.9482129832239241
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.907432e-06
Norm of the params: 12.72763
     Influence (LOO): fixed 495 labels. Loss 0.06215. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01046702
Train loss (w/o reg) on all data: 0.005063226
Test loss (w/o reg) on all data: 0.0070790383
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2607482e-07
Norm of the params: 10.395955
                Loss: fixed 698 labels. Loss 0.00708. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36329493
Train loss (w/o reg) on all data: 0.3577856
Test loss (w/o reg) on all data: 0.18948093
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5299145e-05
Norm of the params: 10.49697
              Random: fixed 123 labels. Loss 0.18948. Accuracy 0.981.
### Flips: 820, rs: 23, checks: 1025
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11438866
Train loss (w/o reg) on all data: 0.106826216
Test loss (w/o reg) on all data: 0.04621392
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.71005e-06
Norm of the params: 12.298329
     Influence (LOO): fixed 553 labels. Loss 0.04621. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061152494
Train loss (w/o reg) on all data: 0.0025894449
Test loss (w/o reg) on all data: 0.0049929204
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1278096e-07
Norm of the params: 8.397386
                Loss: fixed 707 labels. Loss 0.00499. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3423691
Train loss (w/o reg) on all data: 0.33657742
Test loss (w/o reg) on all data: 0.1730625
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.1016792e-05
Norm of the params: 10.7626095
              Random: fixed 171 labels. Loss 0.17306. Accuracy 0.980.
### Flips: 820, rs: 23, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090897456
Train loss (w/o reg) on all data: 0.08330656
Test loss (w/o reg) on all data: 0.03617253
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6051646e-06
Norm of the params: 12.321442
     Influence (LOO): fixed 589 labels. Loss 0.03617. Accuracy 0.998.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0050132778
Train loss (w/o reg) on all data: 0.0020257325
Test loss (w/o reg) on all data: 0.0036778967
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1579236e-08
Norm of the params: 7.7298713
                Loss: fixed 710 labels. Loss 0.00368. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32730234
Train loss (w/o reg) on all data: 0.32134256
Test loss (w/o reg) on all data: 0.16218418
Train acc on all data:  0.8784342329200098
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.634907e-05
Norm of the params: 10.917683
              Random: fixed 205 labels. Loss 0.16218. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40812284
Train loss (w/o reg) on all data: 0.4021516
Test loss (w/o reg) on all data: 0.21406123
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.6350432e-05
Norm of the params: 10.928152
Flipped loss: 0.21406. Accuracy: 0.977
### Flips: 820, rs: 24, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31500956
Train loss (w/o reg) on all data: 0.30570972
Test loss (w/o reg) on all data: 0.15335493
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.201269e-06
Norm of the params: 13.638078
     Influence (LOO): fixed 181 labels. Loss 0.15335. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28479534
Train loss (w/o reg) on all data: 0.27154028
Test loss (w/o reg) on all data: 0.15428005
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.2487834e-06
Norm of the params: 16.281925
                Loss: fixed 205 labels. Loss 0.15428. Accuracy 0.968.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39499182
Train loss (w/o reg) on all data: 0.3888694
Test loss (w/o reg) on all data: 0.20119214
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.6753909e-05
Norm of the params: 11.065623
              Random: fixed  42 labels. Loss 0.20119. Accuracy 0.983.
### Flips: 820, rs: 24, checks: 410
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25638667
Train loss (w/o reg) on all data: 0.24700972
Test loss (w/o reg) on all data: 0.11944778
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.3019767e-06
Norm of the params: 13.694486
     Influence (LOO): fixed 300 labels. Loss 0.11945. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17294961
Train loss (w/o reg) on all data: 0.15533015
Test loss (w/o reg) on all data: 0.096537516
Train acc on all data:  0.9336250911743253
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 5.4349284e-06
Norm of the params: 18.772032
                Loss: fixed 409 labels. Loss 0.09654. Accuracy 0.979.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38286045
Train loss (w/o reg) on all data: 0.37664452
Test loss (w/o reg) on all data: 0.19045778
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.179276e-05
Norm of the params: 11.149829
              Random: fixed  78 labels. Loss 0.19046. Accuracy 0.984.
### Flips: 820, rs: 24, checks: 615
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20129548
Train loss (w/o reg) on all data: 0.1921891
Test loss (w/o reg) on all data: 0.09053839
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.102819e-05
Norm of the params: 13.495473
     Influence (LOO): fixed 403 labels. Loss 0.09054. Accuracy 0.996.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06478131
Train loss (w/o reg) on all data: 0.049421772
Test loss (w/o reg) on all data: 0.034722924
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.957724e-07
Norm of the params: 17.526854
                Loss: fixed 610 labels. Loss 0.03472. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3644059
Train loss (w/o reg) on all data: 0.35779676
Test loss (w/o reg) on all data: 0.17448226
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.815806e-05
Norm of the params: 11.497074
              Random: fixed 128 labels. Loss 0.17448. Accuracy 0.987.
### Flips: 820, rs: 24, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14759058
Train loss (w/o reg) on all data: 0.13848776
Test loss (w/o reg) on all data: 0.061504796
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.222867e-06
Norm of the params: 13.492831
     Influence (LOO): fixed 494 labels. Loss 0.06150. Accuracy 0.998.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216228
Train loss (w/o reg) on all data: 0.004936608
Test loss (w/o reg) on all data: 0.006258105
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6361827e-07
Norm of the params: 10.275816
                Loss: fixed 706 labels. Loss 0.00626. Accuracy 0.999.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35168418
Train loss (w/o reg) on all data: 0.34517834
Test loss (w/o reg) on all data: 0.16385312
Train acc on all data:  0.8670070508144906
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1432681e-05
Norm of the params: 11.406876
              Random: fixed 160 labels. Loss 0.16385. Accuracy 0.987.
### Flips: 820, rs: 24, checks: 1025
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11321423
Train loss (w/o reg) on all data: 0.105542906
Test loss (w/o reg) on all data: 0.046214897
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7419197e-06
Norm of the params: 12.386547
     Influence (LOO): fixed 552 labels. Loss 0.04621. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069558164
Train loss (w/o reg) on all data: 0.0029998051
Test loss (w/o reg) on all data: 0.0051861373
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.7840454e-08
Norm of the params: 8.894956
                Loss: fixed 712 labels. Loss 0.00519. Accuracy 0.999.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33952418
Train loss (w/o reg) on all data: 0.33278397
Test loss (w/o reg) on all data: 0.1552445
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.8064633e-05
Norm of the params: 11.610519
              Random: fixed 190 labels. Loss 0.15524. Accuracy 0.989.
### Flips: 820, rs: 24, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09033142
Train loss (w/o reg) on all data: 0.08340322
Test loss (w/o reg) on all data: 0.03563217
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1624912e-06
Norm of the params: 11.771321
     Influence (LOO): fixed 590 labels. Loss 0.03563. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046372367
Train loss (w/o reg) on all data: 0.0018720011
Test loss (w/o reg) on all data: 0.0037419964
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.5660735e-08
Norm of the params: 7.4367137
                Loss: fixed 716 labels. Loss 0.00374. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32152346
Train loss (w/o reg) on all data: 0.31445852
Test loss (w/o reg) on all data: 0.14490315
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.977871e-06
Norm of the params: 11.886909
              Random: fixed 230 labels. Loss 0.14490. Accuracy 0.991.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39951438
Train loss (w/o reg) on all data: 0.39360023
Test loss (w/o reg) on all data: 0.2165771
Train acc on all data:  0.8358862144420132
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.3128263e-05
Norm of the params: 10.875791
Flipped loss: 0.21658. Accuracy: 0.978
### Flips: 820, rs: 25, checks: 205
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30434743
Train loss (w/o reg) on all data: 0.29399583
Test loss (w/o reg) on all data: 0.16067055
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0248975e-05
Norm of the params: 14.388616
     Influence (LOO): fixed 179 labels. Loss 0.16067. Accuracy 0.980.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27249727
Train loss (w/o reg) on all data: 0.2589495
Test loss (w/o reg) on all data: 0.16654982
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 6.2180975e-06
Norm of the params: 16.46073
                Loss: fixed 205 labels. Loss 0.16655. Accuracy 0.957.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38949096
Train loss (w/o reg) on all data: 0.383599
Test loss (w/o reg) on all data: 0.20311731
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.2570412e-05
Norm of the params: 10.855357
              Random: fixed  28 labels. Loss 0.20312. Accuracy 0.983.
### Flips: 820, rs: 25, checks: 410
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23956095
Train loss (w/o reg) on all data: 0.22901288
Test loss (w/o reg) on all data: 0.119981565
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.8604315e-06
Norm of the params: 14.524508
     Influence (LOO): fixed 309 labels. Loss 0.11998. Accuracy 0.986.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15554778
Train loss (w/o reg) on all data: 0.13824943
Test loss (w/o reg) on all data: 0.11412059
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.1437296e-06
Norm of the params: 18.60019
                Loss: fixed 410 labels. Loss 0.11412. Accuracy 0.967.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37493825
Train loss (w/o reg) on all data: 0.3686128
Test loss (w/o reg) on all data: 0.19275726
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.4683533e-05
Norm of the params: 11.247625
              Random: fixed  63 labels. Loss 0.19276. Accuracy 0.983.
### Flips: 820, rs: 25, checks: 615
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19300485
Train loss (w/o reg) on all data: 0.18295515
Test loss (w/o reg) on all data: 0.0904776
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.7145277e-06
Norm of the params: 14.177239
     Influence (LOO): fixed 404 labels. Loss 0.09048. Accuracy 0.993.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04921452
Train loss (w/o reg) on all data: 0.03502768
Test loss (w/o reg) on all data: 0.038042612
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.3378412e-06
Norm of the params: 16.84449
                Loss: fixed 610 labels. Loss 0.03804. Accuracy 0.987.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3592024
Train loss (w/o reg) on all data: 0.35278392
Test loss (w/o reg) on all data: 0.17964299
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.0787054e-05
Norm of the params: 11.330045
              Random: fixed 101 labels. Loss 0.17964. Accuracy 0.982.
### Flips: 820, rs: 25, checks: 820
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15311716
Train loss (w/o reg) on all data: 0.1444616
Test loss (w/o reg) on all data: 0.06354454
Train acc on all data:  0.9455385363481643
Test acc on all data:   1.0
Norm of the mean of gradients: 9.47813e-06
Norm of the params: 13.15717
     Influence (LOO): fixed 475 labels. Loss 0.06354. Accuracy 1.000.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013925375
Train loss (w/o reg) on all data: 0.0075450735
Test loss (w/o reg) on all data: 0.0081391
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.435223e-07
Norm of the params: 11.296285
                Loss: fixed 684 labels. Loss 0.00814. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34732145
Train loss (w/o reg) on all data: 0.34099102
Test loss (w/o reg) on all data: 0.1670797
Train acc on all data:  0.8674933138828106
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6686954e-05
Norm of the params: 11.252056
              Random: fixed 136 labels. Loss 0.16708. Accuracy 0.988.
### Flips: 820, rs: 25, checks: 1025
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12140301
Train loss (w/o reg) on all data: 0.11412243
Test loss (w/o reg) on all data: 0.048094314
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.785021e-06
Norm of the params: 12.066966
     Influence (LOO): fixed 530 labels. Loss 0.04809. Accuracy 0.999.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006364169
Train loss (w/o reg) on all data: 0.0027683747
Test loss (w/o reg) on all data: 0.0035371957
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5822663e-07
Norm of the params: 8.480324
                Loss: fixed 697 labels. Loss 0.00354. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33337945
Train loss (w/o reg) on all data: 0.32680923
Test loss (w/o reg) on all data: 0.15949029
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.252865e-06
Norm of the params: 11.463173
              Random: fixed 168 labels. Loss 0.15949. Accuracy 0.987.
### Flips: 820, rs: 25, checks: 1230
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08926481
Train loss (w/o reg) on all data: 0.08215667
Test loss (w/o reg) on all data: 0.034396175
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 4.760308e-06
Norm of the params: 11.923204
     Influence (LOO): fixed 578 labels. Loss 0.03440. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051237084
Train loss (w/o reg) on all data: 0.0020533672
Test loss (w/o reg) on all data: 0.0029096403
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7340084e-07
Norm of the params: 7.8362503
                Loss: fixed 699 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32125223
Train loss (w/o reg) on all data: 0.31491706
Test loss (w/o reg) on all data: 0.14889954
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.0250772e-05
Norm of the params: 11.2562685
              Random: fixed 201 labels. Loss 0.14890. Accuracy 0.989.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40178454
Train loss (w/o reg) on all data: 0.39647862
Test loss (w/o reg) on all data: 0.22214775
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1887894e-05
Norm of the params: 10.301378
Flipped loss: 0.22215. Accuracy: 0.985
### Flips: 820, rs: 26, checks: 205
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3129613
Train loss (w/o reg) on all data: 0.30497098
Test loss (w/o reg) on all data: 0.16841783
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.5008966e-05
Norm of the params: 12.64146
     Influence (LOO): fixed 174 labels. Loss 0.16842. Accuracy 0.985.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27405253
Train loss (w/o reg) on all data: 0.26168644
Test loss (w/o reg) on all data: 0.17006429
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 9.553782e-06
Norm of the params: 15.726463
                Loss: fixed 205 labels. Loss 0.17006. Accuracy 0.967.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3934887
Train loss (w/o reg) on all data: 0.3880133
Test loss (w/o reg) on all data: 0.21338046
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1487165e-05
Norm of the params: 10.464616
              Random: fixed  28 labels. Loss 0.21338. Accuracy 0.985.
### Flips: 820, rs: 26, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24193788
Train loss (w/o reg) on all data: 0.23240761
Test loss (w/o reg) on all data: 0.12448818
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.4719095e-06
Norm of the params: 13.805984
     Influence (LOO): fixed 313 labels. Loss 0.12449. Accuracy 0.988.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16151184
Train loss (w/o reg) on all data: 0.14307162
Test loss (w/o reg) on all data: 0.11502418
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.1379225e-06
Norm of the params: 19.204279
                Loss: fixed 409 labels. Loss 0.11502. Accuracy 0.971.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38269144
Train loss (w/o reg) on all data: 0.3770046
Test loss (w/o reg) on all data: 0.2045759
Train acc on all data:  0.8426938973984925
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.094964e-05
Norm of the params: 10.664756
              Random: fixed  56 labels. Loss 0.20458. Accuracy 0.985.
### Flips: 820, rs: 26, checks: 615
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18717685
Train loss (w/o reg) on all data: 0.17717497
Test loss (w/o reg) on all data: 0.090864144
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.804132e-06
Norm of the params: 14.143467
     Influence (LOO): fixed 412 labels. Loss 0.09086. Accuracy 0.998.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058711205
Train loss (w/o reg) on all data: 0.04368315
Test loss (w/o reg) on all data: 0.038043685
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.4210374e-06
Norm of the params: 17.336697
                Loss: fixed 610 labels. Loss 0.03804. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36716622
Train loss (w/o reg) on all data: 0.36087748
Test loss (w/o reg) on all data: 0.1905178
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.374329e-06
Norm of the params: 11.214937
              Random: fixed  96 labels. Loss 0.19052. Accuracy 0.983.
### Flips: 820, rs: 26, checks: 820
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14286955
Train loss (w/o reg) on all data: 0.13263223
Test loss (w/o reg) on all data: 0.06715658
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.652125e-06
Norm of the params: 14.308966
     Influence (LOO): fixed 490 labels. Loss 0.06716. Accuracy 0.997.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010635036
Train loss (w/o reg) on all data: 0.0052241627
Test loss (w/o reg) on all data: 0.0068047615
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1834049e-07
Norm of the params: 10.402762
                Loss: fixed 705 labels. Loss 0.00680. Accuracy 0.999.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3568755
Train loss (w/o reg) on all data: 0.35070926
Test loss (w/o reg) on all data: 0.1778588
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.6815484e-06
Norm of the params: 11.105171
              Random: fixed 130 labels. Loss 0.17786. Accuracy 0.992.
### Flips: 820, rs: 26, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11837121
Train loss (w/o reg) on all data: 0.109152064
Test loss (w/o reg) on all data: 0.050152592
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.856977e-06
Norm of the params: 13.578769
     Influence (LOO): fixed 540 labels. Loss 0.05015. Accuracy 0.998.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006487879
Train loss (w/o reg) on all data: 0.0027159867
Test loss (w/o reg) on all data: 0.0039318088
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7503754e-07
Norm of the params: 8.685496
                Loss: fixed 712 labels. Loss 0.00393. Accuracy 1.000.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34202275
Train loss (w/o reg) on all data: 0.33546537
Test loss (w/o reg) on all data: 0.16541462
Train acc on all data:  0.8687089715536105
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8996561e-05
Norm of the params: 11.451975
              Random: fixed 168 labels. Loss 0.16541. Accuracy 0.990.
### Flips: 820, rs: 26, checks: 1230
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09359809
Train loss (w/o reg) on all data: 0.08540433
Test loss (w/o reg) on all data: 0.037452385
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6168691e-06
Norm of the params: 12.801374
     Influence (LOO): fixed 583 labels. Loss 0.03745. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048846463
Train loss (w/o reg) on all data: 0.0019182592
Test loss (w/o reg) on all data: 0.0027682541
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8954917e-08
Norm of the params: 7.7024503
                Loss: fixed 716 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32850176
Train loss (w/o reg) on all data: 0.32163683
Test loss (w/o reg) on all data: 0.1553723
Train acc on all data:  0.87551665451009
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5618643e-05
Norm of the params: 11.717461
              Random: fixed 202 labels. Loss 0.15537. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4012774
Train loss (w/o reg) on all data: 0.3945159
Test loss (w/o reg) on all data: 0.21040726
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.4482566e-05
Norm of the params: 11.628846
Flipped loss: 0.21041. Accuracy: 0.984
### Flips: 820, rs: 27, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30880222
Train loss (w/o reg) on all data: 0.29853344
Test loss (w/o reg) on all data: 0.15444082
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.5773845e-05
Norm of the params: 14.330923
     Influence (LOO): fixed 181 labels. Loss 0.15444. Accuracy 0.981.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27723
Train loss (w/o reg) on all data: 0.26289386
Test loss (w/o reg) on all data: 0.16106226
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.4592782e-05
Norm of the params: 16.93289
                Loss: fixed 205 labels. Loss 0.16106. Accuracy 0.957.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38811508
Train loss (w/o reg) on all data: 0.38116705
Test loss (w/o reg) on all data: 0.19608364
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.758501e-06
Norm of the params: 11.788152
              Random: fixed  41 labels. Loss 0.19608. Accuracy 0.986.
### Flips: 820, rs: 27, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24740286
Train loss (w/o reg) on all data: 0.23627807
Test loss (w/o reg) on all data: 0.12407482
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.1672246e-05
Norm of the params: 14.916291
     Influence (LOO): fixed 303 labels. Loss 0.12407. Accuracy 0.982.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16921683
Train loss (w/o reg) on all data: 0.1509436
Test loss (w/o reg) on all data: 0.101901926
Train acc on all data:  0.9336250911743253
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.8251295e-06
Norm of the params: 19.117125
                Loss: fixed 408 labels. Loss 0.10190. Accuracy 0.976.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37646446
Train loss (w/o reg) on all data: 0.3695307
Test loss (w/o reg) on all data: 0.18476728
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6344573e-05
Norm of the params: 11.776038
              Random: fixed  78 labels. Loss 0.18477. Accuracy 0.988.
### Flips: 820, rs: 27, checks: 615
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19810499
Train loss (w/o reg) on all data: 0.18794286
Test loss (w/o reg) on all data: 0.090590954
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.6301943e-06
Norm of the params: 14.256316
     Influence (LOO): fixed 402 labels. Loss 0.09059. Accuracy 0.993.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06298703
Train loss (w/o reg) on all data: 0.0470177
Test loss (w/o reg) on all data: 0.042005714
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0953985e-06
Norm of the params: 17.871391
                Loss: fixed 609 labels. Loss 0.04201. Accuracy 0.987.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36548352
Train loss (w/o reg) on all data: 0.35856044
Test loss (w/o reg) on all data: 0.17291479
Train acc on all data:  0.8558230002431315
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.825841e-05
Norm of the params: 11.766983
              Random: fixed 109 labels. Loss 0.17291. Accuracy 0.990.
### Flips: 820, rs: 27, checks: 820
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15348163
Train loss (w/o reg) on all data: 0.14383522
Test loss (w/o reg) on all data: 0.06778071
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.999204e-06
Norm of the params: 13.889862
     Influence (LOO): fixed 488 labels. Loss 0.06778. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014305104
Train loss (w/o reg) on all data: 0.0077220243
Test loss (w/o reg) on all data: 0.007855795
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5570453e-06
Norm of the params: 11.474388
                Loss: fixed 699 labels. Loss 0.00786. Accuracy 0.998.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3534809
Train loss (w/o reg) on all data: 0.34678924
Test loss (w/o reg) on all data: 0.16041243
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.588455e-05
Norm of the params: 11.568632
              Random: fixed 144 labels. Loss 0.16041. Accuracy 0.991.
### Flips: 820, rs: 27, checks: 1025
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12591386
Train loss (w/o reg) on all data: 0.11687316
Test loss (w/o reg) on all data: 0.05318265
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5288891e-06
Norm of the params: 13.4467125
     Influence (LOO): fixed 536 labels. Loss 0.05318. Accuracy 0.997.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00599562
Train loss (w/o reg) on all data: 0.0026142604
Test loss (w/o reg) on all data: 0.0036741132
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.421847e-07
Norm of the params: 8.223575
                Loss: fixed 713 labels. Loss 0.00367. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33822238
Train loss (w/o reg) on all data: 0.3314202
Test loss (w/o reg) on all data: 0.14978208
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.254426e-05
Norm of the params: 11.663759
              Random: fixed 185 labels. Loss 0.14978. Accuracy 0.993.
### Flips: 820, rs: 27, checks: 1230
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093911946
Train loss (w/o reg) on all data: 0.08534586
Test loss (w/o reg) on all data: 0.038615663
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0303973e-06
Norm of the params: 13.088995
     Influence (LOO): fixed 588 labels. Loss 0.03862. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056729317
Train loss (w/o reg) on all data: 0.0023279488
Test loss (w/o reg) on all data: 0.0030670967
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9121635e-08
Norm of the params: 8.179221
                Loss: fixed 714 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3226881
Train loss (w/o reg) on all data: 0.31534347
Test loss (w/o reg) on all data: 0.13814974
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.4823312e-05
Norm of the params: 12.119925
              Random: fixed 220 labels. Loss 0.13815. Accuracy 0.996.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41015872
Train loss (w/o reg) on all data: 0.4050196
Test loss (w/o reg) on all data: 0.21331325
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.878812e-05
Norm of the params: 10.1381445
Flipped loss: 0.21331. Accuracy: 0.991
### Flips: 820, rs: 28, checks: 205
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3185342
Train loss (w/o reg) on all data: 0.3088869
Test loss (w/o reg) on all data: 0.15322185
Train acc on all data:  0.8677364454169706
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.883918e-05
Norm of the params: 13.890509
     Influence (LOO): fixed 183 labels. Loss 0.15322. Accuracy 0.994.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28498328
Train loss (w/o reg) on all data: 0.27359334
Test loss (w/o reg) on all data: 0.15888946
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0423579e-05
Norm of the params: 15.093
                Loss: fixed 205 labels. Loss 0.15889. Accuracy 0.977.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39915714
Train loss (w/o reg) on all data: 0.39387587
Test loss (w/o reg) on all data: 0.2003805
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.5286914e-05
Norm of the params: 10.277419
              Random: fixed  37 labels. Loss 0.20038. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25850642
Train loss (w/o reg) on all data: 0.24833225
Test loss (w/o reg) on all data: 0.11082469
Train acc on all data:  0.8983710187211281
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2110611e-05
Norm of the params: 14.264767
     Influence (LOO): fixed 309 labels. Loss 0.11082. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1695473
Train loss (w/o reg) on all data: 0.15296207
Test loss (w/o reg) on all data: 0.09703516
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.0624915e-05
Norm of the params: 18.212759
                Loss: fixed 410 labels. Loss 0.09704. Accuracy 0.978.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38815337
Train loss (w/o reg) on all data: 0.3828852
Test loss (w/o reg) on all data: 0.19152528
Train acc on all data:  0.8441526866034524
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8833045e-05
Norm of the params: 10.26467
              Random: fixed  71 labels. Loss 0.19153. Accuracy 0.994.
### Flips: 820, rs: 28, checks: 615
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20553829
Train loss (w/o reg) on all data: 0.19527887
Test loss (w/o reg) on all data: 0.08689349
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5004497e-05
Norm of the params: 14.324399
     Influence (LOO): fixed 409 labels. Loss 0.08689. Accuracy 0.998.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06139622
Train loss (w/o reg) on all data: 0.046636567
Test loss (w/o reg) on all data: 0.038570683
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2116772e-06
Norm of the params: 17.181183
                Loss: fixed 614 labels. Loss 0.03857. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37539926
Train loss (w/o reg) on all data: 0.36986274
Test loss (w/o reg) on all data: 0.18132977
Train acc on all data:  0.8529054218332117
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8176184e-05
Norm of the params: 10.52285
              Random: fixed 104 labels. Loss 0.18133. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 820
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1595667
Train loss (w/o reg) on all data: 0.15021068
Test loss (w/o reg) on all data: 0.06392373
Train acc on all data:  0.9428640894724045
Test acc on all data:   1.0
Norm of the mean of gradients: 7.940434e-06
Norm of the params: 13.679199
     Influence (LOO): fixed 490 labels. Loss 0.06392. Accuracy 1.000.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010017311
Train loss (w/o reg) on all data: 0.004844656
Test loss (w/o reg) on all data: 0.00433789
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5359965e-07
Norm of the params: 10.17119
                Loss: fixed 714 labels. Loss 0.00434. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35922113
Train loss (w/o reg) on all data: 0.35353643
Test loss (w/o reg) on all data: 0.170945
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.5939824e-06
Norm of the params: 10.662732
              Random: fixed 143 labels. Loss 0.17095. Accuracy 0.991.
### Flips: 820, rs: 28, checks: 1025
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057697
Train loss (w/o reg) on all data: 0.11194238
Test loss (w/o reg) on all data: 0.04482821
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7549246e-06
Norm of the params: 13.141226
     Influence (LOO): fixed 556 labels. Loss 0.04483. Accuracy 0.999.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062414315
Train loss (w/o reg) on all data: 0.0025345336
Test loss (w/o reg) on all data: 0.0032421327
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.1728104e-08
Norm of the params: 8.610339
                Loss: fixed 720 labels. Loss 0.00324. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34552658
Train loss (w/o reg) on all data: 0.34011152
Test loss (w/o reg) on all data: 0.16162916
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.6692316e-05
Norm of the params: 10.406785
              Random: fixed 178 labels. Loss 0.16163. Accuracy 0.991.
### Flips: 820, rs: 28, checks: 1230
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095703825
Train loss (w/o reg) on all data: 0.087829754
Test loss (w/o reg) on all data: 0.034603294
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9857232e-06
Norm of the params: 12.549161
     Influence (LOO): fixed 596 labels. Loss 0.03460. Accuracy 0.999.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054001887
Train loss (w/o reg) on all data: 0.002115111
Test loss (w/o reg) on all data: 0.003028613
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.9454574e-08
Norm of the params: 8.105649
                Loss: fixed 722 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32869655
Train loss (w/o reg) on all data: 0.32304138
Test loss (w/o reg) on all data: 0.15113811
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.10727005e-05
Norm of the params: 10.635021
              Random: fixed 218 labels. Loss 0.15114. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3975348
Train loss (w/o reg) on all data: 0.39112112
Test loss (w/o reg) on all data: 0.20938776
Train acc on all data:  0.8349136883053732
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.774204e-05
Norm of the params: 11.325796
Flipped loss: 0.20939. Accuracy: 0.978
### Flips: 820, rs: 29, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31186792
Train loss (w/o reg) on all data: 0.3018098
Test loss (w/o reg) on all data: 0.15233491
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2441081e-05
Norm of the params: 14.18318
     Influence (LOO): fixed 176 labels. Loss 0.15233. Accuracy 0.988.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27041197
Train loss (w/o reg) on all data: 0.25752717
Test loss (w/o reg) on all data: 0.16517514
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 3.2936146e-05
Norm of the params: 16.052906
                Loss: fixed 205 labels. Loss 0.16518. Accuracy 0.956.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38578114
Train loss (w/o reg) on all data: 0.37928966
Test loss (w/o reg) on all data: 0.19627799
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.7865025e-05
Norm of the params: 11.394279
              Random: fixed  37 labels. Loss 0.19628. Accuracy 0.981.
### Flips: 820, rs: 29, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24011052
Train loss (w/o reg) on all data: 0.22961873
Test loss (w/o reg) on all data: 0.10705007
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0126983e-05
Norm of the params: 14.485712
     Influence (LOO): fixed 314 labels. Loss 0.10705. Accuracy 0.993.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15534903
Train loss (w/o reg) on all data: 0.13901897
Test loss (w/o reg) on all data: 0.102521405
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.9365992e-06
Norm of the params: 18.072113
                Loss: fixed 410 labels. Loss 0.10252. Accuracy 0.972.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3736567
Train loss (w/o reg) on all data: 0.36714694
Test loss (w/o reg) on all data: 0.18614104
Train acc on all data:  0.8524191587648918
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.3528424e-05
Norm of the params: 11.410294
              Random: fixed  71 labels. Loss 0.18614. Accuracy 0.983.
### Flips: 820, rs: 29, checks: 615
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920077
Train loss (w/o reg) on all data: 0.1797639
Test loss (w/o reg) on all data: 0.08199776
Train acc on all data:  0.9307075127644056
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.650693e-06
Norm of the params: 13.738181
     Influence (LOO): fixed 413 labels. Loss 0.08200. Accuracy 0.995.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04935535
Train loss (w/o reg) on all data: 0.035127796
Test loss (w/o reg) on all data: 0.03656313
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0555314e-06
Norm of the params: 16.868643
                Loss: fixed 610 labels. Loss 0.03656. Accuracy 0.988.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36532518
Train loss (w/o reg) on all data: 0.35889837
Test loss (w/o reg) on all data: 0.17882636
Train acc on all data:  0.8582543155847313
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.3651943e-05
Norm of the params: 11.33739
              Random: fixed  93 labels. Loss 0.17883. Accuracy 0.983.
### Flips: 820, rs: 29, checks: 820
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15137997
Train loss (w/o reg) on all data: 0.14274651
Test loss (w/o reg) on all data: 0.062686026
Train acc on all data:  0.9467541940189642
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3619107e-06
Norm of the params: 13.140368
     Influence (LOO): fixed 480 labels. Loss 0.06269. Accuracy 0.998.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012912847
Train loss (w/o reg) on all data: 0.006492001
Test loss (w/o reg) on all data: 0.007989967
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3749316e-07
Norm of the params: 11.332119
                Loss: fixed 684 labels. Loss 0.00799. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35327652
Train loss (w/o reg) on all data: 0.34687567
Test loss (w/o reg) on all data: 0.16722581
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.8842895e-06
Norm of the params: 11.3144655
              Random: fixed 128 labels. Loss 0.16723. Accuracy 0.986.
### Flips: 820, rs: 29, checks: 1025
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112078
Train loss (w/o reg) on all data: 0.10414526
Test loss (w/o reg) on all data: 0.04525147
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.6291602e-06
Norm of the params: 12.595826
     Influence (LOO): fixed 541 labels. Loss 0.04525. Accuracy 0.995.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006314952
Train loss (w/o reg) on all data: 0.0025974729
Test loss (w/o reg) on all data: 0.0041692834
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.564339e-08
Norm of the params: 8.622621
                Loss: fixed 693 labels. Loss 0.00417. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33853397
Train loss (w/o reg) on all data: 0.33185804
Test loss (w/o reg) on all data: 0.15587555
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.283057e-06
Norm of the params: 11.555026
              Random: fixed 165 labels. Loss 0.15588. Accuracy 0.992.
### Flips: 820, rs: 29, checks: 1230
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08374707
Train loss (w/o reg) on all data: 0.0762561
Test loss (w/o reg) on all data: 0.030081559
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 6.186051e-06
Norm of the params: 12.240073
     Influence (LOO): fixed 587 labels. Loss 0.03008. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048052403
Train loss (w/o reg) on all data: 0.0019451012
Test loss (w/o reg) on all data: 0.0033373372
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2305057e-08
Norm of the params: 7.563253
                Loss: fixed 696 labels. Loss 0.00334. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32615462
Train loss (w/o reg) on all data: 0.3193653
Test loss (w/o reg) on all data: 0.14740777
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2225136e-05
Norm of the params: 11.6527605
              Random: fixed 192 labels. Loss 0.14741. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40062472
Train loss (w/o reg) on all data: 0.39484265
Test loss (w/o reg) on all data: 0.22289282
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.2286677e-05
Norm of the params: 10.753663
Flipped loss: 0.22289. Accuracy: 0.970
### Flips: 820, rs: 30, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30905086
Train loss (w/o reg) on all data: 0.3004042
Test loss (w/o reg) on all data: 0.15997007
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.2226357e-05
Norm of the params: 13.150421
     Influence (LOO): fixed 181 labels. Loss 0.15997. Accuracy 0.982.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27370247
Train loss (w/o reg) on all data: 0.26057187
Test loss (w/o reg) on all data: 0.18078558
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 9.756128e-06
Norm of the params: 16.205309
                Loss: fixed 205 labels. Loss 0.18079. Accuracy 0.941.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38958836
Train loss (w/o reg) on all data: 0.38387185
Test loss (w/o reg) on all data: 0.21145636
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.868787e-06
Norm of the params: 10.692511
              Random: fixed  35 labels. Loss 0.21146. Accuracy 0.976.
### Flips: 820, rs: 30, checks: 410
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24633947
Train loss (w/o reg) on all data: 0.23671599
Test loss (w/o reg) on all data: 0.122435026
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3266144e-05
Norm of the params: 13.873347
     Influence (LOO): fixed 306 labels. Loss 0.12244. Accuracy 0.990.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1605055
Train loss (w/o reg) on all data: 0.14392942
Test loss (w/o reg) on all data: 0.120308965
Train acc on all data:  0.9380014587892049
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.9061783e-06
Norm of the params: 18.207731
                Loss: fixed 410 labels. Loss 0.12031. Accuracy 0.965.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3790398
Train loss (w/o reg) on all data: 0.37319285
Test loss (w/o reg) on all data: 0.2003182
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8807977e-05
Norm of the params: 10.813834
              Random: fixed  67 labels. Loss 0.20032. Accuracy 0.979.
### Flips: 820, rs: 30, checks: 615
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1998456
Train loss (w/o reg) on all data: 0.19057171
Test loss (w/o reg) on all data: 0.093933836
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2597636e-06
Norm of the params: 13.619022
     Influence (LOO): fixed 396 labels. Loss 0.09393. Accuracy 0.997.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052682854
Train loss (w/o reg) on all data: 0.03931253
Test loss (w/o reg) on all data: 0.05783333
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.6237036e-06
Norm of the params: 16.352568
                Loss: fixed 611 labels. Loss 0.05783. Accuracy 0.984.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36623615
Train loss (w/o reg) on all data: 0.3605043
Test loss (w/o reg) on all data: 0.18863142
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.900097e-05
Norm of the params: 10.7068615
              Random: fixed 102 labels. Loss 0.18863. Accuracy 0.984.
### Flips: 820, rs: 30, checks: 820
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15381937
Train loss (w/o reg) on all data: 0.14487784
Test loss (w/o reg) on all data: 0.06599025
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1792981e-06
Norm of the params: 13.372761
     Influence (LOO): fixed 481 labels. Loss 0.06599. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0109256385
Train loss (w/o reg) on all data: 0.005325126
Test loss (w/o reg) on all data: 0.0078931
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.825486e-08
Norm of the params: 10.583489
                Loss: fixed 693 labels. Loss 0.00789. Accuracy 0.998.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35149887
Train loss (w/o reg) on all data: 0.34535787
Test loss (w/o reg) on all data: 0.17520817
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.178015e-06
Norm of the params: 11.082416
              Random: fixed 139 labels. Loss 0.17521. Accuracy 0.990.
### Flips: 820, rs: 30, checks: 1025
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11419422
Train loss (w/o reg) on all data: 0.10568968
Test loss (w/o reg) on all data: 0.046467815
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0862716e-06
Norm of the params: 13.041888
     Influence (LOO): fixed 545 labels. Loss 0.04647. Accuracy 0.996.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054602213
Train loss (w/o reg) on all data: 0.002143149
Test loss (w/o reg) on all data: 0.004622577
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1436259e-07
Norm of the params: 8.145026
                Loss: fixed 703 labels. Loss 0.00462. Accuracy 0.999.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3406042
Train loss (w/o reg) on all data: 0.3345423
Test loss (w/o reg) on all data: 0.16479583
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.4865055e-05
Norm of the params: 11.010794
              Random: fixed 167 labels. Loss 0.16480. Accuracy 0.988.
### Flips: 820, rs: 30, checks: 1230
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088264346
Train loss (w/o reg) on all data: 0.08082959
Test loss (w/o reg) on all data: 0.03390305
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6539954e-06
Norm of the params: 12.194063
     Influence (LOO): fixed 588 labels. Loss 0.03390. Accuracy 0.998.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041835243
Train loss (w/o reg) on all data: 0.0015734275
Test loss (w/o reg) on all data: 0.0037405386
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7916565e-08
Norm of the params: 7.225091
                Loss: fixed 706 labels. Loss 0.00374. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32629347
Train loss (w/o reg) on all data: 0.32021075
Test loss (w/o reg) on all data: 0.15456054
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.316518e-05
Norm of the params: 11.029697
              Random: fixed 202 labels. Loss 0.15456. Accuracy 0.989.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40637156
Train loss (w/o reg) on all data: 0.40027153
Test loss (w/o reg) on all data: 0.21978049
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.3970536e-05
Norm of the params: 11.045387
Flipped loss: 0.21978. Accuracy: 0.977
### Flips: 820, rs: 31, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31967643
Train loss (w/o reg) on all data: 0.310302
Test loss (w/o reg) on all data: 0.15931721
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.9643017e-05
Norm of the params: 13.692663
     Influence (LOO): fixed 181 labels. Loss 0.15932. Accuracy 0.993.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28342927
Train loss (w/o reg) on all data: 0.27030626
Test loss (w/o reg) on all data: 0.17452402
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.6144542e-05
Norm of the params: 16.20062
                Loss: fixed 205 labels. Loss 0.17452. Accuracy 0.954.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3954835
Train loss (w/o reg) on all data: 0.38953242
Test loss (w/o reg) on all data: 0.20555207
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 8.404702e-06
Norm of the params: 10.909698
              Random: fixed  38 labels. Loss 0.20555. Accuracy 0.981.
### Flips: 820, rs: 31, checks: 410
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253603
Train loss (w/o reg) on all data: 0.24298534
Test loss (w/o reg) on all data: 0.11966473
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3302216e-05
Norm of the params: 14.572359
     Influence (LOO): fixed 313 labels. Loss 0.11966. Accuracy 0.993.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16550305
Train loss (w/o reg) on all data: 0.14684461
Test loss (w/o reg) on all data: 0.11627027
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.56589e-06
Norm of the params: 19.31758
                Loss: fixed 410 labels. Loss 0.11627. Accuracy 0.966.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38518715
Train loss (w/o reg) on all data: 0.37917536
Test loss (w/o reg) on all data: 0.19517204
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.9107294e-05
Norm of the params: 10.965217
              Random: fixed  71 labels. Loss 0.19517. Accuracy 0.984.
### Flips: 820, rs: 31, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1992456
Train loss (w/o reg) on all data: 0.18988344
Test loss (w/o reg) on all data: 0.0883834
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0398114e-06
Norm of the params: 13.68369
     Influence (LOO): fixed 419 labels. Loss 0.08838. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061048355
Train loss (w/o reg) on all data: 0.044810127
Test loss (w/o reg) on all data: 0.039620634
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.326517e-06
Norm of the params: 18.021225
                Loss: fixed 611 labels. Loss 0.03962. Accuracy 0.991.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37254542
Train loss (w/o reg) on all data: 0.36630562
Test loss (w/o reg) on all data: 0.1861288
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.2381703e-05
Norm of the params: 11.171203
              Random: fixed 104 labels. Loss 0.18613. Accuracy 0.983.
### Flips: 820, rs: 31, checks: 820
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16123523
Train loss (w/o reg) on all data: 0.15193248
Test loss (w/o reg) on all data: 0.067231305
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.163836e-05
Norm of the params: 13.640199
     Influence (LOO): fixed 486 labels. Loss 0.06723. Accuracy 0.998.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011113446
Train loss (w/o reg) on all data: 0.0054620993
Test loss (w/o reg) on all data: 0.005556151
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.16586556e-07
Norm of the params: 10.6314125
                Loss: fixed 709 labels. Loss 0.00556. Accuracy 0.999.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.362927
Train loss (w/o reg) on all data: 0.35662758
Test loss (w/o reg) on all data: 0.17962363
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.76105e-06
Norm of the params: 11.224441
              Random: fixed 129 labels. Loss 0.17962. Accuracy 0.983.
### Flips: 820, rs: 31, checks: 1025
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12108483
Train loss (w/o reg) on all data: 0.11305589
Test loss (w/o reg) on all data: 0.049946073
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4854066e-06
Norm of the params: 12.671967
     Influence (LOO): fixed 552 labels. Loss 0.04995. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006175466
Train loss (w/o reg) on all data: 0.002668193
Test loss (w/o reg) on all data: 0.004866314
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.6692595e-08
Norm of the params: 8.375289
                Loss: fixed 719 labels. Loss 0.00487. Accuracy 0.998.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34673473
Train loss (w/o reg) on all data: 0.3402372
Test loss (w/o reg) on all data: 0.16856325
Train acc on all data:  0.8672501823486506
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.0392019e-05
Norm of the params: 11.3995905
              Random: fixed 167 labels. Loss 0.16856. Accuracy 0.985.
### Flips: 820, rs: 31, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09022911
Train loss (w/o reg) on all data: 0.082980506
Test loss (w/o reg) on all data: 0.035303365
Train acc on all data:  0.9700948212983224
Test acc on all data:   1.0
Norm of the mean of gradients: 7.570454e-06
Norm of the params: 12.040437
     Influence (LOO): fixed 601 labels. Loss 0.03530. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035895973
Train loss (w/o reg) on all data: 0.0012758318
Test loss (w/o reg) on all data: 0.0026635204
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0276072e-08
Norm of the params: 6.802596
                Loss: fixed 723 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32798448
Train loss (w/o reg) on all data: 0.3211203
Test loss (w/o reg) on all data: 0.15513453
Train acc on all data:  0.8781911013858498
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.207818e-06
Norm of the params: 11.716814
              Random: fixed 211 labels. Loss 0.15513. Accuracy 0.987.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40261558
Train loss (w/o reg) on all data: 0.39770135
Test loss (w/o reg) on all data: 0.21935385
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.1149884e-05
Norm of the params: 9.913851
Flipped loss: 0.21935. Accuracy: 0.977
### Flips: 820, rs: 32, checks: 205
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31395695
Train loss (w/o reg) on all data: 0.30465052
Test loss (w/o reg) on all data: 0.1557097
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.15184e-05
Norm of the params: 13.642888
     Influence (LOO): fixed 178 labels. Loss 0.15571. Accuracy 0.988.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2762543
Train loss (w/o reg) on all data: 0.26504436
Test loss (w/o reg) on all data: 0.17548336
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.502933e-06
Norm of the params: 14.973261
                Loss: fixed 205 labels. Loss 0.17548. Accuracy 0.954.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38658413
Train loss (w/o reg) on all data: 0.3815102
Test loss (w/o reg) on all data: 0.20374785
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.757585e-05
Norm of the params: 10.073674
              Random: fixed  45 labels. Loss 0.20375. Accuracy 0.979.
### Flips: 820, rs: 32, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25205013
Train loss (w/o reg) on all data: 0.24223965
Test loss (w/o reg) on all data: 0.1175274
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.7573064e-06
Norm of the params: 14.007483
     Influence (LOO): fixed 305 labels. Loss 0.11753. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15474333
Train loss (w/o reg) on all data: 0.1392087
Test loss (w/o reg) on all data: 0.121727705
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.0636868e-05
Norm of the params: 17.626474
                Loss: fixed 410 labels. Loss 0.12173. Accuracy 0.961.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37222937
Train loss (w/o reg) on all data: 0.36673445
Test loss (w/o reg) on all data: 0.1903659
Train acc on all data:  0.8516897641624118
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.2856498e-05
Norm of the params: 10.483243
              Random: fixed  83 labels. Loss 0.19037. Accuracy 0.982.
### Flips: 820, rs: 32, checks: 615
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19828226
Train loss (w/o reg) on all data: 0.18865483
Test loss (w/o reg) on all data: 0.08847158
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.12097e-06
Norm of the params: 13.876189
     Influence (LOO): fixed 403 labels. Loss 0.08847. Accuracy 0.997.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05069957
Train loss (w/o reg) on all data: 0.036537524
Test loss (w/o reg) on all data: 0.045833744
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3143382e-06
Norm of the params: 16.829762
                Loss: fixed 613 labels. Loss 0.04583. Accuracy 0.988.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35704654
Train loss (w/o reg) on all data: 0.3513715
Test loss (w/o reg) on all data: 0.1780255
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1406593e-05
Norm of the params: 10.653696
              Random: fixed 122 labels. Loss 0.17803. Accuracy 0.985.
### Flips: 820, rs: 32, checks: 820
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15265894
Train loss (w/o reg) on all data: 0.14398636
Test loss (w/o reg) on all data: 0.062784076
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.629816e-06
Norm of the params: 13.170104
     Influence (LOO): fixed 486 labels. Loss 0.06278. Accuracy 0.998.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0079300525
Train loss (w/o reg) on all data: 0.0033574759
Test loss (w/o reg) on all data: 0.005694498
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.589344e-08
Norm of the params: 9.563029
                Loss: fixed 695 labels. Loss 0.00569. Accuracy 0.999.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34206703
Train loss (w/o reg) on all data: 0.3360243
Test loss (w/o reg) on all data: 0.16878225
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.014422e-06
Norm of the params: 10.993383
              Random: fixed 157 labels. Loss 0.16878. Accuracy 0.982.
### Flips: 820, rs: 32, checks: 1025
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12141559
Train loss (w/o reg) on all data: 0.11376064
Test loss (w/o reg) on all data: 0.045623414
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.9106183e-06
Norm of the params: 12.373321
     Influence (LOO): fixed 539 labels. Loss 0.04562. Accuracy 0.999.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060059526
Train loss (w/o reg) on all data: 0.0023098753
Test loss (w/o reg) on all data: 0.0036200483
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3478254e-07
Norm of the params: 8.597764
                Loss: fixed 699 labels. Loss 0.00362. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32911873
Train loss (w/o reg) on all data: 0.3230706
Test loss (w/o reg) on all data: 0.15914066
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.9668325e-05
Norm of the params: 10.998317
              Random: fixed 188 labels. Loss 0.15914. Accuracy 0.983.
### Flips: 820, rs: 32, checks: 1230
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883145
Train loss (w/o reg) on all data: 0.08120137
Test loss (w/o reg) on all data: 0.031103473
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.992483e-06
Norm of the params: 11.927397
     Influence (LOO): fixed 592 labels. Loss 0.03110. Accuracy 0.999.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056980783
Train loss (w/o reg) on all data: 0.002165936
Test loss (w/o reg) on all data: 0.0034507115
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.100544e-08
Norm of the params: 8.40493
                Loss: fixed 700 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31577873
Train loss (w/o reg) on all data: 0.30971327
Test loss (w/o reg) on all data: 0.14588454
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.601467e-05
Norm of the params: 11.014031
              Random: fixed 223 labels. Loss 0.14588. Accuracy 0.985.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4088102
Train loss (w/o reg) on all data: 0.40330106
Test loss (w/o reg) on all data: 0.21222241
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.08839e-05
Norm of the params: 10.496808
Flipped loss: 0.21222. Accuracy: 0.987
### Flips: 820, rs: 33, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31869057
Train loss (w/o reg) on all data: 0.3091973
Test loss (w/o reg) on all data: 0.15411483
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.666763e-06
Norm of the params: 13.77915
     Influence (LOO): fixed 175 labels. Loss 0.15411. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2845409
Train loss (w/o reg) on all data: 0.27225184
Test loss (w/o reg) on all data: 0.15526368
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9400159e-05
Norm of the params: 15.677402
                Loss: fixed 205 labels. Loss 0.15526. Accuracy 0.966.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4000377
Train loss (w/o reg) on all data: 0.39428967
Test loss (w/o reg) on all data: 0.20423067
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4521075e-05
Norm of the params: 10.72197
              Random: fixed  27 labels. Loss 0.20423. Accuracy 0.985.
### Flips: 820, rs: 33, checks: 410
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252048
Train loss (w/o reg) on all data: 0.24186446
Test loss (w/o reg) on all data: 0.11424182
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.070677e-06
Norm of the params: 14.271318
     Influence (LOO): fixed 309 labels. Loss 0.11424. Accuracy 0.995.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17179316
Train loss (w/o reg) on all data: 0.15579809
Test loss (w/o reg) on all data: 0.10413598
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 6.557993e-06
Norm of the params: 17.885792
                Loss: fixed 410 labels. Loss 0.10414. Accuracy 0.969.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3878215
Train loss (w/o reg) on all data: 0.38201594
Test loss (w/o reg) on all data: 0.19127624
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5089179e-05
Norm of the params: 10.775483
              Random: fixed  63 labels. Loss 0.19128. Accuracy 0.990.
### Flips: 820, rs: 33, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20101482
Train loss (w/o reg) on all data: 0.1913785
Test loss (w/o reg) on all data: 0.08668224
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9493383e-06
Norm of the params: 13.882585
     Influence (LOO): fixed 405 labels. Loss 0.08668. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059740465
Train loss (w/o reg) on all data: 0.044652835
Test loss (w/o reg) on all data: 0.04116404
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.641149e-06
Norm of the params: 17.371029
                Loss: fixed 611 labels. Loss 0.04116. Accuracy 0.990.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37350816
Train loss (w/o reg) on all data: 0.36773574
Test loss (w/o reg) on all data: 0.17547145
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7636152e-05
Norm of the params: 10.744689
              Random: fixed 107 labels. Loss 0.17547. Accuracy 0.992.
### Flips: 820, rs: 33, checks: 820
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15995939
Train loss (w/o reg) on all data: 0.15068078
Test loss (w/o reg) on all data: 0.06649494
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0929996e-05
Norm of the params: 13.622488
     Influence (LOO): fixed 479 labels. Loss 0.06649. Accuracy 0.999.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01032252
Train loss (w/o reg) on all data: 0.0053780815
Test loss (w/o reg) on all data: 0.0059444676
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.512536e-07
Norm of the params: 9.944283
                Loss: fixed 706 labels. Loss 0.00594. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35765907
Train loss (w/o reg) on all data: 0.35145813
Test loss (w/o reg) on all data: 0.16521667
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.3547716e-05
Norm of the params: 11.136368
              Random: fixed 143 labels. Loss 0.16522. Accuracy 0.991.
### Flips: 820, rs: 33, checks: 1025
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12365204
Train loss (w/o reg) on all data: 0.11544994
Test loss (w/o reg) on all data: 0.052226014
Train acc on all data:  0.9567225869195235
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5799052e-06
Norm of the params: 12.80789
     Influence (LOO): fixed 538 labels. Loss 0.05223. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051760897
Train loss (w/o reg) on all data: 0.002005174
Test loss (w/o reg) on all data: 0.0035210452
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2672966e-08
Norm of the params: 7.963562
                Loss: fixed 713 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34580436
Train loss (w/o reg) on all data: 0.33949324
Test loss (w/o reg) on all data: 0.1540623
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6037047e-05
Norm of the params: 11.234869
              Random: fixed 176 labels. Loss 0.15406. Accuracy 0.993.
### Flips: 820, rs: 33, checks: 1230
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096146
Train loss (w/o reg) on all data: 0.08832339
Test loss (w/o reg) on all data: 0.039341606
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 9.300795e-06
Norm of the params: 12.508088
     Influence (LOO): fixed 582 labels. Loss 0.03934. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004452096
Train loss (w/o reg) on all data: 0.0016603386
Test loss (w/o reg) on all data: 0.0032096242
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.566128e-08
Norm of the params: 7.472292
                Loss: fixed 714 labels. Loss 0.00321. Accuracy 1.000.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33031067
Train loss (w/o reg) on all data: 0.32372147
Test loss (w/o reg) on all data: 0.14415304
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.8744755e-06
Norm of the params: 11.479727
              Random: fixed 211 labels. Loss 0.14415. Accuracy 0.993.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40795308
Train loss (w/o reg) on all data: 0.40212476
Test loss (w/o reg) on all data: 0.21758664
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.622394e-05
Norm of the params: 10.79658
Flipped loss: 0.21759. Accuracy: 0.986
### Flips: 820, rs: 34, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31301916
Train loss (w/o reg) on all data: 0.3033827
Test loss (w/o reg) on all data: 0.15595256
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.342289e-05
Norm of the params: 13.882698
     Influence (LOO): fixed 188 labels. Loss 0.15595. Accuracy 0.993.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2828678
Train loss (w/o reg) on all data: 0.2697749
Test loss (w/o reg) on all data: 0.17200066
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 5.4311377e-06
Norm of the params: 16.182013
                Loss: fixed 205 labels. Loss 0.17200. Accuracy 0.954.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39343107
Train loss (w/o reg) on all data: 0.3878148
Test loss (w/o reg) on all data: 0.2035317
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.544178e-05
Norm of the params: 10.598364
              Random: fixed  48 labels. Loss 0.20353. Accuracy 0.985.
### Flips: 820, rs: 34, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25067478
Train loss (w/o reg) on all data: 0.24066523
Test loss (w/o reg) on all data: 0.115240134
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4603567e-05
Norm of the params: 14.148891
     Influence (LOO): fixed 310 labels. Loss 0.11524. Accuracy 0.997.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17065002
Train loss (w/o reg) on all data: 0.15359944
Test loss (w/o reg) on all data: 0.10866206
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.2756491e-05
Norm of the params: 18.466497
                Loss: fixed 410 labels. Loss 0.10866. Accuracy 0.968.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38316235
Train loss (w/o reg) on all data: 0.37746596
Test loss (w/o reg) on all data: 0.19289748
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.624824e-05
Norm of the params: 10.673682
              Random: fixed  79 labels. Loss 0.19290. Accuracy 0.988.
### Flips: 820, rs: 34, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19744392
Train loss (w/o reg) on all data: 0.18711211
Test loss (w/o reg) on all data: 0.08848267
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.925351e-06
Norm of the params: 14.374844
     Influence (LOO): fixed 404 labels. Loss 0.08848. Accuracy 0.995.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05959459
Train loss (w/o reg) on all data: 0.044033002
Test loss (w/o reg) on all data: 0.036881097
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.8342153e-06
Norm of the params: 17.641764
                Loss: fixed 615 labels. Loss 0.03688. Accuracy 0.989.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37169108
Train loss (w/o reg) on all data: 0.36607292
Test loss (w/o reg) on all data: 0.18318264
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.4256027e-05
Norm of the params: 10.600133
              Random: fixed 113 labels. Loss 0.18318. Accuracy 0.988.
### Flips: 820, rs: 34, checks: 820
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15339363
Train loss (w/o reg) on all data: 0.14339915
Test loss (w/o reg) on all data: 0.06514702
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2481956e-06
Norm of the params: 14.138229
     Influence (LOO): fixed 484 labels. Loss 0.06515. Accuracy 0.997.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008913564
Train loss (w/o reg) on all data: 0.004111605
Test loss (w/o reg) on all data: 0.0044506504
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 5.691635e-07
Norm of the params: 9.799957
                Loss: fixed 709 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35864973
Train loss (w/o reg) on all data: 0.3529573
Test loss (w/o reg) on all data: 0.171109
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.0482916e-05
Norm of the params: 10.669983
              Random: fixed 146 labels. Loss 0.17111. Accuracy 0.989.
### Flips: 820, rs: 34, checks: 1025
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1151439
Train loss (w/o reg) on all data: 0.10575992
Test loss (w/o reg) on all data: 0.04692836
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.9072037e-06
Norm of the params: 13.699625
     Influence (LOO): fixed 545 labels. Loss 0.04693. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065856846
Train loss (w/o reg) on all data: 0.0029697318
Test loss (w/o reg) on all data: 0.0039367615
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 9.759016e-08
Norm of the params: 8.504061
                Loss: fixed 713 labels. Loss 0.00394. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34460178
Train loss (w/o reg) on all data: 0.3387728
Test loss (w/o reg) on all data: 0.16153184
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2538723e-05
Norm of the params: 10.7971945
              Random: fixed 181 labels. Loss 0.16153. Accuracy 0.991.
### Flips: 820, rs: 34, checks: 1230
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09841027
Train loss (w/o reg) on all data: 0.08965059
Test loss (w/o reg) on all data: 0.040125586
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7287904e-06
Norm of the params: 13.236071
     Influence (LOO): fixed 573 labels. Loss 0.04013. Accuracy 0.998.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037254128
Train loss (w/o reg) on all data: 0.0014285754
Test loss (w/o reg) on all data: 0.0032918197
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.98379e-08
Norm of the params: 6.7776656
                Loss: fixed 716 labels. Loss 0.00329. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3296639
Train loss (w/o reg) on all data: 0.32362148
Test loss (w/o reg) on all data: 0.14887537
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1473244e-05
Norm of the params: 10.993118
              Random: fixed 216 labels. Loss 0.14888. Accuracy 0.991.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4078686
Train loss (w/o reg) on all data: 0.40185556
Test loss (w/o reg) on all data: 0.22116934
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.8490595e-05
Norm of the params: 10.966348
Flipped loss: 0.22117. Accuracy: 0.979
### Flips: 820, rs: 35, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3206586
Train loss (w/o reg) on all data: 0.31081513
Test loss (w/o reg) on all data: 0.16563676
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.809471e-06
Norm of the params: 14.031008
     Influence (LOO): fixed 174 labels. Loss 0.16564. Accuracy 0.989.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2861131
Train loss (w/o reg) on all data: 0.2731375
Test loss (w/o reg) on all data: 0.17001554
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 8.950845e-06
Norm of the params: 16.109377
                Loss: fixed 205 labels. Loss 0.17002. Accuracy 0.963.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39544284
Train loss (w/o reg) on all data: 0.3892165
Test loss (w/o reg) on all data: 0.21004944
Train acc on all data:  0.8373450036469731
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.0893659e-05
Norm of the params: 11.1591625
              Random: fixed  35 labels. Loss 0.21005. Accuracy 0.979.
### Flips: 820, rs: 35, checks: 410
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2577966
Train loss (w/o reg) on all data: 0.24822079
Test loss (w/o reg) on all data: 0.12161475
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0277017e-05
Norm of the params: 13.838938
     Influence (LOO): fixed 310 labels. Loss 0.12161. Accuracy 0.993.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1735403
Train loss (w/o reg) on all data: 0.15551256
Test loss (w/o reg) on all data: 0.11352164
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.1363021e-05
Norm of the params: 18.98828
                Loss: fixed 410 labels. Loss 0.11352. Accuracy 0.965.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3823439
Train loss (w/o reg) on all data: 0.37577885
Test loss (w/o reg) on all data: 0.19886371
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.947788e-05
Norm of the params: 11.458645
              Random: fixed  70 labels. Loss 0.19886. Accuracy 0.979.
### Flips: 820, rs: 35, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1982506
Train loss (w/o reg) on all data: 0.18865554
Test loss (w/o reg) on all data: 0.08826081
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.766773e-06
Norm of the params: 13.852848
     Influence (LOO): fixed 423 labels. Loss 0.08826. Accuracy 0.996.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063783765
Train loss (w/o reg) on all data: 0.047294874
Test loss (w/o reg) on all data: 0.04089194
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.7049585e-06
Norm of the params: 18.159788
                Loss: fixed 612 labels. Loss 0.04089. Accuracy 0.985.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36768445
Train loss (w/o reg) on all data: 0.3611076
Test loss (w/o reg) on all data: 0.18698408
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.422113e-05
Norm of the params: 11.468968
              Random: fixed 112 labels. Loss 0.18698. Accuracy 0.983.
### Flips: 820, rs: 35, checks: 820
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15917729
Train loss (w/o reg) on all data: 0.15107334
Test loss (w/o reg) on all data: 0.06287981
Train acc on all data:  0.9443228786773644
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9213363e-06
Norm of the params: 12.7310295
     Influence (LOO): fixed 497 labels. Loss 0.06288. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011982882
Train loss (w/o reg) on all data: 0.0057522473
Test loss (w/o reg) on all data: 0.0057809995
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 3.648443e-07
Norm of the params: 11.163007
                Loss: fixed 709 labels. Loss 0.00578. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35476783
Train loss (w/o reg) on all data: 0.34836057
Test loss (w/o reg) on all data: 0.17490916
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.9893013e-05
Norm of the params: 11.320113
              Random: fixed 146 labels. Loss 0.17491. Accuracy 0.986.
### Flips: 820, rs: 35, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11977202
Train loss (w/o reg) on all data: 0.11232134
Test loss (w/o reg) on all data: 0.045373812
Train acc on all data:  0.9606126914660832
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5929522e-06
Norm of the params: 12.207109
     Influence (LOO): fixed 562 labels. Loss 0.04537. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00810018
Train loss (w/o reg) on all data: 0.003644195
Test loss (w/o reg) on all data: 0.0045579267
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8023566e-07
Norm of the params: 9.440324
                Loss: fixed 717 labels. Loss 0.00456. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34030828
Train loss (w/o reg) on all data: 0.33336726
Test loss (w/o reg) on all data: 0.165776
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.2811789e-05
Norm of the params: 11.782213
              Random: fixed 177 labels. Loss 0.16578. Accuracy 0.985.
### Flips: 820, rs: 35, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09193147
Train loss (w/o reg) on all data: 0.085053004
Test loss (w/o reg) on all data: 0.034508582
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7517887e-06
Norm of the params: 11.728992
     Influence (LOO): fixed 605 labels. Loss 0.03451. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061537195
Train loss (w/o reg) on all data: 0.0025956316
Test loss (w/o reg) on all data: 0.0036159747
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4376636e-08
Norm of the params: 8.435743
                Loss: fixed 720 labels. Loss 0.00362. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3284105
Train loss (w/o reg) on all data: 0.3212316
Test loss (w/o reg) on all data: 0.15800641
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.840335e-05
Norm of the params: 11.982407
              Random: fixed 204 labels. Loss 0.15801. Accuracy 0.985.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40528017
Train loss (w/o reg) on all data: 0.39920887
Test loss (w/o reg) on all data: 0.22684105
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.5314801e-05
Norm of the params: 11.019359
Flipped loss: 0.22684. Accuracy: 0.973
### Flips: 820, rs: 36, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3106071
Train loss (w/o reg) on all data: 0.30146173
Test loss (w/o reg) on all data: 0.16601664
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3794364e-05
Norm of the params: 13.524338
     Influence (LOO): fixed 183 labels. Loss 0.16602. Accuracy 0.985.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27948415
Train loss (w/o reg) on all data: 0.26763198
Test loss (w/o reg) on all data: 0.17666255
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 6.0045886e-06
Norm of the params: 15.396219
                Loss: fixed 205 labels. Loss 0.17666. Accuracy 0.954.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39283663
Train loss (w/o reg) on all data: 0.38686442
Test loss (w/o reg) on all data: 0.21306524
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.781702e-05
Norm of the params: 10.929037
              Random: fixed  38 labels. Loss 0.21307. Accuracy 0.975.
### Flips: 820, rs: 36, checks: 410
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24353388
Train loss (w/o reg) on all data: 0.233591
Test loss (w/o reg) on all data: 0.11999946
Train acc on all data:  0.9044493070751276
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.972899e-06
Norm of the params: 14.10168
     Influence (LOO): fixed 316 labels. Loss 0.12000. Accuracy 0.997.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16430748
Train loss (w/o reg) on all data: 0.14856686
Test loss (w/o reg) on all data: 0.12996113
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.6181215e-06
Norm of the params: 17.74295
                Loss: fixed 409 labels. Loss 0.12996. Accuracy 0.962.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38363
Train loss (w/o reg) on all data: 0.37736884
Test loss (w/o reg) on all data: 0.20362753
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.1067346e-05
Norm of the params: 11.190332
              Random: fixed  65 labels. Loss 0.20363. Accuracy 0.978.
### Flips: 820, rs: 36, checks: 615
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19893713
Train loss (w/o reg) on all data: 0.18980576
Test loss (w/o reg) on all data: 0.091947414
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0278859e-05
Norm of the params: 13.51397
     Influence (LOO): fixed 403 labels. Loss 0.09195. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056316428
Train loss (w/o reg) on all data: 0.042913802
Test loss (w/o reg) on all data: 0.045346897
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.392657e-06
Norm of the params: 16.37231
                Loss: fixed 611 labels. Loss 0.04535. Accuracy 0.983.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37367576
Train loss (w/o reg) on all data: 0.36736777
Test loss (w/o reg) on all data: 0.19288045
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.792745e-05
Norm of the params: 11.232076
              Random: fixed  94 labels. Loss 0.19288. Accuracy 0.983.
### Flips: 820, rs: 36, checks: 820
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15496555
Train loss (w/o reg) on all data: 0.14601691
Test loss (w/o reg) on all data: 0.064815156
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.999264e-06
Norm of the params: 13.378068
     Influence (LOO): fixed 484 labels. Loss 0.06482. Accuracy 0.999.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013259575
Train loss (w/o reg) on all data: 0.006473471
Test loss (w/o reg) on all data: 0.00637928
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0976115e-07
Norm of the params: 11.649982
                Loss: fixed 697 labels. Loss 0.00638. Accuracy 0.998.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35886854
Train loss (w/o reg) on all data: 0.35232762
Test loss (w/o reg) on all data: 0.17964906
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.041893e-05
Norm of the params: 11.4376
              Random: fixed 133 labels. Loss 0.17965. Accuracy 0.986.
### Flips: 820, rs: 36, checks: 1025
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12704495
Train loss (w/o reg) on all data: 0.1181654
Test loss (w/o reg) on all data: 0.051736612
Train acc on all data:  0.9547775346462436
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9011119e-06
Norm of the params: 13.326325
     Influence (LOO): fixed 529 labels. Loss 0.05174. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00912368
Train loss (w/o reg) on all data: 0.004330108
Test loss (w/o reg) on all data: 0.0040136673
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.598539e-07
Norm of the params: 9.791396
                Loss: fixed 704 labels. Loss 0.00401. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34487033
Train loss (w/o reg) on all data: 0.33780882
Test loss (w/o reg) on all data: 0.16759127
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.0147123e-05
Norm of the params: 11.884025
              Random: fixed 169 labels. Loss 0.16759. Accuracy 0.989.
### Flips: 820, rs: 36, checks: 1230
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0965671
Train loss (w/o reg) on all data: 0.08877853
Test loss (w/o reg) on all data: 0.038329348
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1378338e-06
Norm of the params: 12.480841
     Influence (LOO): fixed 578 labels. Loss 0.03833. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0055775736
Train loss (w/o reg) on all data: 0.0024535896
Test loss (w/o reg) on all data: 0.0031448407
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9245353e-07
Norm of the params: 7.9044094
                Loss: fixed 709 labels. Loss 0.00314. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33251882
Train loss (w/o reg) on all data: 0.32554078
Test loss (w/o reg) on all data: 0.15658066
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.334796e-06
Norm of the params: 11.813572
              Random: fixed 203 labels. Loss 0.15658. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40272722
Train loss (w/o reg) on all data: 0.39666718
Test loss (w/o reg) on all data: 0.21326882
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1316731e-05
Norm of the params: 11.009108
Flipped loss: 0.21327. Accuracy: 0.982
### Flips: 820, rs: 37, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31260705
Train loss (w/o reg) on all data: 0.30339798
Test loss (w/o reg) on all data: 0.1590489
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.3116753e-06
Norm of the params: 13.571351
     Influence (LOO): fixed 180 labels. Loss 0.15905. Accuracy 0.982.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27644908
Train loss (w/o reg) on all data: 0.26340187
Test loss (w/o reg) on all data: 0.16026819
Train acc on all data:  0.888402625820569
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.6980797e-05
Norm of the params: 16.153765
                Loss: fixed 205 labels. Loss 0.16027. Accuracy 0.962.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39153358
Train loss (w/o reg) on all data: 0.3853967
Test loss (w/o reg) on all data: 0.20204982
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.6830749e-05
Norm of the params: 11.078708
              Random: fixed  34 labels. Loss 0.20205. Accuracy 0.983.
### Flips: 820, rs: 37, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24104473
Train loss (w/o reg) on all data: 0.2306856
Test loss (w/o reg) on all data: 0.11519419
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.84462e-05
Norm of the params: 14.393836
     Influence (LOO): fixed 319 labels. Loss 0.11519. Accuracy 0.990.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16167484
Train loss (w/o reg) on all data: 0.14265458
Test loss (w/o reg) on all data: 0.10215914
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.6170993e-06
Norm of the params: 19.503979
                Loss: fixed 410 labels. Loss 0.10216. Accuracy 0.970.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3768929
Train loss (w/o reg) on all data: 0.37077212
Test loss (w/o reg) on all data: 0.18738914
Train acc on all data:  0.8529054218332117
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.4833293e-05
Norm of the params: 11.064152
              Random: fixed  75 labels. Loss 0.18739. Accuracy 0.986.
### Flips: 820, rs: 37, checks: 615
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19201677
Train loss (w/o reg) on all data: 0.18170151
Test loss (w/o reg) on all data: 0.07960233
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2296144e-05
Norm of the params: 14.363322
     Influence (LOO): fixed 416 labels. Loss 0.07960. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05396717
Train loss (w/o reg) on all data: 0.03832447
Test loss (w/o reg) on all data: 0.033031832
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5132854e-06
Norm of the params: 17.687677
                Loss: fixed 612 labels. Loss 0.03303. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3635261
Train loss (w/o reg) on all data: 0.35742968
Test loss (w/o reg) on all data: 0.17731659
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.5212548e-05
Norm of the params: 11.042115
              Random: fixed 113 labels. Loss 0.17732. Accuracy 0.986.
### Flips: 820, rs: 37, checks: 820
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15415622
Train loss (w/o reg) on all data: 0.144814
Test loss (w/o reg) on all data: 0.058998317
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.659751e-06
Norm of the params: 13.669108
     Influence (LOO): fixed 486 labels. Loss 0.05900. Accuracy 0.998.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010335671
Train loss (w/o reg) on all data: 0.00482343
Test loss (w/o reg) on all data: 0.0052164546
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3479644e-07
Norm of the params: 10.499754
                Loss: fixed 698 labels. Loss 0.00522. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35231042
Train loss (w/o reg) on all data: 0.34600133
Test loss (w/o reg) on all data: 0.168428
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.8358863e-05
Norm of the params: 11.233074
              Random: fixed 143 labels. Loss 0.16843. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 1025
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11638697
Train loss (w/o reg) on all data: 0.107702546
Test loss (w/o reg) on all data: 0.04372216
Train acc on all data:  0.9603695599319232
Test acc on all data:   1.0
Norm of the mean of gradients: 8.942919e-06
Norm of the params: 13.179097
     Influence (LOO): fixed 544 labels. Loss 0.04372. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074169827
Train loss (w/o reg) on all data: 0.0032096147
Test loss (w/o reg) on all data: 0.0041665393
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.327264e-07
Norm of the params: 9.173187
                Loss: fixed 702 labels. Loss 0.00417. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3357873
Train loss (w/o reg) on all data: 0.32937667
Test loss (w/o reg) on all data: 0.15693042
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3532325e-05
Norm of the params: 11.323092
              Random: fixed 182 labels. Loss 0.15693. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 1230
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08253237
Train loss (w/o reg) on all data: 0.07503556
Test loss (w/o reg) on all data: 0.031308755
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.8390405e-06
Norm of the params: 12.244847
     Influence (LOO): fixed 597 labels. Loss 0.03131. Accuracy 0.998.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005600677
Train loss (w/o reg) on all data: 0.00219801
Test loss (w/o reg) on all data: 0.0038119256
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2595823e-07
Norm of the params: 8.249445
                Loss: fixed 705 labels. Loss 0.00381. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31682518
Train loss (w/o reg) on all data: 0.309851
Test loss (w/o reg) on all data: 0.14383803
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.4086454e-06
Norm of the params: 11.810322
              Random: fixed 225 labels. Loss 0.14384. Accuracy 0.991.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40761065
Train loss (w/o reg) on all data: 0.40099198
Test loss (w/o reg) on all data: 0.22315423
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.8422683e-05
Norm of the params: 11.505363
Flipped loss: 0.22315. Accuracy: 0.980
### Flips: 820, rs: 38, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32446152
Train loss (w/o reg) on all data: 0.31373355
Test loss (w/o reg) on all data: 0.16281533
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.623776e-06
Norm of the params: 14.647846
     Influence (LOO): fixed 170 labels. Loss 0.16282. Accuracy 0.990.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28837785
Train loss (w/o reg) on all data: 0.27573645
Test loss (w/o reg) on all data: 0.15955134
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.929883e-05
Norm of the params: 15.900566
                Loss: fixed 204 labels. Loss 0.15955. Accuracy 0.969.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39894745
Train loss (w/o reg) on all data: 0.3922211
Test loss (w/o reg) on all data: 0.21576661
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.1342034e-05
Norm of the params: 11.598589
              Random: fixed  28 labels. Loss 0.21577. Accuracy 0.979.
### Flips: 820, rs: 38, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25385064
Train loss (w/o reg) on all data: 0.24342279
Test loss (w/o reg) on all data: 0.11616461
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.708916e-06
Norm of the params: 14.441509
     Influence (LOO): fixed 311 labels. Loss 0.11616. Accuracy 0.996.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17193566
Train loss (w/o reg) on all data: 0.15441558
Test loss (w/o reg) on all data: 0.10213748
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.329867e-06
Norm of the params: 18.719019
                Loss: fixed 409 labels. Loss 0.10214. Accuracy 0.973.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38282952
Train loss (w/o reg) on all data: 0.3755145
Test loss (w/o reg) on all data: 0.20288587
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.9472736e-05
Norm of the params: 12.095472
              Random: fixed  72 labels. Loss 0.20289. Accuracy 0.981.
### Flips: 820, rs: 38, checks: 615
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19885561
Train loss (w/o reg) on all data: 0.18931197
Test loss (w/o reg) on all data: 0.08727172
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1333848e-05
Norm of the params: 13.815677
     Influence (LOO): fixed 417 labels. Loss 0.08727. Accuracy 0.998.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06390304
Train loss (w/o reg) on all data: 0.047554515
Test loss (w/o reg) on all data: 0.038718976
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.3123234e-06
Norm of the params: 18.082327
                Loss: fixed 603 labels. Loss 0.03872. Accuracy 0.990.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3708103
Train loss (w/o reg) on all data: 0.3633856
Test loss (w/o reg) on all data: 0.19124182
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1551434e-05
Norm of the params: 12.185819
              Random: fixed 103 labels. Loss 0.19124. Accuracy 0.982.
### Flips: 820, rs: 38, checks: 820
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15601823
Train loss (w/o reg) on all data: 0.14716099
Test loss (w/o reg) on all data: 0.06610238
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.782765e-06
Norm of the params: 13.309568
     Influence (LOO): fixed 488 labels. Loss 0.06610. Accuracy 0.997.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01153878
Train loss (w/o reg) on all data: 0.0057897656
Test loss (w/o reg) on all data: 0.0054598325
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4915933e-07
Norm of the params: 10.722886
                Loss: fixed 701 labels. Loss 0.00546. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35559714
Train loss (w/o reg) on all data: 0.34799138
Test loss (w/o reg) on all data: 0.17724226
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3228935e-05
Norm of the params: 12.333495
              Random: fixed 141 labels. Loss 0.17724. Accuracy 0.985.
### Flips: 820, rs: 38, checks: 1025
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11284207
Train loss (w/o reg) on all data: 0.10446165
Test loss (w/o reg) on all data: 0.043824103
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0795368e-05
Norm of the params: 12.946368
     Influence (LOO): fixed 558 labels. Loss 0.04382. Accuracy 0.998.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069361
Train loss (w/o reg) on all data: 0.0030195187
Test loss (w/o reg) on all data: 0.003864247
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11416234e-07
Norm of the params: 8.850515
                Loss: fixed 711 labels. Loss 0.00386. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34320793
Train loss (w/o reg) on all data: 0.33561507
Test loss (w/o reg) on all data: 0.1671156
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.7786655e-05
Norm of the params: 12.32303
              Random: fixed 170 labels. Loss 0.16712. Accuracy 0.988.
### Flips: 820, rs: 38, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084530026
Train loss (w/o reg) on all data: 0.07702118
Test loss (w/o reg) on all data: 0.030955747
Train acc on all data:  0.9720398735716023
Test acc on all data:   1.0
Norm of the mean of gradients: 6.044333e-06
Norm of the params: 12.25467
     Influence (LOO): fixed 603 labels. Loss 0.03096. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053858883
Train loss (w/o reg) on all data: 0.002201286
Test loss (w/o reg) on all data: 0.0038172593
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1141993e-07
Norm of the params: 7.9807296
                Loss: fixed 714 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32871833
Train loss (w/o reg) on all data: 0.32137343
Test loss (w/o reg) on all data: 0.15666638
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.3223046e-06
Norm of the params: 12.120148
              Random: fixed 206 labels. Loss 0.15667. Accuracy 0.989.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40393576
Train loss (w/o reg) on all data: 0.39810276
Test loss (w/o reg) on all data: 0.21662666
Train acc on all data:  0.8315098468271335
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.1111703e-05
Norm of the params: 10.8009205
Flipped loss: 0.21663. Accuracy: 0.970
### Flips: 820, rs: 39, checks: 205
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31846952
Train loss (w/o reg) on all data: 0.30930126
Test loss (w/o reg) on all data: 0.15793177
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.5423404e-05
Norm of the params: 13.541236
     Influence (LOO): fixed 174 labels. Loss 0.15793. Accuracy 0.988.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27745607
Train loss (w/o reg) on all data: 0.26434344
Test loss (w/o reg) on all data: 0.16761528
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.9737496e-05
Norm of the params: 16.194218
                Loss: fixed 205 labels. Loss 0.16762. Accuracy 0.960.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3927844
Train loss (w/o reg) on all data: 0.38692376
Test loss (w/o reg) on all data: 0.20287353
Train acc on all data:  0.838560661317773
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.3334203e-05
Norm of the params: 10.826468
              Random: fixed  35 labels. Loss 0.20287. Accuracy 0.978.
### Flips: 820, rs: 39, checks: 410
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273886
Train loss (w/o reg) on all data: 0.24268636
Test loss (w/o reg) on all data: 0.11939357
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.311946e-05
Norm of the params: 14.17921
     Influence (LOO): fixed 305 labels. Loss 0.11939. Accuracy 0.996.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16609399
Train loss (w/o reg) on all data: 0.1478406
Test loss (w/o reg) on all data: 0.11946393
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.388429e-06
Norm of the params: 19.106749
                Loss: fixed 409 labels. Loss 0.11946. Accuracy 0.962.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37887126
Train loss (w/o reg) on all data: 0.37284398
Test loss (w/o reg) on all data: 0.191225
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.7970545e-05
Norm of the params: 10.97934
              Random: fixed  72 labels. Loss 0.19123. Accuracy 0.979.
### Flips: 820, rs: 39, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19429027
Train loss (w/o reg) on all data: 0.18487729
Test loss (w/o reg) on all data: 0.084832266
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.4685382e-06
Norm of the params: 13.720769
     Influence (LOO): fixed 414 labels. Loss 0.08483. Accuracy 0.994.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061486706
Train loss (w/o reg) on all data: 0.04572577
Test loss (w/o reg) on all data: 0.04205362
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.2667047e-06
Norm of the params: 17.754398
                Loss: fixed 606 labels. Loss 0.04205. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36311695
Train loss (w/o reg) on all data: 0.35711977
Test loss (w/o reg) on all data: 0.1764315
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.097417e-06
Norm of the params: 10.951876
              Random: fixed 117 labels. Loss 0.17643. Accuracy 0.982.
### Flips: 820, rs: 39, checks: 820
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15099573
Train loss (w/o reg) on all data: 0.14205982
Test loss (w/o reg) on all data: 0.06061706
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2166445e-05
Norm of the params: 13.368554
     Influence (LOO): fixed 496 labels. Loss 0.06062. Accuracy 0.999.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0101350155
Train loss (w/o reg) on all data: 0.0049942466
Test loss (w/o reg) on all data: 0.005779917
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 7.407877e-08
Norm of the params: 10.139792
                Loss: fixed 701 labels. Loss 0.00578. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.345336
Train loss (w/o reg) on all data: 0.33934087
Test loss (w/o reg) on all data: 0.1618117
Train acc on all data:  0.8670070508144906
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.8872345e-05
Norm of the params: 10.949997
              Random: fixed 162 labels. Loss 0.16181. Accuracy 0.987.
### Flips: 820, rs: 39, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1110613
Train loss (w/o reg) on all data: 0.10290795
Test loss (w/o reg) on all data: 0.042462517
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 8.5396905e-06
Norm of the params: 12.769769
     Influence (LOO): fixed 562 labels. Loss 0.04246. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004622599
Train loss (w/o reg) on all data: 0.0017597389
Test loss (w/o reg) on all data: 0.0026826751
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4823085e-07
Norm of the params: 7.5668488
                Loss: fixed 710 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33362752
Train loss (w/o reg) on all data: 0.32770845
Test loss (w/o reg) on all data: 0.15297045
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.5320104e-06
Norm of the params: 10.88032
              Random: fixed 194 labels. Loss 0.15297. Accuracy 0.988.
### Flips: 820, rs: 39, checks: 1230
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08688802
Train loss (w/o reg) on all data: 0.07922453
Test loss (w/o reg) on all data: 0.032620117
Train acc on all data:  0.9722830051057623
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0653179e-05
Norm of the params: 12.380221
     Influence (LOO): fixed 599 labels. Loss 0.03262. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046225986
Train loss (w/o reg) on all data: 0.0017597473
Test loss (w/o reg) on all data: 0.0026827042
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5191716e-08
Norm of the params: 7.5668373
                Loss: fixed 710 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32155272
Train loss (w/o reg) on all data: 0.3155388
Test loss (w/o reg) on all data: 0.14158374
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.922551e-06
Norm of the params: 10.9671545
              Random: fixed 224 labels. Loss 0.14158. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4584187
Train loss (w/o reg) on all data: 0.4528632
Test loss (w/o reg) on all data: 0.2752328
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.4588626e-05
Norm of the params: 10.540882
Flipped loss: 0.27523. Accuracy: 0.961
### Flips: 1025, rs: 0, checks: 205
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837506
Train loss (w/o reg) on all data: 0.37424788
Test loss (w/o reg) on all data: 0.21460105
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1299305e-05
Norm of the params: 13.786014
     Influence (LOO): fixed 170 labels. Loss 0.21460. Accuracy 0.986.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34637833
Train loss (w/o reg) on all data: 0.33434746
Test loss (w/o reg) on all data: 0.21775956
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.6909336e-05
Norm of the params: 15.5118475
                Loss: fixed 205 labels. Loss 0.21776. Accuracy 0.941.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44782382
Train loss (w/o reg) on all data: 0.44228223
Test loss (w/o reg) on all data: 0.26032913
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.071351e-06
Norm of the params: 10.527666
              Random: fixed  41 labels. Loss 0.26033. Accuracy 0.970.
### Flips: 1025, rs: 0, checks: 410
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32645655
Train loss (w/o reg) on all data: 0.31622738
Test loss (w/o reg) on all data: 0.17152749
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.705976e-06
Norm of the params: 14.303262
     Influence (LOO): fixed 304 labels. Loss 0.17153. Accuracy 0.990.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24572088
Train loss (w/o reg) on all data: 0.2287436
Test loss (w/o reg) on all data: 0.15909587
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 6.8592162e-06
Norm of the params: 18.426765
                Loss: fixed 410 labels. Loss 0.15910. Accuracy 0.946.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43759337
Train loss (w/o reg) on all data: 0.4322327
Test loss (w/o reg) on all data: 0.2447761
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.7814515e-05
Norm of the params: 10.354378
              Random: fixed  83 labels. Loss 0.24478. Accuracy 0.978.
### Flips: 1025, rs: 0, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27982768
Train loss (w/o reg) on all data: 0.26883525
Test loss (w/o reg) on all data: 0.13943535
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0855276e-05
Norm of the params: 14.8273
     Influence (LOO): fixed 405 labels. Loss 0.13944. Accuracy 0.990.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14644626
Train loss (w/o reg) on all data: 0.12803596
Test loss (w/o reg) on all data: 0.10161318
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.4054794e-05
Norm of the params: 19.188696
                Loss: fixed 610 labels. Loss 0.10161. Accuracy 0.970.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42406604
Train loss (w/o reg) on all data: 0.4185315
Test loss (w/o reg) on all data: 0.23080528
Train acc on all data:  0.8157062971067347
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4615957e-05
Norm of the params: 10.520973
              Random: fixed 128 labels. Loss 0.23081. Accuracy 0.979.
### Flips: 1025, rs: 0, checks: 820
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23231682
Train loss (w/o reg) on all data: 0.22146611
Test loss (w/o reg) on all data: 0.10810287
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.3035064e-06
Norm of the params: 14.731401
     Influence (LOO): fixed 504 labels. Loss 0.10810. Accuracy 0.995.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060467463
Train loss (w/o reg) on all data: 0.045496885
Test loss (w/o reg) on all data: 0.03533968
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1558042e-06
Norm of the params: 17.303514
                Loss: fixed 790 labels. Loss 0.03534. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40957683
Train loss (w/o reg) on all data: 0.40391028
Test loss (w/o reg) on all data: 0.2162004
Train acc on all data:  0.8278628738147338
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.2614168e-05
Norm of the params: 10.6456995
              Random: fixed 177 labels. Loss 0.21620. Accuracy 0.983.
### Flips: 1025, rs: 0, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19280358
Train loss (w/o reg) on all data: 0.18204708
Test loss (w/o reg) on all data: 0.0864536
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.6419473e-06
Norm of the params: 14.667308
     Influence (LOO): fixed 578 labels. Loss 0.08645. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023438744
Train loss (w/o reg) on all data: 0.014475854
Test loss (w/o reg) on all data: 0.012820492
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.882281e-07
Norm of the params: 13.38872
                Loss: fixed 860 labels. Loss 0.01282. Accuracy 0.999.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39676484
Train loss (w/o reg) on all data: 0.39091462
Test loss (w/o reg) on all data: 0.20610465
Train acc on all data:  0.8368587405786531
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.2572134e-05
Norm of the params: 10.816858
              Random: fixed 215 labels. Loss 0.20610. Accuracy 0.985.
### Flips: 1025, rs: 0, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16122654
Train loss (w/o reg) on all data: 0.15135412
Test loss (w/o reg) on all data: 0.06860803
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.241051e-06
Norm of the params: 14.051631
     Influence (LOO): fixed 642 labels. Loss 0.06861. Accuracy 0.998.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014079605
Train loss (w/o reg) on all data: 0.0076575014
Test loss (w/o reg) on all data: 0.0073956805
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.032844e-07
Norm of the params: 11.33323
                Loss: fixed 875 labels. Loss 0.00740. Accuracy 0.999.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38058445
Train loss (w/o reg) on all data: 0.37434712
Test loss (w/o reg) on all data: 0.19181372
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.4117277e-05
Norm of the params: 11.169007
              Random: fixed 259 labels. Loss 0.19181. Accuracy 0.988.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45258984
Train loss (w/o reg) on all data: 0.4466688
Test loss (w/o reg) on all data: 0.28347364
Train acc on all data:  0.7969851689764162
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.658727e-05
Norm of the params: 10.882134
Flipped loss: 0.28347. Accuracy: 0.953
### Flips: 1025, rs: 1, checks: 205
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37893152
Train loss (w/o reg) on all data: 0.36940008
Test loss (w/o reg) on all data: 0.23300818
Train acc on all data:  0.8373450036469731
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0865578e-05
Norm of the params: 13.806834
     Influence (LOO): fixed 171 labels. Loss 0.23301. Accuracy 0.955.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34180328
Train loss (w/o reg) on all data: 0.32847303
Test loss (w/o reg) on all data: 0.2377719
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 8.125715e-06
Norm of the params: 16.32804
                Loss: fixed 205 labels. Loss 0.23777. Accuracy 0.930.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43939197
Train loss (w/o reg) on all data: 0.43333632
Test loss (w/o reg) on all data: 0.26716903
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.7324277e-05
Norm of the params: 11.00515
              Random: fixed  49 labels. Loss 0.26717. Accuracy 0.955.
### Flips: 1025, rs: 1, checks: 410
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32245308
Train loss (w/o reg) on all data: 0.31179485
Test loss (w/o reg) on all data: 0.18056951
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 9.586884e-06
Norm of the params: 14.60017
     Influence (LOO): fixed 307 labels. Loss 0.18057. Accuracy 0.972.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24102512
Train loss (w/o reg) on all data: 0.22325842
Test loss (w/o reg) on all data: 0.18853778
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.1967677e-05
Norm of the params: 18.850307
                Loss: fixed 408 labels. Loss 0.18854. Accuracy 0.942.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42896882
Train loss (w/o reg) on all data: 0.42291555
Test loss (w/o reg) on all data: 0.25596672
Train acc on all data:  0.8174082178458546
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.5339516e-05
Norm of the params: 11.00298
              Random: fixed  89 labels. Loss 0.25597. Accuracy 0.952.
### Flips: 1025, rs: 1, checks: 615
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27554357
Train loss (w/o reg) on all data: 0.2642047
Test loss (w/o reg) on all data: 0.14260057
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.4304412e-05
Norm of the params: 15.059115
     Influence (LOO): fixed 410 labels. Loss 0.14260. Accuracy 0.985.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1422704
Train loss (w/o reg) on all data: 0.12116943
Test loss (w/o reg) on all data: 0.14117426
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.323344e-06
Norm of the params: 20.543108
                Loss: fixed 607 labels. Loss 0.14117. Accuracy 0.952.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41149238
Train loss (w/o reg) on all data: 0.40517205
Test loss (w/o reg) on all data: 0.23517208
Train acc on all data:  0.8264040846097739
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 9.287301e-05
Norm of the params: 11.243067
              Random: fixed 144 labels. Loss 0.23517. Accuracy 0.961.
### Flips: 1025, rs: 1, checks: 820
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23292008
Train loss (w/o reg) on all data: 0.22219363
Test loss (w/o reg) on all data: 0.11219355
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2359231e-05
Norm of the params: 14.646812
     Influence (LOO): fixed 499 labels. Loss 0.11219. Accuracy 0.991.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05977019
Train loss (w/o reg) on all data: 0.041872125
Test loss (w/o reg) on all data: 0.0654908
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.9002233e-06
Norm of the params: 18.919865
                Loss: fixed 779 labels. Loss 0.06549. Accuracy 0.980.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39017543
Train loss (w/o reg) on all data: 0.38360542
Test loss (w/o reg) on all data: 0.21221863
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.880408e-05
Norm of the params: 11.462999
              Random: fixed 207 labels. Loss 0.21222. Accuracy 0.965.
### Flips: 1025, rs: 1, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19605373
Train loss (w/o reg) on all data: 0.18604226
Test loss (w/o reg) on all data: 0.08876433
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5372007e-05
Norm of the params: 14.150236
     Influence (LOO): fixed 575 labels. Loss 0.08876. Accuracy 0.997.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02749683
Train loss (w/o reg) on all data: 0.016378684
Test loss (w/o reg) on all data: 0.020288413
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.1504515e-07
Norm of the params: 14.9118395
                Loss: fixed 846 labels. Loss 0.02029. Accuracy 0.995.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37688538
Train loss (w/o reg) on all data: 0.37029067
Test loss (w/o reg) on all data: 0.19656315
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 9.332449e-06
Norm of the params: 11.48453
              Random: fixed 247 labels. Loss 0.19656. Accuracy 0.973.
### Flips: 1025, rs: 1, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15808728
Train loss (w/o reg) on all data: 0.14844428
Test loss (w/o reg) on all data: 0.06859958
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.745969e-06
Norm of the params: 13.887403
     Influence (LOO): fixed 644 labels. Loss 0.06860. Accuracy 0.999.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01911565
Train loss (w/o reg) on all data: 0.010343756
Test loss (w/o reg) on all data: 0.0125707835
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9031284e-07
Norm of the params: 13.245295
                Loss: fixed 866 labels. Loss 0.01257. Accuracy 0.997.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3633437
Train loss (w/o reg) on all data: 0.3567006
Test loss (w/o reg) on all data: 0.18617065
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4289515e-05
Norm of the params: 11.526562
              Random: fixed 286 labels. Loss 0.18617. Accuracy 0.979.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45846382
Train loss (w/o reg) on all data: 0.453344
Test loss (w/o reg) on all data: 0.26657262
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.1168205e-05
Norm of the params: 10.119118
Flipped loss: 0.26657. Accuracy: 0.975
### Flips: 1025, rs: 2, checks: 205
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38196677
Train loss (w/o reg) on all data: 0.3725928
Test loss (w/o reg) on all data: 0.21228409
Train acc on all data:  0.8353999513736932
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.6400112e-05
Norm of the params: 13.692313
     Influence (LOO): fixed 172 labels. Loss 0.21228. Accuracy 0.981.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34923792
Train loss (w/o reg) on all data: 0.33789897
Test loss (w/o reg) on all data: 0.21606527
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.5750436e-05
Norm of the params: 15.059189
                Loss: fixed 205 labels. Loss 0.21607. Accuracy 0.954.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44855416
Train loss (w/o reg) on all data: 0.44325942
Test loss (w/o reg) on all data: 0.25725138
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.356268e-05
Norm of the params: 10.290521
              Random: fixed  38 labels. Loss 0.25725. Accuracy 0.978.
### Flips: 1025, rs: 2, checks: 410
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32173613
Train loss (w/o reg) on all data: 0.31078282
Test loss (w/o reg) on all data: 0.16847958
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.061141e-05
Norm of the params: 14.8008795
     Influence (LOO): fixed 307 labels. Loss 0.16848. Accuracy 0.983.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24350595
Train loss (w/o reg) on all data: 0.22694486
Test loss (w/o reg) on all data: 0.16343425
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.6656122e-05
Norm of the params: 18.199503
                Loss: fixed 409 labels. Loss 0.16343. Accuracy 0.954.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43610606
Train loss (w/o reg) on all data: 0.43058154
Test loss (w/o reg) on all data: 0.24294879
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.7417133e-05
Norm of the params: 10.511429
              Random: fixed  83 labels. Loss 0.24295. Accuracy 0.983.
### Flips: 1025, rs: 2, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27619085
Train loss (w/o reg) on all data: 0.26531145
Test loss (w/o reg) on all data: 0.13550003
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7824348e-05
Norm of the params: 14.750855
     Influence (LOO): fixed 411 labels. Loss 0.13550. Accuracy 0.995.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13977394
Train loss (w/o reg) on all data: 0.11915414
Test loss (w/o reg) on all data: 0.089759775
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.13909e-06
Norm of the params: 20.307533
                Loss: fixed 612 labels. Loss 0.08976. Accuracy 0.972.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42555177
Train loss (w/o reg) on all data: 0.42004248
Test loss (w/o reg) on all data: 0.23318134
Train acc on all data:  0.8174082178458546
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.0591585e-05
Norm of the params: 10.496949
              Random: fixed 118 labels. Loss 0.23318. Accuracy 0.983.
### Flips: 1025, rs: 2, checks: 820
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23674801
Train loss (w/o reg) on all data: 0.22660081
Test loss (w/o reg) on all data: 0.1083745
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0644875e-05
Norm of the params: 14.245843
     Influence (LOO): fixed 496 labels. Loss 0.10837. Accuracy 0.999.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048898473
Train loss (w/o reg) on all data: 0.03359837
Test loss (w/o reg) on all data: 0.025737906
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.0649994e-07
Norm of the params: 17.492914
                Loss: fixed 798 labels. Loss 0.02574. Accuracy 0.995.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40880758
Train loss (w/o reg) on all data: 0.40307546
Test loss (w/o reg) on all data: 0.2173714
Train acc on all data:  0.8281060053488938
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1054177e-05
Norm of the params: 10.707131
              Random: fixed 166 labels. Loss 0.21737. Accuracy 0.986.
### Flips: 1025, rs: 2, checks: 1025
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19423206
Train loss (w/o reg) on all data: 0.18455961
Test loss (w/o reg) on all data: 0.08584832
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.995974e-06
Norm of the params: 13.908597
     Influence (LOO): fixed 575 labels. Loss 0.08585. Accuracy 0.999.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021746384
Train loss (w/o reg) on all data: 0.012644506
Test loss (w/o reg) on all data: 0.00800417
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2921006e-07
Norm of the params: 13.49213
                Loss: fixed 855 labels. Loss 0.00800. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39396095
Train loss (w/o reg) on all data: 0.38841873
Test loss (w/o reg) on all data: 0.2006801
Train acc on all data:  0.8402625820568927
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1931508e-05
Norm of the params: 10.528262
              Random: fixed 212 labels. Loss 0.20068. Accuracy 0.987.
### Flips: 1025, rs: 2, checks: 1230
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15326637
Train loss (w/o reg) on all data: 0.14425515
Test loss (w/o reg) on all data: 0.066741094
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.46549555e-05
Norm of the params: 13.424774
     Influence (LOO): fixed 648 labels. Loss 0.06674. Accuracy 0.998.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013026288
Train loss (w/o reg) on all data: 0.006567702
Test loss (w/o reg) on all data: 0.0058467644
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6489934e-07
Norm of the params: 11.365374
                Loss: fixed 869 labels. Loss 0.00585. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37683085
Train loss (w/o reg) on all data: 0.37127492
Test loss (w/o reg) on all data: 0.18524322
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.962442e-06
Norm of the params: 10.541275
              Random: fixed 263 labels. Loss 0.18524. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46247506
Train loss (w/o reg) on all data: 0.45659465
Test loss (w/o reg) on all data: 0.2750838
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.1116647e-05
Norm of the params: 10.84475
Flipped loss: 0.27508. Accuracy: 0.959
### Flips: 1025, rs: 3, checks: 205
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3887701
Train loss (w/o reg) on all data: 0.3785477
Test loss (w/o reg) on all data: 0.2147248
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.331425e-05
Norm of the params: 14.298528
     Influence (LOO): fixed 170 labels. Loss 0.21472. Accuracy 0.983.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3580523
Train loss (w/o reg) on all data: 0.34577414
Test loss (w/o reg) on all data: 0.2243247
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.9419656e-05
Norm of the params: 15.670458
                Loss: fixed 205 labels. Loss 0.22432. Accuracy 0.941.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44929525
Train loss (w/o reg) on all data: 0.4433062
Test loss (w/o reg) on all data: 0.26094687
Train acc on all data:  0.799659615852176
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.1075746e-05
Norm of the params: 10.944442
              Random: fixed  47 labels. Loss 0.26095. Accuracy 0.964.
### Flips: 1025, rs: 3, checks: 410
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33302283
Train loss (w/o reg) on all data: 0.32173353
Test loss (w/o reg) on all data: 0.18159509
Train acc on all data:  0.8580111840505713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.113109e-06
Norm of the params: 15.0261755
     Influence (LOO): fixed 292 labels. Loss 0.18160. Accuracy 0.982.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25775975
Train loss (w/o reg) on all data: 0.24102986
Test loss (w/o reg) on all data: 0.17035855
Train acc on all data:  0.8903476780938487
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 4.85194e-05
Norm of the params: 18.29202
                Loss: fixed 410 labels. Loss 0.17036. Accuracy 0.949.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43900636
Train loss (w/o reg) on all data: 0.43308115
Test loss (w/o reg) on all data: 0.24808201
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 9.506527e-06
Norm of the params: 10.885958
              Random: fixed  86 labels. Loss 0.24808. Accuracy 0.967.
### Flips: 1025, rs: 3, checks: 615
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2836868
Train loss (w/o reg) on all data: 0.27252984
Test loss (w/o reg) on all data: 0.14412193
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.172257e-06
Norm of the params: 14.93784
     Influence (LOO): fixed 401 labels. Loss 0.14412. Accuracy 0.990.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15027778
Train loss (w/o reg) on all data: 0.13064554
Test loss (w/o reg) on all data: 0.11647975
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.252389e-06
Norm of the params: 19.815268
                Loss: fixed 612 labels. Loss 0.11648. Accuracy 0.960.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4245186
Train loss (w/o reg) on all data: 0.41840324
Test loss (w/o reg) on all data: 0.23083057
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.0326682e-05
Norm of the params: 11.059252
              Random: fixed 134 labels. Loss 0.23083. Accuracy 0.978.
### Flips: 1025, rs: 3, checks: 820
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24637417
Train loss (w/o reg) on all data: 0.23497225
Test loss (w/o reg) on all data: 0.12338851
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5506548e-05
Norm of the params: 15.100941
     Influence (LOO): fixed 481 labels. Loss 0.12339. Accuracy 0.990.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055991765
Train loss (w/o reg) on all data: 0.040190764
Test loss (w/o reg) on all data: 0.04299808
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.210357e-06
Norm of the params: 17.77695
                Loss: fixed 802 labels. Loss 0.04300. Accuracy 0.984.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4109719
Train loss (w/o reg) on all data: 0.40485552
Test loss (w/o reg) on all data: 0.21454403
Train acc on all data:  0.8283491368830538
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.083541e-05
Norm of the params: 11.06018
              Random: fixed 179 labels. Loss 0.21454. Accuracy 0.982.
### Flips: 1025, rs: 3, checks: 1025
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211398
Train loss (w/o reg) on all data: 0.2008093
Test loss (w/o reg) on all data: 0.09903758
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.7493233e-06
Norm of the params: 14.552464
     Influence (LOO): fixed 558 labels. Loss 0.09904. Accuracy 0.998.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021091778
Train loss (w/o reg) on all data: 0.012695636
Test loss (w/o reg) on all data: 0.009364312
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0633615e-07
Norm of the params: 12.958505
                Loss: fixed 870 labels. Loss 0.00936. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3965817
Train loss (w/o reg) on all data: 0.39030534
Test loss (w/o reg) on all data: 0.20485288
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7212335e-05
Norm of the params: 11.2038965
              Random: fixed 220 labels. Loss 0.20485. Accuracy 0.983.
### Flips: 1025, rs: 3, checks: 1230
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17904162
Train loss (w/o reg) on all data: 0.16893339
Test loss (w/o reg) on all data: 0.07815152
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.302161e-06
Norm of the params: 14.218458
     Influence (LOO): fixed 623 labels. Loss 0.07815. Accuracy 0.998.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010662418
Train loss (w/o reg) on all data: 0.00536223
Test loss (w/o reg) on all data: 0.004989982
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4637452e-07
Norm of the params: 10.295814
                Loss: fixed 887 labels. Loss 0.00499. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38327795
Train loss (w/o reg) on all data: 0.3768762
Test loss (w/o reg) on all data: 0.19450721
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.5759786e-05
Norm of the params: 11.315265
              Random: fixed 257 labels. Loss 0.19451. Accuracy 0.984.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45059514
Train loss (w/o reg) on all data: 0.4454505
Test loss (w/o reg) on all data: 0.26458505
Train acc on all data:  0.7960126428397764
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.9586248e-05
Norm of the params: 10.143583
Flipped loss: 0.26459. Accuracy: 0.970
### Flips: 1025, rs: 4, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3730755
Train loss (w/o reg) on all data: 0.36368158
Test loss (w/o reg) on all data: 0.21430562
Train acc on all data:  0.838074398249453
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 8.310627e-06
Norm of the params: 13.70685
     Influence (LOO): fixed 170 labels. Loss 0.21431. Accuracy 0.971.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33614206
Train loss (w/o reg) on all data: 0.32441536
Test loss (w/o reg) on all data: 0.22057293
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 4.3204596e-05
Norm of the params: 15.314503
                Loss: fixed 205 labels. Loss 0.22057. Accuracy 0.943.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43935755
Train loss (w/o reg) on all data: 0.43390915
Test loss (w/o reg) on all data: 0.24995504
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.0268024e-05
Norm of the params: 10.438763
              Random: fixed  44 labels. Loss 0.24996. Accuracy 0.973.
### Flips: 1025, rs: 4, checks: 410
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31854403
Train loss (w/o reg) on all data: 0.3080642
Test loss (w/o reg) on all data: 0.1696723
Train acc on all data:  0.8653051300753708
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.564008e-06
Norm of the params: 14.477462
     Influence (LOO): fixed 302 labels. Loss 0.16967. Accuracy 0.985.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22772808
Train loss (w/o reg) on all data: 0.21160458
Test loss (w/o reg) on all data: 0.17464611
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.8775206e-05
Norm of the params: 17.95745
                Loss: fixed 410 labels. Loss 0.17465. Accuracy 0.938.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42696503
Train loss (w/o reg) on all data: 0.42137787
Test loss (w/o reg) on all data: 0.23340917
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.310132e-05
Norm of the params: 10.570853
              Random: fixed  89 labels. Loss 0.23341. Accuracy 0.977.
### Flips: 1025, rs: 4, checks: 615
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26947546
Train loss (w/o reg) on all data: 0.25796553
Test loss (w/o reg) on all data: 0.14019455
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.639449e-05
Norm of the params: 15.172287
     Influence (LOO): fixed 402 labels. Loss 0.14019. Accuracy 0.989.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12753673
Train loss (w/o reg) on all data: 0.10935974
Test loss (w/o reg) on all data: 0.10241905
Train acc on all data:  0.9533187454412837
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.853655e-06
Norm of the params: 19.06672
                Loss: fixed 613 labels. Loss 0.10242. Accuracy 0.970.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41197926
Train loss (w/o reg) on all data: 0.4059985
Test loss (w/o reg) on all data: 0.21412507
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.5053676e-05
Norm of the params: 10.936883
              Random: fixed 136 labels. Loss 0.21413. Accuracy 0.988.
### Flips: 1025, rs: 4, checks: 820
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22497272
Train loss (w/o reg) on all data: 0.21357024
Test loss (w/o reg) on all data: 0.1081982
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9635783e-05
Norm of the params: 15.1013155
     Influence (LOO): fixed 499 labels. Loss 0.10820. Accuracy 0.993.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04748253
Train loss (w/o reg) on all data: 0.03323503
Test loss (w/o reg) on all data: 0.03181915
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.414988e-06
Norm of the params: 16.880463
                Loss: fixed 790 labels. Loss 0.03182. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40086943
Train loss (w/o reg) on all data: 0.39483652
Test loss (w/o reg) on all data: 0.20382065
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.58536e-06
Norm of the params: 10.984466
              Random: fixed 175 labels. Loss 0.20382. Accuracy 0.986.
### Flips: 1025, rs: 4, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18835516
Train loss (w/o reg) on all data: 0.17737614
Test loss (w/o reg) on all data: 0.08577778
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.8847587e-06
Norm of the params: 14.818249
     Influence (LOO): fixed 569 labels. Loss 0.08578. Accuracy 0.997.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019493435
Train loss (w/o reg) on all data: 0.010653477
Test loss (w/o reg) on all data: 0.0138987815
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5028052e-06
Norm of the params: 13.296586
                Loss: fixed 843 labels. Loss 0.01390. Accuracy 0.997.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3826843
Train loss (w/o reg) on all data: 0.3765083
Test loss (w/o reg) on all data: 0.18443622
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.0125266e-05
Norm of the params: 11.113954
              Random: fixed 228 labels. Loss 0.18444. Accuracy 0.991.
### Flips: 1025, rs: 4, checks: 1230
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16109782
Train loss (w/o reg) on all data: 0.1503815
Test loss (w/o reg) on all data: 0.070022315
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.63607e-06
Norm of the params: 14.639887
     Influence (LOO): fixed 623 labels. Loss 0.07002. Accuracy 0.997.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011162255
Train loss (w/o reg) on all data: 0.0054937457
Test loss (w/o reg) on all data: 0.0072519034
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4745285e-07
Norm of the params: 10.647544
                Loss: fixed 861 labels. Loss 0.00725. Accuracy 0.998.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36643335
Train loss (w/o reg) on all data: 0.35993376
Test loss (w/o reg) on all data: 0.17211191
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.5358276e-05
Norm of the params: 11.401382
              Random: fixed 266 labels. Loss 0.17211. Accuracy 0.991.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46434167
Train loss (w/o reg) on all data: 0.45935872
Test loss (w/o reg) on all data: 0.27853683
Train acc on all data:  0.7865305130075371
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.998119e-05
Norm of the params: 9.982937
Flipped loss: 0.27854. Accuracy: 0.967
### Flips: 1025, rs: 5, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39286166
Train loss (w/o reg) on all data: 0.38386774
Test loss (w/o reg) on all data: 0.2220584
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.289335e-05
Norm of the params: 13.411867
     Influence (LOO): fixed 166 labels. Loss 0.22206. Accuracy 0.972.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3552713
Train loss (w/o reg) on all data: 0.34438345
Test loss (w/o reg) on all data: 0.22147836
Train acc on all data:  0.837588135181133
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.800876e-06
Norm of the params: 14.756598
                Loss: fixed 205 labels. Loss 0.22148. Accuracy 0.953.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45299977
Train loss (w/o reg) on all data: 0.4480032
Test loss (w/o reg) on all data: 0.26445293
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.1247407e-05
Norm of the params: 9.996579
              Random: fixed  45 labels. Loss 0.26445. Accuracy 0.972.
### Flips: 1025, rs: 5, checks: 410
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33347762
Train loss (w/o reg) on all data: 0.3233778
Test loss (w/o reg) on all data: 0.17803629
Train acc on all data:  0.8550936056406516
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.0148225e-05
Norm of the params: 14.212553
     Influence (LOO): fixed 305 labels. Loss 0.17804. Accuracy 0.983.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25560278
Train loss (w/o reg) on all data: 0.24087438
Test loss (w/o reg) on all data: 0.17130943
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 6.7644924e-06
Norm of the params: 17.162977
                Loss: fixed 408 labels. Loss 0.17131. Accuracy 0.952.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4429711
Train loss (w/o reg) on all data: 0.43784493
Test loss (w/o reg) on all data: 0.2529439
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.1351894e-05
Norm of the params: 10.125379
              Random: fixed  83 labels. Loss 0.25294. Accuracy 0.971.
### Flips: 1025, rs: 5, checks: 615
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2804801
Train loss (w/o reg) on all data: 0.26959965
Test loss (w/o reg) on all data: 0.14229324
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.094082e-06
Norm of the params: 14.751567
     Influence (LOO): fixed 418 labels. Loss 0.14229. Accuracy 0.991.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14889333
Train loss (w/o reg) on all data: 0.13092059
Test loss (w/o reg) on all data: 0.10966605
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.970597e-06
Norm of the params: 18.95929
                Loss: fixed 611 labels. Loss 0.10967. Accuracy 0.966.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4302876
Train loss (w/o reg) on all data: 0.42483056
Test loss (w/o reg) on all data: 0.23704764
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0180767e-05
Norm of the params: 10.447049
              Random: fixed 127 labels. Loss 0.23705. Accuracy 0.976.
### Flips: 1025, rs: 5, checks: 820
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24122721
Train loss (w/o reg) on all data: 0.23084547
Test loss (w/o reg) on all data: 0.114171945
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2865836e-05
Norm of the params: 14.409545
     Influence (LOO): fixed 503 labels. Loss 0.11417. Accuracy 0.994.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061369028
Train loss (w/o reg) on all data: 0.045365244
Test loss (w/o reg) on all data: 0.042265326
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.6782546e-06
Norm of the params: 17.89066
                Loss: fixed 796 labels. Loss 0.04227. Accuracy 0.987.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41636762
Train loss (w/o reg) on all data: 0.41109815
Test loss (w/o reg) on all data: 0.22014475
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.6374442e-05
Norm of the params: 10.265936
              Random: fixed 176 labels. Loss 0.22014. Accuracy 0.983.
### Flips: 1025, rs: 5, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2001826
Train loss (w/o reg) on all data: 0.19001888
Test loss (w/o reg) on all data: 0.09294639
Train acc on all data:  0.9202528567955264
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.103713e-05
Norm of the params: 14.257434
     Influence (LOO): fixed 581 labels. Loss 0.09295. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022335555
Train loss (w/o reg) on all data: 0.013417798
Test loss (w/o reg) on all data: 0.014767987
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.7624553e-07
Norm of the params: 13.354967
                Loss: fixed 876 labels. Loss 0.01477. Accuracy 0.995.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4001451
Train loss (w/o reg) on all data: 0.3944971
Test loss (w/o reg) on all data: 0.20738356
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4136813e-05
Norm of the params: 10.628283
              Random: fixed 220 labels. Loss 0.20738. Accuracy 0.980.
### Flips: 1025, rs: 5, checks: 1230
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16338715
Train loss (w/o reg) on all data: 0.15416966
Test loss (w/o reg) on all data: 0.074316844
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3592763e-06
Norm of the params: 13.577542
     Influence (LOO): fixed 652 labels. Loss 0.07432. Accuracy 0.998.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014869398
Train loss (w/o reg) on all data: 0.0077900896
Test loss (w/o reg) on all data: 0.01132685
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4914714e-07
Norm of the params: 11.898999
                Loss: fixed 891 labels. Loss 0.01133. Accuracy 0.997.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38351792
Train loss (w/o reg) on all data: 0.37761566
Test loss (w/o reg) on all data: 0.1926442
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.5862726e-05
Norm of the params: 10.864863
              Random: fixed 266 labels. Loss 0.19264. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4573757
Train loss (w/o reg) on all data: 0.4521632
Test loss (w/o reg) on all data: 0.28217247
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.71304e-05
Norm of the params: 10.210292
Flipped loss: 0.28217. Accuracy: 0.957
### Flips: 1025, rs: 6, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38444054
Train loss (w/o reg) on all data: 0.37504277
Test loss (w/o reg) on all data: 0.22168428
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.2958948e-05
Norm of the params: 13.709696
     Influence (LOO): fixed 169 labels. Loss 0.22168. Accuracy 0.976.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3457046
Train loss (w/o reg) on all data: 0.33413398
Test loss (w/o reg) on all data: 0.22951429
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.6435417e-05
Norm of the params: 15.212232
                Loss: fixed 205 labels. Loss 0.22951. Accuracy 0.938.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4447904
Train loss (w/o reg) on all data: 0.43957952
Test loss (w/o reg) on all data: 0.26488775
Train acc on all data:  0.799173352783856
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.1314792e-05
Norm of the params: 10.208696
              Random: fixed  49 labels. Loss 0.26489. Accuracy 0.968.
### Flips: 1025, rs: 6, checks: 410
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33061096
Train loss (w/o reg) on all data: 0.3210502
Test loss (w/o reg) on all data: 0.17573118
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.701711e-05
Norm of the params: 13.828065
     Influence (LOO): fixed 296 labels. Loss 0.17573. Accuracy 0.988.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24243394
Train loss (w/o reg) on all data: 0.22604243
Test loss (w/o reg) on all data: 0.16641755
Train acc on all data:  0.8954534403112083
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 3.73091e-06
Norm of the params: 18.10608
                Loss: fixed 409 labels. Loss 0.16642. Accuracy 0.947.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4326663
Train loss (w/o reg) on all data: 0.42733774
Test loss (w/o reg) on all data: 0.2529676
Train acc on all data:  0.8103574033552152
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.8003916e-05
Norm of the params: 10.323325
              Random: fixed  89 labels. Loss 0.25297. Accuracy 0.972.
### Flips: 1025, rs: 6, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28270793
Train loss (w/o reg) on all data: 0.2723742
Test loss (w/o reg) on all data: 0.14239292
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2859433e-05
Norm of the params: 14.376183
     Influence (LOO): fixed 401 labels. Loss 0.14239. Accuracy 0.989.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641544
Train loss (w/o reg) on all data: 0.12756364
Test loss (w/o reg) on all data: 0.09709504
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.4167022e-06
Norm of the params: 19.417418
                Loss: fixed 610 labels. Loss 0.09710. Accuracy 0.970.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41792214
Train loss (w/o reg) on all data: 0.41216257
Test loss (w/o reg) on all data: 0.23615511
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.801239e-05
Norm of the params: 10.732728
              Random: fixed 135 labels. Loss 0.23616. Accuracy 0.975.
### Flips: 1025, rs: 6, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2350587
Train loss (w/o reg) on all data: 0.2245676
Test loss (w/o reg) on all data: 0.11048341
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9196175e-05
Norm of the params: 14.485228
     Influence (LOO): fixed 500 labels. Loss 0.11048. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05878845
Train loss (w/o reg) on all data: 0.042484492
Test loss (w/o reg) on all data: 0.038108893
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.550449e-06
Norm of the params: 18.057661
                Loss: fixed 788 labels. Loss 0.03811. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40464696
Train loss (w/o reg) on all data: 0.39880073
Test loss (w/o reg) on all data: 0.22207825
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.650197e-05
Norm of the params: 10.813171
              Random: fixed 179 labels. Loss 0.22208. Accuracy 0.976.
### Flips: 1025, rs: 6, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1950334
Train loss (w/o reg) on all data: 0.18488963
Test loss (w/o reg) on all data: 0.0888304
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.45309e-06
Norm of the params: 14.243432
     Influence (LOO): fixed 577 labels. Loss 0.08883. Accuracy 0.997.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021609187
Train loss (w/o reg) on all data: 0.01266306
Test loss (w/o reg) on all data: 0.014054333
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0641755e-06
Norm of the params: 13.376194
                Loss: fixed 862 labels. Loss 0.01405. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39020285
Train loss (w/o reg) on all data: 0.38416663
Test loss (w/o reg) on all data: 0.20371656
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.6178535e-05
Norm of the params: 10.987471
              Random: fixed 220 labels. Loss 0.20372. Accuracy 0.983.
### Flips: 1025, rs: 6, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15431912
Train loss (w/o reg) on all data: 0.14470948
Test loss (w/o reg) on all data: 0.06788693
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5703501e-05
Norm of the params: 13.863364
     Influence (LOO): fixed 651 labels. Loss 0.06789. Accuracy 0.995.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01337192
Train loss (w/o reg) on all data: 0.0070696594
Test loss (w/o reg) on all data: 0.009789967
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1656159e-07
Norm of the params: 11.226986
                Loss: fixed 879 labels. Loss 0.00979. Accuracy 0.997.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3764642
Train loss (w/o reg) on all data: 0.370373
Test loss (w/o reg) on all data: 0.19296648
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.21880585e-05
Norm of the params: 11.037369
              Random: fixed 260 labels. Loss 0.19297. Accuracy 0.986.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45782146
Train loss (w/o reg) on all data: 0.45265225
Test loss (w/o reg) on all data: 0.27381274
Train acc on all data:  0.7940675905664965
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.1465191e-05
Norm of the params: 10.167803
Flipped loss: 0.27381. Accuracy: 0.965
### Flips: 1025, rs: 7, checks: 205
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38191035
Train loss (w/o reg) on all data: 0.37257484
Test loss (w/o reg) on all data: 0.22076282
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.412172e-05
Norm of the params: 13.664192
     Influence (LOO): fixed 169 labels. Loss 0.22076. Accuracy 0.980.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34755868
Train loss (w/o reg) on all data: 0.33667406
Test loss (w/o reg) on all data: 0.22782052
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 8.506292e-06
Norm of the params: 14.754401
                Loss: fixed 205 labels. Loss 0.22782. Accuracy 0.945.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44736207
Train loss (w/o reg) on all data: 0.44217396
Test loss (w/o reg) on all data: 0.26164228
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.3351542e-05
Norm of the params: 10.186368
              Random: fixed  43 labels. Loss 0.26164. Accuracy 0.971.
### Flips: 1025, rs: 7, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3287685
Train loss (w/o reg) on all data: 0.31878692
Test loss (w/o reg) on all data: 0.17414531
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6419733e-05
Norm of the params: 14.129099
     Influence (LOO): fixed 299 labels. Loss 0.17415. Accuracy 0.986.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23967777
Train loss (w/o reg) on all data: 0.22365488
Test loss (w/o reg) on all data: 0.177015
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 1.99318e-05
Norm of the params: 17.901337
                Loss: fixed 410 labels. Loss 0.17702. Accuracy 0.940.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4343372
Train loss (w/o reg) on all data: 0.42905796
Test loss (w/o reg) on all data: 0.24373853
Train acc on all data:  0.812545587162655
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.4080955e-05
Norm of the params: 10.27545
              Random: fixed  90 labels. Loss 0.24374. Accuracy 0.977.
### Flips: 1025, rs: 7, checks: 615
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2764349
Train loss (w/o reg) on all data: 0.2663858
Test loss (w/o reg) on all data: 0.14197733
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.806568e-06
Norm of the params: 14.176812
     Influence (LOO): fixed 413 labels. Loss 0.14198. Accuracy 0.988.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13594365
Train loss (w/o reg) on all data: 0.11625029
Test loss (w/o reg) on all data: 0.101885304
Train acc on all data:  0.9511305616338439
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.635402e-06
Norm of the params: 19.846092
                Loss: fixed 609 labels. Loss 0.10189. Accuracy 0.966.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42229077
Train loss (w/o reg) on all data: 0.4168318
Test loss (w/o reg) on all data: 0.23146588
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.184685e-05
Norm of the params: 10.448904
              Random: fixed 129 labels. Loss 0.23147. Accuracy 0.979.
### Flips: 1025, rs: 7, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23376492
Train loss (w/o reg) on all data: 0.22372267
Test loss (w/o reg) on all data: 0.111452125
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4354175e-05
Norm of the params: 14.171979
     Influence (LOO): fixed 504 labels. Loss 0.11145. Accuracy 0.997.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049016967
Train loss (w/o reg) on all data: 0.03374184
Test loss (w/o reg) on all data: 0.036315102
Train acc on all data:  0.987114028689521
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.5889545e-06
Norm of the params: 17.47863
                Loss: fixed 785 labels. Loss 0.03632. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40579727
Train loss (w/o reg) on all data: 0.39999166
Test loss (w/o reg) on all data: 0.21894011
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.2259507e-05
Norm of the params: 10.775529
              Random: fixed 174 labels. Loss 0.21894. Accuracy 0.982.
### Flips: 1025, rs: 7, checks: 1025
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19699958
Train loss (w/o reg) on all data: 0.18692702
Test loss (w/o reg) on all data: 0.08972033
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7291002e-06
Norm of the params: 14.193345
     Influence (LOO): fixed 577 labels. Loss 0.08972. Accuracy 0.999.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019746661
Train loss (w/o reg) on all data: 0.010953951
Test loss (w/o reg) on all data: 0.011721595
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.0589196e-07
Norm of the params: 13.261004
                Loss: fixed 852 labels. Loss 0.01172. Accuracy 0.997.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3921833
Train loss (w/o reg) on all data: 0.38658467
Test loss (w/o reg) on all data: 0.20419945
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.6678968e-05
Norm of the params: 10.581725
              Random: fixed 222 labels. Loss 0.20420. Accuracy 0.981.
### Flips: 1025, rs: 7, checks: 1230
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562993
Train loss (w/o reg) on all data: 0.1469295
Test loss (w/o reg) on all data: 0.06850541
Train acc on all data:  0.9435934840748845
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6996998e-06
Norm of the params: 13.1912365
     Influence (LOO): fixed 649 labels. Loss 0.06851. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01386467
Train loss (w/o reg) on all data: 0.0071239695
Test loss (w/o reg) on all data: 0.0068975673
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.2861707e-07
Norm of the params: 11.610944
                Loss: fixed 864 labels. Loss 0.00690. Accuracy 0.999.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37333465
Train loss (w/o reg) on all data: 0.3676119
Test loss (w/o reg) on all data: 0.1834237
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.9395842e-05
Norm of the params: 10.698359
              Random: fixed 274 labels. Loss 0.18342. Accuracy 0.986.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45490015
Train loss (w/o reg) on all data: 0.44941226
Test loss (w/o reg) on all data: 0.27305242
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.4540522e-05
Norm of the params: 10.476525
Flipped loss: 0.27305. Accuracy: 0.960
### Flips: 1025, rs: 8, checks: 205
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37979603
Train loss (w/o reg) on all data: 0.37007722
Test loss (w/o reg) on all data: 0.21720207
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.229696e-06
Norm of the params: 13.941879
     Influence (LOO): fixed 172 labels. Loss 0.21720. Accuracy 0.974.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34307545
Train loss (w/o reg) on all data: 0.33219436
Test loss (w/o reg) on all data: 0.22596748
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 2.3561579e-05
Norm of the params: 14.75201
                Loss: fixed 205 labels. Loss 0.22597. Accuracy 0.941.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44324028
Train loss (w/o reg) on all data: 0.43754762
Test loss (w/o reg) on all data: 0.25611234
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.7171962e-05
Norm of the params: 10.670199
              Random: fixed  44 labels. Loss 0.25611. Accuracy 0.969.
### Flips: 1025, rs: 8, checks: 410
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31880227
Train loss (w/o reg) on all data: 0.30821472
Test loss (w/o reg) on all data: 0.17401627
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4275287e-05
Norm of the params: 14.55166
     Influence (LOO): fixed 307 labels. Loss 0.17402. Accuracy 0.979.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773435
Train loss (w/o reg) on all data: 0.22256592
Test loss (w/o reg) on all data: 0.16879576
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.41924e-05
Norm of the params: 17.41748
                Loss: fixed 410 labels. Loss 0.16880. Accuracy 0.946.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4313264
Train loss (w/o reg) on all data: 0.42558727
Test loss (w/o reg) on all data: 0.23960024
Train acc on all data:  0.812059324094335
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.5046833e-05
Norm of the params: 10.713662
              Random: fixed  88 labels. Loss 0.23960. Accuracy 0.975.
### Flips: 1025, rs: 8, checks: 615
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2708011
Train loss (w/o reg) on all data: 0.25870413
Test loss (w/o reg) on all data: 0.13916385
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.622948e-06
Norm of the params: 15.554401
     Influence (LOO): fixed 412 labels. Loss 0.13916. Accuracy 0.994.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13200389
Train loss (w/o reg) on all data: 0.115025006
Test loss (w/o reg) on all data: 0.11213356
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.4469805e-06
Norm of the params: 18.427635
                Loss: fixed 612 labels. Loss 0.11213. Accuracy 0.958.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41634545
Train loss (w/o reg) on all data: 0.41065636
Test loss (w/o reg) on all data: 0.22749685
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.2010415e-05
Norm of the params: 10.666859
              Random: fixed 136 labels. Loss 0.22750. Accuracy 0.978.
### Flips: 1025, rs: 8, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23153271
Train loss (w/o reg) on all data: 0.21993396
Test loss (w/o reg) on all data: 0.10826402
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0614121e-05
Norm of the params: 15.230728
     Influence (LOO): fixed 493 labels. Loss 0.10826. Accuracy 0.997.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048726976
Train loss (w/o reg) on all data: 0.03470749
Test loss (w/o reg) on all data: 0.037699368
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.92621e-06
Norm of the params: 16.74484
                Loss: fixed 790 labels. Loss 0.03770. Accuracy 0.987.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40026557
Train loss (w/o reg) on all data: 0.39464504
Test loss (w/o reg) on all data: 0.20877294
Train acc on all data:  0.8334548991004134
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.8092323e-05
Norm of the params: 10.602386
              Random: fixed 186 labels. Loss 0.20877. Accuracy 0.982.
### Flips: 1025, rs: 8, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1913657
Train loss (w/o reg) on all data: 0.18047422
Test loss (w/o reg) on all data: 0.08193214
Train acc on all data:  0.9263311451495259
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4387109e-05
Norm of the params: 14.759055
     Influence (LOO): fixed 571 labels. Loss 0.08193. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01529442
Train loss (w/o reg) on all data: 0.008140363
Test loss (w/o reg) on all data: 0.010286334
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6251996e-07
Norm of the params: 11.961654
                Loss: fixed 860 labels. Loss 0.01029. Accuracy 0.997.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38474646
Train loss (w/o reg) on all data: 0.37895373
Test loss (w/o reg) on all data: 0.19955672
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1782729e-05
Norm of the params: 10.763582
              Random: fixed 227 labels. Loss 0.19956. Accuracy 0.984.
### Flips: 1025, rs: 8, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15777363
Train loss (w/o reg) on all data: 0.14737016
Test loss (w/o reg) on all data: 0.06588962
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2154378e-05
Norm of the params: 14.424607
     Influence (LOO): fixed 632 labels. Loss 0.06589. Accuracy 0.999.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010454077
Train loss (w/o reg) on all data: 0.0050872704
Test loss (w/o reg) on all data: 0.0068140533
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8302562e-07
Norm of the params: 10.360316
                Loss: fixed 870 labels. Loss 0.00681. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36840472
Train loss (w/o reg) on all data: 0.36267778
Test loss (w/o reg) on all data: 0.1851501
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.069208e-05
Norm of the params: 10.702287
              Random: fixed 273 labels. Loss 0.18515. Accuracy 0.983.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45652112
Train loss (w/o reg) on all data: 0.4514613
Test loss (w/o reg) on all data: 0.27666968
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 3.655826e-05
Norm of the params: 10.059667
Flipped loss: 0.27667. Accuracy: 0.963
### Flips: 1025, rs: 9, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38051823
Train loss (w/o reg) on all data: 0.37098473
Test loss (w/o reg) on all data: 0.22193871
Train acc on all data:  0.8312667152929735
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.9667701e-05
Norm of the params: 13.808331
     Influence (LOO): fixed 168 labels. Loss 0.22194. Accuracy 0.974.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34135497
Train loss (w/o reg) on all data: 0.32922938
Test loss (w/o reg) on all data: 0.23066035
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.6150012e-05
Norm of the params: 15.572792
                Loss: fixed 205 labels. Loss 0.23066. Accuracy 0.944.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44517902
Train loss (w/o reg) on all data: 0.43987623
Test loss (w/o reg) on all data: 0.2645447
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.5974081e-05
Norm of the params: 10.298331
              Random: fixed  42 labels. Loss 0.26454. Accuracy 0.965.
### Flips: 1025, rs: 9, checks: 410
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31712833
Train loss (w/o reg) on all data: 0.3057726
Test loss (w/o reg) on all data: 0.17486322
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.6930724e-05
Norm of the params: 15.070316
     Influence (LOO): fixed 304 labels. Loss 0.17486. Accuracy 0.981.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23907113
Train loss (w/o reg) on all data: 0.22236584
Test loss (w/o reg) on all data: 0.17093298
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.0639746e-06
Norm of the params: 18.278564
                Loss: fixed 409 labels. Loss 0.17093. Accuracy 0.944.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43287572
Train loss (w/o reg) on all data: 0.42756993
Test loss (w/o reg) on all data: 0.24953714
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.6040612e-05
Norm of the params: 10.301267
              Random: fixed  84 labels. Loss 0.24954. Accuracy 0.973.
### Flips: 1025, rs: 9, checks: 615
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733754
Train loss (w/o reg) on all data: 0.26198527
Test loss (w/o reg) on all data: 0.14113005
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.093368e-06
Norm of the params: 15.093126
     Influence (LOO): fixed 400 labels. Loss 0.14113. Accuracy 0.983.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13706999
Train loss (w/o reg) on all data: 0.11684107
Test loss (w/o reg) on all data: 0.10811223
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.4371499e-05
Norm of the params: 20.114132
                Loss: fixed 612 labels. Loss 0.10811. Accuracy 0.966.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41753098
Train loss (w/o reg) on all data: 0.4123081
Test loss (w/o reg) on all data: 0.23205565
Train acc on all data:  0.8205689277899344
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.135995e-05
Norm of the params: 10.220464
              Random: fixed 132 labels. Loss 0.23206. Accuracy 0.977.
### Flips: 1025, rs: 9, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2296869
Train loss (w/o reg) on all data: 0.2182392
Test loss (w/o reg) on all data: 0.113535345
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.0794415e-06
Norm of the params: 15.131229
     Influence (LOO): fixed 490 labels. Loss 0.11354. Accuracy 0.988.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049394146
Train loss (w/o reg) on all data: 0.034687553
Test loss (w/o reg) on all data: 0.034373324
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.4760045e-07
Norm of the params: 17.150272
                Loss: fixed 796 labels. Loss 0.03437. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40385923
Train loss (w/o reg) on all data: 0.39846754
Test loss (w/o reg) on all data: 0.21638255
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.1919319e-05
Norm of the params: 10.3843
              Random: fixed 172 labels. Loss 0.21638. Accuracy 0.979.
### Flips: 1025, rs: 9, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19609466
Train loss (w/o reg) on all data: 0.18515365
Test loss (w/o reg) on all data: 0.0934153
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.6859398e-06
Norm of the params: 14.792578
     Influence (LOO): fixed 557 labels. Loss 0.09342. Accuracy 0.990.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017151233
Train loss (w/o reg) on all data: 0.009593885
Test loss (w/o reg) on all data: 0.015192219
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.445726e-07
Norm of the params: 12.294184
                Loss: fixed 859 labels. Loss 0.01519. Accuracy 0.995.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38648126
Train loss (w/o reg) on all data: 0.38044512
Test loss (w/o reg) on all data: 0.20368992
Train acc on all data:  0.8395331874544129
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.099426e-05
Norm of the params: 10.987388
              Random: fixed 217 labels. Loss 0.20369. Accuracy 0.981.
### Flips: 1025, rs: 9, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16673155
Train loss (w/o reg) on all data: 0.1564795
Test loss (w/o reg) on all data: 0.07695882
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.539532e-06
Norm of the params: 14.319256
     Influence (LOO): fixed 612 labels. Loss 0.07696. Accuracy 0.995.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01277959
Train loss (w/o reg) on all data: 0.006693552
Test loss (w/o reg) on all data: 0.010418175
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.436564e-07
Norm of the params: 11.032713
                Loss: fixed 870 labels. Loss 0.01042. Accuracy 0.996.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37276405
Train loss (w/o reg) on all data: 0.3668053
Test loss (w/o reg) on all data: 0.19270284
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.424887e-05
Norm of the params: 10.916742
              Random: fixed 254 labels. Loss 0.19270. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4562268
Train loss (w/o reg) on all data: 0.45074454
Test loss (w/o reg) on all data: 0.26963878
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 4.5601053e-05
Norm of the params: 10.471167
Flipped loss: 0.26964. Accuracy: 0.955
### Flips: 1025, rs: 10, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37696394
Train loss (w/o reg) on all data: 0.3674848
Test loss (w/o reg) on all data: 0.2108584
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.0322426e-05
Norm of the params: 13.7689085
     Influence (LOO): fixed 170 labels. Loss 0.21086. Accuracy 0.973.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33845568
Train loss (w/o reg) on all data: 0.32547304
Test loss (w/o reg) on all data: 0.21557394
Train acc on all data:  0.849015317286652
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 6.71394e-06
Norm of the params: 16.113735
                Loss: fixed 205 labels. Loss 0.21557. Accuracy 0.936.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44354963
Train loss (w/o reg) on all data: 0.4379962
Test loss (w/o reg) on all data: 0.25213256
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.8118538e-05
Norm of the params: 10.538908
              Random: fixed  47 labels. Loss 0.25213. Accuracy 0.971.
### Flips: 1025, rs: 10, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3174385
Train loss (w/o reg) on all data: 0.30743393
Test loss (w/o reg) on all data: 0.16457145
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.381646e-05
Norm of the params: 14.145368
     Influence (LOO): fixed 307 labels. Loss 0.16457. Accuracy 0.986.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23842494
Train loss (w/o reg) on all data: 0.22134767
Test loss (w/o reg) on all data: 0.15950999
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 4.580684e-06
Norm of the params: 18.480947
                Loss: fixed 406 labels. Loss 0.15951. Accuracy 0.947.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336881
Train loss (w/o reg) on all data: 0.42821681
Test loss (w/o reg) on all data: 0.2381071
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.8261436e-05
Norm of the params: 10.460688
              Random: fixed  86 labels. Loss 0.23811. Accuracy 0.972.
### Flips: 1025, rs: 10, checks: 615
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25478402
Train loss (w/o reg) on all data: 0.24364756
Test loss (w/o reg) on all data: 0.1306032
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.542023e-06
Norm of the params: 14.924114
     Influence (LOO): fixed 429 labels. Loss 0.13060. Accuracy 0.990.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13901976
Train loss (w/o reg) on all data: 0.12039521
Test loss (w/o reg) on all data: 0.110091366
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.8020806e-06
Norm of the params: 19.300026
                Loss: fixed 608 labels. Loss 0.11009. Accuracy 0.960.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42143494
Train loss (w/o reg) on all data: 0.4161551
Test loss (w/o reg) on all data: 0.22295855
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 9.858147e-06
Norm of the params: 10.276028
              Random: fixed 128 labels. Loss 0.22296. Accuracy 0.978.
### Flips: 1025, rs: 10, checks: 820
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21868926
Train loss (w/o reg) on all data: 0.20707771
Test loss (w/o reg) on all data: 0.11190466
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.9442533e-06
Norm of the params: 15.239133
     Influence (LOO): fixed 500 labels. Loss 0.11190. Accuracy 0.988.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058711052
Train loss (w/o reg) on all data: 0.043632425
Test loss (w/o reg) on all data: 0.032278936
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.5999049e-06
Norm of the params: 17.365845
                Loss: fixed 787 labels. Loss 0.03228. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40748015
Train loss (w/o reg) on all data: 0.40224394
Test loss (w/o reg) on all data: 0.20958415
Train acc on all data:  0.8283491368830538
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.190285e-05
Norm of the params: 10.233486
              Random: fixed 172 labels. Loss 0.20958. Accuracy 0.983.
### Flips: 1025, rs: 10, checks: 1025
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18827975
Train loss (w/o reg) on all data: 0.176769
Test loss (w/o reg) on all data: 0.09309609
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.3006155e-06
Norm of the params: 15.1728325
     Influence (LOO): fixed 562 labels. Loss 0.09310. Accuracy 0.991.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019800767
Train loss (w/o reg) on all data: 0.011329265
Test loss (w/o reg) on all data: 0.013680539
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.099674e-07
Norm of the params: 13.016529
                Loss: fixed 860 labels. Loss 0.01368. Accuracy 0.997.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3899336
Train loss (w/o reg) on all data: 0.38432056
Test loss (w/o reg) on all data: 0.19430849
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.776748e-06
Norm of the params: 10.595317
              Random: fixed 221 labels. Loss 0.19431. Accuracy 0.982.
### Flips: 1025, rs: 10, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15768026
Train loss (w/o reg) on all data: 0.1466486
Test loss (w/o reg) on all data: 0.07417371
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.439477e-06
Norm of the params: 14.853722
     Influence (LOO): fixed 623 labels. Loss 0.07417. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013158156
Train loss (w/o reg) on all data: 0.0067365617
Test loss (w/o reg) on all data: 0.0071906615
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.293733e-08
Norm of the params: 11.332779
                Loss: fixed 871 labels. Loss 0.00719. Accuracy 0.999.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37059817
Train loss (w/o reg) on all data: 0.36441407
Test loss (w/o reg) on all data: 0.1790243
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.2888274e-05
Norm of the params: 11.121251
              Random: fixed 268 labels. Loss 0.17902. Accuracy 0.986.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45670733
Train loss (w/o reg) on all data: 0.45124716
Test loss (w/o reg) on all data: 0.27406204
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.1300893e-05
Norm of the params: 10.450032
Flipped loss: 0.27406. Accuracy: 0.962
### Flips: 1025, rs: 11, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3815005
Train loss (w/o reg) on all data: 0.37151256
Test loss (w/o reg) on all data: 0.21743599
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.7587248e-05
Norm of the params: 14.133616
     Influence (LOO): fixed 170 labels. Loss 0.21744. Accuracy 0.976.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34765515
Train loss (w/o reg) on all data: 0.33642787
Test loss (w/o reg) on all data: 0.22250235
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 1.0640229e-05
Norm of the params: 14.984855
                Loss: fixed 205 labels. Loss 0.22250. Accuracy 0.940.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44875893
Train loss (w/o reg) on all data: 0.44348672
Test loss (w/o reg) on all data: 0.26236814
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.7460357e-05
Norm of the params: 10.2685995
              Random: fixed  36 labels. Loss 0.26237. Accuracy 0.972.
### Flips: 1025, rs: 11, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32797164
Train loss (w/o reg) on all data: 0.31696063
Test loss (w/o reg) on all data: 0.18059629
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0734377e-05
Norm of the params: 14.839823
     Influence (LOO): fixed 298 labels. Loss 0.18060. Accuracy 0.977.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24468121
Train loss (w/o reg) on all data: 0.2281322
Test loss (w/o reg) on all data: 0.16633682
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 7.3705505e-06
Norm of the params: 18.192856
                Loss: fixed 409 labels. Loss 0.16634. Accuracy 0.948.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43436715
Train loss (w/o reg) on all data: 0.4289389
Test loss (w/o reg) on all data: 0.24753888
Train acc on all data:  0.8098711402868952
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.057361e-05
Norm of the params: 10.419464
              Random: fixed  87 labels. Loss 0.24754. Accuracy 0.973.
### Flips: 1025, rs: 11, checks: 615
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27805674
Train loss (w/o reg) on all data: 0.26719162
Test loss (w/o reg) on all data: 0.14917643
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 8.695557e-06
Norm of the params: 14.741192
     Influence (LOO): fixed 409 labels. Loss 0.14918. Accuracy 0.981.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14024815
Train loss (w/o reg) on all data: 0.12046213
Test loss (w/o reg) on all data: 0.10419889
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.16739e-06
Norm of the params: 19.892725
                Loss: fixed 610 labels. Loss 0.10420. Accuracy 0.966.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42199612
Train loss (w/o reg) on all data: 0.41662452
Test loss (w/o reg) on all data: 0.23186123
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.51098e-05
Norm of the params: 10.364935
              Random: fixed 132 labels. Loss 0.23186. Accuracy 0.979.
### Flips: 1025, rs: 11, checks: 820
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23095383
Train loss (w/o reg) on all data: 0.22074096
Test loss (w/o reg) on all data: 0.11688024
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.082636e-06
Norm of the params: 14.29186
     Influence (LOO): fixed 506 labels. Loss 0.11688. Accuracy 0.986.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058465973
Train loss (w/o reg) on all data: 0.041304544
Test loss (w/o reg) on all data: 0.036711913
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1019936e-06
Norm of the params: 18.526428
                Loss: fixed 790 labels. Loss 0.03671. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.408202
Train loss (w/o reg) on all data: 0.40283096
Test loss (w/o reg) on all data: 0.21654795
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.93953e-05
Norm of the params: 10.364406
              Random: fixed 175 labels. Loss 0.21655. Accuracy 0.982.
### Flips: 1025, rs: 11, checks: 1025
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19388041
Train loss (w/o reg) on all data: 0.18364623
Test loss (w/o reg) on all data: 0.09156881
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.2835883e-06
Norm of the params: 14.306769
     Influence (LOO): fixed 581 labels. Loss 0.09157. Accuracy 0.990.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025823195
Train loss (w/o reg) on all data: 0.01530558
Test loss (w/o reg) on all data: 0.011733172
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6415844e-07
Norm of the params: 14.503528
                Loss: fixed 859 labels. Loss 0.01173. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3885755
Train loss (w/o reg) on all data: 0.3829444
Test loss (w/o reg) on all data: 0.19864482
Train acc on all data:  0.8407488451252128
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.7136934e-05
Norm of the params: 10.612339
              Random: fixed 228 labels. Loss 0.19864. Accuracy 0.985.
### Flips: 1025, rs: 11, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16823013
Train loss (w/o reg) on all data: 0.15824842
Test loss (w/o reg) on all data: 0.077256046
Train acc on all data:  0.9338682227084852
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.414954e-06
Norm of the params: 14.1291895
     Influence (LOO): fixed 627 labels. Loss 0.07726. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01587369
Train loss (w/o reg) on all data: 0.008643518
Test loss (w/o reg) on all data: 0.0069181905
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6275074e-07
Norm of the params: 12.025116
                Loss: fixed 878 labels. Loss 0.00692. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3728787
Train loss (w/o reg) on all data: 0.36712244
Test loss (w/o reg) on all data: 0.18584993
Train acc on all data:  0.8507172380257719
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.21743315e-05
Norm of the params: 10.729646
              Random: fixed 271 labels. Loss 0.18585. Accuracy 0.986.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45779845
Train loss (w/o reg) on all data: 0.4519661
Test loss (w/o reg) on all data: 0.28197494
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 1.481574e-05
Norm of the params: 10.80031
Flipped loss: 0.28197. Accuracy: 0.951
### Flips: 1025, rs: 12, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3840753
Train loss (w/o reg) on all data: 0.37388465
Test loss (w/o reg) on all data: 0.22250423
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.046278e-05
Norm of the params: 14.27632
     Influence (LOO): fixed 172 labels. Loss 0.22250. Accuracy 0.967.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34603846
Train loss (w/o reg) on all data: 0.33304304
Test loss (w/o reg) on all data: 0.23800458
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 3.745468e-05
Norm of the params: 16.12168
                Loss: fixed 205 labels. Loss 0.23800. Accuracy 0.921.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44141448
Train loss (w/o reg) on all data: 0.4353934
Test loss (w/o reg) on all data: 0.26317155
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.0859194e-05
Norm of the params: 10.9736805
              Random: fixed  61 labels. Loss 0.26317. Accuracy 0.962.
### Flips: 1025, rs: 12, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32637477
Train loss (w/o reg) on all data: 0.31456757
Test loss (w/o reg) on all data: 0.18257484
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.1629712e-05
Norm of the params: 15.366985
     Influence (LOO): fixed 305 labels. Loss 0.18257. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24330977
Train loss (w/o reg) on all data: 0.22563426
Test loss (w/o reg) on all data: 0.18404055
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 5.7109664e-06
Norm of the params: 18.801863
                Loss: fixed 409 labels. Loss 0.18404. Accuracy 0.930.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42863238
Train loss (w/o reg) on all data: 0.42210394
Test loss (w/o reg) on all data: 0.24798338
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.5567982e-05
Norm of the params: 11.426676
              Random: fixed 105 labels. Loss 0.24798. Accuracy 0.967.
### Flips: 1025, rs: 12, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28245008
Train loss (w/o reg) on all data: 0.27069014
Test loss (w/o reg) on all data: 0.14866643
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.1420543e-05
Norm of the params: 15.336192
     Influence (LOO): fixed 411 labels. Loss 0.14867. Accuracy 0.988.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14353982
Train loss (w/o reg) on all data: 0.12218842
Test loss (w/o reg) on all data: 0.12213067
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.6606295e-05
Norm of the params: 20.664654
                Loss: fixed 609 labels. Loss 0.12213. Accuracy 0.952.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41240937
Train loss (w/o reg) on all data: 0.40540057
Test loss (w/o reg) on all data: 0.23580118
Train acc on all data:  0.825188426938974
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.3582716e-05
Norm of the params: 11.839577
              Random: fixed 156 labels. Loss 0.23580. Accuracy 0.972.
### Flips: 1025, rs: 12, checks: 820
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2413197
Train loss (w/o reg) on all data: 0.23005351
Test loss (w/o reg) on all data: 0.12223968
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.179423e-06
Norm of the params: 15.010789
     Influence (LOO): fixed 496 labels. Loss 0.12224. Accuracy 0.987.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067878306
Train loss (w/o reg) on all data: 0.050253604
Test loss (w/o reg) on all data: 0.059608262
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.4381487e-06
Norm of the params: 18.774824
                Loss: fixed 779 labels. Loss 0.05961. Accuracy 0.982.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39587638
Train loss (w/o reg) on all data: 0.38895366
Test loss (w/o reg) on all data: 0.21839648
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.1173392e-05
Norm of the params: 11.766662
              Random: fixed 208 labels. Loss 0.21840. Accuracy 0.977.
### Flips: 1025, rs: 12, checks: 1025
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2002114
Train loss (w/o reg) on all data: 0.18944052
Test loss (w/o reg) on all data: 0.09784659
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6933802e-06
Norm of the params: 14.677122
     Influence (LOO): fixed 573 labels. Loss 0.09785. Accuracy 0.994.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031914458
Train loss (w/o reg) on all data: 0.019969147
Test loss (w/o reg) on all data: 0.015804503
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.514132e-06
Norm of the params: 15.456592
                Loss: fixed 857 labels. Loss 0.01580. Accuracy 0.999.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38481367
Train loss (w/o reg) on all data: 0.3779312
Test loss (w/o reg) on all data: 0.20627418
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.7228507e-05
Norm of the params: 11.732404
              Random: fixed 244 labels. Loss 0.20627. Accuracy 0.980.
### Flips: 1025, rs: 12, checks: 1230
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16759482
Train loss (w/o reg) on all data: 0.15722023
Test loss (w/o reg) on all data: 0.07759813
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0844733e-06
Norm of the params: 14.404574
     Influence (LOO): fixed 638 labels. Loss 0.07760. Accuracy 0.997.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01724381
Train loss (w/o reg) on all data: 0.008891035
Test loss (w/o reg) on all data: 0.009366943
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8172403e-07
Norm of the params: 12.924996
                Loss: fixed 879 labels. Loss 0.00937. Accuracy 0.998.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3693017
Train loss (w/o reg) on all data: 0.3625381
Test loss (w/o reg) on all data: 0.18973942
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.2062364e-05
Norm of the params: 11.630665
              Random: fixed 290 labels. Loss 0.18974. Accuracy 0.981.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4554349
Train loss (w/o reg) on all data: 0.45021063
Test loss (w/o reg) on all data: 0.2767153
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.4736188e-05
Norm of the params: 10.221798
Flipped loss: 0.27672. Accuracy: 0.961
### Flips: 1025, rs: 13, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38016972
Train loss (w/o reg) on all data: 0.37101957
Test loss (w/o reg) on all data: 0.2190309
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.2343535e-05
Norm of the params: 13.527864
     Influence (LOO): fixed 169 labels. Loss 0.21903. Accuracy 0.968.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34518275
Train loss (w/o reg) on all data: 0.33429646
Test loss (w/o reg) on all data: 0.23080195
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.9746405e-05
Norm of the params: 14.755527
                Loss: fixed 204 labels. Loss 0.23080. Accuracy 0.934.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44514892
Train loss (w/o reg) on all data: 0.43998033
Test loss (w/o reg) on all data: 0.26140714
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.05959725e-05
Norm of the params: 10.16718
              Random: fixed  41 labels. Loss 0.26141. Accuracy 0.969.
### Flips: 1025, rs: 13, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31911057
Train loss (w/o reg) on all data: 0.30808294
Test loss (w/o reg) on all data: 0.18247332
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4035378e-05
Norm of the params: 14.851016
     Influence (LOO): fixed 303 labels. Loss 0.18247. Accuracy 0.980.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24278343
Train loss (w/o reg) on all data: 0.22743872
Test loss (w/o reg) on all data: 0.17894436
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.0795007e-05
Norm of the params: 17.5184
                Loss: fixed 409 labels. Loss 0.17894. Accuracy 0.944.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43320844
Train loss (w/o reg) on all data: 0.42835978
Test loss (w/o reg) on all data: 0.2446555
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.2921746e-05
Norm of the params: 9.847508
              Random: fixed  87 labels. Loss 0.24466. Accuracy 0.975.
### Flips: 1025, rs: 13, checks: 615
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2708994
Train loss (w/o reg) on all data: 0.25998074
Test loss (w/o reg) on all data: 0.14829426
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.28808e-06
Norm of the params: 14.777456
     Influence (LOO): fixed 409 labels. Loss 0.14829. Accuracy 0.982.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14250441
Train loss (w/o reg) on all data: 0.12383014
Test loss (w/o reg) on all data: 0.11814427
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8021308e-06
Norm of the params: 19.325768
                Loss: fixed 608 labels. Loss 0.11814. Accuracy 0.958.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42074588
Train loss (w/o reg) on all data: 0.4157927
Test loss (w/o reg) on all data: 0.23265895
Train acc on all data:  0.8188670070508145
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.2089025e-05
Norm of the params: 9.953056
              Random: fixed 126 labels. Loss 0.23266. Accuracy 0.975.
### Flips: 1025, rs: 13, checks: 820
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23064461
Train loss (w/o reg) on all data: 0.21997839
Test loss (w/o reg) on all data: 0.12023815
Train acc on all data:  0.9076100170192074
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.2624143e-05
Norm of the params: 14.605636
     Influence (LOO): fixed 495 labels. Loss 0.12024. Accuracy 0.984.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05076549
Train loss (w/o reg) on all data: 0.035951458
Test loss (w/o reg) on all data: 0.048917823
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.9126069e-06
Norm of the params: 17.212803
                Loss: fixed 788 labels. Loss 0.04892. Accuracy 0.986.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40750962
Train loss (w/o reg) on all data: 0.40257728
Test loss (w/o reg) on all data: 0.2197138
Train acc on all data:  0.8281060053488938
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.4499047e-05
Norm of the params: 9.932102
              Random: fixed 169 labels. Loss 0.21971. Accuracy 0.977.
### Flips: 1025, rs: 13, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19304998
Train loss (w/o reg) on all data: 0.18231808
Test loss (w/o reg) on all data: 0.09783757
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5857982e-05
Norm of the params: 14.650535
     Influence (LOO): fixed 567 labels. Loss 0.09784. Accuracy 0.986.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018986132
Train loss (w/o reg) on all data: 0.0107249
Test loss (w/o reg) on all data: 0.013506995
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.115851e-07
Norm of the params: 12.853974
                Loss: fixed 853 labels. Loss 0.01351. Accuracy 0.998.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39396533
Train loss (w/o reg) on all data: 0.38886815
Test loss (w/o reg) on all data: 0.20363358
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1303104e-05
Norm of the params: 10.096706
              Random: fixed 211 labels. Loss 0.20363. Accuracy 0.982.
### Flips: 1025, rs: 13, checks: 1230
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15579247
Train loss (w/o reg) on all data: 0.14524055
Test loss (w/o reg) on all data: 0.07835551
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.132153e-06
Norm of the params: 14.5271635
     Influence (LOO): fixed 636 labels. Loss 0.07836. Accuracy 0.989.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01534229
Train loss (w/o reg) on all data: 0.008421993
Test loss (w/o reg) on all data: 0.008940329
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1073905e-07
Norm of the params: 11.764606
                Loss: fixed 865 labels. Loss 0.00894. Accuracy 1.000.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37940386
Train loss (w/o reg) on all data: 0.37373576
Test loss (w/o reg) on all data: 0.19323003
Train acc on all data:  0.8477996596158521
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2647636e-05
Norm of the params: 10.647161
              Random: fixed 249 labels. Loss 0.19323. Accuracy 0.983.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45584434
Train loss (w/o reg) on all data: 0.45156336
Test loss (w/o reg) on all data: 0.26268202
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.6793627e-05
Norm of the params: 9.253077
Flipped loss: 0.26268. Accuracy: 0.976
### Flips: 1025, rs: 14, checks: 205
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37830403
Train loss (w/o reg) on all data: 0.3697381
Test loss (w/o reg) on all data: 0.19903146
Train acc on all data:  0.8349136883053732
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.404022e-06
Norm of the params: 13.088879
     Influence (LOO): fixed 175 labels. Loss 0.19903. Accuracy 0.984.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34058973
Train loss (w/o reg) on all data: 0.3296081
Test loss (w/o reg) on all data: 0.22395614
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.9319486e-05
Norm of the params: 14.820009
                Loss: fixed 205 labels. Loss 0.22396. Accuracy 0.938.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44051078
Train loss (w/o reg) on all data: 0.43606648
Test loss (w/o reg) on all data: 0.24704657
Train acc on all data:  0.8045222465353756
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.6076e-05
Norm of the params: 9.427947
              Random: fixed  48 labels. Loss 0.24705. Accuracy 0.977.
### Flips: 1025, rs: 14, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31954423
Train loss (w/o reg) on all data: 0.3098751
Test loss (w/o reg) on all data: 0.15893076
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3404895e-05
Norm of the params: 13.906212
     Influence (LOO): fixed 312 labels. Loss 0.15893. Accuracy 0.987.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23255354
Train loss (w/o reg) on all data: 0.21599628
Test loss (w/o reg) on all data: 0.16376425
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4558793e-05
Norm of the params: 18.197392
                Loss: fixed 410 labels. Loss 0.16376. Accuracy 0.956.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4283376
Train loss (w/o reg) on all data: 0.42382777
Test loss (w/o reg) on all data: 0.2346464
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5181726e-05
Norm of the params: 9.497204
              Random: fixed  88 labels. Loss 0.23465. Accuracy 0.976.
### Flips: 1025, rs: 14, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27396068
Train loss (w/o reg) on all data: 0.2631993
Test loss (w/o reg) on all data: 0.13066797
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.139156e-06
Norm of the params: 14.670628
     Influence (LOO): fixed 410 labels. Loss 0.13067. Accuracy 0.993.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12974042
Train loss (w/o reg) on all data: 0.110791594
Test loss (w/o reg) on all data: 0.106244735
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.2466519e-06
Norm of the params: 19.467316
                Loss: fixed 612 labels. Loss 0.10624. Accuracy 0.973.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.416428
Train loss (w/o reg) on all data: 0.41167626
Test loss (w/o reg) on all data: 0.22374514
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.4954333e-05
Norm of the params: 9.748596
              Random: fixed 126 labels. Loss 0.22375. Accuracy 0.982.
### Flips: 1025, rs: 14, checks: 820
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22896503
Train loss (w/o reg) on all data: 0.21835299
Test loss (w/o reg) on all data: 0.10820946
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.5757967e-05
Norm of the params: 14.568487
     Influence (LOO): fixed 500 labels. Loss 0.10821. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049289133
Train loss (w/o reg) on all data: 0.03454115
Test loss (w/o reg) on all data: 0.040560316
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0399876e-06
Norm of the params: 17.17439
                Loss: fixed 792 labels. Loss 0.04056. Accuracy 0.990.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4030697
Train loss (w/o reg) on all data: 0.39846033
Test loss (w/o reg) on all data: 0.21160783
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.6090333e-05
Norm of the params: 9.601441
              Random: fixed 169 labels. Loss 0.21161. Accuracy 0.979.
### Flips: 1025, rs: 14, checks: 1025
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260609
Train loss (w/o reg) on all data: 0.18231994
Test loss (w/o reg) on all data: 0.08597405
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5636666e-05
Norm of the params: 14.343043
     Influence (LOO): fixed 572 labels. Loss 0.08597. Accuracy 0.995.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02032661
Train loss (w/o reg) on all data: 0.0117239235
Test loss (w/o reg) on all data: 0.009317574
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0096903e-06
Norm of the params: 13.116926
                Loss: fixed 854 labels. Loss 0.00932. Accuracy 0.999.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3894111
Train loss (w/o reg) on all data: 0.38456678
Test loss (w/o reg) on all data: 0.19721355
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6016678e-05
Norm of the params: 9.843063
              Random: fixed 211 labels. Loss 0.19721. Accuracy 0.982.
### Flips: 1025, rs: 14, checks: 1230
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1545756
Train loss (w/o reg) on all data: 0.14521816
Test loss (w/o reg) on all data: 0.069076836
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.963603e-06
Norm of the params: 13.680228
     Influence (LOO): fixed 642 labels. Loss 0.06908. Accuracy 0.994.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011795572
Train loss (w/o reg) on all data: 0.0058887415
Test loss (w/o reg) on all data: 0.0051523824
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.57598e-07
Norm of the params: 10.869067
                Loss: fixed 867 labels. Loss 0.00515. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36877567
Train loss (w/o reg) on all data: 0.36369985
Test loss (w/o reg) on all data: 0.17935051
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.8141581e-05
Norm of the params: 10.075517
              Random: fixed 266 labels. Loss 0.17935. Accuracy 0.983.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4519427
Train loss (w/o reg) on all data: 0.44569594
Test loss (w/o reg) on all data: 0.26974815
Train acc on all data:  0.7916362752248967
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 5.3960426e-05
Norm of the params: 11.177457
Flipped loss: 0.26975. Accuracy: 0.965
### Flips: 1025, rs: 15, checks: 205
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37608027
Train loss (w/o reg) on all data: 0.36550426
Test loss (w/o reg) on all data: 0.21651232
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.601209e-05
Norm of the params: 14.543737
     Influence (LOO): fixed 168 labels. Loss 0.21651. Accuracy 0.975.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34230596
Train loss (w/o reg) on all data: 0.3294433
Test loss (w/o reg) on all data: 0.21266459
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 6.796438e-06
Norm of the params: 16.039112
                Loss: fixed 205 labels. Loss 0.21266. Accuracy 0.956.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4409179
Train loss (w/o reg) on all data: 0.43473923
Test loss (w/o reg) on all data: 0.2557293
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.1920052e-05
Norm of the params: 11.116364
              Random: fixed  48 labels. Loss 0.25573. Accuracy 0.969.
### Flips: 1025, rs: 15, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32616574
Train loss (w/o reg) on all data: 0.31458563
Test loss (w/o reg) on all data: 0.17192586
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2845895e-05
Norm of the params: 15.2184725
     Influence (LOO): fixed 296 labels. Loss 0.17193. Accuracy 0.990.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2403428
Train loss (w/o reg) on all data: 0.22330528
Test loss (w/o reg) on all data: 0.15798606
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.10617975e-05
Norm of the params: 18.459415
                Loss: fixed 409 labels. Loss 0.15799. Accuracy 0.954.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42968124
Train loss (w/o reg) on all data: 0.42366505
Test loss (w/o reg) on all data: 0.2406591
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.283388e-05
Norm of the params: 10.969218
              Random: fixed  90 labels. Loss 0.24066. Accuracy 0.974.
### Flips: 1025, rs: 15, checks: 615
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28125796
Train loss (w/o reg) on all data: 0.2690615
Test loss (w/o reg) on all data: 0.1395385
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7760855e-05
Norm of the params: 15.618221
     Influence (LOO): fixed 402 labels. Loss 0.13954. Accuracy 0.994.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14081505
Train loss (w/o reg) on all data: 0.1196543
Test loss (w/o reg) on all data: 0.10245421
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.29190175e-05
Norm of the params: 20.57219
                Loss: fixed 608 labels. Loss 0.10245. Accuracy 0.973.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4160057
Train loss (w/o reg) on all data: 0.40998754
Test loss (w/o reg) on all data: 0.2268162
Train acc on all data:  0.8227571115973742
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.535883e-05
Norm of the params: 10.9710245
              Random: fixed 141 labels. Loss 0.22682. Accuracy 0.975.
### Flips: 1025, rs: 15, checks: 820
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23770148
Train loss (w/o reg) on all data: 0.22616936
Test loss (w/o reg) on all data: 0.111147836
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.953373e-06
Norm of the params: 15.186908
     Influence (LOO): fixed 495 labels. Loss 0.11115. Accuracy 0.997.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054383695
Train loss (w/o reg) on all data: 0.03855531
Test loss (w/o reg) on all data: 0.038727395
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1437393e-06
Norm of the params: 17.792353
                Loss: fixed 783 labels. Loss 0.03873. Accuracy 0.990.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40563744
Train loss (w/o reg) on all data: 0.3994491
Test loss (w/o reg) on all data: 0.21662354
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.557339e-05
Norm of the params: 11.125048
              Random: fixed 175 labels. Loss 0.21662. Accuracy 0.976.
### Flips: 1025, rs: 15, checks: 1025
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1967819
Train loss (w/o reg) on all data: 0.18562965
Test loss (w/o reg) on all data: 0.09053326
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.677428e-06
Norm of the params: 14.934694
     Influence (LOO): fixed 572 labels. Loss 0.09053. Accuracy 0.998.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021416895
Train loss (w/o reg) on all data: 0.012407047
Test loss (w/o reg) on all data: 0.013247588
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.7235765e-07
Norm of the params: 13.423747
                Loss: fixed 856 labels. Loss 0.01325. Accuracy 0.999.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39030746
Train loss (w/o reg) on all data: 0.38405734
Test loss (w/o reg) on all data: 0.20268482
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.31430315e-05
Norm of the params: 11.180437
              Random: fixed 223 labels. Loss 0.20268. Accuracy 0.980.
### Flips: 1025, rs: 15, checks: 1230
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16366363
Train loss (w/o reg) on all data: 0.15320668
Test loss (w/o reg) on all data: 0.07378102
Train acc on all data:  0.937028932652565
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3872191e-05
Norm of the params: 14.46164
     Influence (LOO): fixed 634 labels. Loss 0.07378. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016593792
Train loss (w/o reg) on all data: 0.009240018
Test loss (w/o reg) on all data: 0.0085834935
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1961597e-07
Norm of the params: 12.127467
                Loss: fixed 867 labels. Loss 0.00858. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37395346
Train loss (w/o reg) on all data: 0.3677958
Test loss (w/o reg) on all data: 0.1877652
Train acc on all data:  0.8538779479698517
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.8674878e-05
Norm of the params: 11.097454
              Random: fixed 272 labels. Loss 0.18777. Accuracy 0.983.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4560412
Train loss (w/o reg) on all data: 0.45066908
Test loss (w/o reg) on all data: 0.26157874
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3112769e-05
Norm of the params: 10.365436
Flipped loss: 0.26158. Accuracy: 0.975
### Flips: 1025, rs: 16, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37794274
Train loss (w/o reg) on all data: 0.3688167
Test loss (w/o reg) on all data: 0.20614965
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.7750665e-06
Norm of the params: 13.510031
     Influence (LOO): fixed 173 labels. Loss 0.20615. Accuracy 0.987.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3423682
Train loss (w/o reg) on all data: 0.32970494
Test loss (w/o reg) on all data: 0.21076015
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4294314e-05
Norm of the params: 15.91429
                Loss: fixed 205 labels. Loss 0.21076. Accuracy 0.949.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44206157
Train loss (w/o reg) on all data: 0.4362592
Test loss (w/o reg) on all data: 0.24675749
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.803388e-05
Norm of the params: 10.772518
              Random: fixed  47 labels. Loss 0.24676. Accuracy 0.981.
### Flips: 1025, rs: 16, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31794727
Train loss (w/o reg) on all data: 0.30772093
Test loss (w/o reg) on all data: 0.16422684
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.091899e-06
Norm of the params: 14.301295
     Influence (LOO): fixed 310 labels. Loss 0.16423. Accuracy 0.988.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23948461
Train loss (w/o reg) on all data: 0.22226968
Test loss (w/o reg) on all data: 0.1506725
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 6.5534755e-06
Norm of the params: 18.55528
                Loss: fixed 410 labels. Loss 0.15067. Accuracy 0.957.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43047997
Train loss (w/o reg) on all data: 0.42451382
Test loss (w/o reg) on all data: 0.23653091
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2757497e-05
Norm of the params: 10.923514
              Random: fixed  90 labels. Loss 0.23653. Accuracy 0.979.
### Flips: 1025, rs: 16, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27049896
Train loss (w/o reg) on all data: 0.26016504
Test loss (w/o reg) on all data: 0.13444263
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3574678e-05
Norm of the params: 14.376326
     Influence (LOO): fixed 413 labels. Loss 0.13444. Accuracy 0.993.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13716236
Train loss (w/o reg) on all data: 0.11742647
Test loss (w/o reg) on all data: 0.08280599
Train acc on all data:  0.950401167031364
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 6.1529e-06
Norm of the params: 19.867502
                Loss: fixed 615 labels. Loss 0.08281. Accuracy 0.976.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41640943
Train loss (w/o reg) on all data: 0.41027552
Test loss (w/o reg) on all data: 0.21914667
Train acc on all data:  0.825674690007294
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2221768e-05
Norm of the params: 11.07601
              Random: fixed 139 labels. Loss 0.21915. Accuracy 0.979.
### Flips: 1025, rs: 16, checks: 820
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22021368
Train loss (w/o reg) on all data: 0.20996518
Test loss (w/o reg) on all data: 0.106908225
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.891797e-06
Norm of the params: 14.316768
     Influence (LOO): fixed 509 labels. Loss 0.10691. Accuracy 0.994.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04187885
Train loss (w/o reg) on all data: 0.027412346
Test loss (w/o reg) on all data: 0.024505263
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.4242394e-07
Norm of the params: 17.009705
                Loss: fixed 803 labels. Loss 0.02451. Accuracy 0.993.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40300354
Train loss (w/o reg) on all data: 0.39675418
Test loss (w/o reg) on all data: 0.20656441
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1795243e-05
Norm of the params: 11.1797695
              Random: fixed 180 labels. Loss 0.20656. Accuracy 0.982.
### Flips: 1025, rs: 16, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18310489
Train loss (w/o reg) on all data: 0.17279874
Test loss (w/o reg) on all data: 0.08425222
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.9792893e-05
Norm of the params: 14.356981
     Influence (LOO): fixed 579 labels. Loss 0.08425. Accuracy 0.998.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020652216
Train loss (w/o reg) on all data: 0.011360529
Test loss (w/o reg) on all data: 0.011154525
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4418339e-06
Norm of the params: 13.632086
                Loss: fixed 853 labels. Loss 0.01115. Accuracy 0.999.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38706145
Train loss (w/o reg) on all data: 0.38029733
Test loss (w/o reg) on all data: 0.19374633
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6034039e-05
Norm of the params: 11.6311035
              Random: fixed 223 labels. Loss 0.19375. Accuracy 0.984.
### Flips: 1025, rs: 16, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15621883
Train loss (w/o reg) on all data: 0.14662716
Test loss (w/o reg) on all data: 0.07042802
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.318121e-06
Norm of the params: 13.850392
     Influence (LOO): fixed 629 labels. Loss 0.07043. Accuracy 0.998.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012743596
Train loss (w/o reg) on all data: 0.0062685134
Test loss (w/o reg) on all data: 0.006224231
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0843663e-07
Norm of the params: 11.379879
                Loss: fixed 868 labels. Loss 0.00622. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37135735
Train loss (w/o reg) on all data: 0.36459833
Test loss (w/o reg) on all data: 0.1786801
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2371845e-05
Norm of the params: 11.626701
              Random: fixed 271 labels. Loss 0.17868. Accuracy 0.986.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4492957
Train loss (w/o reg) on all data: 0.4430098
Test loss (w/o reg) on all data: 0.26138517
Train acc on all data:  0.7947969851689765
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3895e-05
Norm of the params: 11.212406
Flipped loss: 0.26139. Accuracy: 0.972
### Flips: 1025, rs: 17, checks: 205
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38043353
Train loss (w/o reg) on all data: 0.37090525
Test loss (w/o reg) on all data: 0.2082738
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0319988e-05
Norm of the params: 13.804549
     Influence (LOO): fixed 162 labels. Loss 0.20827. Accuracy 0.977.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34013516
Train loss (w/o reg) on all data: 0.32770854
Test loss (w/o reg) on all data: 0.20816372
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4064537e-05
Norm of the params: 15.764903
                Loss: fixed 205 labels. Loss 0.20816. Accuracy 0.949.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4347162
Train loss (w/o reg) on all data: 0.42811212
Test loss (w/o reg) on all data: 0.24739158
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.5390693e-05
Norm of the params: 11.492678
              Random: fixed  48 labels. Loss 0.24739. Accuracy 0.971.
### Flips: 1025, rs: 17, checks: 410
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3245089
Train loss (w/o reg) on all data: 0.31344
Test loss (w/o reg) on all data: 0.16219985
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2304865e-05
Norm of the params: 14.878781
     Influence (LOO): fixed 298 labels. Loss 0.16220. Accuracy 0.988.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23856835
Train loss (w/o reg) on all data: 0.2215144
Test loss (w/o reg) on all data: 0.15366451
Train acc on all data:  0.899829807926088
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 5.7977923e-06
Norm of the params: 18.468325
                Loss: fixed 408 labels. Loss 0.15366. Accuracy 0.956.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4234893
Train loss (w/o reg) on all data: 0.41686073
Test loss (w/o reg) on all data: 0.23593229
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.3217084e-05
Norm of the params: 11.513979
              Random: fixed  87 labels. Loss 0.23593. Accuracy 0.973.
### Flips: 1025, rs: 17, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2743718
Train loss (w/o reg) on all data: 0.26316398
Test loss (w/o reg) on all data: 0.13091049
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.47397195e-05
Norm of the params: 14.971855
     Influence (LOO): fixed 409 labels. Loss 0.13091. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13520508
Train loss (w/o reg) on all data: 0.11493914
Test loss (w/o reg) on all data: 0.09290709
Train acc on all data:  0.9523462193046438
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.321746e-06
Norm of the params: 20.132526
                Loss: fixed 607 labels. Loss 0.09291. Accuracy 0.976.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40745685
Train loss (w/o reg) on all data: 0.400589
Test loss (w/o reg) on all data: 0.22422035
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.0179403e-05
Norm of the params: 11.719949
              Random: fixed 133 labels. Loss 0.22422. Accuracy 0.969.
### Flips: 1025, rs: 17, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22440553
Train loss (w/o reg) on all data: 0.21342805
Test loss (w/o reg) on all data: 0.10194998
Train acc on all data:  0.9115001215657671
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2094015e-05
Norm of the params: 14.817203
     Influence (LOO): fixed 514 labels. Loss 0.10195. Accuracy 0.994.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049666554
Train loss (w/o reg) on all data: 0.03377505
Test loss (w/o reg) on all data: 0.039527155
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.167066e-06
Norm of the params: 17.82779
                Loss: fixed 780 labels. Loss 0.03953. Accuracy 0.988.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3928749
Train loss (w/o reg) on all data: 0.3858308
Test loss (w/o reg) on all data: 0.20963284
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0808669e-05
Norm of the params: 11.869377
              Random: fixed 176 labels. Loss 0.20963. Accuracy 0.975.
### Flips: 1025, rs: 17, checks: 1025
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19157916
Train loss (w/o reg) on all data: 0.18124764
Test loss (w/o reg) on all data: 0.084141895
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.14086e-06
Norm of the params: 14.374645
     Influence (LOO): fixed 579 labels. Loss 0.08414. Accuracy 0.996.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027317205
Train loss (w/o reg) on all data: 0.016258737
Test loss (w/o reg) on all data: 0.014285675
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.318626e-06
Norm of the params: 14.871764
                Loss: fixed 832 labels. Loss 0.01429. Accuracy 0.998.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3766378
Train loss (w/o reg) on all data: 0.36958843
Test loss (w/o reg) on all data: 0.19492115
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.4489496e-05
Norm of the params: 11.873805
              Random: fixed 226 labels. Loss 0.19492. Accuracy 0.976.
### Flips: 1025, rs: 17, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15486585
Train loss (w/o reg) on all data: 0.14555076
Test loss (w/o reg) on all data: 0.06706514
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.2875004e-06
Norm of the params: 13.649236
     Influence (LOO): fixed 649 labels. Loss 0.06707. Accuracy 0.994.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020251218
Train loss (w/o reg) on all data: 0.011429893
Test loss (w/o reg) on all data: 0.009101208
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.193463e-07
Norm of the params: 13.282564
                Loss: fixed 849 labels. Loss 0.00910. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3601671
Train loss (w/o reg) on all data: 0.35279182
Test loss (w/o reg) on all data: 0.18525583
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.6241236e-05
Norm of the params: 12.14518
              Random: fixed 269 labels. Loss 0.18526. Accuracy 0.981.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46249828
Train loss (w/o reg) on all data: 0.45749184
Test loss (w/o reg) on all data: 0.27889532
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.7472166e-05
Norm of the params: 10.00642
Flipped loss: 0.27890. Accuracy: 0.966
### Flips: 1025, rs: 18, checks: 205
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38607186
Train loss (w/o reg) on all data: 0.37693682
Test loss (w/o reg) on all data: 0.22011676
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.5251851e-05
Norm of the params: 13.516694
     Influence (LOO): fixed 170 labels. Loss 0.22012. Accuracy 0.983.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3512927
Train loss (w/o reg) on all data: 0.34055752
Test loss (w/o reg) on all data: 0.22659884
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 2.580885e-05
Norm of the params: 14.65277
                Loss: fixed 205 labels. Loss 0.22660. Accuracy 0.940.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44922546
Train loss (w/o reg) on all data: 0.44408983
Test loss (w/o reg) on all data: 0.2588999
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6720967e-05
Norm of the params: 10.134711
              Random: fixed  52 labels. Loss 0.25890. Accuracy 0.973.
### Flips: 1025, rs: 18, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3279386
Train loss (w/o reg) on all data: 0.31711134
Test loss (w/o reg) on all data: 0.1764967
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.8798644e-05
Norm of the params: 14.715468
     Influence (LOO): fixed 301 labels. Loss 0.17650. Accuracy 0.983.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24778362
Train loss (w/o reg) on all data: 0.2326822
Test loss (w/o reg) on all data: 0.16608396
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 8.243191e-06
Norm of the params: 17.378963
                Loss: fixed 409 labels. Loss 0.16608. Accuracy 0.955.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4405687
Train loss (w/o reg) on all data: 0.43526796
Test loss (w/o reg) on all data: 0.24849145
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 6.9340414e-05
Norm of the params: 10.296325
              Random: fixed  83 labels. Loss 0.24849. Accuracy 0.979.
### Flips: 1025, rs: 18, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28241724
Train loss (w/o reg) on all data: 0.27193883
Test loss (w/o reg) on all data: 0.14236763
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5233707e-05
Norm of the params: 14.476469
     Influence (LOO): fixed 400 labels. Loss 0.14237. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14662786
Train loss (w/o reg) on all data: 0.12816687
Test loss (w/o reg) on all data: 0.102737084
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.2173546e-06
Norm of the params: 19.215092
                Loss: fixed 612 labels. Loss 0.10274. Accuracy 0.970.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42563024
Train loss (w/o reg) on all data: 0.42034212
Test loss (w/o reg) on all data: 0.23173675
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.25087845e-05
Norm of the params: 10.284098
              Random: fixed 130 labels. Loss 0.23174. Accuracy 0.984.
### Flips: 1025, rs: 18, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23954569
Train loss (w/o reg) on all data: 0.22881626
Test loss (w/o reg) on all data: 0.11716475
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.593086e-05
Norm of the params: 14.648842
     Influence (LOO): fixed 484 labels. Loss 0.11716. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05404059
Train loss (w/o reg) on all data: 0.03961494
Test loss (w/o reg) on all data: 0.036243472
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6740314e-06
Norm of the params: 16.985672
                Loss: fixed 805 labels. Loss 0.03624. Accuracy 0.989.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41410384
Train loss (w/o reg) on all data: 0.40860918
Test loss (w/o reg) on all data: 0.22047462
Train acc on all data:  0.824702163870654
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.2338478e-05
Norm of the params: 10.482986
              Random: fixed 168 labels. Loss 0.22047. Accuracy 0.983.
### Flips: 1025, rs: 18, checks: 1025
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2017337
Train loss (w/o reg) on all data: 0.19147015
Test loss (w/o reg) on all data: 0.09350866
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.935562e-06
Norm of the params: 14.327275
     Influence (LOO): fixed 565 labels. Loss 0.09351. Accuracy 0.997.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014521884
Train loss (w/o reg) on all data: 0.0076785865
Test loss (w/o reg) on all data: 0.007935281
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.254144e-07
Norm of the params: 11.698972
                Loss: fixed 883 labels. Loss 0.00794. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40128472
Train loss (w/o reg) on all data: 0.39550823
Test loss (w/o reg) on all data: 0.21217333
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3076031e-05
Norm of the params: 10.748489
              Random: fixed 203 labels. Loss 0.21217. Accuracy 0.983.
### Flips: 1025, rs: 18, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16479409
Train loss (w/o reg) on all data: 0.15435527
Test loss (w/o reg) on all data: 0.074067295
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.23713e-06
Norm of the params: 14.449092
     Influence (LOO): fixed 638 labels. Loss 0.07407. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00980421
Train loss (w/o reg) on all data: 0.004831326
Test loss (w/o reg) on all data: 0.0050345045
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.785917e-07
Norm of the params: 9.972847
                Loss: fixed 892 labels. Loss 0.00503. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38475314
Train loss (w/o reg) on all data: 0.37864938
Test loss (w/o reg) on all data: 0.1974452
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0947793e-05
Norm of the params: 11.048771
              Random: fixed 249 labels. Loss 0.19745. Accuracy 0.983.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45182028
Train loss (w/o reg) on all data: 0.4468468
Test loss (w/o reg) on all data: 0.26221377
Train acc on all data:  0.7923656698273767
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.841089e-05
Norm of the params: 9.973425
Flipped loss: 0.26221. Accuracy: 0.974
### Flips: 1025, rs: 19, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3688234
Train loss (w/o reg) on all data: 0.35981536
Test loss (w/o reg) on all data: 0.19719717
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.921659e-05
Norm of the params: 13.422409
     Influence (LOO): fixed 174 labels. Loss 0.19720. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33776066
Train loss (w/o reg) on all data: 0.32618582
Test loss (w/o reg) on all data: 0.20307826
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.236674e-06
Norm of the params: 15.215015
                Loss: fixed 204 labels. Loss 0.20308. Accuracy 0.954.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.441239
Train loss (w/o reg) on all data: 0.43603933
Test loss (w/o reg) on all data: 0.24722494
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.1320475e-05
Norm of the params: 10.197714
              Random: fixed  42 labels. Loss 0.24722. Accuracy 0.975.
### Flips: 1025, rs: 19, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3143834
Train loss (w/o reg) on all data: 0.30420604
Test loss (w/o reg) on all data: 0.15594102
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2510261e-05
Norm of the params: 14.266977
     Influence (LOO): fixed 303 labels. Loss 0.15594. Accuracy 0.990.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23514712
Train loss (w/o reg) on all data: 0.21801051
Test loss (w/o reg) on all data: 0.15602697
Train acc on all data:  0.8952103087770484
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 5.3189196e-06
Norm of the params: 18.513023
                Loss: fixed 409 labels. Loss 0.15603. Accuracy 0.952.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4282373
Train loss (w/o reg) on all data: 0.42283016
Test loss (w/o reg) on all data: 0.2321981
Train acc on all data:  0.812302455628495
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.288717e-05
Norm of the params: 10.399152
              Random: fixed  89 labels. Loss 0.23220. Accuracy 0.982.
### Flips: 1025, rs: 19, checks: 615
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26464653
Train loss (w/o reg) on all data: 0.2533173
Test loss (w/o reg) on all data: 0.12927097
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.144095e-06
Norm of the params: 15.052725
     Influence (LOO): fixed 408 labels. Loss 0.12927. Accuracy 0.988.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13418792
Train loss (w/o reg) on all data: 0.114423074
Test loss (w/o reg) on all data: 0.08378211
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.1214e-06
Norm of the params: 19.882074
                Loss: fixed 608 labels. Loss 0.08378. Accuracy 0.970.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4147871
Train loss (w/o reg) on all data: 0.4091371
Test loss (w/o reg) on all data: 0.21985453
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.7827896e-05
Norm of the params: 10.630151
              Random: fixed 134 labels. Loss 0.21985. Accuracy 0.983.
### Flips: 1025, rs: 19, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22348022
Train loss (w/o reg) on all data: 0.21227664
Test loss (w/o reg) on all data: 0.1028849
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.372843e-06
Norm of the params: 14.969027
     Influence (LOO): fixed 493 labels. Loss 0.10288. Accuracy 0.995.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045842923
Train loss (w/o reg) on all data: 0.031647634
Test loss (w/o reg) on all data: 0.029324692
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.3218055e-06
Norm of the params: 16.849504
                Loss: fixed 791 labels. Loss 0.02932. Accuracy 0.989.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39885244
Train loss (w/o reg) on all data: 0.39295417
Test loss (w/o reg) on all data: 0.20513785
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.914158e-06
Norm of the params: 10.861179
              Random: fixed 179 labels. Loss 0.20514. Accuracy 0.983.
### Flips: 1025, rs: 19, checks: 1025
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864627
Train loss (w/o reg) on all data: 0.17849195
Test loss (w/o reg) on all data: 0.084811114
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.969912e-05
Norm of the params: 14.250835
     Influence (LOO): fixed 562 labels. Loss 0.08481. Accuracy 0.997.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017918944
Train loss (w/o reg) on all data: 0.010635327
Test loss (w/o reg) on all data: 0.006929225
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3012907e-07
Norm of the params: 12.06948
                Loss: fixed 854 labels. Loss 0.00693. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38462192
Train loss (w/o reg) on all data: 0.37862527
Test loss (w/o reg) on all data: 0.19044143
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.4666646e-05
Norm of the params: 10.951379
              Random: fixed 219 labels. Loss 0.19044. Accuracy 0.985.
### Flips: 1025, rs: 19, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15327477
Train loss (w/o reg) on all data: 0.14328615
Test loss (w/o reg) on all data: 0.06910455
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.2322186e-06
Norm of the params: 14.134085
     Influence (LOO): fixed 624 labels. Loss 0.06910. Accuracy 0.998.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010347586
Train loss (w/o reg) on all data: 0.0053877453
Test loss (w/o reg) on all data: 0.0042151813
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 8.587097e-08
Norm of the params: 9.959761
                Loss: fixed 865 labels. Loss 0.00422. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36744273
Train loss (w/o reg) on all data: 0.3611915
Test loss (w/o reg) on all data: 0.17694746
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.648843e-06
Norm of the params: 11.181429
              Random: fixed 265 labels. Loss 0.17695. Accuracy 0.988.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4482012
Train loss (w/o reg) on all data: 0.44268134
Test loss (w/o reg) on all data: 0.26756346
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.2179357e-05
Norm of the params: 10.507021
Flipped loss: 0.26756. Accuracy: 0.957
### Flips: 1025, rs: 20, checks: 205
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3778715
Train loss (w/o reg) on all data: 0.36831608
Test loss (w/o reg) on all data: 0.21543477
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.964154e-05
Norm of the params: 13.824209
     Influence (LOO): fixed 165 labels. Loss 0.21543. Accuracy 0.972.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33696982
Train loss (w/o reg) on all data: 0.3257069
Test loss (w/o reg) on all data: 0.21484326
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 3.9168404e-05
Norm of the params: 15.008612
                Loss: fixed 205 labels. Loss 0.21484. Accuracy 0.938.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43549517
Train loss (w/o reg) on all data: 0.42961398
Test loss (w/o reg) on all data: 0.25077817
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.345523e-05
Norm of the params: 10.845445
              Random: fixed  45 labels. Loss 0.25078. Accuracy 0.965.
### Flips: 1025, rs: 20, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32376012
Train loss (w/o reg) on all data: 0.31337208
Test loss (w/o reg) on all data: 0.16709465
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1961424e-05
Norm of the params: 14.4139185
     Influence (LOO): fixed 296 labels. Loss 0.16709. Accuracy 0.984.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23289675
Train loss (w/o reg) on all data: 0.21719769
Test loss (w/o reg) on all data: 0.16338019
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 9.4378165e-06
Norm of the params: 17.719519
                Loss: fixed 408 labels. Loss 0.16338. Accuracy 0.948.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42291296
Train loss (w/o reg) on all data: 0.41683412
Test loss (w/o reg) on all data: 0.23925382
Train acc on all data:  0.8135181132992949
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8120433e-05
Norm of the params: 11.026197
              Random: fixed  88 labels. Loss 0.23925. Accuracy 0.967.
### Flips: 1025, rs: 20, checks: 615
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27361792
Train loss (w/o reg) on all data: 0.2630944
Test loss (w/o reg) on all data: 0.134885
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4649352e-05
Norm of the params: 14.507605
     Influence (LOO): fixed 405 labels. Loss 0.13488. Accuracy 0.990.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12834178
Train loss (w/o reg) on all data: 0.10845916
Test loss (w/o reg) on all data: 0.103540055
Train acc on all data:  0.9545344031120836
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.1557169e-06
Norm of the params: 19.941223
                Loss: fixed 609 labels. Loss 0.10354. Accuracy 0.967.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4098097
Train loss (w/o reg) on all data: 0.40365425
Test loss (w/o reg) on all data: 0.22444472
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.0676594e-05
Norm of the params: 11.095466
              Random: fixed 133 labels. Loss 0.22444. Accuracy 0.975.
### Flips: 1025, rs: 20, checks: 820
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22576025
Train loss (w/o reg) on all data: 0.21495828
Test loss (w/o reg) on all data: 0.11074521
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7974588e-05
Norm of the params: 14.6982765
     Influence (LOO): fixed 499 labels. Loss 0.11075. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05296564
Train loss (w/o reg) on all data: 0.037365526
Test loss (w/o reg) on all data: 0.03744796
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7492774e-06
Norm of the params: 17.663586
                Loss: fixed 773 labels. Loss 0.03745. Accuracy 0.990.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39814603
Train loss (w/o reg) on all data: 0.39193526
Test loss (w/o reg) on all data: 0.21313941
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6002192e-05
Norm of the params: 11.145205
              Random: fixed 167 labels. Loss 0.21314. Accuracy 0.973.
### Flips: 1025, rs: 20, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18667577
Train loss (w/o reg) on all data: 0.17590983
Test loss (w/o reg) on all data: 0.08812589
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.942851e-06
Norm of the params: 14.673747
     Influence (LOO): fixed 574 labels. Loss 0.08813. Accuracy 0.998.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027025394
Train loss (w/o reg) on all data: 0.016796073
Test loss (w/o reg) on all data: 0.012316063
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.35763e-07
Norm of the params: 14.30337
                Loss: fixed 837 labels. Loss 0.01232. Accuracy 0.998.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38519225
Train loss (w/o reg) on all data: 0.378831
Test loss (w/o reg) on all data: 0.20072138
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.7870114e-05
Norm of the params: 11.279406
              Random: fixed 205 labels. Loss 0.20072. Accuracy 0.975.
### Flips: 1025, rs: 20, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15832204
Train loss (w/o reg) on all data: 0.14777866
Test loss (w/o reg) on all data: 0.07144405
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.004599e-06
Norm of the params: 14.521281
     Influence (LOO): fixed 629 labels. Loss 0.07144. Accuracy 0.999.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01506971
Train loss (w/o reg) on all data: 0.007901257
Test loss (w/o reg) on all data: 0.0069428165
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6049474e-07
Norm of the params: 11.973682
                Loss: fixed 863 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37198117
Train loss (w/o reg) on all data: 0.36555338
Test loss (w/o reg) on all data: 0.18677305
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0172004e-05
Norm of the params: 11.338246
              Random: fixed 247 labels. Loss 0.18677. Accuracy 0.983.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.457394
Train loss (w/o reg) on all data: 0.4519908
Test loss (w/o reg) on all data: 0.27977362
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.5506763e-05
Norm of the params: 10.395388
Flipped loss: 0.27977. Accuracy: 0.961
### Flips: 1025, rs: 21, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38254863
Train loss (w/o reg) on all data: 0.37300345
Test loss (w/o reg) on all data: 0.22250308
Train acc on all data:  0.8293216630196937
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 9.578038e-06
Norm of the params: 13.81678
     Influence (LOO): fixed 167 labels. Loss 0.22250. Accuracy 0.973.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3406743
Train loss (w/o reg) on all data: 0.32783562
Test loss (w/o reg) on all data: 0.23744819
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 3.1966065e-05
Norm of the params: 16.024166
                Loss: fixed 204 labels. Loss 0.23745. Accuracy 0.925.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4458603
Train loss (w/o reg) on all data: 0.44024947
Test loss (w/o reg) on all data: 0.2670555
Train acc on all data:  0.8001458789204959
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 4.619061e-05
Norm of the params: 10.593231
              Random: fixed  42 labels. Loss 0.26706. Accuracy 0.965.
### Flips: 1025, rs: 21, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32285902
Train loss (w/o reg) on all data: 0.31142256
Test loss (w/o reg) on all data: 0.17710733
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.657779e-06
Norm of the params: 15.123801
     Influence (LOO): fixed 302 labels. Loss 0.17711. Accuracy 0.983.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24187572
Train loss (w/o reg) on all data: 0.22455394
Test loss (w/o reg) on all data: 0.17025943
Train acc on all data:  0.8947240457087284
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 1.2391139e-05
Norm of the params: 18.612783
                Loss: fixed 407 labels. Loss 0.17026. Accuracy 0.942.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4346554
Train loss (w/o reg) on all data: 0.42899293
Test loss (w/o reg) on all data: 0.25170347
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.2284617e-05
Norm of the params: 10.641863
              Random: fixed  86 labels. Loss 0.25170. Accuracy 0.972.
### Flips: 1025, rs: 21, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2789208
Train loss (w/o reg) on all data: 0.26694378
Test loss (w/o reg) on all data: 0.15083922
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.9856727e-05
Norm of the params: 15.477084
     Influence (LOO): fixed 400 labels. Loss 0.15084. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14398277
Train loss (w/o reg) on all data: 0.12371153
Test loss (w/o reg) on all data: 0.09967751
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.416016e-06
Norm of the params: 20.135159
                Loss: fixed 610 labels. Loss 0.09968. Accuracy 0.969.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41938278
Train loss (w/o reg) on all data: 0.41344562
Test loss (w/o reg) on all data: 0.23739731
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.256727e-05
Norm of the params: 10.896944
              Random: fixed 134 labels. Loss 0.23740. Accuracy 0.975.
### Flips: 1025, rs: 21, checks: 820
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773022
Train loss (w/o reg) on all data: 0.22589456
Test loss (w/o reg) on all data: 0.12057792
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2145192e-05
Norm of the params: 15.385491
     Influence (LOO): fixed 494 labels. Loss 0.12058. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05845254
Train loss (w/o reg) on all data: 0.04203002
Test loss (w/o reg) on all data: 0.037921052
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.3798059e-06
Norm of the params: 18.123198
                Loss: fixed 787 labels. Loss 0.03792. Accuracy 0.987.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40787506
Train loss (w/o reg) on all data: 0.4019208
Test loss (w/o reg) on all data: 0.22128226
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.6899387e-05
Norm of the params: 10.912622
              Random: fixed 175 labels. Loss 0.22128. Accuracy 0.979.
### Flips: 1025, rs: 21, checks: 1025
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19619827
Train loss (w/o reg) on all data: 0.18451492
Test loss (w/o reg) on all data: 0.09646781
Train acc on all data:  0.9212253829321663
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.014776e-06
Norm of the params: 15.286165
     Influence (LOO): fixed 573 labels. Loss 0.09647. Accuracy 0.993.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024493326
Train loss (w/o reg) on all data: 0.014707129
Test loss (w/o reg) on all data: 0.013652317
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.772997e-07
Norm of the params: 13.990137
                Loss: fixed 858 labels. Loss 0.01365. Accuracy 0.997.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39492902
Train loss (w/o reg) on all data: 0.3892212
Test loss (w/o reg) on all data: 0.20945695
Train acc on all data:  0.8368587405786531
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2894676e-05
Norm of the params: 10.684406
              Random: fixed 214 labels. Loss 0.20946. Accuracy 0.983.
### Flips: 1025, rs: 21, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167008
Train loss (w/o reg) on all data: 0.15616827
Test loss (w/o reg) on all data: 0.07999391
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.118193e-06
Norm of the params: 14.723943
     Influence (LOO): fixed 631 labels. Loss 0.07999. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018418279
Train loss (w/o reg) on all data: 0.010493911
Test loss (w/o reg) on all data: 0.008312402
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 3.036382e-06
Norm of the params: 12.589175
                Loss: fixed 874 labels. Loss 0.00831. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37814373
Train loss (w/o reg) on all data: 0.37241516
Test loss (w/o reg) on all data: 0.19534497
Train acc on all data:  0.8473133965475322
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.685841e-06
Norm of the params: 10.703812
              Random: fixed 260 labels. Loss 0.19534. Accuracy 0.983.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4574934
Train loss (w/o reg) on all data: 0.45228183
Test loss (w/o reg) on all data: 0.26814216
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2005291e-05
Norm of the params: 10.209359
Flipped loss: 0.26814. Accuracy: 0.976
### Flips: 1025, rs: 22, checks: 205
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38142058
Train loss (w/o reg) on all data: 0.3712625
Test loss (w/o reg) on all data: 0.20994608
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.6273184e-05
Norm of the params: 14.253485
     Influence (LOO): fixed 169 labels. Loss 0.20995. Accuracy 0.973.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34561422
Train loss (w/o reg) on all data: 0.33352485
Test loss (w/o reg) on all data: 0.21480799
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 3.8324513e-05
Norm of the params: 15.549521
                Loss: fixed 205 labels. Loss 0.21481. Accuracy 0.947.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44546258
Train loss (w/o reg) on all data: 0.440107
Test loss (w/o reg) on all data: 0.25361636
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.0346156e-05
Norm of the params: 10.349487
              Random: fixed  44 labels. Loss 0.25362. Accuracy 0.974.
### Flips: 1025, rs: 22, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32051337
Train loss (w/o reg) on all data: 0.3086564
Test loss (w/o reg) on all data: 0.1704361
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.82828e-06
Norm of the params: 15.399332
     Influence (LOO): fixed 309 labels. Loss 0.17044. Accuracy 0.981.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24138877
Train loss (w/o reg) on all data: 0.2259
Test loss (w/o reg) on all data: 0.16370656
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.0214381e-05
Norm of the params: 17.600445
                Loss: fixed 409 labels. Loss 0.16371. Accuracy 0.945.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43221498
Train loss (w/o reg) on all data: 0.4267491
Test loss (w/o reg) on all data: 0.23878717
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0379993e-05
Norm of the params: 10.455489
              Random: fixed  92 labels. Loss 0.23879. Accuracy 0.976.
### Flips: 1025, rs: 22, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27220225
Train loss (w/o reg) on all data: 0.2595248
Test loss (w/o reg) on all data: 0.13738142
Train acc on all data:  0.888645757354729
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.839097e-06
Norm of the params: 15.923238
     Influence (LOO): fixed 414 labels. Loss 0.13738. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13862363
Train loss (w/o reg) on all data: 0.12026225
Test loss (w/o reg) on all data: 0.09232742
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.803825e-06
Norm of the params: 19.163176
                Loss: fixed 613 labels. Loss 0.09233. Accuracy 0.970.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4135188
Train loss (w/o reg) on all data: 0.4077887
Test loss (w/o reg) on all data: 0.22153586
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1508473e-05
Norm of the params: 10.70523
              Random: fixed 150 labels. Loss 0.22154. Accuracy 0.979.
### Flips: 1025, rs: 22, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23470119
Train loss (w/o reg) on all data: 0.22270454
Test loss (w/o reg) on all data: 0.11630609
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2210032e-05
Norm of the params: 15.489766
     Influence (LOO): fixed 489 labels. Loss 0.11631. Accuracy 0.994.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050101835
Train loss (w/o reg) on all data: 0.03586258
Test loss (w/o reg) on all data: 0.030677678
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.113209e-06
Norm of the params: 16.875576
                Loss: fixed 796 labels. Loss 0.03068. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40072608
Train loss (w/o reg) on all data: 0.39492518
Test loss (w/o reg) on all data: 0.20801279
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.3818207e-05
Norm of the params: 10.771169
              Random: fixed 192 labels. Loss 0.20801. Accuracy 0.984.
### Flips: 1025, rs: 22, checks: 1025
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19821818
Train loss (w/o reg) on all data: 0.18694563
Test loss (w/o reg) on all data: 0.09154862
Train acc on all data:  0.9214685144663263
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.259518e-06
Norm of the params: 15.015023
     Influence (LOO): fixed 563 labels. Loss 0.09155. Accuracy 0.998.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011289705
Train loss (w/o reg) on all data: 0.0058989283
Test loss (w/o reg) on all data: 0.0066110194
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8899274e-07
Norm of the params: 10.383426
                Loss: fixed 870 labels. Loss 0.00661. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38725913
Train loss (w/o reg) on all data: 0.381241
Test loss (w/o reg) on all data: 0.19596112
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.9677866e-05
Norm of the params: 10.971002
              Random: fixed 229 labels. Loss 0.19596. Accuracy 0.986.
### Flips: 1025, rs: 22, checks: 1230
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16387987
Train loss (w/o reg) on all data: 0.15360941
Test loss (w/o reg) on all data: 0.07449488
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0556407e-06
Norm of the params: 14.332103
     Influence (LOO): fixed 621 labels. Loss 0.07449. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008192946
Train loss (w/o reg) on all data: 0.0039579016
Test loss (w/o reg) on all data: 0.00586431
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2704268e-07
Norm of the params: 9.203309
                Loss: fixed 875 labels. Loss 0.00586. Accuracy 0.999.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3677604
Train loss (w/o reg) on all data: 0.36163273
Test loss (w/o reg) on all data: 0.18075357
Train acc on all data:  0.8563092633114515
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.1550855e-05
Norm of the params: 11.070366
              Random: fixed 280 labels. Loss 0.18075. Accuracy 0.988.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4551509
Train loss (w/o reg) on all data: 0.4495559
Test loss (w/o reg) on all data: 0.26728356
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 6.241033e-05
Norm of the params: 10.57828
Flipped loss: 0.26728. Accuracy: 0.962
### Flips: 1025, rs: 23, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37867603
Train loss (w/o reg) on all data: 0.36848882
Test loss (w/o reg) on all data: 0.21432056
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.860136e-06
Norm of the params: 14.273904
     Influence (LOO): fixed 176 labels. Loss 0.21432. Accuracy 0.978.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3481109
Train loss (w/o reg) on all data: 0.3360983
Test loss (w/o reg) on all data: 0.21534225
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 8.094193e-06
Norm of the params: 15.500059
                Loss: fixed 204 labels. Loss 0.21534. Accuracy 0.946.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44642362
Train loss (w/o reg) on all data: 0.4406748
Test loss (w/o reg) on all data: 0.25589022
Train acc on all data:  0.798930221249696
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.4373006e-05
Norm of the params: 10.722686
              Random: fixed  38 labels. Loss 0.25589. Accuracy 0.967.
### Flips: 1025, rs: 23, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32614213
Train loss (w/o reg) on all data: 0.31525835
Test loss (w/o reg) on all data: 0.17648256
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.2070939e-05
Norm of the params: 14.753834
     Influence (LOO): fixed 299 labels. Loss 0.17648. Accuracy 0.982.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24512556
Train loss (w/o reg) on all data: 0.22849241
Test loss (w/o reg) on all data: 0.15568282
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.6653903e-05
Norm of the params: 18.239048
                Loss: fixed 409 labels. Loss 0.15568. Accuracy 0.959.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4343516
Train loss (w/o reg) on all data: 0.4284002
Test loss (w/o reg) on all data: 0.24338758
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.8700599e-05
Norm of the params: 10.909994
              Random: fixed  81 labels. Loss 0.24339. Accuracy 0.970.
### Flips: 1025, rs: 23, checks: 615
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27624366
Train loss (w/o reg) on all data: 0.26493493
Test loss (w/o reg) on all data: 0.14204209
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.7774115e-05
Norm of the params: 15.039107
     Influence (LOO): fixed 407 labels. Loss 0.14204. Accuracy 0.989.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14390928
Train loss (w/o reg) on all data: 0.12462291
Test loss (w/o reg) on all data: 0.08964045
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.742522e-06
Norm of the params: 19.639942
                Loss: fixed 610 labels. Loss 0.08964. Accuracy 0.970.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42219153
Train loss (w/o reg) on all data: 0.4161876
Test loss (w/o reg) on all data: 0.2286194
Train acc on all data:  0.8154631655725748
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2560128e-05
Norm of the params: 10.958026
              Random: fixed 125 labels. Loss 0.22862. Accuracy 0.978.
### Flips: 1025, rs: 23, checks: 820
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23128583
Train loss (w/o reg) on all data: 0.22008516
Test loss (w/o reg) on all data: 0.113088734
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.324163e-06
Norm of the params: 14.967072
     Influence (LOO): fixed 502 labels. Loss 0.11309. Accuracy 0.993.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060881846
Train loss (w/o reg) on all data: 0.045259185
Test loss (w/o reg) on all data: 0.035350773
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0618354e-06
Norm of the params: 17.676348
                Loss: fixed 778 labels. Loss 0.03535. Accuracy 0.990.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40782982
Train loss (w/o reg) on all data: 0.40196002
Test loss (w/o reg) on all data: 0.21316876
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.014551e-05
Norm of the params: 10.834961
              Random: fixed 173 labels. Loss 0.21317. Accuracy 0.980.
### Flips: 1025, rs: 23, checks: 1025
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19145
Train loss (w/o reg) on all data: 0.18102504
Test loss (w/o reg) on all data: 0.08592858
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.0729365e-06
Norm of the params: 14.4395
     Influence (LOO): fixed 578 labels. Loss 0.08593. Accuracy 0.995.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020749543
Train loss (w/o reg) on all data: 0.011651804
Test loss (w/o reg) on all data: 0.0108534815
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.653916e-07
Norm of the params: 13.48906
                Loss: fixed 855 labels. Loss 0.01085. Accuracy 0.998.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39355057
Train loss (w/o reg) on all data: 0.38758713
Test loss (w/o reg) on all data: 0.1953058
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2278698e-05
Norm of the params: 10.921027
              Random: fixed 218 labels. Loss 0.19531. Accuracy 0.988.
### Flips: 1025, rs: 23, checks: 1230
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16184191
Train loss (w/o reg) on all data: 0.15194209
Test loss (w/o reg) on all data: 0.07230654
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1323317e-05
Norm of the params: 14.071123
     Influence (LOO): fixed 633 labels. Loss 0.07231. Accuracy 0.998.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0147462115
Train loss (w/o reg) on all data: 0.007643876
Test loss (w/o reg) on all data: 0.006668423
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0190614e-06
Norm of the params: 11.918336
                Loss: fixed 870 labels. Loss 0.00667. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37851107
Train loss (w/o reg) on all data: 0.37253976
Test loss (w/o reg) on all data: 0.18126588
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.3020773e-05
Norm of the params: 10.92824
              Random: fixed 262 labels. Loss 0.18127. Accuracy 0.988.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45264798
Train loss (w/o reg) on all data: 0.4466591
Test loss (w/o reg) on all data: 0.27235547
Train acc on all data:  0.7938244590323364
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.0201767e-05
Norm of the params: 10.944298
Flipped loss: 0.27236. Accuracy: 0.966
### Flips: 1025, rs: 24, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37472293
Train loss (w/o reg) on all data: 0.36483902
Test loss (w/o reg) on all data: 0.21859153
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.097397e-05
Norm of the params: 14.059807
     Influence (LOO): fixed 167 labels. Loss 0.21859. Accuracy 0.974.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34267315
Train loss (w/o reg) on all data: 0.33050296
Test loss (w/o reg) on all data: 0.22300684
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 7.5196267e-06
Norm of the params: 15.601416
                Loss: fixed 205 labels. Loss 0.22301. Accuracy 0.939.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44163868
Train loss (w/o reg) on all data: 0.43571877
Test loss (w/o reg) on all data: 0.25599086
Train acc on all data:  0.8040359834670556
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.0340569e-05
Norm of the params: 10.881094
              Random: fixed  44 labels. Loss 0.25599. Accuracy 0.971.
### Flips: 1025, rs: 24, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3211674
Train loss (w/o reg) on all data: 0.30995464
Test loss (w/o reg) on all data: 0.1694673
Train acc on all data:  0.862630683199611
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1562252e-05
Norm of the params: 14.975161
     Influence (LOO): fixed 303 labels. Loss 0.16947. Accuracy 0.989.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23901959
Train loss (w/o reg) on all data: 0.22201775
Test loss (w/o reg) on all data: 0.16363695
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 4.49953e-06
Norm of the params: 18.440086
                Loss: fixed 410 labels. Loss 0.16364. Accuracy 0.950.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42807835
Train loss (w/o reg) on all data: 0.42207363
Test loss (w/o reg) on all data: 0.23760091
Train acc on all data:  0.8157062971067347
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.8153188e-05
Norm of the params: 10.958761
              Random: fixed  93 labels. Loss 0.23760. Accuracy 0.978.
### Flips: 1025, rs: 24, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26452324
Train loss (w/o reg) on all data: 0.25288662
Test loss (w/o reg) on all data: 0.1388754
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.8081954e-06
Norm of the params: 15.25556
     Influence (LOO): fixed 421 labels. Loss 0.13888. Accuracy 0.991.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13666208
Train loss (w/o reg) on all data: 0.117848255
Test loss (w/o reg) on all data: 0.09718825
Train acc on all data:  0.949428640894724
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.0454907e-05
Norm of the params: 19.397848
                Loss: fixed 614 labels. Loss 0.09719. Accuracy 0.970.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4173519
Train loss (w/o reg) on all data: 0.4113016
Test loss (w/o reg) on all data: 0.22616984
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.7730323e-05
Norm of the params: 11.000271
              Random: fixed 131 labels. Loss 0.22617. Accuracy 0.979.
### Flips: 1025, rs: 24, checks: 820
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21997157
Train loss (w/o reg) on all data: 0.20839013
Test loss (w/o reg) on all data: 0.11004566
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3409417e-06
Norm of the params: 15.219355
     Influence (LOO): fixed 509 labels. Loss 0.11005. Accuracy 0.996.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05188679
Train loss (w/o reg) on all data: 0.036661725
Test loss (w/o reg) on all data: 0.03119034
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8740986e-06
Norm of the params: 17.449965
                Loss: fixed 791 labels. Loss 0.03119. Accuracy 0.994.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4004843
Train loss (w/o reg) on all data: 0.39412323
Test loss (w/o reg) on all data: 0.2154727
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.9770941e-05
Norm of the params: 11.279236
              Random: fixed 180 labels. Loss 0.21547. Accuracy 0.983.
### Flips: 1025, rs: 24, checks: 1025
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1928452
Train loss (w/o reg) on all data: 0.1817233
Test loss (w/o reg) on all data: 0.09605089
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6707738e-05
Norm of the params: 14.91435
     Influence (LOO): fixed 563 labels. Loss 0.09605. Accuracy 0.997.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021883056
Train loss (w/o reg) on all data: 0.012225456
Test loss (w/o reg) on all data: 0.010827803
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.7767016e-07
Norm of the params: 13.897914
                Loss: fixed 849 labels. Loss 0.01083. Accuracy 0.999.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39101738
Train loss (w/o reg) on all data: 0.3846063
Test loss (w/o reg) on all data: 0.2035948
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.949175e-06
Norm of the params: 11.323486
              Random: fixed 210 labels. Loss 0.20359. Accuracy 0.984.
### Flips: 1025, rs: 24, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16363741
Train loss (w/o reg) on all data: 0.15253991
Test loss (w/o reg) on all data: 0.07964786
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0350119e-05
Norm of the params: 14.897993
     Influence (LOO): fixed 619 labels. Loss 0.07965. Accuracy 0.998.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01192976
Train loss (w/o reg) on all data: 0.0059566554
Test loss (w/o reg) on all data: 0.005341676
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7098972e-07
Norm of the params: 10.929872
                Loss: fixed 869 labels. Loss 0.00534. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37734306
Train loss (w/o reg) on all data: 0.3708212
Test loss (w/o reg) on all data: 0.18997686
Train acc on all data:  0.849258448820812
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 5.2057967e-05
Norm of the params: 11.420914
              Random: fixed 248 labels. Loss 0.18998. Accuracy 0.985.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45621532
Train loss (w/o reg) on all data: 0.45064592
Test loss (w/o reg) on all data: 0.27837864
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.859613e-05
Norm of the params: 10.554052
Flipped loss: 0.27838. Accuracy: 0.954
### Flips: 1025, rs: 25, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38044953
Train loss (w/o reg) on all data: 0.37133127
Test loss (w/o reg) on all data: 0.22098954
Train acc on all data:  0.8288353999513737
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.7264392e-05
Norm of the params: 13.504274
     Influence (LOO): fixed 163 labels. Loss 0.22099. Accuracy 0.971.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34621617
Train loss (w/o reg) on all data: 0.3346974
Test loss (w/o reg) on all data: 0.23706912
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.1359048e-05
Norm of the params: 15.178131
                Loss: fixed 205 labels. Loss 0.23707. Accuracy 0.925.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44524756
Train loss (w/o reg) on all data: 0.4396174
Test loss (w/o reg) on all data: 0.26389635
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.4746724e-05
Norm of the params: 10.611481
              Random: fixed  40 labels. Loss 0.26390. Accuracy 0.966.
### Flips: 1025, rs: 25, checks: 410
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3249725
Train loss (w/o reg) on all data: 0.31479216
Test loss (w/o reg) on all data: 0.18024515
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.3717373e-05
Norm of the params: 14.269104
     Influence (LOO): fixed 294 labels. Loss 0.18025. Accuracy 0.980.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24336469
Train loss (w/o reg) on all data: 0.22735888
Test loss (w/o reg) on all data: 0.18239681
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 8.954279e-06
Norm of the params: 17.891792
                Loss: fixed 409 labels. Loss 0.18240. Accuracy 0.939.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43114766
Train loss (w/o reg) on all data: 0.42526758
Test loss (w/o reg) on all data: 0.24957174
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.848065e-05
Norm of the params: 10.844428
              Random: fixed  85 labels. Loss 0.24957. Accuracy 0.967.
### Flips: 1025, rs: 25, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2742975
Train loss (w/o reg) on all data: 0.26305807
Test loss (w/o reg) on all data: 0.14255333
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.7152987e-06
Norm of the params: 14.992967
     Influence (LOO): fixed 409 labels. Loss 0.14255. Accuracy 0.985.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14309219
Train loss (w/o reg) on all data: 0.12272564
Test loss (w/o reg) on all data: 0.11308082
Train acc on all data:  0.9482129832239241
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 7.788208e-06
Norm of the params: 20.182438
                Loss: fixed 609 labels. Loss 0.11308. Accuracy 0.960.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41600204
Train loss (w/o reg) on all data: 0.40993774
Test loss (w/o reg) on all data: 0.23548357
Train acc on all data:  0.8198395331874544
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.5733998e-05
Norm of the params: 11.0130005
              Random: fixed 130 labels. Loss 0.23548. Accuracy 0.968.
### Flips: 1025, rs: 25, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23705573
Train loss (w/o reg) on all data: 0.22659506
Test loss (w/o reg) on all data: 0.12224157
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.4236337e-06
Norm of the params: 14.464211
     Influence (LOO): fixed 483 labels. Loss 0.12224. Accuracy 0.987.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053725004
Train loss (w/o reg) on all data: 0.03758747
Test loss (w/o reg) on all data: 0.040740903
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.322568e-06
Norm of the params: 17.965263
                Loss: fixed 792 labels. Loss 0.04074. Accuracy 0.991.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4030861
Train loss (w/o reg) on all data: 0.3969724
Test loss (w/o reg) on all data: 0.21891075
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.396128e-05
Norm of the params: 11.05776
              Random: fixed 174 labels. Loss 0.21891. Accuracy 0.975.
### Flips: 1025, rs: 25, checks: 1025
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19474241
Train loss (w/o reg) on all data: 0.18549356
Test loss (w/o reg) on all data: 0.09312153
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.158279e-06
Norm of the params: 13.600629
     Influence (LOO): fixed 572 labels. Loss 0.09312. Accuracy 0.995.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020416234
Train loss (w/o reg) on all data: 0.011401596
Test loss (w/o reg) on all data: 0.013877205
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5154069e-06
Norm of the params: 13.427313
                Loss: fixed 859 labels. Loss 0.01388. Accuracy 0.997.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38801953
Train loss (w/o reg) on all data: 0.38171238
Test loss (w/o reg) on all data: 0.20649925
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.925354e-05
Norm of the params: 11.23135
              Random: fixed 213 labels. Loss 0.20650. Accuracy 0.975.
### Flips: 1025, rs: 25, checks: 1230
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15755783
Train loss (w/o reg) on all data: 0.14846511
Test loss (w/o reg) on all data: 0.07479096
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.5965476e-06
Norm of the params: 13.485336
     Influence (LOO): fixed 639 labels. Loss 0.07479. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011839092
Train loss (w/o reg) on all data: 0.0056333714
Test loss (w/o reg) on all data: 0.007724261
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7550104e-07
Norm of the params: 11.140665
                Loss: fixed 873 labels. Loss 0.00772. Accuracy 0.998.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37068257
Train loss (w/o reg) on all data: 0.36429557
Test loss (w/o reg) on all data: 0.19116531
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.0866511e-05
Norm of the params: 11.302217
              Random: fixed 264 labels. Loss 0.19117. Accuracy 0.976.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4490468
Train loss (w/o reg) on all data: 0.44262215
Test loss (w/o reg) on all data: 0.27471882
Train acc on all data:  0.7957695113056164
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.1461046e-05
Norm of the params: 11.335452
Flipped loss: 0.27472. Accuracy: 0.953
### Flips: 1025, rs: 26, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37749302
Train loss (w/o reg) on all data: 0.3672436
Test loss (w/o reg) on all data: 0.22848897
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.429743e-06
Norm of the params: 14.317423
     Influence (LOO): fixed 159 labels. Loss 0.22849. Accuracy 0.960.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33634007
Train loss (w/o reg) on all data: 0.322732
Test loss (w/o reg) on all data: 0.22695386
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9319727891156463
Norm of the mean of gradients: 2.1126325e-05
Norm of the params: 16.497309
                Loss: fixed 205 labels. Loss 0.22695. Accuracy 0.932.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43682522
Train loss (w/o reg) on all data: 0.43020993
Test loss (w/o reg) on all data: 0.25888675
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 2.7687356e-05
Norm of the params: 11.502424
              Random: fixed  46 labels. Loss 0.25889. Accuracy 0.961.
### Flips: 1025, rs: 26, checks: 410
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32374486
Train loss (w/o reg) on all data: 0.31245774
Test loss (w/o reg) on all data: 0.17918707
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.1086208e-05
Norm of the params: 15.024725
     Influence (LOO): fixed 295 labels. Loss 0.17919. Accuracy 0.981.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2384246
Train loss (w/o reg) on all data: 0.22024179
Test loss (w/o reg) on all data: 0.16956604
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 7.4841155e-06
Norm of the params: 19.069769
                Loss: fixed 407 labels. Loss 0.16957. Accuracy 0.943.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42108056
Train loss (w/o reg) on all data: 0.41430372
Test loss (w/o reg) on all data: 0.24227649
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.336508e-06
Norm of the params: 11.642024
              Random: fixed 101 labels. Loss 0.24228. Accuracy 0.966.
### Flips: 1025, rs: 26, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27301076
Train loss (w/o reg) on all data: 0.2617169
Test loss (w/o reg) on all data: 0.14554726
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1197393e-05
Norm of the params: 15.029219
     Influence (LOO): fixed 403 labels. Loss 0.14555. Accuracy 0.987.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13938504
Train loss (w/o reg) on all data: 0.11841672
Test loss (w/o reg) on all data: 0.10245044
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.555228e-06
Norm of the params: 20.478437
                Loss: fixed 610 labels. Loss 0.10245. Accuracy 0.963.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4100583
Train loss (w/o reg) on all data: 0.40318972
Test loss (w/o reg) on all data: 0.22847112
Train acc on all data:  0.824945295404814
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.3153654e-05
Norm of the params: 11.7205515
              Random: fixed 140 labels. Loss 0.22847. Accuracy 0.968.
### Flips: 1025, rs: 26, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231764
Train loss (w/o reg) on all data: 0.22082753
Test loss (w/o reg) on all data: 0.115613535
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5781836e-05
Norm of the params: 14.789504
     Influence (LOO): fixed 496 labels. Loss 0.11561. Accuracy 0.991.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060720444
Train loss (w/o reg) on all data: 0.04384524
Test loss (w/o reg) on all data: 0.04208387
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.8723982e-06
Norm of the params: 18.371284
                Loss: fixed 780 labels. Loss 0.04208. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39583102
Train loss (w/o reg) on all data: 0.38866362
Test loss (w/o reg) on all data: 0.21679333
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.1681471e-05
Norm of the params: 11.972794
              Random: fixed 184 labels. Loss 0.21679. Accuracy 0.971.
### Flips: 1025, rs: 26, checks: 1025
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1907042
Train loss (w/o reg) on all data: 0.18015853
Test loss (w/o reg) on all data: 0.09275575
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7460632e-05
Norm of the params: 14.522863
     Influence (LOO): fixed 569 labels. Loss 0.09276. Accuracy 0.991.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029512152
Train loss (w/o reg) on all data: 0.019283554
Test loss (w/o reg) on all data: 0.021330275
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.984482e-07
Norm of the params: 14.302866
                Loss: fixed 842 labels. Loss 0.02133. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3828411
Train loss (w/o reg) on all data: 0.37554133
Test loss (w/o reg) on all data: 0.20169589
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.4034178e-05
Norm of the params: 12.082873
              Random: fixed 226 labels. Loss 0.20170. Accuracy 0.978.
### Flips: 1025, rs: 26, checks: 1230
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15342456
Train loss (w/o reg) on all data: 0.14379087
Test loss (w/o reg) on all data: 0.07241406
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.2885416e-06
Norm of the params: 13.880702
     Influence (LOO): fixed 635 labels. Loss 0.07241. Accuracy 0.995.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013027685
Train loss (w/o reg) on all data: 0.007183336
Test loss (w/o reg) on all data: 0.0102075795
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5246137e-07
Norm of the params: 10.811427
                Loss: fixed 872 labels. Loss 0.01021. Accuracy 0.998.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36805817
Train loss (w/o reg) on all data: 0.36053044
Test loss (w/o reg) on all data: 0.19132747
Train acc on all data:  0.8548504741064916
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.654108e-05
Norm of the params: 12.270064
              Random: fixed 267 labels. Loss 0.19133. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44958055
Train loss (w/o reg) on all data: 0.4439531
Test loss (w/o reg) on all data: 0.27525023
Train acc on all data:  0.7972283005105762
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 5.5762663e-05
Norm of the params: 10.608921
Flipped loss: 0.27525. Accuracy: 0.956
### Flips: 1025, rs: 27, checks: 205
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37934557
Train loss (w/o reg) on all data: 0.3698151
Test loss (w/o reg) on all data: 0.22363168
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3155122e-05
Norm of the params: 13.806135
     Influence (LOO): fixed 163 labels. Loss 0.22363. Accuracy 0.972.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33384535
Train loss (w/o reg) on all data: 0.32222646
Test loss (w/o reg) on all data: 0.23098354
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 9.187484e-06
Norm of the params: 15.243945
                Loss: fixed 205 labels. Loss 0.23098. Accuracy 0.928.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43749788
Train loss (w/o reg) on all data: 0.43169814
Test loss (w/o reg) on all data: 0.26258224
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.2734337e-05
Norm of the params: 10.7701
              Random: fixed  41 labels. Loss 0.26258. Accuracy 0.959.
### Flips: 1025, rs: 27, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32362124
Train loss (w/o reg) on all data: 0.31278446
Test loss (w/o reg) on all data: 0.17799567
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.57e-05
Norm of the params: 14.721947
     Influence (LOO): fixed 303 labels. Loss 0.17800. Accuracy 0.977.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23123011
Train loss (w/o reg) on all data: 0.21473525
Test loss (w/o reg) on all data: 0.17781074
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.6814352e-05
Norm of the params: 18.163067
                Loss: fixed 409 labels. Loss 0.17781. Accuracy 0.937.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4277896
Train loss (w/o reg) on all data: 0.42175686
Test loss (w/o reg) on all data: 0.24970533
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 0.0001073355
Norm of the params: 10.984297
              Random: fixed  75 labels. Loss 0.24971. Accuracy 0.962.
### Flips: 1025, rs: 27, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275273
Train loss (w/o reg) on all data: 0.26458207
Test loss (w/o reg) on all data: 0.1433001
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.153034e-05
Norm of the params: 14.622545
     Influence (LOO): fixed 413 labels. Loss 0.14330. Accuracy 0.985.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13344605
Train loss (w/o reg) on all data: 0.11382699
Test loss (w/o reg) on all data: 0.11371741
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 7.68649e-06
Norm of the params: 19.808613
                Loss: fixed 610 labels. Loss 0.11372. Accuracy 0.955.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4156409
Train loss (w/o reg) on all data: 0.4096436
Test loss (w/o reg) on all data: 0.23886417
Train acc on all data:  0.825188426938974
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.6652542e-05
Norm of the params: 10.951995
              Random: fixed 117 labels. Loss 0.23886. Accuracy 0.967.
### Flips: 1025, rs: 27, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22452095
Train loss (w/o reg) on all data: 0.21388438
Test loss (w/o reg) on all data: 0.107867286
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.089251e-06
Norm of the params: 14.585317
     Influence (LOO): fixed 516 labels. Loss 0.10787. Accuracy 0.993.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05310968
Train loss (w/o reg) on all data: 0.037791133
Test loss (w/o reg) on all data: 0.046841405
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.62954e-06
Norm of the params: 17.503454
                Loss: fixed 779 labels. Loss 0.04684. Accuracy 0.981.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40104356
Train loss (w/o reg) on all data: 0.3949051
Test loss (w/o reg) on all data: 0.22662903
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.8158287e-05
Norm of the params: 11.080146
              Random: fixed 165 labels. Loss 0.22663. Accuracy 0.968.
### Flips: 1025, rs: 27, checks: 1025
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18264869
Train loss (w/o reg) on all data: 0.17247202
Test loss (w/o reg) on all data: 0.08874439
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.0204195e-06
Norm of the params: 14.266516
     Influence (LOO): fixed 586 labels. Loss 0.08874. Accuracy 0.994.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022330888
Train loss (w/o reg) on all data: 0.012980679
Test loss (w/o reg) on all data: 0.018399768
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.240631e-06
Norm of the params: 13.674948
                Loss: fixed 840 labels. Loss 0.01840. Accuracy 0.996.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38728255
Train loss (w/o reg) on all data: 0.3809995
Test loss (w/o reg) on all data: 0.21395533
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.3667448e-05
Norm of the params: 11.209854
              Random: fixed 204 labels. Loss 0.21396. Accuracy 0.971.
### Flips: 1025, rs: 27, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14628448
Train loss (w/o reg) on all data: 0.13682002
Test loss (w/o reg) on all data: 0.07178673
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0490221e-05
Norm of the params: 13.758236
     Influence (LOO): fixed 652 labels. Loss 0.07179. Accuracy 0.994.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014806569
Train loss (w/o reg) on all data: 0.007559031
Test loss (w/o reg) on all data: 0.010929859
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.3311777e-07
Norm of the params: 12.039551
                Loss: fixed 858 labels. Loss 0.01093. Accuracy 0.998.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36798185
Train loss (w/o reg) on all data: 0.3616449
Test loss (w/o reg) on all data: 0.19860849
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.773768e-05
Norm of the params: 11.257837
              Random: fixed 258 labels. Loss 0.19861. Accuracy 0.971.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4540684
Train loss (w/o reg) on all data: 0.450153
Test loss (w/o reg) on all data: 0.2667503
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.9948584e-05
Norm of the params: 8.849171
Flipped loss: 0.26675. Accuracy: 0.964
### Flips: 1025, rs: 28, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37766838
Train loss (w/o reg) on all data: 0.3686985
Test loss (w/o reg) on all data: 0.2052502
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.968411e-05
Norm of the params: 13.393936
     Influence (LOO): fixed 174 labels. Loss 0.20525. Accuracy 0.977.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34018147
Train loss (w/o reg) on all data: 0.33023787
Test loss (w/o reg) on all data: 0.2128866
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.22532865e-05
Norm of the params: 14.102195
                Loss: fixed 204 labels. Loss 0.21289. Accuracy 0.938.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4419818
Train loss (w/o reg) on all data: 0.43779826
Test loss (w/o reg) on all data: 0.25550848
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.45969e-05
Norm of the params: 9.14716
              Random: fixed  41 labels. Loss 0.25551. Accuracy 0.963.
### Flips: 1025, rs: 28, checks: 410
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32015142
Train loss (w/o reg) on all data: 0.3100721
Test loss (w/o reg) on all data: 0.16666171
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.760118e-05
Norm of the params: 14.198112
     Influence (LOO): fixed 299 labels. Loss 0.16666. Accuracy 0.983.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23455852
Train loss (w/o reg) on all data: 0.21949388
Test loss (w/o reg) on all data: 0.15391575
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 5.380495e-06
Norm of the params: 17.357792
                Loss: fixed 409 labels. Loss 0.15392. Accuracy 0.949.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42247534
Train loss (w/o reg) on all data: 0.41801688
Test loss (w/o reg) on all data: 0.23392767
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0827055e-05
Norm of the params: 9.442952
              Random: fixed 106 labels. Loss 0.23393. Accuracy 0.975.
### Flips: 1025, rs: 28, checks: 615
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27515402
Train loss (w/o reg) on all data: 0.26411805
Test loss (w/o reg) on all data: 0.13289103
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.8831707e-06
Norm of the params: 14.856631
     Influence (LOO): fixed 408 labels. Loss 0.13289. Accuracy 0.993.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13255838
Train loss (w/o reg) on all data: 0.116062716
Test loss (w/o reg) on all data: 0.094605096
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.6404043e-06
Norm of the params: 18.163513
                Loss: fixed 608 labels. Loss 0.09461. Accuracy 0.971.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4076105
Train loss (w/o reg) on all data: 0.40314755
Test loss (w/o reg) on all data: 0.21713546
Train acc on all data:  0.8261609530756139
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.3751583e-05
Norm of the params: 9.4477215
              Random: fixed 156 labels. Loss 0.21714. Accuracy 0.979.
### Flips: 1025, rs: 28, checks: 820
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24061312
Train loss (w/o reg) on all data: 0.22973315
Test loss (w/o reg) on all data: 0.11322858
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1129394e-05
Norm of the params: 14.751246
     Influence (LOO): fixed 481 labels. Loss 0.11323. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04928376
Train loss (w/o reg) on all data: 0.03600045
Test loss (w/o reg) on all data: 0.042801242
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.781501e-07
Norm of the params: 16.299273
                Loss: fixed 784 labels. Loss 0.04280. Accuracy 0.987.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3954448
Train loss (w/o reg) on all data: 0.39067498
Test loss (w/o reg) on all data: 0.20309804
Train acc on all data:  0.8353999513736932
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.5040354e-05
Norm of the params: 9.767121
              Random: fixed 194 labels. Loss 0.20310. Accuracy 0.984.
### Flips: 1025, rs: 28, checks: 1025
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20325717
Train loss (w/o reg) on all data: 0.19274932
Test loss (w/o reg) on all data: 0.091080084
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.956726e-06
Norm of the params: 14.496799
     Influence (LOO): fixed 557 labels. Loss 0.09108. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017946538
Train loss (w/o reg) on all data: 0.010348224
Test loss (w/o reg) on all data: 0.016210863
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1581584e-07
Norm of the params: 12.327461
                Loss: fixed 849 labels. Loss 0.01621. Accuracy 0.995.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38106582
Train loss (w/o reg) on all data: 0.37589514
Test loss (w/o reg) on all data: 0.18986629
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.5529553e-05
Norm of the params: 10.169235
              Random: fixed 239 labels. Loss 0.18987. Accuracy 0.988.
### Flips: 1025, rs: 28, checks: 1230
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15897529
Train loss (w/o reg) on all data: 0.14908856
Test loss (w/o reg) on all data: 0.069607586
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.748169e-06
Norm of the params: 14.061811
     Influence (LOO): fixed 638 labels. Loss 0.06961. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010771645
Train loss (w/o reg) on all data: 0.0055479845
Test loss (w/o reg) on all data: 0.0073591373
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.2870972e-07
Norm of the params: 10.221214
                Loss: fixed 868 labels. Loss 0.00736. Accuracy 0.998.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36533922
Train loss (w/o reg) on all data: 0.35992408
Test loss (w/o reg) on all data: 0.17598093
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.2128027e-05
Norm of the params: 10.40686
              Random: fixed 283 labels. Loss 0.17598. Accuracy 0.989.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45753968
Train loss (w/o reg) on all data: 0.45234317
Test loss (w/o reg) on all data: 0.27437064
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.6016994e-05
Norm of the params: 10.19463
Flipped loss: 0.27437. Accuracy: 0.967
### Flips: 1025, rs: 29, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3827662
Train loss (w/o reg) on all data: 0.37340316
Test loss (w/o reg) on all data: 0.22259536
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.490719e-05
Norm of the params: 13.684324
     Influence (LOO): fixed 172 labels. Loss 0.22260. Accuracy 0.973.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.348393
Train loss (w/o reg) on all data: 0.33694968
Test loss (w/o reg) on all data: 0.23249203
Train acc on all data:  0.8426938973984925
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 1.3423197e-05
Norm of the params: 15.128326
                Loss: fixed 203 labels. Loss 0.23249. Accuracy 0.939.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44533074
Train loss (w/o reg) on all data: 0.43979862
Test loss (w/o reg) on all data: 0.261823
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.27556095e-05
Norm of the params: 10.518679
              Random: fixed  45 labels. Loss 0.26182. Accuracy 0.968.
### Flips: 1025, rs: 29, checks: 410
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3230626
Train loss (w/o reg) on all data: 0.31276348
Test loss (w/o reg) on all data: 0.18125339
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 6.4218866e-06
Norm of the params: 14.352085
     Influence (LOO): fixed 307 labels. Loss 0.18125. Accuracy 0.980.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24536552
Train loss (w/o reg) on all data: 0.22995454
Test loss (w/o reg) on all data: 0.17586327
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 8.8664465e-06
Norm of the params: 17.556185
                Loss: fixed 408 labels. Loss 0.17586. Accuracy 0.942.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43156356
Train loss (w/o reg) on all data: 0.42609718
Test loss (w/o reg) on all data: 0.24647334
Train acc on all data:  0.8103574033552152
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.859255e-05
Norm of the params: 10.455975
              Random: fixed  94 labels. Loss 0.24647. Accuracy 0.975.
### Flips: 1025, rs: 29, checks: 615
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2648088
Train loss (w/o reg) on all data: 0.25416765
Test loss (w/o reg) on all data: 0.14681283
Train acc on all data:  0.8888888888888888
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.0891806e-06
Norm of the params: 14.588454
     Influence (LOO): fixed 422 labels. Loss 0.14681. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14149085
Train loss (w/o reg) on all data: 0.12330551
Test loss (w/o reg) on all data: 0.10453207
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.5343934e-05
Norm of the params: 19.071098
                Loss: fixed 610 labels. Loss 0.10453. Accuracy 0.968.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42012486
Train loss (w/o reg) on all data: 0.41464752
Test loss (w/o reg) on all data: 0.23526573
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2573289e-05
Norm of the params: 10.466449
              Random: fixed 132 labels. Loss 0.23527. Accuracy 0.976.
### Flips: 1025, rs: 29, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173692
Train loss (w/o reg) on all data: 0.21146293
Test loss (w/o reg) on all data: 0.113477066
Train acc on all data:  0.9093119377583273
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.799812e-06
Norm of the params: 14.334568
     Influence (LOO): fixed 516 labels. Loss 0.11348. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049532235
Train loss (w/o reg) on all data: 0.0349564
Test loss (w/o reg) on all data: 0.028932484
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.2455495e-06
Norm of the params: 17.07386
                Loss: fixed 802 labels. Loss 0.02893. Accuracy 0.993.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4057873
Train loss (w/o reg) on all data: 0.40022224
Test loss (w/o reg) on all data: 0.2199807
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.1140001e-05
Norm of the params: 10.549923
              Random: fixed 179 labels. Loss 0.21998. Accuracy 0.979.
### Flips: 1025, rs: 29, checks: 1025
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17952316
Train loss (w/o reg) on all data: 0.16903171
Test loss (w/o reg) on all data: 0.090132974
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9780695e-05
Norm of the params: 14.485477
     Influence (LOO): fixed 593 labels. Loss 0.09013. Accuracy 0.993.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018242676
Train loss (w/o reg) on all data: 0.0094019
Test loss (w/o reg) on all data: 0.008447472
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.0592154e-07
Norm of the params: 13.297198
                Loss: fixed 861 labels. Loss 0.00845. Accuracy 0.999.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39255506
Train loss (w/o reg) on all data: 0.38669696
Test loss (w/o reg) on all data: 0.20722592
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5386719e-05
Norm of the params: 10.824139
              Random: fixed 219 labels. Loss 0.20723. Accuracy 0.981.
### Flips: 1025, rs: 29, checks: 1230
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14882445
Train loss (w/o reg) on all data: 0.13854562
Test loss (w/o reg) on all data: 0.072633564
Train acc on all data:  0.9416484318016046
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.626377e-06
Norm of the params: 14.337951
     Influence (LOO): fixed 648 labels. Loss 0.07263. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010413123
Train loss (w/o reg) on all data: 0.0046182713
Test loss (w/o reg) on all data: 0.00458579
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4374636e-07
Norm of the params: 10.76555
                Loss: fixed 880 labels. Loss 0.00459. Accuracy 1.000.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37562293
Train loss (w/o reg) on all data: 0.36966285
Test loss (w/o reg) on all data: 0.19348058
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 9.395284e-06
Norm of the params: 10.917934
              Random: fixed 265 labels. Loss 0.19348. Accuracy 0.979.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45093253
Train loss (w/o reg) on all data: 0.44535735
Test loss (w/o reg) on all data: 0.28389448
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.3617437e-05
Norm of the params: 10.559517
Flipped loss: 0.28389. Accuracy: 0.964
### Flips: 1025, rs: 30, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37447396
Train loss (w/o reg) on all data: 0.36515006
Test loss (w/o reg) on all data: 0.22903241
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.5852243e-05
Norm of the params: 13.655696
     Influence (LOO): fixed 171 labels. Loss 0.22903. Accuracy 0.968.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3383243
Train loss (w/o reg) on all data: 0.32647014
Test loss (w/o reg) on all data: 0.2319889
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 9.86883e-06
Norm of the params: 15.397507
                Loss: fixed 204 labels. Loss 0.23199. Accuracy 0.940.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43587956
Train loss (w/o reg) on all data: 0.43002042
Test loss (w/o reg) on all data: 0.26663288
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.0198067e-05
Norm of the params: 10.825105
              Random: fixed  51 labels. Loss 0.26663. Accuracy 0.962.
### Flips: 1025, rs: 30, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32230595
Train loss (w/o reg) on all data: 0.31234545
Test loss (w/o reg) on all data: 0.17850384
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 9.6066915e-06
Norm of the params: 14.114182
     Influence (LOO): fixed 296 labels. Loss 0.17850. Accuracy 0.978.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23284392
Train loss (w/o reg) on all data: 0.21552841
Test loss (w/o reg) on all data: 0.18077624
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 7.3336446e-06
Norm of the params: 18.60941
                Loss: fixed 408 labels. Loss 0.18078. Accuracy 0.946.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42433423
Train loss (w/o reg) on all data: 0.4184011
Test loss (w/o reg) on all data: 0.25230816
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.1770766e-05
Norm of the params: 10.893235
              Random: fixed  92 labels. Loss 0.25231. Accuracy 0.965.
### Flips: 1025, rs: 30, checks: 615
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27276337
Train loss (w/o reg) on all data: 0.262523
Test loss (w/o reg) on all data: 0.13872539
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.4482546e-05
Norm of the params: 14.311105
     Influence (LOO): fixed 407 labels. Loss 0.13873. Accuracy 0.989.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13659927
Train loss (w/o reg) on all data: 0.11705416
Test loss (w/o reg) on all data: 0.11570354
Train acc on all data:  0.949428640894724
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 9.2812925e-06
Norm of the params: 19.771248
                Loss: fixed 606 labels. Loss 0.11570. Accuracy 0.968.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41058508
Train loss (w/o reg) on all data: 0.40469262
Test loss (w/o reg) on all data: 0.23296048
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.3315714e-05
Norm of the params: 10.855829
              Random: fixed 142 labels. Loss 0.23296. Accuracy 0.974.
### Flips: 1025, rs: 30, checks: 820
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23218215
Train loss (w/o reg) on all data: 0.22098932
Test loss (w/o reg) on all data: 0.11484489
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.7321607e-06
Norm of the params: 14.961835
     Influence (LOO): fixed 489 labels. Loss 0.11484. Accuracy 0.993.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056515846
Train loss (w/o reg) on all data: 0.040542156
Test loss (w/o reg) on all data: 0.04289599
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9430245e-06
Norm of the params: 17.87383
                Loss: fixed 779 labels. Loss 0.04290. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39314598
Train loss (w/o reg) on all data: 0.38681564
Test loss (w/o reg) on all data: 0.21415287
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.4242128e-05
Norm of the params: 11.2519655
              Random: fixed 194 labels. Loss 0.21415. Accuracy 0.983.
### Flips: 1025, rs: 30, checks: 1025
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19234109
Train loss (w/o reg) on all data: 0.18163839
Test loss (w/o reg) on all data: 0.091122545
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.996773e-06
Norm of the params: 14.630584
     Influence (LOO): fixed 570 labels. Loss 0.09112. Accuracy 0.994.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026105512
Train loss (w/o reg) on all data: 0.015595371
Test loss (w/o reg) on all data: 0.01862358
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.6471015e-06
Norm of the params: 14.498374
                Loss: fixed 838 labels. Loss 0.01862. Accuracy 0.997.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37893066
Train loss (w/o reg) on all data: 0.37282717
Test loss (w/o reg) on all data: 0.1976033
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.84873e-05
Norm of the params: 11.048504
              Random: fixed 241 labels. Loss 0.19760. Accuracy 0.984.
### Flips: 1025, rs: 30, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16256042
Train loss (w/o reg) on all data: 0.15241788
Test loss (w/o reg) on all data: 0.07332643
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.521684e-05
Norm of the params: 14.242569
     Influence (LOO): fixed 626 labels. Loss 0.07333. Accuracy 0.997.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019535946
Train loss (w/o reg) on all data: 0.010950994
Test loss (w/o reg) on all data: 0.015217306
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6524207e-07
Norm of the params: 13.103398
                Loss: fixed 855 labels. Loss 0.01522. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36090264
Train loss (w/o reg) on all data: 0.35459718
Test loss (w/o reg) on all data: 0.18071243
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.7271325e-06
Norm of the params: 11.229829
              Random: fixed 287 labels. Loss 0.18071. Accuracy 0.988.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45397356
Train loss (w/o reg) on all data: 0.44761252
Test loss (w/o reg) on all data: 0.28073063
Train acc on all data:  0.7906637490882568
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 7.027623e-05
Norm of the params: 11.279208
Flipped loss: 0.28073. Accuracy: 0.961
### Flips: 1025, rs: 31, checks: 205
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38034433
Train loss (w/o reg) on all data: 0.3707392
Test loss (w/o reg) on all data: 0.22172217
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.4703498e-05
Norm of the params: 13.860119
     Influence (LOO): fixed 170 labels. Loss 0.22172. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3446513
Train loss (w/o reg) on all data: 0.33193144
Test loss (w/o reg) on all data: 0.2320115
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.0277639e-05
Norm of the params: 15.949847
                Loss: fixed 205 labels. Loss 0.23201. Accuracy 0.941.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44112107
Train loss (w/o reg) on all data: 0.43468004
Test loss (w/o reg) on all data: 0.2626884
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.2978216e-05
Norm of the params: 11.349922
              Random: fixed  48 labels. Loss 0.26269. Accuracy 0.961.
### Flips: 1025, rs: 31, checks: 410
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32396963
Train loss (w/o reg) on all data: 0.31292737
Test loss (w/o reg) on all data: 0.17777202
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.303897e-05
Norm of the params: 14.86086
     Influence (LOO): fixed 302 labels. Loss 0.17777. Accuracy 0.984.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24518412
Train loss (w/o reg) on all data: 0.22773212
Test loss (w/o reg) on all data: 0.17674321
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 3.301353e-05
Norm of the params: 18.682615
                Loss: fixed 410 labels. Loss 0.17674. Accuracy 0.944.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4267953
Train loss (w/o reg) on all data: 0.4207821
Test loss (w/o reg) on all data: 0.24401638
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 3.228376e-05
Norm of the params: 10.966519
              Random: fixed 100 labels. Loss 0.24402. Accuracy 0.969.
### Flips: 1025, rs: 31, checks: 615
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27469936
Train loss (w/o reg) on all data: 0.26361045
Test loss (w/o reg) on all data: 0.13864377
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.951254e-06
Norm of the params: 14.89222
     Influence (LOO): fixed 411 labels. Loss 0.13864. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14688577
Train loss (w/o reg) on all data: 0.12569585
Test loss (w/o reg) on all data: 0.113512166
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 3.4819839e-06
Norm of the params: 20.58636
                Loss: fixed 610 labels. Loss 0.11351. Accuracy 0.960.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4158153
Train loss (w/o reg) on all data: 0.40964827
Test loss (w/o reg) on all data: 0.23329948
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.5860169e-05
Norm of the params: 11.105873
              Random: fixed 136 labels. Loss 0.23330. Accuracy 0.976.
### Flips: 1025, rs: 31, checks: 820
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22680844
Train loss (w/o reg) on all data: 0.21474123
Test loss (w/o reg) on all data: 0.10842062
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0296274e-05
Norm of the params: 15.535259
     Influence (LOO): fixed 507 labels. Loss 0.10842. Accuracy 0.994.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056658387
Train loss (w/o reg) on all data: 0.038859297
Test loss (w/o reg) on all data: 0.048157297
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.2437616e-06
Norm of the params: 18.86748
                Loss: fixed 794 labels. Loss 0.04816. Accuracy 0.986.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4024329
Train loss (w/o reg) on all data: 0.3960994
Test loss (w/o reg) on all data: 0.21562426
Train acc on all data:  0.8336980306345733
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.7783326e-05
Norm of the params: 11.254766
              Random: fixed 178 labels. Loss 0.21562. Accuracy 0.981.
### Flips: 1025, rs: 31, checks: 1025
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19892941
Train loss (w/o reg) on all data: 0.18800895
Test loss (w/o reg) on all data: 0.0929074
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.144221e-06
Norm of the params: 14.778676
     Influence (LOO): fixed 565 labels. Loss 0.09291. Accuracy 0.994.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02666018
Train loss (w/o reg) on all data: 0.01545795
Test loss (w/o reg) on all data: 0.015807657
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2269616e-06
Norm of the params: 14.968119
                Loss: fixed 860 labels. Loss 0.01581. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38728008
Train loss (w/o reg) on all data: 0.3807896
Test loss (w/o reg) on all data: 0.20210576
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1450613e-05
Norm of the params: 11.393393
              Random: fixed 222 labels. Loss 0.20211. Accuracy 0.983.
### Flips: 1025, rs: 31, checks: 1230
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16476779
Train loss (w/o reg) on all data: 0.15430798
Test loss (w/o reg) on all data: 0.07425186
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.8552056e-06
Norm of the params: 14.4636135
     Influence (LOO): fixed 627 labels. Loss 0.07425. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017608475
Train loss (w/o reg) on all data: 0.009102568
Test loss (w/o reg) on all data: 0.011500696
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.7411247e-07
Norm of the params: 13.042934
                Loss: fixed 873 labels. Loss 0.01150. Accuracy 0.997.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3729878
Train loss (w/o reg) on all data: 0.36639023
Test loss (w/o reg) on all data: 0.18966453
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.308385e-06
Norm of the params: 11.487012
              Random: fixed 263 labels. Loss 0.18966. Accuracy 0.986.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4580301
Train loss (w/o reg) on all data: 0.45231462
Test loss (w/o reg) on all data: 0.27820918
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 3.5209443e-05
Norm of the params: 10.69157
Flipped loss: 0.27821. Accuracy: 0.969
### Flips: 1025, rs: 32, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3820804
Train loss (w/o reg) on all data: 0.37244254
Test loss (w/o reg) on all data: 0.22468854
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.929118e-05
Norm of the params: 13.883715
     Influence (LOO): fixed 168 labels. Loss 0.22469. Accuracy 0.981.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34753364
Train loss (w/o reg) on all data: 0.33601463
Test loss (w/o reg) on all data: 0.22747749
Train acc on all data:  0.8395331874544129
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.4464417e-05
Norm of the params: 15.178294
                Loss: fixed 205 labels. Loss 0.22748. Accuracy 0.945.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4460506
Train loss (w/o reg) on all data: 0.44015032
Test loss (w/o reg) on all data: 0.26346645
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.354696e-05
Norm of the params: 10.863059
              Random: fixed  43 labels. Loss 0.26347. Accuracy 0.978.
### Flips: 1025, rs: 32, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32534862
Train loss (w/o reg) on all data: 0.31416994
Test loss (w/o reg) on all data: 0.183851
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.301856e-06
Norm of the params: 14.952381
     Influence (LOO): fixed 299 labels. Loss 0.18385. Accuracy 0.987.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24647781
Train loss (w/o reg) on all data: 0.2311799
Test loss (w/o reg) on all data: 0.16780026
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 2.4497853e-05
Norm of the params: 17.491665
                Loss: fixed 410 labels. Loss 0.16780. Accuracy 0.952.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43557894
Train loss (w/o reg) on all data: 0.42965776
Test loss (w/o reg) on all data: 0.25497678
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.0341486e-05
Norm of the params: 10.882258
              Random: fixed  77 labels. Loss 0.25498. Accuracy 0.971.
### Flips: 1025, rs: 32, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28269
Train loss (w/o reg) on all data: 0.27152738
Test loss (w/o reg) on all data: 0.14842312
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3080776e-05
Norm of the params: 14.941628
     Influence (LOO): fixed 400 labels. Loss 0.14842. Accuracy 0.990.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14016758
Train loss (w/o reg) on all data: 0.12201685
Test loss (w/o reg) on all data: 0.10606205
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.3384748e-05
Norm of the params: 19.05294
                Loss: fixed 613 labels. Loss 0.10606. Accuracy 0.964.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42210454
Train loss (w/o reg) on all data: 0.41618878
Test loss (w/o reg) on all data: 0.24013624
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8119737e-05
Norm of the params: 10.877293
              Random: fixed 124 labels. Loss 0.24014. Accuracy 0.979.
### Flips: 1025, rs: 32, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23345643
Train loss (w/o reg) on all data: 0.22211018
Test loss (w/o reg) on all data: 0.11865602
Train acc on all data:  0.9049355701434476
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.545173e-06
Norm of the params: 15.064036
     Influence (LOO): fixed 495 labels. Loss 0.11866. Accuracy 0.993.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053748406
Train loss (w/o reg) on all data: 0.0393198
Test loss (w/o reg) on all data: 0.039507974
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1710001e-06
Norm of the params: 16.987411
                Loss: fixed 801 labels. Loss 0.03951. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40514654
Train loss (w/o reg) on all data: 0.39898515
Test loss (w/o reg) on all data: 0.22298257
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.2352026e-05
Norm of the params: 11.100814
              Random: fixed 173 labels. Loss 0.22298. Accuracy 0.977.
### Flips: 1025, rs: 32, checks: 1025
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19618191
Train loss (w/o reg) on all data: 0.1848508
Test loss (w/o reg) on all data: 0.09198365
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5274134e-05
Norm of the params: 15.05398
     Influence (LOO): fixed 570 labels. Loss 0.09198. Accuracy 0.995.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014639707
Train loss (w/o reg) on all data: 0.007591403
Test loss (w/o reg) on all data: 0.008203067
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2758504e-07
Norm of the params: 11.872913
                Loss: fixed 873 labels. Loss 0.00820. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39168465
Train loss (w/o reg) on all data: 0.38549438
Test loss (w/o reg) on all data: 0.20657438
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.0909112e-05
Norm of the params: 11.12678
              Random: fixed 218 labels. Loss 0.20657. Accuracy 0.982.
### Flips: 1025, rs: 32, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1570502
Train loss (w/o reg) on all data: 0.14628601
Test loss (w/o reg) on all data: 0.07313635
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.020832e-05
Norm of the params: 14.672558
     Influence (LOO): fixed 640 labels. Loss 0.07314. Accuracy 0.994.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008824365
Train loss (w/o reg) on all data: 0.0040984172
Test loss (w/o reg) on all data: 0.004623014
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3688826e-07
Norm of the params: 9.722086
                Loss: fixed 883 labels. Loss 0.00462. Accuracy 1.000.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37857524
Train loss (w/o reg) on all data: 0.37244016
Test loss (w/o reg) on all data: 0.19267127
Train acc on all data:  0.8477996596158521
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2204376e-05
Norm of the params: 11.077074
              Random: fixed 256 labels. Loss 0.19267. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45586264
Train loss (w/o reg) on all data: 0.4507094
Test loss (w/o reg) on all data: 0.2779382
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 4.3062304e-05
Norm of the params: 10.152081
Flipped loss: 0.27794. Accuracy: 0.950
### Flips: 1025, rs: 33, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3793803
Train loss (w/o reg) on all data: 0.3699992
Test loss (w/o reg) on all data: 0.22283082
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0714492e-05
Norm of the params: 13.697503
     Influence (LOO): fixed 175 labels. Loss 0.22283. Accuracy 0.955.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34004647
Train loss (w/o reg) on all data: 0.32681325
Test loss (w/o reg) on all data: 0.23927224
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 2.1135582e-05
Norm of the params: 16.268501
                Loss: fixed 205 labels. Loss 0.23927. Accuracy 0.915.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44503957
Train loss (w/o reg) on all data: 0.43991268
Test loss (w/o reg) on all data: 0.2619617
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 3.10268e-05
Norm of the params: 10.1261
              Random: fixed  46 labels. Loss 0.26196. Accuracy 0.957.
### Flips: 1025, rs: 33, checks: 410
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3214204
Train loss (w/o reg) on all data: 0.31107643
Test loss (w/o reg) on all data: 0.1818159
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.7986625e-05
Norm of the params: 14.383304
     Influence (LOO): fixed 304 labels. Loss 0.18182. Accuracy 0.966.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23440565
Train loss (w/o reg) on all data: 0.21490207
Test loss (w/o reg) on all data: 0.18479007
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 4.404537e-06
Norm of the params: 19.75023
                Loss: fixed 410 labels. Loss 0.18479. Accuracy 0.930.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43344894
Train loss (w/o reg) on all data: 0.42800727
Test loss (w/o reg) on all data: 0.24746634
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 5.9454043e-05
Norm of the params: 10.432312
              Random: fixed  86 labels. Loss 0.24747. Accuracy 0.964.
### Flips: 1025, rs: 33, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2773005
Train loss (w/o reg) on all data: 0.26685938
Test loss (w/o reg) on all data: 0.15211931
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.64857e-06
Norm of the params: 14.450698
     Influence (LOO): fixed 403 labels. Loss 0.15212. Accuracy 0.979.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14355908
Train loss (w/o reg) on all data: 0.12180511
Test loss (w/o reg) on all data: 0.115549274
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0234554e-05
Norm of the params: 20.858559
                Loss: fixed 608 labels. Loss 0.11555. Accuracy 0.955.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42033583
Train loss (w/o reg) on all data: 0.41464227
Test loss (w/o reg) on all data: 0.2339366
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.6571839e-05
Norm of the params: 10.671044
              Random: fixed 127 labels. Loss 0.23394. Accuracy 0.966.
### Flips: 1025, rs: 33, checks: 820
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23068945
Train loss (w/o reg) on all data: 0.21996422
Test loss (w/o reg) on all data: 0.11959996
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.912724e-06
Norm of the params: 14.645978
     Influence (LOO): fixed 503 labels. Loss 0.11960. Accuracy 0.987.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06266421
Train loss (w/o reg) on all data: 0.045499418
Test loss (w/o reg) on all data: 0.048783567
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3710264e-06
Norm of the params: 18.528246
                Loss: fixed 782 labels. Loss 0.04878. Accuracy 0.985.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40743455
Train loss (w/o reg) on all data: 0.4017654
Test loss (w/o reg) on all data: 0.2176203
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.3294775e-05
Norm of the params: 10.648136
              Random: fixed 174 labels. Loss 0.21762. Accuracy 0.979.
### Flips: 1025, rs: 33, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19164039
Train loss (w/o reg) on all data: 0.18132584
Test loss (w/o reg) on all data: 0.09708158
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.6992995e-05
Norm of the params: 14.362837
     Influence (LOO): fixed 580 labels. Loss 0.09708. Accuracy 0.991.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023392703
Train loss (w/o reg) on all data: 0.013773534
Test loss (w/o reg) on all data: 0.0172156
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.7896575e-07
Norm of the params: 13.870234
                Loss: fixed 864 labels. Loss 0.01722. Accuracy 0.995.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39603043
Train loss (w/o reg) on all data: 0.3904997
Test loss (w/o reg) on all data: 0.20211808
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.206532e-06
Norm of the params: 10.517327
              Random: fixed 214 labels. Loss 0.20212. Accuracy 0.985.
### Flips: 1025, rs: 33, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15866816
Train loss (w/o reg) on all data: 0.14827152
Test loss (w/o reg) on all data: 0.0790988
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.146179e-06
Norm of the params: 14.419883
     Influence (LOO): fixed 645 labels. Loss 0.07910. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014599968
Train loss (w/o reg) on all data: 0.007856891
Test loss (w/o reg) on all data: 0.008044442
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0372678e-07
Norm of the params: 11.61299
                Loss: fixed 881 labels. Loss 0.00804. Accuracy 0.999.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37631708
Train loss (w/o reg) on all data: 0.37050942
Test loss (w/o reg) on all data: 0.18495801
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.072985e-05
Norm of the params: 10.777444
              Random: fixed 269 labels. Loss 0.18496. Accuracy 0.989.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4471832
Train loss (w/o reg) on all data: 0.44139257
Test loss (w/o reg) on all data: 0.28515393
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.7146567e-05
Norm of the params: 10.761604
Flipped loss: 0.28515. Accuracy: 0.956
### Flips: 1025, rs: 34, checks: 205
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3717565
Train loss (w/o reg) on all data: 0.36200973
Test loss (w/o reg) on all data: 0.2288069
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.47932e-05
Norm of the params: 13.961928
     Influence (LOO): fixed 168 labels. Loss 0.22881. Accuracy 0.967.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33263215
Train loss (w/o reg) on all data: 0.31988758
Test loss (w/o reg) on all data: 0.24702123
Train acc on all data:  0.849258448820812
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 1.2882593e-05
Norm of the params: 15.965324
                Loss: fixed 205 labels. Loss 0.24702. Accuracy 0.927.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43794793
Train loss (w/o reg) on all data: 0.43242103
Test loss (w/o reg) on all data: 0.27037656
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 5.388004e-05
Norm of the params: 10.513711
              Random: fixed  38 labels. Loss 0.27038. Accuracy 0.960.
### Flips: 1025, rs: 34, checks: 410
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3166928
Train loss (w/o reg) on all data: 0.305298
Test loss (w/o reg) on all data: 0.17990679
Train acc on all data:  0.8677364454169706
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.50631695e-05
Norm of the params: 15.096216
     Influence (LOO): fixed 293 labels. Loss 0.17991. Accuracy 0.977.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22960919
Train loss (w/o reg) on all data: 0.21181694
Test loss (w/o reg) on all data: 0.19186312
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 6.772788e-06
Norm of the params: 18.86386
                Loss: fixed 407 labels. Loss 0.19186. Accuracy 0.948.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4273568
Train loss (w/o reg) on all data: 0.42160824
Test loss (w/o reg) on all data: 0.25413805
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.7047394e-05
Norm of the params: 10.722483
              Random: fixed  77 labels. Loss 0.25414. Accuracy 0.968.
### Flips: 1025, rs: 34, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26550338
Train loss (w/o reg) on all data: 0.25324813
Test loss (w/o reg) on all data: 0.14744295
Train acc on all data:  0.8913202042304887
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 7.95748e-06
Norm of the params: 15.65583
     Influence (LOO): fixed 403 labels. Loss 0.14744. Accuracy 0.985.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13207264
Train loss (w/o reg) on all data: 0.11173128
Test loss (w/o reg) on all data: 0.13350695
Train acc on all data:  0.9518599562363238
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 4.871747e-06
Norm of the params: 20.169956
                Loss: fixed 608 labels. Loss 0.13351. Accuracy 0.964.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41665405
Train loss (w/o reg) on all data: 0.41074154
Test loss (w/o reg) on all data: 0.24028537
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.31955185e-05
Norm of the params: 10.874303
              Random: fixed 112 labels. Loss 0.24029. Accuracy 0.968.
### Flips: 1025, rs: 34, checks: 820
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22665523
Train loss (w/o reg) on all data: 0.2145559
Test loss (w/o reg) on all data: 0.1156654
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3522458e-05
Norm of the params: 15.555915
     Influence (LOO): fixed 490 labels. Loss 0.11567. Accuracy 0.987.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05326591
Train loss (w/o reg) on all data: 0.036821797
Test loss (w/o reg) on all data: 0.07422977
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.5649733e-06
Norm of the params: 18.13511
                Loss: fixed 774 labels. Loss 0.07423. Accuracy 0.978.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40496543
Train loss (w/o reg) on all data: 0.39918104
Test loss (w/o reg) on all data: 0.22910017
Train acc on all data:  0.8288353999513737
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.2720046e-05
Norm of the params: 10.755828
              Random: fixed 151 labels. Loss 0.22910. Accuracy 0.972.
### Flips: 1025, rs: 34, checks: 1025
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18526568
Train loss (w/o reg) on all data: 0.17410062
Test loss (w/o reg) on all data: 0.09015882
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9648578e-05
Norm of the params: 14.943258
     Influence (LOO): fixed 573 labels. Loss 0.09016. Accuracy 0.994.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025645632
Train loss (w/o reg) on all data: 0.015496802
Test loss (w/o reg) on all data: 0.030259306
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.851084e-07
Norm of the params: 14.246986
                Loss: fixed 832 labels. Loss 0.03026. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3925038
Train loss (w/o reg) on all data: 0.3867776
Test loss (w/o reg) on all data: 0.21438734
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.1710677e-05
Norm of the params: 10.70157
              Random: fixed 194 labels. Loss 0.21439. Accuracy 0.973.
### Flips: 1025, rs: 34, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15283822
Train loss (w/o reg) on all data: 0.14301169
Test loss (w/o reg) on all data: 0.06855248
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0660636e-05
Norm of the params: 14.018938
     Influence (LOO): fixed 642 labels. Loss 0.06855. Accuracy 0.997.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015907245
Train loss (w/o reg) on all data: 0.008722977
Test loss (w/o reg) on all data: 0.015642364
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.3974368e-07
Norm of the params: 11.986882
                Loss: fixed 854 labels. Loss 0.01564. Accuracy 0.995.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37982213
Train loss (w/o reg) on all data: 0.3738912
Test loss (w/o reg) on all data: 0.20439252
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.0104733e-05
Norm of the params: 10.891229
              Random: fixed 227 labels. Loss 0.20439. Accuracy 0.976.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45443696
Train loss (w/o reg) on all data: 0.4491248
Test loss (w/o reg) on all data: 0.28732872
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.13373335e-05
Norm of the params: 10.307406
Flipped loss: 0.28733. Accuracy: 0.950
### Flips: 1025, rs: 35, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3804947
Train loss (w/o reg) on all data: 0.37108082
Test loss (w/o reg) on all data: 0.21977088
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 3.5621888e-05
Norm of the params: 13.72145
     Influence (LOO): fixed 168 labels. Loss 0.21977. Accuracy 0.976.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33905873
Train loss (w/o reg) on all data: 0.3273184
Test loss (w/o reg) on all data: 0.24207567
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.1926437e-05
Norm of the params: 15.323406
                Loss: fixed 205 labels. Loss 0.24208. Accuracy 0.931.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44071865
Train loss (w/o reg) on all data: 0.43492222
Test loss (w/o reg) on all data: 0.26980877
Train acc on all data:  0.7982008266472161
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 7.343597e-05
Norm of the params: 10.767011
              Random: fixed  45 labels. Loss 0.26981. Accuracy 0.958.
### Flips: 1025, rs: 35, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31928316
Train loss (w/o reg) on all data: 0.30840373
Test loss (w/o reg) on all data: 0.16988559
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.9879693e-05
Norm of the params: 14.750888
     Influence (LOO): fixed 307 labels. Loss 0.16989. Accuracy 0.989.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23570746
Train loss (w/o reg) on all data: 0.21966548
Test loss (w/o reg) on all data: 0.18373771
Train acc on all data:  0.8920495988329686
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.346775e-06
Norm of the params: 17.911999
                Loss: fixed 410 labels. Loss 0.18374. Accuracy 0.938.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42827535
Train loss (w/o reg) on all data: 0.42239237
Test loss (w/o reg) on all data: 0.25531122
Train acc on all data:  0.8064672988086555
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 3.2433003e-05
Norm of the params: 10.847086
              Random: fixed  84 labels. Loss 0.25531. Accuracy 0.959.
### Flips: 1025, rs: 35, checks: 615
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733321
Train loss (w/o reg) on all data: 0.26222694
Test loss (w/o reg) on all data: 0.1405348
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.9264353e-06
Norm of the params: 14.903111
     Influence (LOO): fixed 412 labels. Loss 0.14053. Accuracy 0.990.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14185306
Train loss (w/o reg) on all data: 0.1242321
Test loss (w/o reg) on all data: 0.11194819
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.3166758e-05
Norm of the params: 18.772833
                Loss: fixed 609 labels. Loss 0.11195. Accuracy 0.958.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41813943
Train loss (w/o reg) on all data: 0.41256934
Test loss (w/o reg) on all data: 0.24389161
Train acc on all data:  0.8169219547775346
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 4.319736e-05
Norm of the params: 10.554687
              Random: fixed 122 labels. Loss 0.24389. Accuracy 0.964.
### Flips: 1025, rs: 35, checks: 820
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23486644
Train loss (w/o reg) on all data: 0.2234253
Test loss (w/o reg) on all data: 0.117796704
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.481621e-06
Norm of the params: 15.126897
     Influence (LOO): fixed 487 labels. Loss 0.11780. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060801357
Train loss (w/o reg) on all data: 0.04537556
Test loss (w/o reg) on all data: 0.04775567
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.8362222e-06
Norm of the params: 17.564623
                Loss: fixed 775 labels. Loss 0.04776. Accuracy 0.980.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4084551
Train loss (w/o reg) on all data: 0.40300056
Test loss (w/o reg) on all data: 0.23123923
Train acc on all data:  0.824945295404814
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 7.78909e-06
Norm of the params: 10.44466
              Random: fixed 157 labels. Loss 0.23124. Accuracy 0.974.
### Flips: 1025, rs: 35, checks: 1025
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19245684
Train loss (w/o reg) on all data: 0.18123144
Test loss (w/o reg) on all data: 0.09433278
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1895313e-05
Norm of the params: 14.983589
     Influence (LOO): fixed 567 labels. Loss 0.09433. Accuracy 0.996.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026004422
Train loss (w/o reg) on all data: 0.015432868
Test loss (w/o reg) on all data: 0.013610072
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.5295686e-07
Norm of the params: 14.54067
                Loss: fixed 852 labels. Loss 0.01361. Accuracy 0.998.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3947741
Train loss (w/o reg) on all data: 0.3894945
Test loss (w/o reg) on all data: 0.2109512
Train acc on all data:  0.838074398249453
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.345575e-05
Norm of the params: 10.275802
              Random: fixed 209 labels. Loss 0.21095. Accuracy 0.983.
### Flips: 1025, rs: 35, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15772817
Train loss (w/o reg) on all data: 0.14733705
Test loss (w/o reg) on all data: 0.07268444
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7974833e-06
Norm of the params: 14.4160385
     Influence (LOO): fixed 634 labels. Loss 0.07268. Accuracy 0.999.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015217355
Train loss (w/o reg) on all data: 0.007880276
Test loss (w/o reg) on all data: 0.0076369406
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.868496e-07
Norm of the params: 12.113694
                Loss: fixed 869 labels. Loss 0.00764. Accuracy 0.999.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37841117
Train loss (w/o reg) on all data: 0.37256894
Test loss (w/o reg) on all data: 0.19606833
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.809409e-05
Norm of the params: 10.809481
              Random: fixed 255 labels. Loss 0.19607. Accuracy 0.985.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46266285
Train loss (w/o reg) on all data: 0.45796564
Test loss (w/o reg) on all data: 0.27695218
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.3929368e-05
Norm of the params: 9.692479
Flipped loss: 0.27695. Accuracy: 0.961
### Flips: 1025, rs: 36, checks: 205
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38010052
Train loss (w/o reg) on all data: 0.37135962
Test loss (w/o reg) on all data: 0.22324581
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.700064e-05
Norm of the params: 13.221878
     Influence (LOO): fixed 176 labels. Loss 0.22325. Accuracy 0.973.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3523404
Train loss (w/o reg) on all data: 0.34120736
Test loss (w/o reg) on all data: 0.22623338
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 4.4082863e-05
Norm of the params: 14.921837
                Loss: fixed 205 labels. Loss 0.22623. Accuracy 0.937.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45247597
Train loss (w/o reg) on all data: 0.44771
Test loss (w/o reg) on all data: 0.26175007
Train acc on all data:  0.7986870897155361
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 5.16882e-05
Norm of the params: 9.76316
              Random: fixed  42 labels. Loss 0.26175. Accuracy 0.969.
### Flips: 1025, rs: 36, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32420582
Train loss (w/o reg) on all data: 0.31466398
Test loss (w/o reg) on all data: 0.1713545
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.5347917e-06
Norm of the params: 13.81437
     Influence (LOO): fixed 307 labels. Loss 0.17135. Accuracy 0.988.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2470191
Train loss (w/o reg) on all data: 0.23151492
Test loss (w/o reg) on all data: 0.17482631
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.06069e-05
Norm of the params: 17.609192
                Loss: fixed 409 labels. Loss 0.17483. Accuracy 0.944.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4395411
Train loss (w/o reg) on all data: 0.43447387
Test loss (w/o reg) on all data: 0.24984409
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.826785e-05
Norm of the params: 10.066998
              Random: fixed  86 labels. Loss 0.24984. Accuracy 0.969.
### Flips: 1025, rs: 36, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27242747
Train loss (w/o reg) on all data: 0.26172903
Test loss (w/o reg) on all data: 0.1383203
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.159519e-05
Norm of the params: 14.627676
     Influence (LOO): fixed 414 labels. Loss 0.13832. Accuracy 0.993.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146081
Train loss (w/o reg) on all data: 0.12677307
Test loss (w/o reg) on all data: 0.11138716
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 8.890969e-06
Norm of the params: 19.650915
                Loss: fixed 612 labels. Loss 0.11139. Accuracy 0.961.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4250637
Train loss (w/o reg) on all data: 0.41990027
Test loss (w/o reg) on all data: 0.23501822
Train acc on all data:  0.8178944809141746
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.099161e-05
Norm of the params: 10.162113
              Random: fixed 133 labels. Loss 0.23502. Accuracy 0.973.
### Flips: 1025, rs: 36, checks: 820
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2368121
Train loss (w/o reg) on all data: 0.22588457
Test loss (w/o reg) on all data: 0.11444828
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.13031465e-05
Norm of the params: 14.78346
     Influence (LOO): fixed 492 labels. Loss 0.11445. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054281205
Train loss (w/o reg) on all data: 0.039052356
Test loss (w/o reg) on all data: 0.045134645
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.396421e-06
Norm of the params: 17.452135
                Loss: fixed 800 labels. Loss 0.04513. Accuracy 0.984.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41082323
Train loss (w/o reg) on all data: 0.40541416
Test loss (w/o reg) on all data: 0.21558087
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.9149118e-05
Norm of the params: 10.4010105
              Random: fixed 184 labels. Loss 0.21558. Accuracy 0.980.
### Flips: 1025, rs: 36, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20091774
Train loss (w/o reg) on all data: 0.19000629
Test loss (w/o reg) on all data: 0.09309894
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.2550645e-06
Norm of the params: 14.77258
     Influence (LOO): fixed 564 labels. Loss 0.09310. Accuracy 0.998.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020420041
Train loss (w/o reg) on all data: 0.012119658
Test loss (w/o reg) on all data: 0.012891593
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.97371e-07
Norm of the params: 12.884396
                Loss: fixed 870 labels. Loss 0.01289. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3988007
Train loss (w/o reg) on all data: 0.39333534
Test loss (w/o reg) on all data: 0.20455846
Train acc on all data:  0.838560661317773
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.6188907e-05
Norm of the params: 10.455013
              Random: fixed 218 labels. Loss 0.20456. Accuracy 0.981.
### Flips: 1025, rs: 36, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1685893
Train loss (w/o reg) on all data: 0.15866542
Test loss (w/o reg) on all data: 0.0762717
Train acc on all data:  0.9348407488451252
Test acc on all data:   1.0
Norm of the mean of gradients: 6.598716e-06
Norm of the params: 14.088206
     Influence (LOO): fixed 631 labels. Loss 0.07627. Accuracy 1.000.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008141652
Train loss (w/o reg) on all data: 0.003864219
Test loss (w/o reg) on all data: 0.004447329
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1689724e-07
Norm of the params: 9.249251
                Loss: fixed 892 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38217935
Train loss (w/o reg) on all data: 0.37648705
Test loss (w/o reg) on all data: 0.1911808
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.589113e-05
Norm of the params: 10.669874
              Random: fixed 265 labels. Loss 0.19118. Accuracy 0.983.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45629716
Train loss (w/o reg) on all data: 0.45146155
Test loss (w/o reg) on all data: 0.27476794
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.6070082e-05
Norm of the params: 9.83422
Flipped loss: 0.27477. Accuracy: 0.959
### Flips: 1025, rs: 37, checks: 205
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3796233
Train loss (w/o reg) on all data: 0.3703362
Test loss (w/o reg) on all data: 0.21602182
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 8.850798e-06
Norm of the params: 13.628704
     Influence (LOO): fixed 168 labels. Loss 0.21602. Accuracy 0.973.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34150285
Train loss (w/o reg) on all data: 0.32973787
Test loss (w/o reg) on all data: 0.23299098
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.258325e-06
Norm of the params: 15.339479
                Loss: fixed 205 labels. Loss 0.23299. Accuracy 0.938.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44252166
Train loss (w/o reg) on all data: 0.43767327
Test loss (w/o reg) on all data: 0.2596113
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.7752885e-05
Norm of the params: 9.847213
              Random: fixed  48 labels. Loss 0.25961. Accuracy 0.961.
### Flips: 1025, rs: 37, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3229689
Train loss (w/o reg) on all data: 0.31260717
Test loss (w/o reg) on all data: 0.17635523
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.5395965e-05
Norm of the params: 14.395639
     Influence (LOO): fixed 299 labels. Loss 0.17636. Accuracy 0.984.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23522606
Train loss (w/o reg) on all data: 0.21818773
Test loss (w/o reg) on all data: 0.17788564
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 9.789233e-06
Norm of the params: 18.459866
                Loss: fixed 409 labels. Loss 0.17789. Accuracy 0.948.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43190503
Train loss (w/o reg) on all data: 0.42703763
Test loss (w/o reg) on all data: 0.24649288
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.5115519e-05
Norm of the params: 9.866503
              Random: fixed  91 labels. Loss 0.24649. Accuracy 0.960.
### Flips: 1025, rs: 37, checks: 615
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27338842
Train loss (w/o reg) on all data: 0.2621621
Test loss (w/o reg) on all data: 0.14297149
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.948464e-06
Norm of the params: 14.984217
     Influence (LOO): fixed 411 labels. Loss 0.14297. Accuracy 0.990.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13700157
Train loss (w/o reg) on all data: 0.117287055
Test loss (w/o reg) on all data: 0.11226727
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 7.1012573e-06
Norm of the params: 19.856743
                Loss: fixed 611 labels. Loss 0.11227. Accuracy 0.965.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4183232
Train loss (w/o reg) on all data: 0.4135113
Test loss (w/o reg) on all data: 0.22925182
Train acc on all data:  0.825188426938974
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.6634542e-05
Norm of the params: 9.810084
              Random: fixed 140 labels. Loss 0.22925. Accuracy 0.967.
### Flips: 1025, rs: 37, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22702655
Train loss (w/o reg) on all data: 0.21579584
Test loss (w/o reg) on all data: 0.11604945
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.299731e-05
Norm of the params: 14.987133
     Influence (LOO): fixed 505 labels. Loss 0.11605. Accuracy 0.994.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05173573
Train loss (w/o reg) on all data: 0.036500603
Test loss (w/o reg) on all data: 0.0476925
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1779238e-06
Norm of the params: 17.45573
                Loss: fixed 786 labels. Loss 0.04769. Accuracy 0.989.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40319416
Train loss (w/o reg) on all data: 0.39803016
Test loss (w/o reg) on all data: 0.2153859
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.347866e-05
Norm of the params: 10.162686
              Random: fixed 186 labels. Loss 0.21539. Accuracy 0.970.
### Flips: 1025, rs: 37, checks: 1025
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864788
Train loss (w/o reg) on all data: 0.17815427
Test loss (w/o reg) on all data: 0.08942221
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.090098e-06
Norm of the params: 14.486967
     Influence (LOO): fixed 582 labels. Loss 0.08942. Accuracy 0.994.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019021474
Train loss (w/o reg) on all data: 0.010670327
Test loss (w/o reg) on all data: 0.0111967595
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6792775e-07
Norm of the params: 12.923736
                Loss: fixed 854 labels. Loss 0.01120. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38751543
Train loss (w/o reg) on all data: 0.38207474
Test loss (w/o reg) on all data: 0.19944996
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.0258634e-05
Norm of the params: 10.431365
              Random: fixed 233 labels. Loss 0.19945. Accuracy 0.973.
### Flips: 1025, rs: 37, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15542395
Train loss (w/o reg) on all data: 0.14576058
Test loss (w/o reg) on all data: 0.07120835
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.528187e-05
Norm of the params: 13.902067
     Influence (LOO): fixed 646 labels. Loss 0.07121. Accuracy 0.994.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014780827
Train loss (w/o reg) on all data: 0.0077660633
Test loss (w/o reg) on all data: 0.007249986
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 3.94207e-07
Norm of the params: 11.844631
                Loss: fixed 866 labels. Loss 0.00725. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3693578
Train loss (w/o reg) on all data: 0.36360848
Test loss (w/o reg) on all data: 0.18239518
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2262912e-05
Norm of the params: 10.723176
              Random: fixed 282 labels. Loss 0.18240. Accuracy 0.979.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4570764
Train loss (w/o reg) on all data: 0.45138207
Test loss (w/o reg) on all data: 0.2800536
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.0360774e-05
Norm of the params: 10.671768
Flipped loss: 0.28005. Accuracy: 0.963
### Flips: 1025, rs: 38, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3850462
Train loss (w/o reg) on all data: 0.3758408
Test loss (w/o reg) on all data: 0.21814552
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.559845e-05
Norm of the params: 13.568635
     Influence (LOO): fixed 170 labels. Loss 0.21815. Accuracy 0.975.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3446472
Train loss (w/o reg) on all data: 0.33305565
Test loss (w/o reg) on all data: 0.2303886
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.5335241e-05
Norm of the params: 15.225996
                Loss: fixed 205 labels. Loss 0.23039. Accuracy 0.937.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44471863
Train loss (w/o reg) on all data: 0.43887952
Test loss (w/o reg) on all data: 0.2648147
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.5354553e-05
Norm of the params: 10.806586
              Random: fixed  46 labels. Loss 0.26481. Accuracy 0.969.
### Flips: 1025, rs: 38, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3307038
Train loss (w/o reg) on all data: 0.3198753
Test loss (w/o reg) on all data: 0.17766389
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8155013e-05
Norm of the params: 14.71631
     Influence (LOO): fixed 297 labels. Loss 0.17766. Accuracy 0.983.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24097289
Train loss (w/o reg) on all data: 0.22407795
Test loss (w/o reg) on all data: 0.17719147
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.5893469e-05
Norm of the params: 18.382025
                Loss: fixed 408 labels. Loss 0.17719. Accuracy 0.938.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43265018
Train loss (w/o reg) on all data: 0.42682633
Test loss (w/o reg) on all data: 0.25052726
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.2539671e-05
Norm of the params: 10.792447
              Random: fixed  91 labels. Loss 0.25053. Accuracy 0.967.
### Flips: 1025, rs: 38, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27797177
Train loss (w/o reg) on all data: 0.26703805
Test loss (w/o reg) on all data: 0.13674733
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.134658e-06
Norm of the params: 14.787646
     Influence (LOO): fixed 415 labels. Loss 0.13675. Accuracy 0.991.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13226269
Train loss (w/o reg) on all data: 0.111077175
Test loss (w/o reg) on all data: 0.111575186
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.4931904e-05
Norm of the params: 20.584225
                Loss: fixed 613 labels. Loss 0.11158. Accuracy 0.953.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4187497
Train loss (w/o reg) on all data: 0.41280067
Test loss (w/o reg) on all data: 0.23332813
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.5727404e-05
Norm of the params: 10.90782
              Random: fixed 138 labels. Loss 0.23333. Accuracy 0.967.
### Flips: 1025, rs: 38, checks: 820
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23486009
Train loss (w/o reg) on all data: 0.22450148
Test loss (w/o reg) on all data: 0.11058515
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.6131495e-06
Norm of the params: 14.39348
     Influence (LOO): fixed 503 labels. Loss 0.11059. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051175933
Train loss (w/o reg) on all data: 0.034712862
Test loss (w/o reg) on all data: 0.029206507
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0884195e-06
Norm of the params: 18.145563
                Loss: fixed 793 labels. Loss 0.02921. Accuracy 0.995.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4023179
Train loss (w/o reg) on all data: 0.39650667
Test loss (w/o reg) on all data: 0.21464607
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.3779483e-05
Norm of the params: 10.78077
              Random: fixed 192 labels. Loss 0.21465. Accuracy 0.978.
### Flips: 1025, rs: 38, checks: 1025
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139095
Train loss (w/o reg) on all data: 0.18118247
Test loss (w/o reg) on all data: 0.08667886
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0982738e-05
Norm of the params: 14.288789
     Influence (LOO): fixed 586 labels. Loss 0.08668. Accuracy 0.995.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026291538
Train loss (w/o reg) on all data: 0.0151804695
Test loss (w/o reg) on all data: 0.014549064
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.923903e-07
Norm of the params: 14.907091
                Loss: fixed 846 labels. Loss 0.01455. Accuracy 0.997.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38528788
Train loss (w/o reg) on all data: 0.37948912
Test loss (w/o reg) on all data: 0.19935209
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.755552e-05
Norm of the params: 10.769187
              Random: fixed 241 labels. Loss 0.19935. Accuracy 0.978.
### Flips: 1025, rs: 38, checks: 1230
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15501486
Train loss (w/o reg) on all data: 0.14550452
Test loss (w/o reg) on all data: 0.06533627
Train acc on all data:  0.9404327741308047
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4248884e-06
Norm of the params: 13.791542
     Influence (LOO): fixed 654 labels. Loss 0.06534. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016674351
Train loss (w/o reg) on all data: 0.0091157025
Test loss (w/o reg) on all data: 0.0066748657
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0004408e-07
Norm of the params: 12.295241
                Loss: fixed 869 labels. Loss 0.00667. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37155935
Train loss (w/o reg) on all data: 0.3658135
Test loss (w/o reg) on all data: 0.18688205
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.779472e-06
Norm of the params: 10.71995
              Random: fixed 282 labels. Loss 0.18688. Accuracy 0.980.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4508928
Train loss (w/o reg) on all data: 0.44501084
Test loss (w/o reg) on all data: 0.27614775
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.8157254e-05
Norm of the params: 10.846169
Flipped loss: 0.27615. Accuracy: 0.956
### Flips: 1025, rs: 39, checks: 205
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37669322
Train loss (w/o reg) on all data: 0.36690193
Test loss (w/o reg) on all data: 0.22210631
Train acc on all data:  0.8344274252370533
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.0489904e-05
Norm of the params: 13.993771
     Influence (LOO): fixed 166 labels. Loss 0.22211. Accuracy 0.972.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33685312
Train loss (w/o reg) on all data: 0.323617
Test loss (w/o reg) on all data: 0.2289085
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.2639598e-05
Norm of the params: 16.27028
                Loss: fixed 204 labels. Loss 0.22891. Accuracy 0.938.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43977445
Train loss (w/o reg) on all data: 0.4338547
Test loss (w/o reg) on all data: 0.26238412
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.3369546e-05
Norm of the params: 10.880964
              Random: fixed  44 labels. Loss 0.26238. Accuracy 0.956.
### Flips: 1025, rs: 39, checks: 410
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773332
Train loss (w/o reg) on all data: 0.3072587
Test loss (w/o reg) on all data: 0.17756417
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4848721e-05
Norm of the params: 14.473848
     Influence (LOO): fixed 298 labels. Loss 0.17756. Accuracy 0.982.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2377851
Train loss (w/o reg) on all data: 0.2202852
Test loss (w/o reg) on all data: 0.17020033
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 5.5770847e-06
Norm of the params: 18.708225
                Loss: fixed 406 labels. Loss 0.17020. Accuracy 0.950.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43017548
Train loss (w/o reg) on all data: 0.42423785
Test loss (w/o reg) on all data: 0.24792738
Train acc on all data:  0.8096280087527352
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.1924617e-05
Norm of the params: 10.897375
              Random: fixed  79 labels. Loss 0.24793. Accuracy 0.967.
### Flips: 1025, rs: 39, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27242345
Train loss (w/o reg) on all data: 0.26150492
Test loss (w/o reg) on all data: 0.14361922
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1571676e-05
Norm of the params: 14.777374
     Influence (LOO): fixed 402 labels. Loss 0.14362. Accuracy 0.988.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13851103
Train loss (w/o reg) on all data: 0.11861428
Test loss (w/o reg) on all data: 0.105425134
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.332758e-06
Norm of the params: 19.948309
                Loss: fixed 605 labels. Loss 0.10543. Accuracy 0.970.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41674423
Train loss (w/o reg) on all data: 0.41056985
Test loss (w/o reg) on all data: 0.23334599
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 7.2866937e-06
Norm of the params: 11.112499
              Random: fixed 126 labels. Loss 0.23335. Accuracy 0.974.
### Flips: 1025, rs: 39, checks: 820
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22395031
Train loss (w/o reg) on all data: 0.21336381
Test loss (w/o reg) on all data: 0.107709564
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.7993217e-06
Norm of the params: 14.550942
     Influence (LOO): fixed 502 labels. Loss 0.10771. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056367833
Train loss (w/o reg) on all data: 0.040394183
Test loss (w/o reg) on all data: 0.048165392
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.1822012e-06
Norm of the params: 17.87381
                Loss: fixed 777 labels. Loss 0.04817. Accuracy 0.987.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40177932
Train loss (w/o reg) on all data: 0.39551935
Test loss (w/o reg) on all data: 0.21696417
Train acc on all data:  0.8332117675662534
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.7077578e-05
Norm of the params: 11.18927
              Random: fixed 173 labels. Loss 0.21696. Accuracy 0.980.
### Flips: 1025, rs: 39, checks: 1025
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1896139
Train loss (w/o reg) on all data: 0.17968622
Test loss (w/o reg) on all data: 0.086863376
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.803566e-06
Norm of the params: 14.090899
     Influence (LOO): fixed 572 labels. Loss 0.08686. Accuracy 0.997.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024608962
Train loss (w/o reg) on all data: 0.014939981
Test loss (w/o reg) on all data: 0.016745139
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.4191026e-07
Norm of the params: 13.906102
                Loss: fixed 846 labels. Loss 0.01675. Accuracy 0.994.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38782766
Train loss (w/o reg) on all data: 0.38134083
Test loss (w/o reg) on all data: 0.20067953
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.3699281e-05
Norm of the params: 11.390201
              Random: fixed 215 labels. Loss 0.20068. Accuracy 0.985.
### Flips: 1025, rs: 39, checks: 1230
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15464728
Train loss (w/o reg) on all data: 0.14567
Test loss (w/o reg) on all data: 0.06752756
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.739813e-06
Norm of the params: 13.399457
     Influence (LOO): fixed 637 labels. Loss 0.06753. Accuracy 0.999.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015344581
Train loss (w/o reg) on all data: 0.008079082
Test loss (w/o reg) on all data: 0.015645
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.8406933e-07
Norm of the params: 12.054459
                Loss: fixed 859 labels. Loss 0.01564. Accuracy 0.995.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37103972
Train loss (w/o reg) on all data: 0.36439535
Test loss (w/o reg) on all data: 0.18631022
Train acc on all data:  0.8514466326282519
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.742109e-06
Norm of the params: 11.527667
              Random: fixed 261 labels. Loss 0.18631. Accuracy 0.987.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4967608
Train loss (w/o reg) on all data: 0.49179205
Test loss (w/o reg) on all data: 0.3280967
Train acc on all data:  0.7544371504984196
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 3.2178636e-05
Norm of the params: 9.9686775
Flipped loss: 0.32810. Accuracy: 0.950
### Flips: 1230, rs: 0, checks: 205
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43525904
Train loss (w/o reg) on all data: 0.42578787
Test loss (w/o reg) on all data: 0.2721158
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.3450702e-05
Norm of the params: 13.763129
     Influence (LOO): fixed 155 labels. Loss 0.27212. Accuracy 0.965.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39535257
Train loss (w/o reg) on all data: 0.3830104
Test loss (w/o reg) on all data: 0.27352276
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 1.5749125e-05
Norm of the params: 15.711256
                Loss: fixed 203 labels. Loss 0.27352. Accuracy 0.923.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4858729
Train loss (w/o reg) on all data: 0.4807679
Test loss (w/o reg) on all data: 0.31219152
Train acc on all data:  0.7690250425480185
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 4.1953732e-05
Norm of the params: 10.104429
              Random: fixed  52 labels. Loss 0.31219. Accuracy 0.949.
### Flips: 1230, rs: 0, checks: 410
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38446438
Train loss (w/o reg) on all data: 0.3726919
Test loss (w/o reg) on all data: 0.23183258
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.197378e-05
Norm of the params: 15.3443775
     Influence (LOO): fixed 286 labels. Loss 0.23183. Accuracy 0.968.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30696788
Train loss (w/o reg) on all data: 0.2895118
Test loss (w/o reg) on all data: 0.21161975
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 4.2594937e-05
Norm of the params: 18.684792
                Loss: fixed 405 labels. Loss 0.21162. Accuracy 0.941.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47489348
Train loss (w/o reg) on all data: 0.4698561
Test loss (w/o reg) on all data: 0.29182255
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 5.363249e-05
Norm of the params: 10.037299
              Random: fixed 104 labels. Loss 0.29182. Accuracy 0.960.
### Flips: 1230, rs: 0, checks: 615
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34011295
Train loss (w/o reg) on all data: 0.32719794
Test loss (w/o reg) on all data: 0.20767075
Train acc on all data:  0.8402625820568927
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.6891248e-05
Norm of the params: 16.071722
     Influence (LOO): fixed 395 labels. Loss 0.20767. Accuracy 0.972.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21641271
Train loss (w/o reg) on all data: 0.1962607
Test loss (w/o reg) on all data: 0.15296766
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4577443e-05
Norm of the params: 20.075857
                Loss: fixed 602 labels. Loss 0.15297. Accuracy 0.956.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46471187
Train loss (w/o reg) on all data: 0.45951468
Test loss (w/o reg) on all data: 0.27676705
Train acc on all data:  0.7858011184050572
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.3332e-05
Norm of the params: 10.19528
              Random: fixed 146 labels. Loss 0.27677. Accuracy 0.968.
### Flips: 1230, rs: 0, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29879177
Train loss (w/o reg) on all data: 0.285799
Test loss (w/o reg) on all data: 0.17116706
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.0368731e-05
Norm of the params: 16.120028
     Influence (LOO): fixed 498 labels. Loss 0.17117. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13185243
Train loss (w/o reg) on all data: 0.11027695
Test loss (w/o reg) on all data: 0.09435313
Train acc on all data:  0.949914903963044
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.4510557e-06
Norm of the params: 20.772808
                Loss: fixed 792 labels. Loss 0.09435. Accuracy 0.969.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45258304
Train loss (w/o reg) on all data: 0.4475686
Test loss (w/o reg) on all data: 0.25935513
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.3313412e-05
Norm of the params: 10.014433
              Random: fixed 195 labels. Loss 0.25936. Accuracy 0.981.
### Flips: 1230, rs: 0, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2614743
Train loss (w/o reg) on all data: 0.2482641
Test loss (w/o reg) on all data: 0.14348066
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.2149345e-06
Norm of the params: 16.254353
     Influence (LOO): fixed 584 labels. Loss 0.14348. Accuracy 0.986.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063015565
Train loss (w/o reg) on all data: 0.045754027
Test loss (w/o reg) on all data: 0.037744243
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3020265e-06
Norm of the params: 18.58039
                Loss: fixed 944 labels. Loss 0.03774. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43846884
Train loss (w/o reg) on all data: 0.4334986
Test loss (w/o reg) on all data: 0.2409161
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.0570075e-05
Norm of the params: 9.9702
              Random: fixed 250 labels. Loss 0.24092. Accuracy 0.982.
### Flips: 1230, rs: 0, checks: 1230
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22971484
Train loss (w/o reg) on all data: 0.2170295
Test loss (w/o reg) on all data: 0.121861465
Train acc on all data:  0.8986141502552881
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.276989e-05
Norm of the params: 15.928183
     Influence (LOO): fixed 654 labels. Loss 0.12186. Accuracy 0.993.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034105167
Train loss (w/o reg) on all data: 0.02093731
Test loss (w/o reg) on all data: 0.020092765
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.1385719e-06
Norm of the params: 16.228281
                Loss: fixed 1004 labels. Loss 0.02009. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42415953
Train loss (w/o reg) on all data: 0.41893557
Test loss (w/o reg) on all data: 0.22584958
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.842516e-06
Norm of the params: 10.221517
              Random: fixed 299 labels. Loss 0.22585. Accuracy 0.985.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4988366
Train loss (w/o reg) on all data: 0.49396574
Test loss (w/o reg) on all data: 0.32657197
Train acc on all data:  0.7520058351568198
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.5058e-05
Norm of the params: 9.87001
Flipped loss: 0.32657. Accuracy: 0.948
### Flips: 1230, rs: 1, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43609995
Train loss (w/o reg) on all data: 0.4266837
Test loss (w/o reg) on all data: 0.2747425
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 6.314964e-05
Norm of the params: 13.72316
     Influence (LOO): fixed 159 labels. Loss 0.27474. Accuracy 0.945.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39814463
Train loss (w/o reg) on all data: 0.3866526
Test loss (w/o reg) on all data: 0.2690457
Train acc on all data:  0.8069535618769754
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 3.0795363e-05
Norm of the params: 15.160508
                Loss: fixed 205 labels. Loss 0.26905. Accuracy 0.919.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4886165
Train loss (w/o reg) on all data: 0.48377502
Test loss (w/o reg) on all data: 0.30729085
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.4996024e-05
Norm of the params: 9.840205
              Random: fixed  57 labels. Loss 0.30729. Accuracy 0.952.
### Flips: 1230, rs: 1, checks: 410
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39096907
Train loss (w/o reg) on all data: 0.37942752
Test loss (w/o reg) on all data: 0.23859556
Train acc on all data:  0.8188670070508145
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 4.998756e-05
Norm of the params: 15.193125
     Influence (LOO): fixed 282 labels. Loss 0.23860. Accuracy 0.965.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30429357
Train loss (w/o reg) on all data: 0.28692883
Test loss (w/o reg) on all data: 0.21857794
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 2.9041903e-05
Norm of the params: 18.635849
                Loss: fixed 409 labels. Loss 0.21858. Accuracy 0.926.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47460085
Train loss (w/o reg) on all data: 0.46960565
Test loss (w/o reg) on all data: 0.2885201
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.0550662e-05
Norm of the params: 9.995207
              Random: fixed 112 labels. Loss 0.28852. Accuracy 0.958.
### Flips: 1230, rs: 1, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34826523
Train loss (w/o reg) on all data: 0.33602917
Test loss (w/o reg) on all data: 0.20492314
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.638703e-06
Norm of the params: 15.643569
     Influence (LOO): fixed 393 labels. Loss 0.20492. Accuracy 0.976.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21558754
Train loss (w/o reg) on all data: 0.19404602
Test loss (w/o reg) on all data: 0.16088326
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.3579454e-05
Norm of the params: 20.756453
                Loss: fixed 607 labels. Loss 0.16088. Accuracy 0.946.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46076548
Train loss (w/o reg) on all data: 0.455668
Test loss (w/o reg) on all data: 0.2652882
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.3204288e-05
Norm of the params: 10.09701
              Random: fixed 168 labels. Loss 0.26529. Accuracy 0.968.
### Flips: 1230, rs: 1, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30421206
Train loss (w/o reg) on all data: 0.2919132
Test loss (w/o reg) on all data: 0.17161298
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.9201048e-05
Norm of the params: 15.683649
     Influence (LOO): fixed 499 labels. Loss 0.17161. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13272631
Train loss (w/o reg) on all data: 0.1091408
Test loss (w/o reg) on all data: 0.10296866
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.1486376e-06
Norm of the params: 21.718891
                Loss: fixed 792 labels. Loss 0.10297. Accuracy 0.965.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44648504
Train loss (w/o reg) on all data: 0.4412622
Test loss (w/o reg) on all data: 0.24754481
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2170648e-05
Norm of the params: 10.220434
              Random: fixed 224 labels. Loss 0.24754. Accuracy 0.978.
### Flips: 1230, rs: 1, checks: 1025
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26424915
Train loss (w/o reg) on all data: 0.2523191
Test loss (w/o reg) on all data: 0.14401257
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.8635837e-05
Norm of the params: 15.446712
     Influence (LOO): fixed 587 labels. Loss 0.14401. Accuracy 0.984.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0667266
Train loss (w/o reg) on all data: 0.048173316
Test loss (w/o reg) on all data: 0.052364215
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.1667284e-06
Norm of the params: 19.263067
                Loss: fixed 937 labels. Loss 0.05236. Accuracy 0.982.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4303817
Train loss (w/o reg) on all data: 0.42481396
Test loss (w/o reg) on all data: 0.23396727
Train acc on all data:  0.812545587162655
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.5817956e-05
Norm of the params: 10.552471
              Random: fixed 279 labels. Loss 0.23397. Accuracy 0.982.
### Flips: 1230, rs: 1, checks: 1230
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23000471
Train loss (w/o reg) on all data: 0.21871942
Test loss (w/o reg) on all data: 0.118186526
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.570932e-06
Norm of the params: 15.023505
     Influence (LOO): fixed 660 labels. Loss 0.11819. Accuracy 0.994.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040536992
Train loss (w/o reg) on all data: 0.026353525
Test loss (w/o reg) on all data: 0.026451295
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.1383988e-06
Norm of the params: 16.842487
                Loss: fixed 1004 labels. Loss 0.02645. Accuracy 0.991.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41427958
Train loss (w/o reg) on all data: 0.40849817
Test loss (w/o reg) on all data: 0.21570528
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4003462e-05
Norm of the params: 10.753056
              Random: fixed 329 labels. Loss 0.21571. Accuracy 0.982.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49609748
Train loss (w/o reg) on all data: 0.4914747
Test loss (w/o reg) on all data: 0.3327336
Train acc on all data:  0.761974228057379
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 5.4712345e-05
Norm of the params: 9.615394
Flipped loss: 0.33273. Accuracy: 0.931
### Flips: 1230, rs: 2, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43355578
Train loss (w/o reg) on all data: 0.42470974
Test loss (w/o reg) on all data: 0.28316444
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.0645467e-05
Norm of the params: 13.301164
     Influence (LOO): fixed 159 labels. Loss 0.28316. Accuracy 0.947.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39332965
Train loss (w/o reg) on all data: 0.382342
Test loss (w/o reg) on all data: 0.28860265
Train acc on all data:  0.8169219547775346
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 2.1799136e-05
Norm of the params: 14.82406
                Loss: fixed 203 labels. Loss 0.28860. Accuracy 0.915.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4887879
Train loss (w/o reg) on all data: 0.4840174
Test loss (w/o reg) on all data: 0.31575498
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.268473e-05
Norm of the params: 9.767795
              Random: fixed  42 labels. Loss 0.31575. Accuracy 0.956.
### Flips: 1230, rs: 2, checks: 410
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38875702
Train loss (w/o reg) on all data: 0.37777156
Test loss (w/o reg) on all data: 0.24022292
Train acc on all data:  0.8195964016532944
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 8.183679e-06
Norm of the params: 14.822592
     Influence (LOO): fixed 274 labels. Loss 0.24022. Accuracy 0.971.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2979805
Train loss (w/o reg) on all data: 0.28231117
Test loss (w/o reg) on all data: 0.23209444
Train acc on all data:  0.8657913931436907
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 8.0633745e-06
Norm of the params: 17.702723
                Loss: fixed 406 labels. Loss 0.23209. Accuracy 0.922.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47553527
Train loss (w/o reg) on all data: 0.4704973
Test loss (w/o reg) on all data: 0.29994494
Train acc on all data:  0.7802090931193776
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.6426778e-05
Norm of the params: 10.037879
              Random: fixed  95 labels. Loss 0.29994. Accuracy 0.956.
### Flips: 1230, rs: 2, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3424614
Train loss (w/o reg) on all data: 0.33048156
Test loss (w/o reg) on all data: 0.19907428
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0239037e-05
Norm of the params: 15.478927
     Influence (LOO): fixed 393 labels. Loss 0.19907. Accuracy 0.983.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20704883
Train loss (w/o reg) on all data: 0.18666674
Test loss (w/o reg) on all data: 0.1753892
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 3.9527033e-05
Norm of the params: 20.190142
                Loss: fixed 604 labels. Loss 0.17539. Accuracy 0.942.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4614031
Train loss (w/o reg) on all data: 0.45594594
Test loss (w/o reg) on all data: 0.27993882
Train acc on all data:  0.7892049598832969
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.47802975e-05
Norm of the params: 10.447169
              Random: fixed 148 labels. Loss 0.27994. Accuracy 0.964.
### Flips: 1230, rs: 2, checks: 820
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29739362
Train loss (w/o reg) on all data: 0.28529516
Test loss (w/o reg) on all data: 0.1685206
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0997604e-05
Norm of the params: 15.555352
     Influence (LOO): fixed 494 labels. Loss 0.16852. Accuracy 0.980.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12011933
Train loss (w/o reg) on all data: 0.09779823
Test loss (w/o reg) on all data: 0.117824934
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 9.106133e-06
Norm of the params: 21.128702
                Loss: fixed 791 labels. Loss 0.11782. Accuracy 0.962.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44690245
Train loss (w/o reg) on all data: 0.44121215
Test loss (w/o reg) on all data: 0.26346454
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 6.34985e-05
Norm of the params: 10.668006
              Random: fixed 196 labels. Loss 0.26346. Accuracy 0.967.
### Flips: 1230, rs: 2, checks: 1025
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2605065
Train loss (w/o reg) on all data: 0.24838355
Test loss (w/o reg) on all data: 0.14976016
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.2290935e-06
Norm of the params: 15.571102
     Influence (LOO): fixed 578 labels. Loss 0.14976. Accuracy 0.979.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06347131
Train loss (w/o reg) on all data: 0.044633325
Test loss (w/o reg) on all data: 0.07004807
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.5099243e-06
Norm of the params: 19.4103
                Loss: fixed 923 labels. Loss 0.07005. Accuracy 0.982.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4318539
Train loss (w/o reg) on all data: 0.42581826
Test loss (w/o reg) on all data: 0.24899396
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.095869e-05
Norm of the params: 10.986924
              Random: fixed 245 labels. Loss 0.24899. Accuracy 0.974.
### Flips: 1230, rs: 2, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22715054
Train loss (w/o reg) on all data: 0.21530886
Test loss (w/o reg) on all data: 0.1231506
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.6766979e-05
Norm of the params: 15.3894005
     Influence (LOO): fixed 655 labels. Loss 0.12315. Accuracy 0.990.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0399279
Train loss (w/o reg) on all data: 0.025421716
Test loss (w/o reg) on all data: 0.047069043
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.0357882e-06
Norm of the params: 17.033018
                Loss: fixed 974 labels. Loss 0.04707. Accuracy 0.985.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41889924
Train loss (w/o reg) on all data: 0.41278723
Test loss (w/o reg) on all data: 0.2352116
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.8455244e-05
Norm of the params: 11.056234
              Random: fixed 293 labels. Loss 0.23521. Accuracy 0.970.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49898657
Train loss (w/o reg) on all data: 0.49404323
Test loss (w/o reg) on all data: 0.34989694
Train acc on all data:  0.74981765134938
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 2.886375e-05
Norm of the params: 9.9431715
Flipped loss: 0.34990. Accuracy: 0.917
### Flips: 1230, rs: 3, checks: 205
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44210535
Train loss (w/o reg) on all data: 0.4330348
Test loss (w/o reg) on all data: 0.29781467
Train acc on all data:  0.788475565280817
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 3.2013657e-05
Norm of the params: 13.468889
     Influence (LOO): fixed 151 labels. Loss 0.29781. Accuracy 0.935.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4026018
Train loss (w/o reg) on all data: 0.39202943
Test loss (w/o reg) on all data: 0.3113403
Train acc on all data:  0.8016046681254558
Test acc on all data:   0.8785228377065112
Norm of the mean of gradients: 2.2785367e-05
Norm of the params: 14.541236
                Loss: fixed 203 labels. Loss 0.31134. Accuracy 0.879.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48544213
Train loss (w/o reg) on all data: 0.48033953
Test loss (w/o reg) on all data: 0.32751596
Train acc on all data:  0.7612448334548991
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 2.1710004e-05
Norm of the params: 10.102093
              Random: fixed  60 labels. Loss 0.32752. Accuracy 0.931.
### Flips: 1230, rs: 3, checks: 410
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3939327
Train loss (w/o reg) on all data: 0.38325238
Test loss (w/o reg) on all data: 0.25257754
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 4.6211397e-05
Norm of the params: 14.615272
     Influence (LOO): fixed 280 labels. Loss 0.25258. Accuracy 0.956.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31373823
Train loss (w/o reg) on all data: 0.29901123
Test loss (w/o reg) on all data: 0.26939067
Train acc on all data:  0.8524191587648918
Test acc on all data:   0.892128279883382
Norm of the mean of gradients: 3.6580386e-05
Norm of the params: 17.16217
                Loss: fixed 403 labels. Loss 0.26939. Accuracy 0.892.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47541994
Train loss (w/o reg) on all data: 0.47025692
Test loss (w/o reg) on all data: 0.30743644
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.6280235e-05
Norm of the params: 10.1616955
              Random: fixed 110 labels. Loss 0.30744. Accuracy 0.937.
### Flips: 1230, rs: 3, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3548315
Train loss (w/o reg) on all data: 0.34366983
Test loss (w/o reg) on all data: 0.22133343
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.5667023e-05
Norm of the params: 14.9409895
     Influence (LOO): fixed 384 labels. Loss 0.22133. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22414647
Train loss (w/o reg) on all data: 0.20502988
Test loss (w/o reg) on all data: 0.20492186
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 5.1121415e-06
Norm of the params: 19.553307
                Loss: fixed 602 labels. Loss 0.20492. Accuracy 0.916.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4635362
Train loss (w/o reg) on all data: 0.4584117
Test loss (w/o reg) on all data: 0.2914731
Train acc on all data:  0.7845854607342573
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.6008877e-05
Norm of the params: 10.123747
              Random: fixed 160 labels. Loss 0.29147. Accuracy 0.944.
### Flips: 1230, rs: 3, checks: 820
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31182474
Train loss (w/o reg) on all data: 0.30065873
Test loss (w/o reg) on all data: 0.19085889
Train acc on all data:  0.8633600778020909
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.187879e-05
Norm of the params: 14.943899
     Influence (LOO): fixed 488 labels. Loss 0.19086. Accuracy 0.968.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13784537
Train loss (w/o reg) on all data: 0.116832174
Test loss (w/o reg) on all data: 0.14824164
Train acc on all data:  0.950158035497204
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 9.03574e-06
Norm of the params: 20.50034
                Loss: fixed 787 labels. Loss 0.14824. Accuracy 0.948.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44895592
Train loss (w/o reg) on all data: 0.44385642
Test loss (w/o reg) on all data: 0.26526845
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.0890045e-05
Norm of the params: 10.099007
              Random: fixed 221 labels. Loss 0.26527. Accuracy 0.966.
### Flips: 1230, rs: 3, checks: 1025
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272144
Train loss (w/o reg) on all data: 0.26014608
Test loss (w/o reg) on all data: 0.157212
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 8.646234e-06
Norm of the params: 15.490588
     Influence (LOO): fixed 580 labels. Loss 0.15721. Accuracy 0.980.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07327852
Train loss (w/o reg) on all data: 0.055541266
Test loss (w/o reg) on all data: 0.066854134
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 7.116932e-06
Norm of the params: 18.834679
                Loss: fixed 934 labels. Loss 0.06685. Accuracy 0.976.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43238035
Train loss (w/o reg) on all data: 0.42706174
Test loss (w/o reg) on all data: 0.24503241
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7315138e-05
Norm of the params: 10.313685
              Random: fixed 280 labels. Loss 0.24503. Accuracy 0.974.
### Flips: 1230, rs: 3, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2253887
Train loss (w/o reg) on all data: 0.21369214
Test loss (w/o reg) on all data: 0.123370536
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 7.3311658e-06
Norm of the params: 15.2948065
     Influence (LOO): fixed 673 labels. Loss 0.12337. Accuracy 0.985.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046980105
Train loss (w/o reg) on all data: 0.032652866
Test loss (w/o reg) on all data: 0.03570935
Train acc on all data:  0.987357160223681
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.5045415e-06
Norm of the params: 16.927635
                Loss: fixed 991 labels. Loss 0.03571. Accuracy 0.989.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4126532
Train loss (w/o reg) on all data: 0.40736583
Test loss (w/o reg) on all data: 0.21935356
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.7282525e-05
Norm of the params: 10.283363
              Random: fixed 344 labels. Loss 0.21935. Accuracy 0.982.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49691716
Train loss (w/o reg) on all data: 0.4922665
Test loss (w/o reg) on all data: 0.32494703
Train acc on all data:  0.7554096766350595
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.2768391e-05
Norm of the params: 9.6443205
Flipped loss: 0.32495. Accuracy: 0.956
### Flips: 1230, rs: 4, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43175635
Train loss (w/o reg) on all data: 0.42213696
Test loss (w/o reg) on all data: 0.2821728
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.2657581e-05
Norm of the params: 13.870381
     Influence (LOO): fixed 161 labels. Loss 0.28217. Accuracy 0.948.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39268684
Train loss (w/o reg) on all data: 0.38176066
Test loss (w/o reg) on all data: 0.27616665
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9076773566569485
Norm of the mean of gradients: 3.8343438e-05
Norm of the params: 14.782555
                Loss: fixed 205 labels. Loss 0.27617. Accuracy 0.908.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48789695
Train loss (w/o reg) on all data: 0.48328227
Test loss (w/o reg) on all data: 0.30985248
Train acc on all data:  0.7631898857281789
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.2408167e-05
Norm of the params: 9.6069565
              Random: fixed  43 labels. Loss 0.30985. Accuracy 0.960.
### Flips: 1230, rs: 4, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38564807
Train loss (w/o reg) on all data: 0.37414047
Test loss (w/o reg) on all data: 0.2382127
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.9480858e-05
Norm of the params: 15.170756
     Influence (LOO): fixed 283 labels. Loss 0.23821. Accuracy 0.959.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30205908
Train loss (w/o reg) on all data: 0.28653955
Test loss (w/o reg) on all data: 0.22513366
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 9.275141e-06
Norm of the params: 17.617903
                Loss: fixed 406 labels. Loss 0.22513. Accuracy 0.916.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47356695
Train loss (w/o reg) on all data: 0.46857268
Test loss (w/o reg) on all data: 0.29114777
Train acc on all data:  0.7768052516411379
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.3482607e-05
Norm of the params: 9.994258
              Random: fixed  98 labels. Loss 0.29115. Accuracy 0.963.
### Flips: 1230, rs: 4, checks: 615
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3410771
Train loss (w/o reg) on all data: 0.32865664
Test loss (w/o reg) on all data: 0.2042223
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.1740508e-05
Norm of the params: 15.761001
     Influence (LOO): fixed 396 labels. Loss 0.20422. Accuracy 0.966.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21277374
Train loss (w/o reg) on all data: 0.19392456
Test loss (w/o reg) on all data: 0.16384469
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 6.7796595e-06
Norm of the params: 19.416069
                Loss: fixed 602 labels. Loss 0.16384. Accuracy 0.947.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.458096
Train loss (w/o reg) on all data: 0.4532003
Test loss (w/o reg) on all data: 0.271884
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7560838e-05
Norm of the params: 9.895137
              Random: fixed 157 labels. Loss 0.27188. Accuracy 0.964.
### Flips: 1230, rs: 4, checks: 820
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29747024
Train loss (w/o reg) on all data: 0.28469944
Test loss (w/o reg) on all data: 0.1714913
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.0185637e-05
Norm of the params: 15.981747
     Influence (LOO): fixed 497 labels. Loss 0.17149. Accuracy 0.972.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12961824
Train loss (w/o reg) on all data: 0.11093468
Test loss (w/o reg) on all data: 0.092698805
Train acc on all data:  0.9523462193046438
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.439616e-06
Norm of the params: 19.330574
                Loss: fixed 787 labels. Loss 0.09270. Accuracy 0.974.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44708622
Train loss (w/o reg) on all data: 0.44225195
Test loss (w/o reg) on all data: 0.25673726
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.033853e-05
Norm of the params: 9.832867
              Random: fixed 202 labels. Loss 0.25674. Accuracy 0.973.
### Flips: 1230, rs: 4, checks: 1025
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26159567
Train loss (w/o reg) on all data: 0.24889426
Test loss (w/o reg) on all data: 0.14922182
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.6089752e-06
Norm of the params: 15.938262
     Influence (LOO): fixed 578 labels. Loss 0.14922. Accuracy 0.976.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0590714
Train loss (w/o reg) on all data: 0.043158565
Test loss (w/o reg) on all data: 0.04835117
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1156254e-06
Norm of the params: 17.839752
                Loss: fixed 937 labels. Loss 0.04835. Accuracy 0.986.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43133432
Train loss (w/o reg) on all data: 0.42631605
Test loss (w/o reg) on all data: 0.23661388
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.497043e-06
Norm of the params: 10.018243
              Random: fixed 262 labels. Loss 0.23661. Accuracy 0.976.
### Flips: 1230, rs: 4, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22939712
Train loss (w/o reg) on all data: 0.21686433
Test loss (w/o reg) on all data: 0.12514192
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8910601e-05
Norm of the params: 15.832112
     Influence (LOO): fixed 653 labels. Loss 0.12514. Accuracy 0.983.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032520905
Train loss (w/o reg) on all data: 0.020397548
Test loss (w/o reg) on all data: 0.023709312
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.6811003e-07
Norm of the params: 15.571359
                Loss: fixed 992 labels. Loss 0.02371. Accuracy 0.993.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41300586
Train loss (w/o reg) on all data: 0.4075395
Test loss (w/o reg) on all data: 0.22062804
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.0575793e-05
Norm of the params: 10.455982
              Random: fixed 314 labels. Loss 0.22063. Accuracy 0.978.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49810806
Train loss (w/o reg) on all data: 0.49299547
Test loss (w/o reg) on all data: 0.3313857
Train acc on all data:  0.7532214928276197
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.0187234e-05
Norm of the params: 10.111972
Flipped loss: 0.33139. Accuracy: 0.943
### Flips: 1230, rs: 5, checks: 205
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43010002
Train loss (w/o reg) on all data: 0.42106888
Test loss (w/o reg) on all data: 0.274487
Train acc on all data:  0.7957695113056164
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 3.101192e-05
Norm of the params: 13.439609
     Influence (LOO): fixed 168 labels. Loss 0.27449. Accuracy 0.953.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39837393
Train loss (w/o reg) on all data: 0.38627142
Test loss (w/o reg) on all data: 0.28630963
Train acc on all data:  0.8086554826160953
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 8.182315e-06
Norm of the params: 15.557964
                Loss: fixed 203 labels. Loss 0.28631. Accuracy 0.915.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48875567
Train loss (w/o reg) on all data: 0.48348236
Test loss (w/o reg) on all data: 0.31625926
Train acc on all data:  0.7639192803306589
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 8.113506e-05
Norm of the params: 10.269673
              Random: fixed  47 labels. Loss 0.31626. Accuracy 0.946.
### Flips: 1230, rs: 5, checks: 410
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837493
Train loss (w/o reg) on all data: 0.37292823
Test loss (w/o reg) on all data: 0.24144423
Train acc on all data:  0.8210551908582543
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.0641758e-05
Norm of the params: 14.711275
     Influence (LOO): fixed 279 labels. Loss 0.24144. Accuracy 0.950.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31080782
Train loss (w/o reg) on all data: 0.29347855
Test loss (w/o reg) on all data: 0.2294739
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9290573372206026
Norm of the mean of gradients: 3.768135e-05
Norm of the params: 18.616802
                Loss: fixed 403 labels. Loss 0.22947. Accuracy 0.929.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47606447
Train loss (w/o reg) on all data: 0.4709184
Test loss (w/o reg) on all data: 0.29645434
Train acc on all data:  0.7741308047653781
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.504733e-05
Norm of the params: 10.145045
              Random: fixed 108 labels. Loss 0.29645. Accuracy 0.958.
### Flips: 1230, rs: 5, checks: 615
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33244994
Train loss (w/o reg) on all data: 0.32043618
Test loss (w/o reg) on all data: 0.20149969
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.4357285e-05
Norm of the params: 15.500818
     Influence (LOO): fixed 400 labels. Loss 0.20150. Accuracy 0.963.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22162853
Train loss (w/o reg) on all data: 0.19953044
Test loss (w/o reg) on all data: 0.17725065
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.3064807e-05
Norm of the params: 21.022892
                Loss: fixed 605 labels. Loss 0.17725. Accuracy 0.948.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45960847
Train loss (w/o reg) on all data: 0.45408833
Test loss (w/o reg) on all data: 0.27484667
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.1702734e-05
Norm of the params: 10.507268
              Random: fixed 176 labels. Loss 0.27485. Accuracy 0.964.
### Flips: 1230, rs: 5, checks: 820
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29397088
Train loss (w/o reg) on all data: 0.2815757
Test loss (w/o reg) on all data: 0.17248337
Train acc on all data:  0.8691952346219305
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.3210584e-05
Norm of the params: 15.744954
     Influence (LOO): fixed 491 labels. Loss 0.17248. Accuracy 0.970.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1311624
Train loss (w/o reg) on all data: 0.10748605
Test loss (w/o reg) on all data: 0.11888866
Train acc on all data:  0.9530756139071238
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 8.316332e-06
Norm of the params: 21.760681
                Loss: fixed 799 labels. Loss 0.11889. Accuracy 0.969.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44301862
Train loss (w/o reg) on all data: 0.43734074
Test loss (w/o reg) on all data: 0.2550728
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.7556e-05
Norm of the params: 10.656327
              Random: fixed 231 labels. Loss 0.25507. Accuracy 0.967.
### Flips: 1230, rs: 5, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26376456
Train loss (w/o reg) on all data: 0.25148386
Test loss (w/o reg) on all data: 0.14617418
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.703166e-06
Norm of the params: 15.672085
     Influence (LOO): fixed 564 labels. Loss 0.14617. Accuracy 0.981.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065283425
Train loss (w/o reg) on all data: 0.0477299
Test loss (w/o reg) on all data: 0.04961691
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.8446914e-06
Norm of the params: 18.736876
                Loss: fixed 950 labels. Loss 0.04962. Accuracy 0.985.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43089274
Train loss (w/o reg) on all data: 0.42483413
Test loss (w/o reg) on all data: 0.2434319
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.717428e-05
Norm of the params: 11.007812
              Random: fixed 273 labels. Loss 0.24343. Accuracy 0.972.
### Flips: 1230, rs: 5, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22332136
Train loss (w/o reg) on all data: 0.21065043
Test loss (w/o reg) on all data: 0.12048056
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3782687e-05
Norm of the params: 15.919133
     Influence (LOO): fixed 652 labels. Loss 0.12048. Accuracy 0.988.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038561936
Train loss (w/o reg) on all data: 0.024706744
Test loss (w/o reg) on all data: 0.027134003
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.431029e-06
Norm of the params: 16.646437
                Loss: fixed 1002 labels. Loss 0.02713. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41416195
Train loss (w/o reg) on all data: 0.40789452
Test loss (w/o reg) on all data: 0.22832045
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.1047566e-05
Norm of the params: 11.195929
              Random: fixed 327 labels. Loss 0.22832. Accuracy 0.972.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49288565
Train loss (w/o reg) on all data: 0.48815596
Test loss (w/o reg) on all data: 0.33221036
Train acc on all data:  0.7522489666909798
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 2.4841045e-05
Norm of the params: 9.725931
Flipped loss: 0.33221. Accuracy: 0.934
### Flips: 1230, rs: 6, checks: 205
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42873523
Train loss (w/o reg) on all data: 0.4195449
Test loss (w/o reg) on all data: 0.27882782
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.5518895e-05
Norm of the params: 13.557528
     Influence (LOO): fixed 158 labels. Loss 0.27883. Accuracy 0.943.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38980088
Train loss (w/o reg) on all data: 0.37939587
Test loss (w/o reg) on all data: 0.2872891
Train acc on all data:  0.8067104303428154
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 2.7260656e-05
Norm of the params: 14.425668
                Loss: fixed 204 labels. Loss 0.28729. Accuracy 0.912.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48199147
Train loss (w/o reg) on all data: 0.47699583
Test loss (w/o reg) on all data: 0.31316495
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 2.423148e-05
Norm of the params: 9.99565
              Random: fixed  51 labels. Loss 0.31316. Accuracy 0.941.
### Flips: 1230, rs: 6, checks: 410
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38456473
Train loss (w/o reg) on all data: 0.3739383
Test loss (w/o reg) on all data: 0.2448046
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.3431081e-05
Norm of the params: 14.578362
     Influence (LOO): fixed 270 labels. Loss 0.24480. Accuracy 0.943.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2978216
Train loss (w/o reg) on all data: 0.28239903
Test loss (w/o reg) on all data: 0.23724033
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 6.250476e-05
Norm of the params: 17.562792
                Loss: fixed 405 labels. Loss 0.23724. Accuracy 0.924.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46894908
Train loss (w/o reg) on all data: 0.46376574
Test loss (w/o reg) on all data: 0.2962672
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 9.2463124e-05
Norm of the params: 10.181696
              Random: fixed 105 labels. Loss 0.29627. Accuracy 0.947.
### Flips: 1230, rs: 6, checks: 615
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3384028
Train loss (w/o reg) on all data: 0.32764438
Test loss (w/o reg) on all data: 0.20916294
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.413578e-06
Norm of the params: 14.668634
     Influence (LOO): fixed 390 labels. Loss 0.20916. Accuracy 0.960.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21011594
Train loss (w/o reg) on all data: 0.19100049
Test loss (w/o reg) on all data: 0.17254034
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 4.1485528e-06
Norm of the params: 19.552725
                Loss: fixed 601 labels. Loss 0.17254. Accuracy 0.947.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45514444
Train loss (w/o reg) on all data: 0.44972998
Test loss (w/o reg) on all data: 0.2771373
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 9.767141e-06
Norm of the params: 10.406204
              Random: fixed 160 labels. Loss 0.27714. Accuracy 0.951.
### Flips: 1230, rs: 6, checks: 820
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30344203
Train loss (w/o reg) on all data: 0.29252854
Test loss (w/o reg) on all data: 0.1794703
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.9085337e-05
Norm of the params: 14.7739525
     Influence (LOO): fixed 474 labels. Loss 0.17947. Accuracy 0.965.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12263348
Train loss (w/o reg) on all data: 0.100613005
Test loss (w/o reg) on all data: 0.10823623
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.8543044e-06
Norm of the params: 20.985935
                Loss: fixed 787 labels. Loss 0.10824. Accuracy 0.961.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43990552
Train loss (w/o reg) on all data: 0.43435103
Test loss (w/o reg) on all data: 0.2585082
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.6571327e-05
Norm of the params: 10.539914
              Random: fixed 222 labels. Loss 0.25851. Accuracy 0.958.
### Flips: 1230, rs: 6, checks: 1025
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2650608
Train loss (w/o reg) on all data: 0.25430492
Test loss (w/o reg) on all data: 0.15295485
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.204087e-05
Norm of the params: 14.6669035
     Influence (LOO): fixed 567 labels. Loss 0.15295. Accuracy 0.975.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07086964
Train loss (w/o reg) on all data: 0.05311261
Test loss (w/o reg) on all data: 0.065175615
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5838755e-06
Norm of the params: 18.845175
                Loss: fixed 916 labels. Loss 0.06518. Accuracy 0.976.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42448595
Train loss (w/o reg) on all data: 0.4188525
Test loss (w/o reg) on all data: 0.24423642
Train acc on all data:  0.8135181132992949
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.9043886e-05
Norm of the params: 10.614555
              Random: fixed 274 labels. Loss 0.24424. Accuracy 0.962.
### Flips: 1230, rs: 6, checks: 1230
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22731955
Train loss (w/o reg) on all data: 0.21571942
Test loss (w/o reg) on all data: 0.12760249
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.514496e-06
Norm of the params: 15.231637
     Influence (LOO): fixed 650 labels. Loss 0.12760. Accuracy 0.983.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03940978
Train loss (w/o reg) on all data: 0.025720067
Test loss (w/o reg) on all data: 0.031220391
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1947877e-06
Norm of the params: 16.54673
                Loss: fixed 986 labels. Loss 0.03122. Accuracy 0.990.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41174054
Train loss (w/o reg) on all data: 0.40604672
Test loss (w/o reg) on all data: 0.22904104
Train acc on all data:  0.8244590323364941
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.3305744e-05
Norm of the params: 10.671298
              Random: fixed 319 labels. Loss 0.22904. Accuracy 0.969.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.50041455
Train loss (w/o reg) on all data: 0.49554905
Test loss (w/o reg) on all data: 0.32859537
Train acc on all data:  0.7517627036226598
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 2.760683e-05
Norm of the params: 9.864599
Flipped loss: 0.32860. Accuracy: 0.952
### Flips: 1230, rs: 7, checks: 205
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4380524
Train loss (w/o reg) on all data: 0.42962533
Test loss (w/o reg) on all data: 0.2672169
Train acc on all data:  0.7860442499392171
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 8.816078e-06
Norm of the params: 12.982332
     Influence (LOO): fixed 163 labels. Loss 0.26722. Accuracy 0.965.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4012851
Train loss (w/o reg) on all data: 0.39041448
Test loss (w/o reg) on all data: 0.28786767
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 9.850248e-06
Norm of the params: 14.744917
                Loss: fixed 203 labels. Loss 0.28787. Accuracy 0.895.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48825938
Train loss (w/o reg) on all data: 0.48337024
Test loss (w/o reg) on all data: 0.30629197
Train acc on all data:  0.7646486749331388
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.6176642e-05
Norm of the params: 9.888505
              Random: fixed  60 labels. Loss 0.30629. Accuracy 0.959.
### Flips: 1230, rs: 7, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.391042
Train loss (w/o reg) on all data: 0.38093323
Test loss (w/o reg) on all data: 0.22977069
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.872984e-06
Norm of the params: 14.218837
     Influence (LOO): fixed 286 labels. Loss 0.22977. Accuracy 0.980.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30803216
Train loss (w/o reg) on all data: 0.29256323
Test loss (w/o reg) on all data: 0.22831592
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 9.448879e-06
Norm of the params: 17.589167
                Loss: fixed 407 labels. Loss 0.22832. Accuracy 0.928.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47731957
Train loss (w/o reg) on all data: 0.47240934
Test loss (w/o reg) on all data: 0.29090017
Train acc on all data:  0.7765621201069779
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.0829406e-05
Norm of the params: 9.909815
              Random: fixed 109 labels. Loss 0.29090. Accuracy 0.965.
### Flips: 1230, rs: 7, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33921325
Train loss (w/o reg) on all data: 0.32753745
Test loss (w/o reg) on all data: 0.19449471
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4253306e-05
Norm of the params: 15.281234
     Influence (LOO): fixed 410 labels. Loss 0.19449. Accuracy 0.982.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21328646
Train loss (w/o reg) on all data: 0.19379328
Test loss (w/o reg) on all data: 0.16251428
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.6889124e-05
Norm of the params: 19.744965
                Loss: fixed 611 labels. Loss 0.16251. Accuracy 0.947.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46215665
Train loss (w/o reg) on all data: 0.4568612
Test loss (w/o reg) on all data: 0.27546474
Train acc on all data:  0.787503039144177
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 7.8964564e-05
Norm of the params: 10.291209
              Random: fixed 167 labels. Loss 0.27546. Accuracy 0.969.
### Flips: 1230, rs: 7, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2991797
Train loss (w/o reg) on all data: 0.2869325
Test loss (w/o reg) on all data: 0.16981016
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.8777702e-05
Norm of the params: 15.650698
     Influence (LOO): fixed 499 labels. Loss 0.16981. Accuracy 0.985.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12413197
Train loss (w/o reg) on all data: 0.10302196
Test loss (w/o reg) on all data: 0.10535765
Train acc on all data:  0.9547775346462436
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 9.531461e-06
Norm of the params: 20.547514
                Loss: fixed 806 labels. Loss 0.10536. Accuracy 0.957.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44698054
Train loss (w/o reg) on all data: 0.44189718
Test loss (w/o reg) on all data: 0.25359532
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.7913837e-05
Norm of the params: 10.082995
              Random: fixed 223 labels. Loss 0.25360. Accuracy 0.979.
### Flips: 1230, rs: 7, checks: 1025
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26296505
Train loss (w/o reg) on all data: 0.24994925
Test loss (w/o reg) on all data: 0.14625958
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.472633e-06
Norm of the params: 16.134315
     Influence (LOO): fixed 577 labels. Loss 0.14626. Accuracy 0.990.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06257322
Train loss (w/o reg) on all data: 0.04571575
Test loss (w/o reg) on all data: 0.044849224
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.8485292e-06
Norm of the params: 18.361626
                Loss: fixed 948 labels. Loss 0.04485. Accuracy 0.986.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4308184
Train loss (w/o reg) on all data: 0.42548406
Test loss (w/o reg) on all data: 0.23252709
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.237838e-05
Norm of the params: 10.328933
              Random: fixed 280 labels. Loss 0.23253. Accuracy 0.985.
### Flips: 1230, rs: 7, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23056945
Train loss (w/o reg) on all data: 0.21794677
Test loss (w/o reg) on all data: 0.12544723
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.54568e-06
Norm of the params: 15.888794
     Influence (LOO): fixed 651 labels. Loss 0.12545. Accuracy 0.991.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030177912
Train loss (w/o reg) on all data: 0.018450066
Test loss (w/o reg) on all data: 0.016634619
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.0707506e-07
Norm of the params: 15.315251
                Loss: fixed 1017 labels. Loss 0.01663. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41650563
Train loss (w/o reg) on all data: 0.4112925
Test loss (w/o reg) on all data: 0.21790357
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.1761949e-05
Norm of the params: 10.210914
              Random: fixed 330 labels. Loss 0.21790. Accuracy 0.986.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4976432
Train loss (w/o reg) on all data: 0.49258703
Test loss (w/o reg) on all data: 0.3327962
Train acc on all data:  0.7571115973741794
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 5.159131e-05
Norm of the params: 10.0560055
Flipped loss: 0.33280. Accuracy: 0.949
### Flips: 1230, rs: 8, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4351969
Train loss (w/o reg) on all data: 0.42557383
Test loss (w/o reg) on all data: 0.28197667
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.0375712e-05
Norm of the params: 13.873053
     Influence (LOO): fixed 154 labels. Loss 0.28198. Accuracy 0.949.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4007701
Train loss (w/o reg) on all data: 0.38958555
Test loss (w/o reg) on all data: 0.28482428
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 2.8644321e-05
Norm of the params: 14.956294
                Loss: fixed 203 labels. Loss 0.28482. Accuracy 0.917.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4829908
Train loss (w/o reg) on all data: 0.47780567
Test loss (w/o reg) on all data: 0.31351057
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 4.6443947e-05
Norm of the params: 10.183443
              Random: fixed  63 labels. Loss 0.31351. Accuracy 0.955.
### Flips: 1230, rs: 8, checks: 410
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38832507
Train loss (w/o reg) on all data: 0.37739724
Test loss (w/o reg) on all data: 0.23063128
Train acc on all data:  0.8227571115973742
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.9021236e-05
Norm of the params: 14.78366
     Influence (LOO): fixed 286 labels. Loss 0.23063. Accuracy 0.968.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31036207
Train loss (w/o reg) on all data: 0.29470044
Test loss (w/o reg) on all data: 0.22447346
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.6617736e-05
Norm of the params: 17.69837
                Loss: fixed 406 labels. Loss 0.22447. Accuracy 0.931.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46983716
Train loss (w/o reg) on all data: 0.46466526
Test loss (w/o reg) on all data: 0.29375148
Train acc on all data:  0.7804522246535376
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.4203713e-05
Norm of the params: 10.170443
              Random: fixed 117 labels. Loss 0.29375. Accuracy 0.963.
### Flips: 1230, rs: 8, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34377426
Train loss (w/o reg) on all data: 0.33233467
Test loss (w/o reg) on all data: 0.19338782
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.7874463e-05
Norm of the params: 15.125857
     Influence (LOO): fixed 401 labels. Loss 0.19339. Accuracy 0.981.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21504779
Train loss (w/o reg) on all data: 0.19589603
Test loss (w/o reg) on all data: 0.1604844
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.1194117e-05
Norm of the params: 19.571283
                Loss: fixed 607 labels. Loss 0.16048. Accuracy 0.941.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4556443
Train loss (w/o reg) on all data: 0.44994348
Test loss (w/o reg) on all data: 0.27933812
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.709349e-05
Norm of the params: 10.677845
              Random: fixed 168 labels. Loss 0.27934. Accuracy 0.971.
### Flips: 1230, rs: 8, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3052305
Train loss (w/o reg) on all data: 0.2943946
Test loss (w/o reg) on all data: 0.16489951
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2919876e-05
Norm of the params: 14.721331
     Influence (LOO): fixed 491 labels. Loss 0.16490. Accuracy 0.986.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12028439
Train loss (w/o reg) on all data: 0.09910402
Test loss (w/o reg) on all data: 0.10665513
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.180131e-06
Norm of the params: 20.581728
                Loss: fixed 800 labels. Loss 0.10666. Accuracy 0.963.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4402273
Train loss (w/o reg) on all data: 0.4344277
Test loss (w/o reg) on all data: 0.2602445
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.915088e-05
Norm of the params: 10.769943
              Random: fixed 224 labels. Loss 0.26024. Accuracy 0.972.
### Flips: 1230, rs: 8, checks: 1025
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26816753
Train loss (w/o reg) on all data: 0.25774947
Test loss (w/o reg) on all data: 0.13979341
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2046857e-05
Norm of the params: 14.434727
     Influence (LOO): fixed 572 labels. Loss 0.13979. Accuracy 0.994.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057270136
Train loss (w/o reg) on all data: 0.04047893
Test loss (w/o reg) on all data: 0.045867372
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4010152e-06
Norm of the params: 18.325504
                Loss: fixed 946 labels. Loss 0.04587. Accuracy 0.987.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42597777
Train loss (w/o reg) on all data: 0.4198457
Test loss (w/o reg) on all data: 0.24461271
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.5486568e-05
Norm of the params: 11.074365
              Random: fixed 271 labels. Loss 0.24461. Accuracy 0.970.
### Flips: 1230, rs: 8, checks: 1230
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22883949
Train loss (w/o reg) on all data: 0.21796162
Test loss (w/o reg) on all data: 0.11509671
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7727257e-06
Norm of the params: 14.749818
     Influence (LOO): fixed 652 labels. Loss 0.11510. Accuracy 0.994.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03129732
Train loss (w/o reg) on all data: 0.018881591
Test loss (w/o reg) on all data: 0.025590464
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9006316e-06
Norm of the params: 15.757998
                Loss: fixed 998 labels. Loss 0.02559. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40827578
Train loss (w/o reg) on all data: 0.40203387
Test loss (w/o reg) on all data: 0.23085053
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.7326154e-05
Norm of the params: 11.173104
              Random: fixed 320 labels. Loss 0.23085. Accuracy 0.969.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49769023
Train loss (w/o reg) on all data: 0.49316365
Test loss (w/o reg) on all data: 0.33540922
Train acc on all data:  0.7520058351568198
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 5.9296166e-05
Norm of the params: 9.514809
Flipped loss: 0.33541. Accuracy: 0.937
### Flips: 1230, rs: 9, checks: 205
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43796453
Train loss (w/o reg) on all data: 0.42955416
Test loss (w/o reg) on all data: 0.28009093
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.8309802e-05
Norm of the params: 12.969466
     Influence (LOO): fixed 156 labels. Loss 0.28009. Accuracy 0.959.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39867678
Train loss (w/o reg) on all data: 0.3877086
Test loss (w/o reg) on all data: 0.286141
Train acc on all data:  0.8011184050571359
Test acc on all data:   0.9135082604470359
Norm of the mean of gradients: 2.4210123e-05
Norm of the params: 14.810921
                Loss: fixed 202 labels. Loss 0.28614. Accuracy 0.914.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4819071
Train loss (w/o reg) on all data: 0.47679168
Test loss (w/o reg) on all data: 0.31512865
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.2496535e-05
Norm of the params: 10.114772
              Random: fixed  63 labels. Loss 0.31513. Accuracy 0.955.
### Flips: 1230, rs: 9, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38598052
Train loss (w/o reg) on all data: 0.3751417
Test loss (w/o reg) on all data: 0.2397267
Train acc on all data:  0.8208120593240943
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.4008468e-05
Norm of the params: 14.723316
     Influence (LOO): fixed 292 labels. Loss 0.23973. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30539626
Train loss (w/o reg) on all data: 0.28970546
Test loss (w/o reg) on all data: 0.23625816
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9096209912536443
Norm of the mean of gradients: 1.4885491e-05
Norm of the params: 17.714846
                Loss: fixed 407 labels. Loss 0.23626. Accuracy 0.910.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4699978
Train loss (w/o reg) on all data: 0.46465352
Test loss (w/o reg) on all data: 0.2941488
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.5333127e-05
Norm of the params: 10.338532
              Random: fixed 116 labels. Loss 0.29415. Accuracy 0.962.
### Flips: 1230, rs: 9, checks: 615
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34095043
Train loss (w/o reg) on all data: 0.32948688
Test loss (w/o reg) on all data: 0.20485182
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.1751982e-05
Norm of the params: 15.141698
     Influence (LOO): fixed 401 labels. Loss 0.20485. Accuracy 0.964.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21911496
Train loss (w/o reg) on all data: 0.19940808
Test loss (w/o reg) on all data: 0.16950332
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 3.979107e-06
Norm of the params: 19.852894
                Loss: fixed 600 labels. Loss 0.16950. Accuracy 0.940.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45687386
Train loss (w/o reg) on all data: 0.4513119
Test loss (w/o reg) on all data: 0.27994537
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 6.23277e-05
Norm of the params: 10.547007
              Random: fixed 171 labels. Loss 0.27995. Accuracy 0.969.
### Flips: 1230, rs: 9, checks: 820
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30177954
Train loss (w/o reg) on all data: 0.28931063
Test loss (w/o reg) on all data: 0.17296723
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.000701e-05
Norm of the params: 15.791711
     Influence (LOO): fixed 492 labels. Loss 0.17297. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1404486
Train loss (w/o reg) on all data: 0.11951188
Test loss (w/o reg) on all data: 0.11499951
Train acc on all data:  0.9467541940189642
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.543687e-06
Norm of the params: 20.463
                Loss: fixed 786 labels. Loss 0.11500. Accuracy 0.964.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4443078
Train loss (w/o reg) on all data: 0.43903634
Test loss (w/o reg) on all data: 0.26179513
Train acc on all data:  0.7960126428397764
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.2340704e-05
Norm of the params: 10.267863
              Random: fixed 226 labels. Loss 0.26180. Accuracy 0.973.
### Flips: 1230, rs: 9, checks: 1025
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2654505
Train loss (w/o reg) on all data: 0.25270706
Test loss (w/o reg) on all data: 0.14464852
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6366675e-05
Norm of the params: 15.964608
     Influence (LOO): fixed 577 labels. Loss 0.14465. Accuracy 0.984.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074586876
Train loss (w/o reg) on all data: 0.05575164
Test loss (w/o reg) on all data: 0.058020476
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.3165412e-06
Norm of the params: 19.408882
                Loss: fixed 937 labels. Loss 0.05802. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42549902
Train loss (w/o reg) on all data: 0.42027566
Test loss (w/o reg) on all data: 0.24215019
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.6352968e-05
Norm of the params: 10.220931
              Random: fixed 290 labels. Loss 0.24215. Accuracy 0.977.
### Flips: 1230, rs: 9, checks: 1230
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22117758
Train loss (w/o reg) on all data: 0.2080562
Test loss (w/o reg) on all data: 0.11565098
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.2123363e-05
Norm of the params: 16.199615
     Influence (LOO): fixed 672 labels. Loss 0.11565. Accuracy 0.988.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042933848
Train loss (w/o reg) on all data: 0.028864626
Test loss (w/o reg) on all data: 0.03890931
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.816857e-06
Norm of the params: 16.774519
                Loss: fixed 1003 labels. Loss 0.03891. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41059035
Train loss (w/o reg) on all data: 0.40512365
Test loss (w/o reg) on all data: 0.2251066
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.9235655e-06
Norm of the params: 10.4562845
              Random: fixed 343 labels. Loss 0.22511. Accuracy 0.976.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49444184
Train loss (w/o reg) on all data: 0.48953032
Test loss (w/o reg) on all data: 0.33522192
Train acc on all data:  0.7527352297592997
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 1.2999789e-05
Norm of the params: 9.911107
Flipped loss: 0.33522. Accuracy: 0.919
### Flips: 1230, rs: 10, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43225938
Train loss (w/o reg) on all data: 0.42235634
Test loss (w/o reg) on all data: 0.28525016
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9290573372206026
Norm of the mean of gradients: 1.4040918e-05
Norm of the params: 14.07341
     Influence (LOO): fixed 155 labels. Loss 0.28525. Accuracy 0.929.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3967858
Train loss (w/o reg) on all data: 0.38623923
Test loss (w/o reg) on all data: 0.28696764
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.8999028182701652
Norm of the mean of gradients: 1.3353487e-05
Norm of the params: 14.523476
                Loss: fixed 203 labels. Loss 0.28697. Accuracy 0.900.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48441464
Train loss (w/o reg) on all data: 0.47959977
Test loss (w/o reg) on all data: 0.31915918
Train acc on all data:  0.762703622659859
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 2.0949132e-05
Norm of the params: 9.813124
              Random: fixed  51 labels. Loss 0.31916. Accuracy 0.928.
### Flips: 1230, rs: 10, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38695353
Train loss (w/o reg) on all data: 0.37574333
Test loss (w/o reg) on all data: 0.24332134
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.7805034e-05
Norm of the params: 14.973438
     Influence (LOO): fixed 280 labels. Loss 0.24332. Accuracy 0.951.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30360818
Train loss (w/o reg) on all data: 0.28784266
Test loss (w/o reg) on all data: 0.22941439
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 2.5049325e-05
Norm of the params: 17.75698
                Loss: fixed 406 labels. Loss 0.22941. Accuracy 0.918.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47277057
Train loss (w/o reg) on all data: 0.4675705
Test loss (w/o reg) on all data: 0.30364558
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 9.2285896e-05
Norm of the params: 10.198101
              Random: fixed  99 labels. Loss 0.30365. Accuracy 0.940.
### Flips: 1230, rs: 10, checks: 615
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3427432
Train loss (w/o reg) on all data: 0.33077857
Test loss (w/o reg) on all data: 0.20827258
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.8262464e-05
Norm of the params: 15.469076
     Influence (LOO): fixed 391 labels. Loss 0.20827. Accuracy 0.966.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21168515
Train loss (w/o reg) on all data: 0.19067477
Test loss (w/o reg) on all data: 0.17939919
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 7.941904e-06
Norm of the params: 20.498966
                Loss: fixed 603 labels. Loss 0.17940. Accuracy 0.934.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4590647
Train loss (w/o reg) on all data: 0.45384005
Test loss (w/o reg) on all data: 0.28256708
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.1637003e-05
Norm of the params: 10.2221775
              Random: fixed 152 labels. Loss 0.28257. Accuracy 0.950.
### Flips: 1230, rs: 10, checks: 820
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30721056
Train loss (w/o reg) on all data: 0.29515
Test loss (w/o reg) on all data: 0.17878366
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 8.182439e-06
Norm of the params: 15.530967
     Influence (LOO): fixed 482 labels. Loss 0.17878. Accuracy 0.972.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13098365
Train loss (w/o reg) on all data: 0.108941875
Test loss (w/o reg) on all data: 0.1273787
Train acc on all data:  0.9528324823729638
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.530242e-06
Norm of the params: 20.99608
                Loss: fixed 778 labels. Loss 0.12738. Accuracy 0.953.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44528693
Train loss (w/o reg) on all data: 0.44001955
Test loss (w/o reg) on all data: 0.26093516
Train acc on all data:  0.799173352783856
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.3314852e-05
Norm of the params: 10.263898
              Random: fixed 212 labels. Loss 0.26094. Accuracy 0.957.
### Flips: 1230, rs: 10, checks: 1025
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27103227
Train loss (w/o reg) on all data: 0.2591557
Test loss (w/o reg) on all data: 0.14545316
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.617198e-05
Norm of the params: 15.41206
     Influence (LOO): fixed 573 labels. Loss 0.14545. Accuracy 0.982.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0770452
Train loss (w/o reg) on all data: 0.058130212
Test loss (w/o reg) on all data: 0.07313235
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 6.134038e-06
Norm of the params: 19.449928
                Loss: fixed 903 labels. Loss 0.07313. Accuracy 0.978.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4304749
Train loss (w/o reg) on all data: 0.42512745
Test loss (w/o reg) on all data: 0.2428613
Train acc on all data:  0.8106005348893751
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.3425722e-05
Norm of the params: 10.34162
              Random: fixed 262 labels. Loss 0.24286. Accuracy 0.967.
### Flips: 1230, rs: 10, checks: 1230
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23584634
Train loss (w/o reg) on all data: 0.22412845
Test loss (w/o reg) on all data: 0.12181577
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.8050425e-05
Norm of the params: 15.308743
     Influence (LOO): fixed 652 labels. Loss 0.12182. Accuracy 0.988.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046567593
Train loss (w/o reg) on all data: 0.03139157
Test loss (w/o reg) on all data: 0.039759055
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7310593e-06
Norm of the params: 17.42184
                Loss: fixed 971 labels. Loss 0.03976. Accuracy 0.991.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41233912
Train loss (w/o reg) on all data: 0.40685794
Test loss (w/o reg) on all data: 0.2220068
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7358188e-05
Norm of the params: 10.4701185
              Random: fixed 320 labels. Loss 0.22201. Accuracy 0.974.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49470803
Train loss (w/o reg) on all data: 0.49052903
Test loss (w/o reg) on all data: 0.34079403
Train acc on all data:  0.7597860442499392
Test acc on all data:   0.9203109815354713
Norm of the mean of gradients: 9.3913324e-05
Norm of the params: 9.142202
Flipped loss: 0.34079. Accuracy: 0.920
### Flips: 1230, rs: 11, checks: 205
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43350756
Train loss (w/o reg) on all data: 0.42485008
Test loss (w/o reg) on all data: 0.28836706
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 8.308413e-05
Norm of the params: 13.158634
     Influence (LOO): fixed 151 labels. Loss 0.28837. Accuracy 0.931.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3900232
Train loss (w/o reg) on all data: 0.37898943
Test loss (w/o reg) on all data: 0.29375905
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.892128279883382
Norm of the mean of gradients: 1.3906108e-05
Norm of the params: 14.855149
                Loss: fixed 201 labels. Loss 0.29376. Accuracy 0.892.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4842782
Train loss (w/o reg) on all data: 0.48003784
Test loss (w/o reg) on all data: 0.32389927
Train acc on all data:  0.7665937272064187
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 1.19536935e-05
Norm of the params: 9.209077
              Random: fixed  51 labels. Loss 0.32390. Accuracy 0.928.
### Flips: 1230, rs: 11, checks: 410
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3861868
Train loss (w/o reg) on all data: 0.37584713
Test loss (w/o reg) on all data: 0.24585512
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.9464387e-05
Norm of the params: 14.380317
     Influence (LOO): fixed 280 labels. Loss 0.24586. Accuracy 0.944.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2956541
Train loss (w/o reg) on all data: 0.28039968
Test loss (w/o reg) on all data: 0.24848707
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.901846452866861
Norm of the mean of gradients: 1.9584088e-05
Norm of the params: 17.466764
                Loss: fixed 403 labels. Loss 0.24849. Accuracy 0.902.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47045276
Train loss (w/o reg) on all data: 0.46608037
Test loss (w/o reg) on all data: 0.30039802
Train acc on all data:  0.7804522246535376
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 4.7477526e-05
Norm of the params: 9.351352
              Random: fixed 111 labels. Loss 0.30040. Accuracy 0.940.
### Flips: 1230, rs: 11, checks: 615
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34336752
Train loss (w/o reg) on all data: 0.33235478
Test loss (w/o reg) on all data: 0.20880479
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.0912748e-05
Norm of the params: 14.8409815
     Influence (LOO): fixed 391 labels. Loss 0.20880. Accuracy 0.967.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20126712
Train loss (w/o reg) on all data: 0.18090378
Test loss (w/o reg) on all data: 0.19142467
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 1.6406284e-05
Norm of the params: 20.180851
                Loss: fixed 603 labels. Loss 0.19142. Accuracy 0.926.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45997685
Train loss (w/o reg) on all data: 0.45558843
Test loss (w/o reg) on all data: 0.28548604
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.0745103e-05
Norm of the params: 9.36847
              Random: fixed 161 labels. Loss 0.28549. Accuracy 0.948.
### Flips: 1230, rs: 11, checks: 820
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2997615
Train loss (w/o reg) on all data: 0.288271
Test loss (w/o reg) on all data: 0.17295998
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.0561038e-05
Norm of the params: 15.159477
     Influence (LOO): fixed 498 labels. Loss 0.17296. Accuracy 0.978.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11956409
Train loss (w/o reg) on all data: 0.097344525
Test loss (w/o reg) on all data: 0.1259792
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.182942e-06
Norm of the params: 21.080595
                Loss: fixed 784 labels. Loss 0.12598. Accuracy 0.952.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44540542
Train loss (w/o reg) on all data: 0.4409236
Test loss (w/o reg) on all data: 0.26313776
Train acc on all data:  0.8001458789204959
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.066901e-05
Norm of the params: 9.467643
              Random: fixed 220 labels. Loss 0.26314. Accuracy 0.956.
### Flips: 1230, rs: 11, checks: 1025
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25932536
Train loss (w/o reg) on all data: 0.24811776
Test loss (w/o reg) on all data: 0.14211756
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.375018e-06
Norm of the params: 14.971713
     Influence (LOO): fixed 592 labels. Loss 0.14212. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071851455
Train loss (w/o reg) on all data: 0.053145394
Test loss (w/o reg) on all data: 0.07481948
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.953597e-06
Norm of the params: 19.342213
                Loss: fixed 914 labels. Loss 0.07482. Accuracy 0.975.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42866653
Train loss (w/o reg) on all data: 0.4243217
Test loss (w/o reg) on all data: 0.23750906
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 4.3370386e-05
Norm of the params: 9.321816
              Random: fixed 288 labels. Loss 0.23751. Accuracy 0.971.
### Flips: 1230, rs: 11, checks: 1230
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21689357
Train loss (w/o reg) on all data: 0.20592289
Test loss (w/o reg) on all data: 0.11141248
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.65228e-06
Norm of the params: 14.812614
     Influence (LOO): fixed 682 labels. Loss 0.11141. Accuracy 0.993.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043372385
Train loss (w/o reg) on all data: 0.028524501
Test loss (w/o reg) on all data: 0.04390974
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3895626e-06
Norm of the params: 17.232462
                Loss: fixed 981 labels. Loss 0.04391. Accuracy 0.988.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40923727
Train loss (w/o reg) on all data: 0.40474427
Test loss (w/o reg) on all data: 0.22058383
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.4623772e-05
Norm of the params: 9.479443
              Random: fixed 349 labels. Loss 0.22058. Accuracy 0.973.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49389735
Train loss (w/o reg) on all data: 0.4885865
Test loss (w/o reg) on all data: 0.3389629
Train acc on all data:  0.7585703865791393
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.4792533e-05
Norm of the params: 10.306178
Flipped loss: 0.33896. Accuracy: 0.938
### Flips: 1230, rs: 12, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42476118
Train loss (w/o reg) on all data: 0.41470775
Test loss (w/o reg) on all data: 0.2888504
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.2012113e-05
Norm of the params: 14.1798725
     Influence (LOO): fixed 161 labels. Loss 0.28885. Accuracy 0.943.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3899081
Train loss (w/o reg) on all data: 0.37667257
Test loss (w/o reg) on all data: 0.2964325
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.9057337220602527
Norm of the mean of gradients: 5.412564e-05
Norm of the params: 16.269932
                Loss: fixed 203 labels. Loss 0.29643. Accuracy 0.906.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4812934
Train loss (w/o reg) on all data: 0.47590265
Test loss (w/o reg) on all data: 0.3226474
Train acc on all data:  0.7699975686846584
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 4.5885543e-05
Norm of the params: 10.383413
              Random: fixed  51 labels. Loss 0.32265. Accuracy 0.943.
### Flips: 1230, rs: 12, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38100997
Train loss (w/o reg) on all data: 0.36991432
Test loss (w/o reg) on all data: 0.253325
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 2.987805e-05
Norm of the params: 14.896738
     Influence (LOO): fixed 278 labels. Loss 0.25332. Accuracy 0.950.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3047573
Train loss (w/o reg) on all data: 0.28758425
Test loss (w/o reg) on all data: 0.23993438
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 0.000103139966
Norm of the params: 18.532698
                Loss: fixed 404 labels. Loss 0.23993. Accuracy 0.915.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4698742
Train loss (w/o reg) on all data: 0.46445715
Test loss (w/o reg) on all data: 0.3013295
Train acc on all data:  0.7792365669827377
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 9.822205e-05
Norm of the params: 10.408686
              Random: fixed 103 labels. Loss 0.30133. Accuracy 0.952.
### Flips: 1230, rs: 12, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3377144
Train loss (w/o reg) on all data: 0.3255871
Test loss (w/o reg) on all data: 0.21912678
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.9704868e-05
Norm of the params: 15.573894
     Influence (LOO): fixed 386 labels. Loss 0.21913. Accuracy 0.963.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2140279
Train loss (w/o reg) on all data: 0.19401248
Test loss (w/o reg) on all data: 0.18549725
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.884607e-05
Norm of the params: 20.00771
                Loss: fixed 605 labels. Loss 0.18550. Accuracy 0.935.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45848888
Train loss (w/o reg) on all data: 0.45289764
Test loss (w/o reg) on all data: 0.2851703
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.890491e-05
Norm of the params: 10.574717
              Random: fixed 149 labels. Loss 0.28517. Accuracy 0.966.
### Flips: 1230, rs: 12, checks: 820
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2960757
Train loss (w/o reg) on all data: 0.28441426
Test loss (w/o reg) on all data: 0.18104935
Train acc on all data:  0.8674933138828106
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 6.286405e-05
Norm of the params: 15.2718315
     Influence (LOO): fixed 495 labels. Loss 0.18105. Accuracy 0.971.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122412495
Train loss (w/o reg) on all data: 0.1010125
Test loss (w/o reg) on all data: 0.12263648
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 3.6897852e-06
Norm of the params: 20.68816
                Loss: fixed 799 labels. Loss 0.12264. Accuracy 0.956.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44345748
Train loss (w/o reg) on all data: 0.43758053
Test loss (w/o reg) on all data: 0.26571605
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.6890193e-05
Norm of the params: 10.841558
              Random: fixed 203 labels. Loss 0.26572. Accuracy 0.970.
### Flips: 1230, rs: 12, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2606696
Train loss (w/o reg) on all data: 0.24825588
Test loss (w/o reg) on all data: 0.15288122
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.7336047e-06
Norm of the params: 15.756726
     Influence (LOO): fixed 576 labels. Loss 0.15288. Accuracy 0.976.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063852325
Train loss (w/o reg) on all data: 0.045894403
Test loss (w/o reg) on all data: 0.06174299
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.348556e-06
Norm of the params: 18.951477
                Loss: fixed 931 labels. Loss 0.06174. Accuracy 0.977.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42784652
Train loss (w/o reg) on all data: 0.4216977
Test loss (w/o reg) on all data: 0.24538776
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6431779e-05
Norm of the params: 11.089463
              Random: fixed 255 labels. Loss 0.24539. Accuracy 0.973.
### Flips: 1230, rs: 12, checks: 1230
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22858523
Train loss (w/o reg) on all data: 0.21627532
Test loss (w/o reg) on all data: 0.12978262
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.7172271e-05
Norm of the params: 15.690705
     Influence (LOO): fixed 649 labels. Loss 0.12978. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040388152
Train loss (w/o reg) on all data: 0.025681917
Test loss (w/o reg) on all data: 0.04458088
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.801893e-06
Norm of the params: 17.150064
                Loss: fixed 977 labels. Loss 0.04458. Accuracy 0.984.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41199678
Train loss (w/o reg) on all data: 0.40588456
Test loss (w/o reg) on all data: 0.22905853
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.561154e-05
Norm of the params: 11.056421
              Random: fixed 311 labels. Loss 0.22906. Accuracy 0.976.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5006862
Train loss (w/o reg) on all data: 0.4959083
Test loss (w/o reg) on all data: 0.33140805
Train acc on all data:  0.7486019936785802
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 0.00013073979
Norm of the params: 9.7754345
Flipped loss: 0.33141. Accuracy: 0.947
### Flips: 1230, rs: 13, checks: 205
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4358788
Train loss (w/o reg) on all data: 0.42622185
Test loss (w/o reg) on all data: 0.27219763
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.3561177e-05
Norm of the params: 13.897467
     Influence (LOO): fixed 160 labels. Loss 0.27220. Accuracy 0.959.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4045907
Train loss (w/o reg) on all data: 0.39357424
Test loss (w/o reg) on all data: 0.28081045
Train acc on all data:  0.8028203257962557
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 7.415978e-06
Norm of the params: 14.843488
                Loss: fixed 202 labels. Loss 0.28081. Accuracy 0.921.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48781067
Train loss (w/o reg) on all data: 0.48301378
Test loss (w/o reg) on all data: 0.31194723
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 2.9872794e-05
Norm of the params: 9.794797
              Random: fixed  60 labels. Loss 0.31195. Accuracy 0.954.
### Flips: 1230, rs: 13, checks: 410
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3869424
Train loss (w/o reg) on all data: 0.3760599
Test loss (w/o reg) on all data: 0.23617153
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.6911621e-05
Norm of the params: 14.75296
     Influence (LOO): fixed 286 labels. Loss 0.23617. Accuracy 0.964.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3148533
Train loss (w/o reg) on all data: 0.29955873
Test loss (w/o reg) on all data: 0.22823726
Train acc on all data:  0.8553367371748116
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 2.678864e-05
Norm of the params: 17.489765
                Loss: fixed 405 labels. Loss 0.22824. Accuracy 0.925.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47623178
Train loss (w/o reg) on all data: 0.4717034
Test loss (w/o reg) on all data: 0.29403585
Train acc on all data:  0.7731582786287381
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 5.2888714e-05
Norm of the params: 9.516694
              Random: fixed 117 labels. Loss 0.29404. Accuracy 0.964.
### Flips: 1230, rs: 13, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35009322
Train loss (w/o reg) on all data: 0.3379214
Test loss (w/o reg) on all data: 0.2072137
Train acc on all data:  0.8344274252370533
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.0847257e-05
Norm of the params: 15.602435
     Influence (LOO): fixed 378 labels. Loss 0.20721. Accuracy 0.973.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2209985
Train loss (w/o reg) on all data: 0.20109895
Test loss (w/o reg) on all data: 0.1715361
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.580543e-06
Norm of the params: 19.949715
                Loss: fixed 606 labels. Loss 0.17154. Accuracy 0.944.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46332362
Train loss (w/o reg) on all data: 0.45857635
Test loss (w/o reg) on all data: 0.27960274
Train acc on all data:  0.7828835399951374
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 4.50332e-05
Norm of the params: 9.743986
              Random: fixed 166 labels. Loss 0.27960. Accuracy 0.968.
### Flips: 1230, rs: 13, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30748922
Train loss (w/o reg) on all data: 0.29458696
Test loss (w/o reg) on all data: 0.17535627
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2567843e-05
Norm of the params: 16.063776
     Influence (LOO): fixed 485 labels. Loss 0.17536. Accuracy 0.976.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13491829
Train loss (w/o reg) on all data: 0.11449195
Test loss (w/o reg) on all data: 0.10608749
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.1817126e-05
Norm of the params: 20.21205
                Loss: fixed 795 labels. Loss 0.10609. Accuracy 0.966.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45230514
Train loss (w/o reg) on all data: 0.4474167
Test loss (w/o reg) on all data: 0.2688309
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.6693964e-05
Norm of the params: 9.887823
              Random: fixed 207 labels. Loss 0.26883. Accuracy 0.972.
### Flips: 1230, rs: 13, checks: 1025
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2679795
Train loss (w/o reg) on all data: 0.255097
Test loss (w/o reg) on all data: 0.14793344
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.5200241e-05
Norm of the params: 16.051489
     Influence (LOO): fixed 572 labels. Loss 0.14793. Accuracy 0.982.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07020034
Train loss (w/o reg) on all data: 0.053264458
Test loss (w/o reg) on all data: 0.05074891
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.1994498e-06
Norm of the params: 18.404284
                Loss: fixed 942 labels. Loss 0.05075. Accuracy 0.986.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43924427
Train loss (w/o reg) on all data: 0.434227
Test loss (w/o reg) on all data: 0.25486934
Train acc on all data:  0.8045222465353756
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.161906e-05
Norm of the params: 10.01728
              Random: fixed 255 labels. Loss 0.25487. Accuracy 0.967.
### Flips: 1230, rs: 13, checks: 1230
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23619564
Train loss (w/o reg) on all data: 0.22389498
Test loss (w/o reg) on all data: 0.12570117
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.9914593e-06
Norm of the params: 15.684808
     Influence (LOO): fixed 646 labels. Loss 0.12570. Accuracy 0.984.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038343206
Train loss (w/o reg) on all data: 0.02551542
Test loss (w/o reg) on all data: 0.028456623
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.599836e-06
Norm of the params: 16.017357
                Loss: fixed 1014 labels. Loss 0.02846. Accuracy 0.993.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4271267
Train loss (w/o reg) on all data: 0.42226213
Test loss (w/o reg) on all data: 0.24095598
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.4195219e-05
Norm of the params: 9.863638
              Random: fixed 300 labels. Loss 0.24096. Accuracy 0.973.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49727947
Train loss (w/o reg) on all data: 0.49251822
Test loss (w/o reg) on all data: 0.3366398
Train acc on all data:  0.7566253343058594
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.3433024e-05
Norm of the params: 9.758339
Flipped loss: 0.33664. Accuracy: 0.946
### Flips: 1230, rs: 14, checks: 205
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336527
Train loss (w/o reg) on all data: 0.4246253
Test loss (w/o reg) on all data: 0.2852143
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.8290022e-05
Norm of the params: 13.436812
     Influence (LOO): fixed 160 labels. Loss 0.28521. Accuracy 0.952.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39519668
Train loss (w/o reg) on all data: 0.38438475
Test loss (w/o reg) on all data: 0.28802043
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 1.29014325e-05
Norm of the params: 14.705057
                Loss: fixed 205 labels. Loss 0.28802. Accuracy 0.919.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48568895
Train loss (w/o reg) on all data: 0.48079452
Test loss (w/o reg) on all data: 0.31924725
Train acc on all data:  0.7663505956722587
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.5440592e-05
Norm of the params: 9.893867
              Random: fixed  52 labels. Loss 0.31925. Accuracy 0.948.
### Flips: 1230, rs: 14, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38776386
Train loss (w/o reg) on all data: 0.376844
Test loss (w/o reg) on all data: 0.24288663
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.4427998e-05
Norm of the params: 14.778278
     Influence (LOO): fixed 285 labels. Loss 0.24289. Accuracy 0.959.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304603
Train loss (w/o reg) on all data: 0.28942195
Test loss (w/o reg) on all data: 0.23878123
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 1.2092102e-05
Norm of the params: 17.424732
                Loss: fixed 406 labels. Loss 0.23878. Accuracy 0.917.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4725088
Train loss (w/o reg) on all data: 0.4675017
Test loss (w/o reg) on all data: 0.2967365
Train acc on all data:  0.7782640408460978
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 2.6181644e-05
Norm of the params: 10.007075
              Random: fixed 106 labels. Loss 0.29674. Accuracy 0.960.
### Flips: 1230, rs: 14, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3533538
Train loss (w/o reg) on all data: 0.3414711
Test loss (w/o reg) on all data: 0.21234292
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 8.33175e-06
Norm of the params: 15.416021
     Influence (LOO): fixed 377 labels. Loss 0.21234. Accuracy 0.974.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20923671
Train loss (w/o reg) on all data: 0.19034034
Test loss (w/o reg) on all data: 0.18561472
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 7.3678116e-06
Norm of the params: 19.440357
                Loss: fixed 608 labels. Loss 0.18561. Accuracy 0.937.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4615665
Train loss (w/o reg) on all data: 0.45643553
Test loss (w/o reg) on all data: 0.28319365
Train acc on all data:  0.7848285922684172
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8381444e-05
Norm of the params: 10.130133
              Random: fixed 151 labels. Loss 0.28319. Accuracy 0.958.
### Flips: 1230, rs: 14, checks: 820
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3150272
Train loss (w/o reg) on all data: 0.30284843
Test loss (w/o reg) on all data: 0.18029045
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.7986853e-06
Norm of the params: 15.60691
     Influence (LOO): fixed 476 labels. Loss 0.18029. Accuracy 0.981.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122622535
Train loss (w/o reg) on all data: 0.10243452
Test loss (w/o reg) on all data: 0.12399266
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 3.735744e-06
Norm of the params: 20.093784
                Loss: fixed 793 labels. Loss 0.12399. Accuracy 0.957.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44902012
Train loss (w/o reg) on all data: 0.44393688
Test loss (w/o reg) on all data: 0.26032406
Train acc on all data:  0.799173352783856
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8358237e-05
Norm of the params: 10.082903
              Random: fixed 207 labels. Loss 0.26032. Accuracy 0.967.
### Flips: 1230, rs: 14, checks: 1025
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28109506
Train loss (w/o reg) on all data: 0.26869568
Test loss (w/o reg) on all data: 0.15021352
Train acc on all data:  0.8767323121808899
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.378641e-06
Norm of the params: 15.747615
     Influence (LOO): fixed 560 labels. Loss 0.15021. Accuracy 0.987.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06317333
Train loss (w/o reg) on all data: 0.046312302
Test loss (w/o reg) on all data: 0.06535828
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.6312035e-06
Norm of the params: 18.363567
                Loss: fixed 933 labels. Loss 0.06536. Accuracy 0.977.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4351575
Train loss (w/o reg) on all data: 0.42971686
Test loss (w/o reg) on all data: 0.2414962
Train acc on all data:  0.8098711402868952
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.4678654e-05
Norm of the params: 10.431343
              Random: fixed 257 labels. Loss 0.24150. Accuracy 0.976.
### Flips: 1230, rs: 14, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23886059
Train loss (w/o reg) on all data: 0.22661868
Test loss (w/o reg) on all data: 0.1209309
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1558428e-05
Norm of the params: 15.647315
     Influence (LOO): fixed 651 labels. Loss 0.12093. Accuracy 0.990.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032341786
Train loss (w/o reg) on all data: 0.020351104
Test loss (w/o reg) on all data: 0.03374367
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.690609e-07
Norm of the params: 15.485918
                Loss: fixed 996 labels. Loss 0.03374. Accuracy 0.986.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41831914
Train loss (w/o reg) on all data: 0.41282678
Test loss (w/o reg) on all data: 0.22706746
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.7433345e-05
Norm of the params: 10.480813
              Random: fixed 312 labels. Loss 0.22707. Accuracy 0.978.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49587384
Train loss (w/o reg) on all data: 0.4909528
Test loss (w/o reg) on all data: 0.3260308
Train acc on all data:  0.74933138828106
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 3.3683908e-05
Norm of the params: 9.920734
Flipped loss: 0.32603. Accuracy: 0.952
### Flips: 1230, rs: 15, checks: 205
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42643344
Train loss (w/o reg) on all data: 0.41666064
Test loss (w/o reg) on all data: 0.27925524
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 8.9876994e-05
Norm of the params: 13.980562
     Influence (LOO): fixed 163 labels. Loss 0.27926. Accuracy 0.955.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39285496
Train loss (w/o reg) on all data: 0.38096604
Test loss (w/o reg) on all data: 0.27529338
Train acc on all data:  0.8042791150012156
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 4.5638004e-05
Norm of the params: 15.420059
                Loss: fixed 205 labels. Loss 0.27529. Accuracy 0.917.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48456174
Train loss (w/o reg) on all data: 0.47952688
Test loss (w/o reg) on all data: 0.3052325
Train acc on all data:  0.7636761487964989
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.7307283e-05
Norm of the params: 10.0348
              Random: fixed  63 labels. Loss 0.30523. Accuracy 0.960.
### Flips: 1230, rs: 15, checks: 410
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3794086
Train loss (w/o reg) on all data: 0.3676442
Test loss (w/o reg) on all data: 0.23259166
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9636183e-05
Norm of the params: 15.339102
     Influence (LOO): fixed 289 labels. Loss 0.23259. Accuracy 0.966.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30287316
Train loss (w/o reg) on all data: 0.28651717
Test loss (w/o reg) on all data: 0.21978042
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 8.594058e-06
Norm of the params: 18.086454
                Loss: fixed 409 labels. Loss 0.21978. Accuracy 0.918.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47639987
Train loss (w/o reg) on all data: 0.471752
Test loss (w/o reg) on all data: 0.29122567
Train acc on all data:  0.7721857524920982
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.128793e-05
Norm of the params: 9.641463
              Random: fixed 104 labels. Loss 0.29123. Accuracy 0.969.
### Flips: 1230, rs: 15, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34457856
Train loss (w/o reg) on all data: 0.33205226
Test loss (w/o reg) on all data: 0.20442908
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.8579674e-05
Norm of the params: 15.828019
     Influence (LOO): fixed 380 labels. Loss 0.20443. Accuracy 0.980.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21658252
Train loss (w/o reg) on all data: 0.19742294
Test loss (w/o reg) on all data: 0.15533546
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 8.043811e-06
Norm of the params: 19.575281
                Loss: fixed 612 labels. Loss 0.15534. Accuracy 0.944.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46371868
Train loss (w/o reg) on all data: 0.4586495
Test loss (w/o reg) on all data: 0.2753692
Train acc on all data:  0.7845854607342573
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 6.3647065e-05
Norm of the params: 10.068961
              Random: fixed 155 labels. Loss 0.27537. Accuracy 0.973.
### Flips: 1230, rs: 15, checks: 820
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30642396
Train loss (w/o reg) on all data: 0.29385856
Test loss (w/o reg) on all data: 0.17351975
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1038186e-05
Norm of the params: 15.852698
     Influence (LOO): fixed 474 labels. Loss 0.17352. Accuracy 0.987.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13541424
Train loss (w/o reg) on all data: 0.11569356
Test loss (w/o reg) on all data: 0.10174092
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.7035873e-06
Norm of the params: 19.859848
                Loss: fixed 790 labels. Loss 0.10174. Accuracy 0.965.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44824332
Train loss (w/o reg) on all data: 0.44309196
Test loss (w/o reg) on all data: 0.25482705
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.0154096e-05
Norm of the params: 10.150222
              Random: fixed 213 labels. Loss 0.25483. Accuracy 0.982.
### Flips: 1230, rs: 15, checks: 1025
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2677146
Train loss (w/o reg) on all data: 0.2545858
Test loss (w/o reg) on all data: 0.14665553
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.543125e-06
Norm of the params: 16.204184
     Influence (LOO): fixed 558 labels. Loss 0.14666. Accuracy 0.986.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06854752
Train loss (w/o reg) on all data: 0.05241788
Test loss (w/o reg) on all data: 0.043031063
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.4572305e-06
Norm of the params: 17.960869
                Loss: fixed 935 labels. Loss 0.04303. Accuracy 0.986.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4376119
Train loss (w/o reg) on all data: 0.43250838
Test loss (w/o reg) on all data: 0.24188766
Train acc on all data:  0.8071966934111354
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.562679e-05
Norm of the params: 10.10299
              Random: fixed 256 labels. Loss 0.24189. Accuracy 0.984.
### Flips: 1230, rs: 15, checks: 1230
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23342107
Train loss (w/o reg) on all data: 0.22050871
Test loss (w/o reg) on all data: 0.124956004
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.9782968e-05
Norm of the params: 16.070078
     Influence (LOO): fixed 632 labels. Loss 0.12496. Accuracy 0.987.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033969373
Train loss (w/o reg) on all data: 0.021744885
Test loss (w/o reg) on all data: 0.016886484
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5279069e-06
Norm of the params: 15.636169
                Loss: fixed 1010 labels. Loss 0.01689. Accuracy 0.998.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4247201
Train loss (w/o reg) on all data: 0.41946945
Test loss (w/o reg) on all data: 0.22841713
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.03609e-05
Norm of the params: 10.2476
              Random: fixed 299 labels. Loss 0.22842. Accuracy 0.983.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4993089
Train loss (w/o reg) on all data: 0.4946123
Test loss (w/o reg) on all data: 0.33074114
Train acc on all data:  0.7522489666909798
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.8218334e-05
Norm of the params: 9.691866
Flipped loss: 0.33074. Accuracy: 0.947
### Flips: 1230, rs: 16, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4335985
Train loss (w/o reg) on all data: 0.42478693
Test loss (w/o reg) on all data: 0.2783394
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.7078586e-05
Norm of the params: 13.275211
     Influence (LOO): fixed 162 labels. Loss 0.27834. Accuracy 0.950.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40079775
Train loss (w/o reg) on all data: 0.3899153
Test loss (w/o reg) on all data: 0.27307153
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 3.8544847e-05
Norm of the params: 14.752936
                Loss: fixed 204 labels. Loss 0.27307. Accuracy 0.918.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4878856
Train loss (w/o reg) on all data: 0.48298702
Test loss (w/o reg) on all data: 0.31210554
Train acc on all data:  0.762217359591539
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 9.407749e-06
Norm of the params: 9.898066
              Random: fixed  56 labels. Loss 0.31211. Accuracy 0.956.
### Flips: 1230, rs: 16, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38778022
Train loss (w/o reg) on all data: 0.37753654
Test loss (w/o reg) on all data: 0.23821828
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.015974e-05
Norm of the params: 14.313403
     Influence (LOO): fixed 286 labels. Loss 0.23822. Accuracy 0.953.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30910203
Train loss (w/o reg) on all data: 0.2937462
Test loss (w/o reg) on all data: 0.21340296
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 9.269482e-06
Norm of the params: 17.524742
                Loss: fixed 405 labels. Loss 0.21340. Accuracy 0.927.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47687495
Train loss (w/o reg) on all data: 0.47207198
Test loss (w/o reg) on all data: 0.29520908
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.07975275e-05
Norm of the params: 9.800979
              Random: fixed 108 labels. Loss 0.29521. Accuracy 0.958.
### Flips: 1230, rs: 16, checks: 615
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35155135
Train loss (w/o reg) on all data: 0.33986202
Test loss (w/o reg) on all data: 0.2031264
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.0527225e-05
Norm of the params: 15.290089
     Influence (LOO): fixed 389 labels. Loss 0.20313. Accuracy 0.976.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21904714
Train loss (w/o reg) on all data: 0.19899546
Test loss (w/o reg) on all data: 0.15674928
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 9.755859e-06
Norm of the params: 20.025826
                Loss: fixed 603 labels. Loss 0.15675. Accuracy 0.941.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46507165
Train loss (w/o reg) on all data: 0.46005306
Test loss (w/o reg) on all data: 0.28237727
Train acc on all data:  0.7828835399951374
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.1613936e-05
Norm of the params: 10.018562
              Random: fixed 154 labels. Loss 0.28238. Accuracy 0.963.
### Flips: 1230, rs: 16, checks: 820
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3155032
Train loss (w/o reg) on all data: 0.30310813
Test loss (w/o reg) on all data: 0.1812764
Train acc on all data:  0.8533916849015317
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 7.018226e-05
Norm of the params: 15.744903
     Influence (LOO): fixed 472 labels. Loss 0.18128. Accuracy 0.975.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13738835
Train loss (w/o reg) on all data: 0.11601808
Test loss (w/o reg) on all data: 0.109226786
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 3.609322e-06
Norm of the params: 20.673782
                Loss: fixed 790 labels. Loss 0.10923. Accuracy 0.960.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45064822
Train loss (w/o reg) on all data: 0.44568253
Test loss (w/o reg) on all data: 0.26187548
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.7941638e-05
Norm of the params: 9.96564
              Random: fixed 208 labels. Loss 0.26188. Accuracy 0.966.
### Flips: 1230, rs: 16, checks: 1025
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2748263
Train loss (w/o reg) on all data: 0.26311052
Test loss (w/o reg) on all data: 0.15115865
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.613042e-06
Norm of the params: 15.307356
     Influence (LOO): fixed 570 labels. Loss 0.15116. Accuracy 0.983.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06673139
Train loss (w/o reg) on all data: 0.04880143
Test loss (w/o reg) on all data: 0.04954192
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.1053455e-06
Norm of the params: 18.936718
                Loss: fixed 947 labels. Loss 0.04954. Accuracy 0.981.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336961
Train loss (w/o reg) on all data: 0.42883873
Test loss (w/o reg) on all data: 0.24334443
Train acc on all data:  0.8106005348893751
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.1534724e-05
Norm of the params: 9.85634
              Random: fixed 270 labels. Loss 0.24334. Accuracy 0.966.
### Flips: 1230, rs: 16, checks: 1230
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24263553
Train loss (w/o reg) on all data: 0.23163722
Test loss (w/o reg) on all data: 0.1268603
Train acc on all data:  0.8952103087770484
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.634756e-06
Norm of the params: 14.831259
     Influence (LOO): fixed 641 labels. Loss 0.12686. Accuracy 0.988.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042710308
Train loss (w/o reg) on all data: 0.028663239
Test loss (w/o reg) on all data: 0.030845145
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.5249004e-07
Norm of the params: 16.761307
                Loss: fixed 1003 labels. Loss 0.03085. Accuracy 0.989.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41672155
Train loss (w/o reg) on all data: 0.4117001
Test loss (w/o reg) on all data: 0.22503176
Train acc on all data:  0.8244590323364941
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.6460337e-05
Norm of the params: 10.021436
              Random: fixed 321 labels. Loss 0.22503. Accuracy 0.973.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5014724
Train loss (w/o reg) on all data: 0.49689496
Test loss (w/o reg) on all data: 0.33875754
Train acc on all data:  0.7461706783369803
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.2039792e-05
Norm of the params: 9.568117
Flipped loss: 0.33876. Accuracy: 0.937
### Flips: 1230, rs: 17, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43378592
Train loss (w/o reg) on all data: 0.4243938
Test loss (w/o reg) on all data: 0.28650758
Train acc on all data:  0.7892049598832969
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.2523435e-05
Norm of the params: 13.705545
     Influence (LOO): fixed 162 labels. Loss 0.28651. Accuracy 0.935.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39999905
Train loss (w/o reg) on all data: 0.38881376
Test loss (w/o reg) on all data: 0.28446034
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 3.0978168e-05
Norm of the params: 14.956794
                Loss: fixed 205 labels. Loss 0.28446. Accuracy 0.913.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48998326
Train loss (w/o reg) on all data: 0.48531076
Test loss (w/o reg) on all data: 0.31991044
Train acc on all data:  0.7583272550449793
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 7.353727e-05
Norm of the params: 9.666957
              Random: fixed  59 labels. Loss 0.31991. Accuracy 0.947.
### Flips: 1230, rs: 17, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38587284
Train loss (w/o reg) on all data: 0.375738
Test loss (w/o reg) on all data: 0.24243875
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.6948564e-05
Norm of the params: 14.237163
     Influence (LOO): fixed 289 labels. Loss 0.24244. Accuracy 0.963.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3107168
Train loss (w/o reg) on all data: 0.29533508
Test loss (w/o reg) on all data: 0.22935307
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 2.1668116e-05
Norm of the params: 17.539503
                Loss: fixed 407 labels. Loss 0.22935. Accuracy 0.921.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47968218
Train loss (w/o reg) on all data: 0.47478402
Test loss (w/o reg) on all data: 0.30578202
Train acc on all data:  0.7690250425480185
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 4.0418752e-05
Norm of the params: 9.897639
              Random: fixed 105 labels. Loss 0.30578. Accuracy 0.951.
### Flips: 1230, rs: 17, checks: 615
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34606275
Train loss (w/o reg) on all data: 0.33452067
Test loss (w/o reg) on all data: 0.21288916
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.054606e-05
Norm of the params: 15.193468
     Influence (LOO): fixed 386 labels. Loss 0.21289. Accuracy 0.968.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22010455
Train loss (w/o reg) on all data: 0.20030841
Test loss (w/o reg) on all data: 0.1777864
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 8.142007e-06
Norm of the params: 19.89781
                Loss: fixed 606 labels. Loss 0.17779. Accuracy 0.930.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46314785
Train loss (w/o reg) on all data: 0.4580305
Test loss (w/o reg) on all data: 0.28549474
Train acc on all data:  0.7855579868708972
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 6.361544e-05
Norm of the params: 10.116668
              Random: fixed 171 labels. Loss 0.28549. Accuracy 0.956.
### Flips: 1230, rs: 17, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3125991
Train loss (w/o reg) on all data: 0.30040422
Test loss (w/o reg) on all data: 0.18477318
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.9225343e-05
Norm of the params: 15.617215
     Influence (LOO): fixed 469 labels. Loss 0.18477. Accuracy 0.973.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13051143
Train loss (w/o reg) on all data: 0.108675964
Test loss (w/o reg) on all data: 0.11133565
Train acc on all data:  0.9538050085096037
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 9.051377e-06
Norm of the params: 20.897593
                Loss: fixed 799 labels. Loss 0.11134. Accuracy 0.958.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45184433
Train loss (w/o reg) on all data: 0.4465041
Test loss (w/o reg) on all data: 0.26873446
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.6507842e-05
Norm of the params: 10.334643
              Random: fixed 214 labels. Loss 0.26873. Accuracy 0.971.
### Flips: 1230, rs: 17, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27752724
Train loss (w/o reg) on all data: 0.2652869
Test loss (w/o reg) on all data: 0.15918216
Train acc on all data:  0.8767323121808899
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.223437e-05
Norm of the params: 15.646308
     Influence (LOO): fixed 554 labels. Loss 0.15918. Accuracy 0.976.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071897894
Train loss (w/o reg) on all data: 0.053456325
Test loss (w/o reg) on all data: 0.056721326
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.3641155e-06
Norm of the params: 19.204987
                Loss: fixed 930 labels. Loss 0.05672. Accuracy 0.978.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43804166
Train loss (w/o reg) on all data: 0.43287316
Test loss (w/o reg) on all data: 0.25103095
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.713428e-05
Norm of the params: 10.16709
              Random: fixed 265 labels. Loss 0.25103. Accuracy 0.974.
### Flips: 1230, rs: 17, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24465127
Train loss (w/o reg) on all data: 0.23220158
Test loss (w/o reg) on all data: 0.13762836
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3839644e-05
Norm of the params: 15.779541
     Influence (LOO): fixed 627 labels. Loss 0.13763. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03771825
Train loss (w/o reg) on all data: 0.024248453
Test loss (w/o reg) on all data: 0.030236073
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.687171e-06
Norm of the params: 16.413288
                Loss: fixed 1011 labels. Loss 0.03024. Accuracy 0.990.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41949382
Train loss (w/o reg) on all data: 0.41416296
Test loss (w/o reg) on all data: 0.23302034
Train acc on all data:  0.8205689277899344
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4700622e-05
Norm of the params: 10.325558
              Random: fixed 328 labels. Loss 0.23302. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49424252
Train loss (w/o reg) on all data: 0.48867956
Test loss (w/o reg) on all data: 0.34115124
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 3.0189522e-05
Norm of the params: 10.547958
Flipped loss: 0.34115. Accuracy: 0.934
### Flips: 1230, rs: 18, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4308268
Train loss (w/o reg) on all data: 0.4204114
Test loss (w/o reg) on all data: 0.29522294
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9319727891156463
Norm of the mean of gradients: 2.0539428e-05
Norm of the params: 14.432892
     Influence (LOO): fixed 151 labels. Loss 0.29522. Accuracy 0.932.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3961607
Train loss (w/o reg) on all data: 0.38355196
Test loss (w/o reg) on all data: 0.291339
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9135082604470359
Norm of the mean of gradients: 6.6835114e-06
Norm of the params: 15.880011
                Loss: fixed 201 labels. Loss 0.29134. Accuracy 0.914.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48213854
Train loss (w/o reg) on all data: 0.47670054
Test loss (w/o reg) on all data: 0.31969485
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.3340226e-05
Norm of the params: 10.4288
              Random: fixed  61 labels. Loss 0.31969. Accuracy 0.942.
### Flips: 1230, rs: 18, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38116252
Train loss (w/o reg) on all data: 0.36899891
Test loss (w/o reg) on all data: 0.24859568
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 2.8413577e-05
Norm of the params: 15.597184
     Influence (LOO): fixed 288 labels. Loss 0.24860. Accuracy 0.949.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31085458
Train loss (w/o reg) on all data: 0.2937375
Test loss (w/o reg) on all data: 0.24238665
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 6.6769375e-05
Norm of the params: 18.502478
                Loss: fixed 399 labels. Loss 0.24239. Accuracy 0.914.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46692488
Train loss (w/o reg) on all data: 0.4614745
Test loss (w/o reg) on all data: 0.29570222
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.8840787e-05
Norm of the params: 10.440663
              Random: fixed 130 labels. Loss 0.29570. Accuracy 0.954.
### Flips: 1230, rs: 18, checks: 615
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33697388
Train loss (w/o reg) on all data: 0.32412896
Test loss (w/o reg) on all data: 0.20484124
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.9827726e-05
Norm of the params: 16.028053
     Influence (LOO): fixed 402 labels. Loss 0.20484. Accuracy 0.967.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2263171
Train loss (w/o reg) on all data: 0.205342
Test loss (w/o reg) on all data: 0.17805009
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 2.063606e-05
Norm of the params: 20.481747
                Loss: fixed 597 labels. Loss 0.17805. Accuracy 0.940.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45348853
Train loss (w/o reg) on all data: 0.44765553
Test loss (w/o reg) on all data: 0.27755487
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.9503914e-05
Norm of the params: 10.800919
              Random: fixed 180 labels. Loss 0.27755. Accuracy 0.957.
### Flips: 1230, rs: 18, checks: 820
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30321756
Train loss (w/o reg) on all data: 0.29020676
Test loss (w/o reg) on all data: 0.1776946
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.5427904e-05
Norm of the params: 16.131214
     Influence (LOO): fixed 493 labels. Loss 0.17769. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14391607
Train loss (w/o reg) on all data: 0.12213265
Test loss (w/o reg) on all data: 0.115847126
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.04379515e-05
Norm of the params: 20.87267
                Loss: fixed 784 labels. Loss 0.11585. Accuracy 0.958.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4414137
Train loss (w/o reg) on all data: 0.43581963
Test loss (w/o reg) on all data: 0.25875533
Train acc on all data:  0.798930221249696
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.0518602e-05
Norm of the params: 10.577403
              Random: fixed 234 labels. Loss 0.25876. Accuracy 0.975.
### Flips: 1230, rs: 18, checks: 1025
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26496404
Train loss (w/o reg) on all data: 0.25136968
Test loss (w/o reg) on all data: 0.15236953
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 6.922714e-06
Norm of the params: 16.489004
     Influence (LOO): fixed 577 labels. Loss 0.15237. Accuracy 0.979.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0807173
Train loss (w/o reg) on all data: 0.061788447
Test loss (w/o reg) on all data: 0.062419146
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.379778e-06
Norm of the params: 19.457058
                Loss: fixed 925 labels. Loss 0.06242. Accuracy 0.981.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4291813
Train loss (w/o reg) on all data: 0.42343923
Test loss (w/o reg) on all data: 0.24486813
Train acc on all data:  0.8091417456844152
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.816218e-05
Norm of the params: 10.716401
              Random: fixed 277 labels. Loss 0.24487. Accuracy 0.977.
### Flips: 1230, rs: 18, checks: 1230
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22773623
Train loss (w/o reg) on all data: 0.21454394
Test loss (w/o reg) on all data: 0.12758364
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.9233244e-06
Norm of the params: 16.243336
     Influence (LOO): fixed 657 labels. Loss 0.12758. Accuracy 0.981.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045752328
Train loss (w/o reg) on all data: 0.03176361
Test loss (w/o reg) on all data: 0.029977025
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.0947496e-06
Norm of the params: 16.726456
                Loss: fixed 994 labels. Loss 0.02998. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41182977
Train loss (w/o reg) on all data: 0.40621004
Test loss (w/o reg) on all data: 0.22334519
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.8085582e-05
Norm of the params: 10.601646
              Random: fixed 337 labels. Loss 0.22335. Accuracy 0.981.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5016705
Train loss (w/o reg) on all data: 0.49742082
Test loss (w/o reg) on all data: 0.32860476
Train acc on all data:  0.7527352297592997
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.87467e-05
Norm of the params: 9.21916
Flipped loss: 0.32860. Accuracy: 0.958
### Flips: 1230, rs: 19, checks: 205
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44063815
Train loss (w/o reg) on all data: 0.43117845
Test loss (w/o reg) on all data: 0.27873453
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.1317095e-05
Norm of the params: 13.754782
     Influence (LOO): fixed 158 labels. Loss 0.27873. Accuracy 0.970.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40615562
Train loss (w/o reg) on all data: 0.39632225
Test loss (w/o reg) on all data: 0.2774264
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 2.1502086e-05
Norm of the params: 14.023822
                Loss: fixed 201 labels. Loss 0.27743. Accuracy 0.927.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4896266
Train loss (w/o reg) on all data: 0.48515415
Test loss (w/o reg) on all data: 0.311045
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.0718952e-05
Norm of the params: 9.457742
              Random: fixed  51 labels. Loss 0.31104. Accuracy 0.959.
### Flips: 1230, rs: 19, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3926049
Train loss (w/o reg) on all data: 0.38188785
Test loss (w/o reg) on all data: 0.22992054
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 7.2068215e-05
Norm of the params: 14.640377
     Influence (LOO): fixed 296 labels. Loss 0.22992. Accuracy 0.980.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31393623
Train loss (w/o reg) on all data: 0.29954478
Test loss (w/o reg) on all data: 0.22254136
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.0052953e-05
Norm of the params: 16.965523
                Loss: fixed 405 labels. Loss 0.22254. Accuracy 0.935.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47912723
Train loss (w/o reg) on all data: 0.4747723
Test loss (w/o reg) on all data: 0.29374927
Train acc on all data:  0.7734014101628981
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.7874298e-05
Norm of the params: 9.332661
              Random: fixed 103 labels. Loss 0.29375. Accuracy 0.962.
### Flips: 1230, rs: 19, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34913883
Train loss (w/o reg) on all data: 0.33836246
Test loss (w/o reg) on all data: 0.19877295
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.2162863e-05
Norm of the params: 14.680854
     Influence (LOO): fixed 405 labels. Loss 0.19877. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22058222
Train loss (w/o reg) on all data: 0.20190261
Test loss (w/o reg) on all data: 0.1721523
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.7329676e-06
Norm of the params: 19.32853
                Loss: fixed 604 labels. Loss 0.17215. Accuracy 0.948.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4642941
Train loss (w/o reg) on all data: 0.45981494
Test loss (w/o reg) on all data: 0.27307358
Train acc on all data:  0.787989302212497
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.4184443e-05
Norm of the params: 9.464864
              Random: fixed 160 labels. Loss 0.27307. Accuracy 0.974.
### Flips: 1230, rs: 19, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3122495
Train loss (w/o reg) on all data: 0.30107394
Test loss (w/o reg) on all data: 0.17632876
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0516791e-05
Norm of the params: 14.950302
     Influence (LOO): fixed 488 labels. Loss 0.17633. Accuracy 0.983.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12972245
Train loss (w/o reg) on all data: 0.10858144
Test loss (w/o reg) on all data: 0.10672312
Train acc on all data:  0.9535618769754437
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.386494e-05
Norm of the params: 20.56259
                Loss: fixed 798 labels. Loss 0.10672. Accuracy 0.965.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4495845
Train loss (w/o reg) on all data: 0.44466016
Test loss (w/o reg) on all data: 0.25346372
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 9.81784e-06
Norm of the params: 9.924075
              Random: fixed 217 labels. Loss 0.25346. Accuracy 0.979.
### Flips: 1230, rs: 19, checks: 1025
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2673526
Train loss (w/o reg) on all data: 0.2560632
Test loss (w/o reg) on all data: 0.15088671
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.243371e-06
Norm of the params: 15.02625
     Influence (LOO): fixed 583 labels. Loss 0.15089. Accuracy 0.983.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059294656
Train loss (w/o reg) on all data: 0.04294224
Test loss (w/o reg) on all data: 0.052528944
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.945002e-06
Norm of the params: 18.084478
                Loss: fixed 958 labels. Loss 0.05253. Accuracy 0.988.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43379843
Train loss (w/o reg) on all data: 0.42835656
Test loss (w/o reg) on all data: 0.23710291
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.2722299e-05
Norm of the params: 10.432524
              Random: fixed 267 labels. Loss 0.23710. Accuracy 0.977.
### Flips: 1230, rs: 19, checks: 1230
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23693201
Train loss (w/o reg) on all data: 0.22590694
Test loss (w/o reg) on all data: 0.12849905
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.8965766e-05
Norm of the params: 14.849296
     Influence (LOO): fixed 654 labels. Loss 0.12850. Accuracy 0.985.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033622563
Train loss (w/o reg) on all data: 0.021493226
Test loss (w/o reg) on all data: 0.04046124
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.020592e-06
Norm of the params: 15.575197
                Loss: fixed 1012 labels. Loss 0.04046. Accuracy 0.990.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4168246
Train loss (w/o reg) on all data: 0.41086355
Test loss (w/o reg) on all data: 0.22304589
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.0235096e-05
Norm of the params: 10.91885
              Random: fixed 320 labels. Loss 0.22305. Accuracy 0.977.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4954253
Train loss (w/o reg) on all data: 0.48987606
Test loss (w/o reg) on all data: 0.3329862
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 5.4992357e-05
Norm of the params: 10.534957
Flipped loss: 0.33299. Accuracy: 0.936
### Flips: 1230, rs: 20, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43075642
Train loss (w/o reg) on all data: 0.42181188
Test loss (w/o reg) on all data: 0.27771762
Train acc on all data:  0.7947969851689765
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.7677881e-05
Norm of the params: 13.375002
     Influence (LOO): fixed 158 labels. Loss 0.27772. Accuracy 0.947.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39617223
Train loss (w/o reg) on all data: 0.38409844
Test loss (w/o reg) on all data: 0.27370623
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.43409925e-05
Norm of the params: 15.539486
                Loss: fixed 205 labels. Loss 0.27371. Accuracy 0.925.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48512563
Train loss (w/o reg) on all data: 0.47955614
Test loss (w/o reg) on all data: 0.3115725
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.970136e-05
Norm of the params: 10.554143
              Random: fixed  51 labels. Loss 0.31157. Accuracy 0.944.
### Flips: 1230, rs: 20, checks: 410
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38221067
Train loss (w/o reg) on all data: 0.37093997
Test loss (w/o reg) on all data: 0.2389771
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.5594558e-05
Norm of the params: 15.0138
     Influence (LOO): fixed 282 labels. Loss 0.23898. Accuracy 0.953.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3032152
Train loss (w/o reg) on all data: 0.28541103
Test loss (w/o reg) on all data: 0.2165552
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 2.145216e-05
Norm of the params: 18.870169
                Loss: fixed 409 labels. Loss 0.21656. Accuracy 0.921.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4742791
Train loss (w/o reg) on all data: 0.4685724
Test loss (w/o reg) on all data: 0.29718226
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 6.0916773e-05
Norm of the params: 10.683341
              Random: fixed  94 labels. Loss 0.29718. Accuracy 0.951.
### Flips: 1230, rs: 20, checks: 615
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33950257
Train loss (w/o reg) on all data: 0.32853815
Test loss (w/o reg) on all data: 0.20042793
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.3503115e-05
Norm of the params: 14.808385
     Influence (LOO): fixed 393 labels. Loss 0.20043. Accuracy 0.965.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21366656
Train loss (w/o reg) on all data: 0.19217259
Test loss (w/o reg) on all data: 0.15541531
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 7.818876e-06
Norm of the params: 20.733538
                Loss: fixed 607 labels. Loss 0.15542. Accuracy 0.947.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46205446
Train loss (w/o reg) on all data: 0.45631203
Test loss (w/o reg) on all data: 0.2757871
Train acc on all data:  0.7843423292000973
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.3958065e-05
Norm of the params: 10.716733
              Random: fixed 148 labels. Loss 0.27579. Accuracy 0.961.
### Flips: 1230, rs: 20, checks: 820
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293643
Train loss (w/o reg) on all data: 0.2821251
Test loss (w/o reg) on all data: 0.16788426
Train acc on all data:  0.8660345246778507
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.3627845e-06
Norm of the params: 15.177552
     Influence (LOO): fixed 499 labels. Loss 0.16788. Accuracy 0.977.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13086975
Train loss (w/o reg) on all data: 0.108219326
Test loss (w/o reg) on all data: 0.091882624
Train acc on all data:  0.9557500607828835
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.6680136e-06
Norm of the params: 21.283997
                Loss: fixed 793 labels. Loss 0.09188. Accuracy 0.970.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44886795
Train loss (w/o reg) on all data: 0.44278032
Test loss (w/o reg) on all data: 0.2568386
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.653175e-05
Norm of the params: 11.034157
              Random: fixed 201 labels. Loss 0.25684. Accuracy 0.973.
### Flips: 1230, rs: 20, checks: 1025
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2574367
Train loss (w/o reg) on all data: 0.24564695
Test loss (w/o reg) on all data: 0.14215316
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.35146e-06
Norm of the params: 15.355612
     Influence (LOO): fixed 578 labels. Loss 0.14215. Accuracy 0.983.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06915779
Train loss (w/o reg) on all data: 0.05053554
Test loss (w/o reg) on all data: 0.04769276
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9843665e-06
Norm of the params: 19.298832
                Loss: fixed 925 labels. Loss 0.04769. Accuracy 0.984.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43247217
Train loss (w/o reg) on all data: 0.42646083
Test loss (w/o reg) on all data: 0.24028632
Train acc on all data:  0.8071966934111354
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.0889393e-05
Norm of the params: 10.9648075
              Random: fixed 262 labels. Loss 0.24029. Accuracy 0.979.
### Flips: 1230, rs: 20, checks: 1230
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22598203
Train loss (w/o reg) on all data: 0.21398124
Test loss (w/o reg) on all data: 0.11772218
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.926952e-06
Norm of the params: 15.492436
     Influence (LOO): fixed 650 labels. Loss 0.11772. Accuracy 0.987.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044236112
Train loss (w/o reg) on all data: 0.029667307
Test loss (w/o reg) on all data: 0.02221721
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.01045e-06
Norm of the params: 17.069742
                Loss: fixed 988 labels. Loss 0.02222. Accuracy 0.996.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41489768
Train loss (w/o reg) on all data: 0.40858707
Test loss (w/o reg) on all data: 0.22544436
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.9258845e-05
Norm of the params: 11.234428
              Random: fixed 313 labels. Loss 0.22544. Accuracy 0.978.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49176192
Train loss (w/o reg) on all data: 0.48606423
Test loss (w/o reg) on all data: 0.35225502
Train acc on all data:  0.7539508874300996
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 5.3360447e-05
Norm of the params: 10.674919
Flipped loss: 0.35226. Accuracy: 0.921
### Flips: 1230, rs: 21, checks: 205
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43026397
Train loss (w/o reg) on all data: 0.42069733
Test loss (w/o reg) on all data: 0.29694036
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.40117e-05
Norm of the params: 13.832297
     Influence (LOO): fixed 157 labels. Loss 0.29694. Accuracy 0.935.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39439586
Train loss (w/o reg) on all data: 0.3825867
Test loss (w/o reg) on all data: 0.30272275
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 6.269727e-05
Norm of the params: 15.368257
                Loss: fixed 202 labels. Loss 0.30272. Accuracy 0.895.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48247635
Train loss (w/o reg) on all data: 0.47685295
Test loss (w/o reg) on all data: 0.33332276
Train acc on all data:  0.7648918064672988
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 1.641204e-05
Norm of the params: 10.60509
              Random: fixed  51 labels. Loss 0.33332. Accuracy 0.926.
### Flips: 1230, rs: 21, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38157603
Train loss (w/o reg) on all data: 0.37011185
Test loss (w/o reg) on all data: 0.2503776
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.0695159e-05
Norm of the params: 15.142112
     Influence (LOO): fixed 284 labels. Loss 0.25038. Accuracy 0.951.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30001795
Train loss (w/o reg) on all data: 0.28420484
Test loss (w/o reg) on all data: 0.25933176
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9008746355685131
Norm of the mean of gradients: 1.4050616e-05
Norm of the params: 17.78376
                Loss: fixed 402 labels. Loss 0.25933. Accuracy 0.901.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47156048
Train loss (w/o reg) on all data: 0.4659528
Test loss (w/o reg) on all data: 0.31460562
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.46282555e-05
Norm of the params: 10.590251
              Random: fixed 100 labels. Loss 0.31461. Accuracy 0.941.
### Flips: 1230, rs: 21, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33599243
Train loss (w/o reg) on all data: 0.32394627
Test loss (w/o reg) on all data: 0.21290022
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.3675562e-05
Norm of the params: 15.521701
     Influence (LOO): fixed 397 labels. Loss 0.21290. Accuracy 0.959.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20979956
Train loss (w/o reg) on all data: 0.19053534
Test loss (w/o reg) on all data: 0.19982699
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.25678025e-05
Norm of the params: 19.628668
                Loss: fixed 596 labels. Loss 0.19983. Accuracy 0.931.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45987126
Train loss (w/o reg) on all data: 0.45425186
Test loss (w/o reg) on all data: 0.29808253
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 3.0760835e-05
Norm of the params: 10.601322
              Random: fixed 149 labels. Loss 0.29808. Accuracy 0.945.
### Flips: 1230, rs: 21, checks: 820
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29770976
Train loss (w/o reg) on all data: 0.28499678
Test loss (w/o reg) on all data: 0.18305433
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.1861758e-05
Norm of the params: 15.945528
     Influence (LOO): fixed 492 labels. Loss 0.18305. Accuracy 0.969.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13565221
Train loss (w/o reg) on all data: 0.115858644
Test loss (w/o reg) on all data: 0.14006364
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.688069e-06
Norm of the params: 19.89652
                Loss: fixed 770 labels. Loss 0.14006. Accuracy 0.953.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44420013
Train loss (w/o reg) on all data: 0.43837625
Test loss (w/o reg) on all data: 0.27963966
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 2.8630862e-05
Norm of the params: 10.792489
              Random: fixed 206 labels. Loss 0.27964. Accuracy 0.945.
### Flips: 1230, rs: 21, checks: 1025
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26596808
Train loss (w/o reg) on all data: 0.253657
Test loss (w/o reg) on all data: 0.15123074
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 7.5000626e-06
Norm of the params: 15.691452
     Influence (LOO): fixed 576 labels. Loss 0.15123. Accuracy 0.980.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07481564
Train loss (w/o reg) on all data: 0.05686137
Test loss (w/o reg) on all data: 0.08355781
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6824167e-06
Norm of the params: 18.949549
                Loss: fixed 911 labels. Loss 0.08356. Accuracy 0.974.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42941502
Train loss (w/o reg) on all data: 0.42301434
Test loss (w/o reg) on all data: 0.25663
Train acc on all data:  0.812302455628495
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.1982992e-05
Norm of the params: 11.314306
              Random: fixed 262 labels. Loss 0.25663. Accuracy 0.961.
### Flips: 1230, rs: 21, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23160233
Train loss (w/o reg) on all data: 0.21989992
Test loss (w/o reg) on all data: 0.123403534
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0357936e-05
Norm of the params: 15.298627
     Influence (LOO): fixed 655 labels. Loss 0.12340. Accuracy 0.988.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04547131
Train loss (w/o reg) on all data: 0.031586297
Test loss (w/o reg) on all data: 0.056603193
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.5650004e-07
Norm of the params: 16.664343
                Loss: fixed 973 labels. Loss 0.05660. Accuracy 0.982.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4160917
Train loss (w/o reg) on all data: 0.4095217
Test loss (w/o reg) on all data: 0.24249889
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.2324181e-05
Norm of the params: 11.462986
              Random: fixed 307 labels. Loss 0.24250. Accuracy 0.970.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4987831
Train loss (w/o reg) on all data: 0.4941123
Test loss (w/o reg) on all data: 0.32084218
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 3.9054463e-05
Norm of the params: 9.665204
Flipped loss: 0.32084. Accuracy: 0.949
### Flips: 1230, rs: 22, checks: 205
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43323147
Train loss (w/o reg) on all data: 0.42368788
Test loss (w/o reg) on all data: 0.26787117
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.0844877e-05
Norm of the params: 13.815649
     Influence (LOO): fixed 162 labels. Loss 0.26787. Accuracy 0.950.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39949962
Train loss (w/o reg) on all data: 0.38907343
Test loss (w/o reg) on all data: 0.26920196
Train acc on all data:  0.8110867979576951
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 9.786515e-06
Norm of the params: 14.440361
                Loss: fixed 203 labels. Loss 0.26920. Accuracy 0.919.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48617932
Train loss (w/o reg) on all data: 0.4813909
Test loss (w/o reg) on all data: 0.3001715
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.7782672e-05
Norm of the params: 9.786154
              Random: fixed  60 labels. Loss 0.30017. Accuracy 0.959.
### Flips: 1230, rs: 22, checks: 410
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38340497
Train loss (w/o reg) on all data: 0.37297896
Test loss (w/o reg) on all data: 0.2275369
Train acc on all data:  0.825188426938974
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.0059718e-05
Norm of the params: 14.440221
     Influence (LOO): fixed 296 labels. Loss 0.22754. Accuracy 0.970.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30504096
Train loss (w/o reg) on all data: 0.2898404
Test loss (w/o reg) on all data: 0.22198308
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 2.4174118e-05
Norm of the params: 17.43591
                Loss: fixed 407 labels. Loss 0.22198. Accuracy 0.926.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47503132
Train loss (w/o reg) on all data: 0.47048652
Test loss (w/o reg) on all data: 0.27993745
Train acc on all data:  0.7802090931193776
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.8089822e-05
Norm of the params: 9.533946
              Random: fixed 113 labels. Loss 0.27994. Accuracy 0.966.
### Flips: 1230, rs: 22, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3403634
Train loss (w/o reg) on all data: 0.32911307
Test loss (w/o reg) on all data: 0.1972594
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.229216e-06
Norm of the params: 15.000227
     Influence (LOO): fixed 400 labels. Loss 0.19726. Accuracy 0.978.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20642021
Train loss (w/o reg) on all data: 0.18643914
Test loss (w/o reg) on all data: 0.1664464
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 4.6905034e-06
Norm of the params: 19.990534
                Loss: fixed 609 labels. Loss 0.16645. Accuracy 0.944.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4617291
Train loss (w/o reg) on all data: 0.45704874
Test loss (w/o reg) on all data: 0.26096556
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.2578256e-05
Norm of the params: 9.675096
              Random: fixed 168 labels. Loss 0.26097. Accuracy 0.974.
### Flips: 1230, rs: 22, checks: 820
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3031448
Train loss (w/o reg) on all data: 0.2913971
Test loss (w/o reg) on all data: 0.16936465
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.9873043e-05
Norm of the params: 15.328216
     Influence (LOO): fixed 490 labels. Loss 0.16936. Accuracy 0.979.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122375816
Train loss (w/o reg) on all data: 0.10070727
Test loss (w/o reg) on all data: 0.103906736
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 7.4567847e-06
Norm of the params: 20.81756
                Loss: fixed 796 labels. Loss 0.10391. Accuracy 0.965.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44615093
Train loss (w/o reg) on all data: 0.44107038
Test loss (w/o reg) on all data: 0.24559706
Train acc on all data:  0.8064672988086555
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.3918379e-05
Norm of the params: 10.0802355
              Random: fixed 224 labels. Loss 0.24560. Accuracy 0.976.
### Flips: 1230, rs: 22, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27340955
Train loss (w/o reg) on all data: 0.2615153
Test loss (w/o reg) on all data: 0.14607282
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.047971e-06
Norm of the params: 15.423516
     Influence (LOO): fixed 564 labels. Loss 0.14607. Accuracy 0.987.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0638784
Train loss (w/o reg) on all data: 0.046415854
Test loss (w/o reg) on all data: 0.048395015
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.0921851e-06
Norm of the params: 18.688255
                Loss: fixed 936 labels. Loss 0.04840. Accuracy 0.983.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43198314
Train loss (w/o reg) on all data: 0.42666128
Test loss (w/o reg) on all data: 0.23188482
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.6999434e-05
Norm of the params: 10.316851
              Random: fixed 274 labels. Loss 0.23188. Accuracy 0.980.
### Flips: 1230, rs: 22, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2349242
Train loss (w/o reg) on all data: 0.22304896
Test loss (w/o reg) on all data: 0.12366014
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.223123e-06
Norm of the params: 15.411187
     Influence (LOO): fixed 644 labels. Loss 0.12366. Accuracy 0.986.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03785694
Train loss (w/o reg) on all data: 0.02498669
Test loss (w/o reg) on all data: 0.025368152
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2957036e-06
Norm of the params: 16.043846
                Loss: fixed 988 labels. Loss 0.02537. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41870213
Train loss (w/o reg) on all data: 0.41344568
Test loss (w/o reg) on all data: 0.21975806
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4724632e-05
Norm of the params: 10.253225
              Random: fixed 319 labels. Loss 0.21976. Accuracy 0.982.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49610233
Train loss (w/o reg) on all data: 0.49092954
Test loss (w/o reg) on all data: 0.34909847
Train acc on all data:  0.7546802820325796
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 2.4077963e-05
Norm of the params: 10.171313
Flipped loss: 0.34910. Accuracy: 0.927
### Flips: 1230, rs: 23, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4322936
Train loss (w/o reg) on all data: 0.4226773
Test loss (w/o reg) on all data: 0.29576996
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.1794211e-05
Norm of the params: 13.868157
     Influence (LOO): fixed 163 labels. Loss 0.29577. Accuracy 0.938.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39867932
Train loss (w/o reg) on all data: 0.3878231
Test loss (w/o reg) on all data: 0.3004529
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9037900874635568
Norm of the mean of gradients: 1.4930913e-05
Norm of the params: 14.73514
                Loss: fixed 204 labels. Loss 0.30045. Accuracy 0.904.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4847195
Train loss (w/o reg) on all data: 0.47942725
Test loss (w/o reg) on all data: 0.32935417
Train acc on all data:  0.762703622659859
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 5.3258962e-05
Norm of the params: 10.288125
              Random: fixed  56 labels. Loss 0.32935. Accuracy 0.937.
### Flips: 1230, rs: 23, checks: 410
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38558865
Train loss (w/o reg) on all data: 0.37367707
Test loss (w/o reg) on all data: 0.2566312
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.2220778e-05
Norm of the params: 15.434754
     Influence (LOO): fixed 289 labels. Loss 0.25663. Accuracy 0.945.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30823082
Train loss (w/o reg) on all data: 0.2927486
Test loss (w/o reg) on all data: 0.25225574
Train acc on all data:  0.8582543155847313
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 1.1920927e-05
Norm of the params: 17.596716
                Loss: fixed 404 labels. Loss 0.25226. Accuracy 0.913.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47064415
Train loss (w/o reg) on all data: 0.46513295
Test loss (w/o reg) on all data: 0.31036142
Train acc on all data:  0.7782640408460978
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 6.0863073e-05
Norm of the params: 10.498759
              Random: fixed 116 labels. Loss 0.31036. Accuracy 0.948.
### Flips: 1230, rs: 23, checks: 615
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34678462
Train loss (w/o reg) on all data: 0.33514497
Test loss (w/o reg) on all data: 0.2166005
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4697727e-05
Norm of the params: 15.257554
     Influence (LOO): fixed 397 labels. Loss 0.21660. Accuracy 0.956.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2167979
Train loss (w/o reg) on all data: 0.19742708
Test loss (w/o reg) on all data: 0.19263762
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 4.157699e-05
Norm of the params: 19.682896
                Loss: fixed 603 labels. Loss 0.19264. Accuracy 0.940.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45553616
Train loss (w/o reg) on all data: 0.4497876
Test loss (w/o reg) on all data: 0.29123095
Train acc on all data:  0.7913931436907367
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 0.00010043105
Norm of the params: 10.722467
              Random: fixed 174 labels. Loss 0.29123. Accuracy 0.952.
### Flips: 1230, rs: 23, checks: 820
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30175486
Train loss (w/o reg) on all data: 0.28944924
Test loss (w/o reg) on all data: 0.1803909
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.109048e-05
Norm of the params: 15.687965
     Influence (LOO): fixed 501 labels. Loss 0.18039. Accuracy 0.971.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13551825
Train loss (w/o reg) on all data: 0.11343998
Test loss (w/o reg) on all data: 0.12967423
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 4.5651905e-06
Norm of the params: 21.01346
                Loss: fixed 784 labels. Loss 0.12967. Accuracy 0.957.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4429563
Train loss (w/o reg) on all data: 0.4373621
Test loss (w/o reg) on all data: 0.26922446
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.673181e-05
Norm of the params: 10.57753
              Random: fixed 228 labels. Loss 0.26922. Accuracy 0.959.
### Flips: 1230, rs: 23, checks: 1025
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26530105
Train loss (w/o reg) on all data: 0.25316194
Test loss (w/o reg) on all data: 0.15200469
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.2383193e-06
Norm of the params: 15.581475
     Influence (LOO): fixed 588 labels. Loss 0.15200. Accuracy 0.977.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079168804
Train loss (w/o reg) on all data: 0.06047699
Test loss (w/o reg) on all data: 0.05755741
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.3048844e-06
Norm of the params: 19.334846
                Loss: fixed 919 labels. Loss 0.05756. Accuracy 0.983.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4256083
Train loss (w/o reg) on all data: 0.41973266
Test loss (w/o reg) on all data: 0.25258955
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.2699664e-05
Norm of the params: 10.840332
              Random: fixed 282 labels. Loss 0.25259. Accuracy 0.967.
### Flips: 1230, rs: 23, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23143797
Train loss (w/o reg) on all data: 0.21942592
Test loss (w/o reg) on all data: 0.13024862
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.597397e-06
Norm of the params: 15.499705
     Influence (LOO): fixed 662 labels. Loss 0.13025. Accuracy 0.983.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04300254
Train loss (w/o reg) on all data: 0.028676046
Test loss (w/o reg) on all data: 0.027062234
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5055875e-06
Norm of the params: 16.92719
                Loss: fixed 994 labels. Loss 0.02706. Accuracy 0.994.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41249812
Train loss (w/o reg) on all data: 0.40664294
Test loss (w/o reg) on all data: 0.23595913
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.020684e-05
Norm of the params: 10.821442
              Random: fixed 328 labels. Loss 0.23596. Accuracy 0.971.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4896999
Train loss (w/o reg) on all data: 0.4835923
Test loss (w/o reg) on all data: 0.3328841
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 5.883449e-05
Norm of the params: 11.052241
Flipped loss: 0.33288. Accuracy: 0.926
### Flips: 1230, rs: 24, checks: 205
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42500713
Train loss (w/o reg) on all data: 0.41476536
Test loss (w/o reg) on all data: 0.2773836
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.8364819e-05
Norm of the params: 14.312074
     Influence (LOO): fixed 156 labels. Loss 0.27738. Accuracy 0.946.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39015853
Train loss (w/o reg) on all data: 0.37641796
Test loss (w/o reg) on all data: 0.26908726
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 1.47558485e-05
Norm of the params: 16.577429
                Loss: fixed 203 labels. Loss 0.26909. Accuracy 0.923.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4790836
Train loss (w/o reg) on all data: 0.4729912
Test loss (w/o reg) on all data: 0.3136082
Train acc on all data:  0.7653780695356188
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.6106873e-05
Norm of the params: 11.038472
              Random: fixed  50 labels. Loss 0.31361. Accuracy 0.943.
### Flips: 1230, rs: 24, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37823227
Train loss (w/o reg) on all data: 0.3667268
Test loss (w/o reg) on all data: 0.23415922
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.5183886e-05
Norm of the params: 15.169375
     Influence (LOO): fixed 286 labels. Loss 0.23416. Accuracy 0.957.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3037972
Train loss (w/o reg) on all data: 0.28567123
Test loss (w/o reg) on all data: 0.21585283
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 9.762428e-06
Norm of the params: 19.039925
                Loss: fixed 407 labels. Loss 0.21585. Accuracy 0.927.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.466504
Train loss (w/o reg) on all data: 0.46016347
Test loss (w/o reg) on all data: 0.29518944
Train acc on all data:  0.7789934354485777
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.1214693e-05
Norm of the params: 11.261015
              Random: fixed 107 labels. Loss 0.29519. Accuracy 0.946.
### Flips: 1230, rs: 24, checks: 615
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3370346
Train loss (w/o reg) on all data: 0.3247203
Test loss (w/o reg) on all data: 0.2032487
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.399026e-05
Norm of the params: 15.693523
     Influence (LOO): fixed 388 labels. Loss 0.20325. Accuracy 0.970.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21707107
Train loss (w/o reg) on all data: 0.19540021
Test loss (w/o reg) on all data: 0.15662597
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4804014e-05
Norm of the params: 20.818678
                Loss: fixed 600 labels. Loss 0.15663. Accuracy 0.949.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45563355
Train loss (w/o reg) on all data: 0.44938254
Test loss (w/o reg) on all data: 0.2756076
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.2288062e-05
Norm of the params: 11.181249
              Random: fixed 157 labels. Loss 0.27561. Accuracy 0.960.
### Flips: 1230, rs: 24, checks: 820
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30230758
Train loss (w/o reg) on all data: 0.2899463
Test loss (w/o reg) on all data: 0.17164236
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.4871972e-05
Norm of the params: 15.72341
     Influence (LOO): fixed 482 labels. Loss 0.17164. Accuracy 0.974.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1366086
Train loss (w/o reg) on all data: 0.11393823
Test loss (w/o reg) on all data: 0.09755233
Train acc on all data:  0.9508874300996839
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.7356267e-06
Norm of the params: 21.29337
                Loss: fixed 777 labels. Loss 0.09755. Accuracy 0.973.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44362193
Train loss (w/o reg) on all data: 0.43717724
Test loss (w/o reg) on all data: 0.259114
Train acc on all data:  0.8008752735229759
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 3.3104316e-05
Norm of the params: 11.353136
              Random: fixed 206 labels. Loss 0.25911. Accuracy 0.976.
### Flips: 1230, rs: 24, checks: 1025
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26498175
Train loss (w/o reg) on all data: 0.25275767
Test loss (w/o reg) on all data: 0.14966643
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1304666e-05
Norm of the params: 15.635916
     Influence (LOO): fixed 564 labels. Loss 0.14967. Accuracy 0.979.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07435261
Train loss (w/o reg) on all data: 0.056133863
Test loss (w/o reg) on all data: 0.051298488
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.9605496e-06
Norm of the params: 19.08861
                Loss: fixed 915 labels. Loss 0.05130. Accuracy 0.986.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42953947
Train loss (w/o reg) on all data: 0.42310497
Test loss (w/o reg) on all data: 0.23961525
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.2592199e-05
Norm of the params: 11.344151
              Random: fixed 256 labels. Loss 0.23962. Accuracy 0.980.
### Flips: 1230, rs: 24, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22644185
Train loss (w/o reg) on all data: 0.21417734
Test loss (w/o reg) on all data: 0.122716166
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.4032826e-06
Norm of the params: 15.661738
     Influence (LOO): fixed 650 labels. Loss 0.12272. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03830815
Train loss (w/o reg) on all data: 0.025261462
Test loss (w/o reg) on all data: 0.022543004
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4708758e-06
Norm of the params: 16.153446
                Loss: fixed 992 labels. Loss 0.02254. Accuracy 0.995.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4143345
Train loss (w/o reg) on all data: 0.4079569
Test loss (w/o reg) on all data: 0.22583508
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.080399e-05
Norm of the params: 11.293907
              Random: fixed 310 labels. Loss 0.22584. Accuracy 0.983.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49517465
Train loss (w/o reg) on all data: 0.48954788
Test loss (w/o reg) on all data: 0.33931044
Train acc on all data:  0.7537077558959397
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.0596502e-05
Norm of the params: 10.608266
Flipped loss: 0.33931. Accuracy: 0.937
### Flips: 1230, rs: 25, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4286913
Train loss (w/o reg) on all data: 0.41875485
Test loss (w/o reg) on all data: 0.29198253
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.6003029e-05
Norm of the params: 14.097118
     Influence (LOO): fixed 162 labels. Loss 0.29198. Accuracy 0.947.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3926891
Train loss (w/o reg) on all data: 0.37952504
Test loss (w/o reg) on all data: 0.29208815
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 1.6501985e-05
Norm of the params: 16.225952
                Loss: fixed 205 labels. Loss 0.29209. Accuracy 0.916.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48336288
Train loss (w/o reg) on all data: 0.47754094
Test loss (w/o reg) on all data: 0.3230644
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.398658e-05
Norm of the params: 10.790675
              Random: fixed  57 labels. Loss 0.32306. Accuracy 0.938.
### Flips: 1230, rs: 25, checks: 410
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38142544
Train loss (w/o reg) on all data: 0.370085
Test loss (w/o reg) on all data: 0.24351916
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.505347e-06
Norm of the params: 15.060172
     Influence (LOO): fixed 294 labels. Loss 0.24352. Accuracy 0.963.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30429482
Train loss (w/o reg) on all data: 0.28633922
Test loss (w/o reg) on all data: 0.23723325
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 9.253903e-06
Norm of the params: 18.950245
                Loss: fixed 407 labels. Loss 0.23723. Accuracy 0.921.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47002938
Train loss (w/o reg) on all data: 0.46381256
Test loss (w/o reg) on all data: 0.30058578
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 1.1002233e-05
Norm of the params: 11.150623
              Random: fixed 115 labels. Loss 0.30059. Accuracy 0.948.
### Flips: 1230, rs: 25, checks: 615
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34019193
Train loss (w/o reg) on all data: 0.32813665
Test loss (w/o reg) on all data: 0.21353997
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 7.583256e-06
Norm of the params: 15.52757
     Influence (LOO): fixed 397 labels. Loss 0.21354. Accuracy 0.963.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2157282
Train loss (w/o reg) on all data: 0.19381613
Test loss (w/o reg) on all data: 0.17536661
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.054826e-05
Norm of the params: 20.934214
                Loss: fixed 609 labels. Loss 0.17537. Accuracy 0.943.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45597312
Train loss (w/o reg) on all data: 0.44973505
Test loss (w/o reg) on all data: 0.28121987
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.6798596e-05
Norm of the params: 11.169668
              Random: fixed 171 labels. Loss 0.28122. Accuracy 0.958.
### Flips: 1230, rs: 25, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30027097
Train loss (w/o reg) on all data: 0.28782287
Test loss (w/o reg) on all data: 0.18244004
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6104377e-05
Norm of the params: 15.778532
     Influence (LOO): fixed 489 labels. Loss 0.18244. Accuracy 0.973.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13129334
Train loss (w/o reg) on all data: 0.10783482
Test loss (w/o reg) on all data: 0.1111666
Train acc on all data:  0.9552637977145636
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 8.148647e-06
Norm of the params: 21.660343
                Loss: fixed 798 labels. Loss 0.11117. Accuracy 0.969.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44036493
Train loss (w/o reg) on all data: 0.43398386
Test loss (w/o reg) on all data: 0.2581312
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.663387e-06
Norm of the params: 11.296956
              Random: fixed 237 labels. Loss 0.25813. Accuracy 0.966.
### Flips: 1230, rs: 25, checks: 1025
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2678728
Train loss (w/o reg) on all data: 0.2551896
Test loss (w/o reg) on all data: 0.15988714
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.6312386e-06
Norm of the params: 15.926836
     Influence (LOO): fixed 560 labels. Loss 0.15989. Accuracy 0.974.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07493505
Train loss (w/o reg) on all data: 0.054854166
Test loss (w/o reg) on all data: 0.061083987
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.3725753e-06
Norm of the params: 20.0404
                Loss: fixed 926 labels. Loss 0.06108. Accuracy 0.983.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42619595
Train loss (w/o reg) on all data: 0.41986462
Test loss (w/o reg) on all data: 0.24065243
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2450907e-05
Norm of the params: 11.252854
              Random: fixed 289 labels. Loss 0.24065. Accuracy 0.976.
### Flips: 1230, rs: 25, checks: 1230
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23449051
Train loss (w/o reg) on all data: 0.22192872
Test loss (w/o reg) on all data: 0.13244198
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.4491724e-05
Norm of the params: 15.850423
     Influence (LOO): fixed 634 labels. Loss 0.13244. Accuracy 0.982.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048503824
Train loss (w/o reg) on all data: 0.032541204
Test loss (w/o reg) on all data: 0.03535535
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4725422e-06
Norm of the params: 17.867638
                Loss: fixed 988 labels. Loss 0.03536. Accuracy 0.991.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41341838
Train loss (w/o reg) on all data: 0.40706074
Test loss (w/o reg) on all data: 0.22589126
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.1616848e-05
Norm of the params: 11.276207
              Random: fixed 331 labels. Loss 0.22589. Accuracy 0.975.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4901399
Train loss (w/o reg) on all data: 0.48440722
Test loss (w/o reg) on all data: 0.33414897
Train acc on all data:  0.7556528081692195
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 5.1181356e-05
Norm of the params: 10.707635
Flipped loss: 0.33415. Accuracy: 0.938
### Flips: 1230, rs: 26, checks: 205
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42797163
Train loss (w/o reg) on all data: 0.4184561
Test loss (w/o reg) on all data: 0.27967575
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.0342833e-05
Norm of the params: 13.795317
     Influence (LOO): fixed 159 labels. Loss 0.27968. Accuracy 0.948.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39190874
Train loss (w/o reg) on all data: 0.37974524
Test loss (w/o reg) on all data: 0.27629757
Train acc on all data:  0.8101142718210552
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 3.8384565e-05
Norm of the params: 15.597108
                Loss: fixed 204 labels. Loss 0.27630. Accuracy 0.912.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47782245
Train loss (w/o reg) on all data: 0.47205067
Test loss (w/o reg) on all data: 0.3113254
Train acc on all data:  0.7704838317529784
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.9116166e-05
Norm of the params: 10.744101
              Random: fixed  62 labels. Loss 0.31133. Accuracy 0.953.
### Flips: 1230, rs: 26, checks: 410
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3792752
Train loss (w/o reg) on all data: 0.36827943
Test loss (w/o reg) on all data: 0.232228
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.0802936e-05
Norm of the params: 14.829549
     Influence (LOO): fixed 295 labels. Loss 0.23223. Accuracy 0.963.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2987111
Train loss (w/o reg) on all data: 0.28158516
Test loss (w/o reg) on all data: 0.22264326
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.00737125e-05
Norm of the params: 18.507256
                Loss: fixed 405 labels. Loss 0.22264. Accuracy 0.925.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46685132
Train loss (w/o reg) on all data: 0.46133325
Test loss (w/o reg) on all data: 0.29208383
Train acc on all data:  0.7823972769268174
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.264375e-05
Norm of the params: 10.505325
              Random: fixed 120 labels. Loss 0.29208. Accuracy 0.961.
### Flips: 1230, rs: 26, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33642808
Train loss (w/o reg) on all data: 0.32507244
Test loss (w/o reg) on all data: 0.19879273
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.4817406e-05
Norm of the params: 15.070252
     Influence (LOO): fixed 399 labels. Loss 0.19879. Accuracy 0.969.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20768465
Train loss (w/o reg) on all data: 0.18656163
Test loss (w/o reg) on all data: 0.16560015
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 9.875029e-06
Norm of the params: 20.553843
                Loss: fixed 608 labels. Loss 0.16560. Accuracy 0.944.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45435464
Train loss (w/o reg) on all data: 0.44870394
Test loss (w/o reg) on all data: 0.27855614
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 9.99938e-06
Norm of the params: 10.630795
              Random: fixed 166 labels. Loss 0.27856. Accuracy 0.961.
### Flips: 1230, rs: 26, checks: 820
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29099748
Train loss (w/o reg) on all data: 0.27846208
Test loss (w/o reg) on all data: 0.1673412
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.16101955e-05
Norm of the params: 15.833754
     Influence (LOO): fixed 507 labels. Loss 0.16734. Accuracy 0.975.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12654059
Train loss (w/o reg) on all data: 0.104786694
Test loss (w/o reg) on all data: 0.0945053
Train acc on all data:  0.9542912715779237
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.9442006e-05
Norm of the params: 20.858522
                Loss: fixed 795 labels. Loss 0.09451. Accuracy 0.972.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44032234
Train loss (w/o reg) on all data: 0.4347438
Test loss (w/o reg) on all data: 0.2554707
Train acc on all data:  0.8037928519328957
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.2882064e-05
Norm of the params: 10.562723
              Random: fixed 226 labels. Loss 0.25547. Accuracy 0.972.
### Flips: 1230, rs: 26, checks: 1025
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25108233
Train loss (w/o reg) on all data: 0.23841748
Test loss (w/o reg) on all data: 0.14097314
Train acc on all data:  0.8903476780938487
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.361331e-06
Norm of the params: 15.915311
     Influence (LOO): fixed 588 labels. Loss 0.14097. Accuracy 0.980.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06836124
Train loss (w/o reg) on all data: 0.0508085
Test loss (w/o reg) on all data: 0.059967425
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0953146e-06
Norm of the params: 18.736458
                Loss: fixed 923 labels. Loss 0.05997. Accuracy 0.976.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42468914
Train loss (w/o reg) on all data: 0.41915217
Test loss (w/o reg) on all data: 0.23427372
Train acc on all data:  0.8164356917092147
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.40753855e-05
Norm of the params: 10.523293
              Random: fixed 282 labels. Loss 0.23427. Accuracy 0.984.
### Flips: 1230, rs: 26, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21582532
Train loss (w/o reg) on all data: 0.20301992
Test loss (w/o reg) on all data: 0.114869796
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.3686296e-05
Norm of the params: 16.003378
     Influence (LOO): fixed 659 labels. Loss 0.11487. Accuracy 0.986.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039429262
Train loss (w/o reg) on all data: 0.026601344
Test loss (w/o reg) on all data: 0.033433944
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.2417698e-06
Norm of the params: 16.01744
                Loss: fixed 986 labels. Loss 0.03343. Accuracy 0.990.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40846324
Train loss (w/o reg) on all data: 0.4029881
Test loss (w/o reg) on all data: 0.22042103
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.199951e-05
Norm of the params: 10.464353
              Random: fixed 337 labels. Loss 0.22042. Accuracy 0.981.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4989986
Train loss (w/o reg) on all data: 0.49445987
Test loss (w/o reg) on all data: 0.33934498
Train acc on all data:  0.7515195720884998
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.5453295e-05
Norm of the params: 9.527578
Flipped loss: 0.33934. Accuracy: 0.942
### Flips: 1230, rs: 27, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42929736
Train loss (w/o reg) on all data: 0.4194877
Test loss (w/o reg) on all data: 0.28183958
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 4.176684e-05
Norm of the params: 14.006881
     Influence (LOO): fixed 162 labels. Loss 0.28184. Accuracy 0.948.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3978272
Train loss (w/o reg) on all data: 0.38688585
Test loss (w/o reg) on all data: 0.28663835
Train acc on all data:  0.8040359834670556
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 3.68869e-05
Norm of the params: 14.792808
                Loss: fixed 204 labels. Loss 0.28664. Accuracy 0.917.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48485166
Train loss (w/o reg) on all data: 0.47997898
Test loss (w/o reg) on all data: 0.31857526
Train acc on all data:  0.7670799902747386
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.1937392e-05
Norm of the params: 9.871858
              Random: fixed  58 labels. Loss 0.31858. Accuracy 0.949.
### Flips: 1230, rs: 27, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3861494
Train loss (w/o reg) on all data: 0.37576348
Test loss (w/o reg) on all data: 0.23796962
Train acc on all data:  0.8203257962557744
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.5107087e-06
Norm of the params: 14.412438
     Influence (LOO): fixed 285 labels. Loss 0.23797. Accuracy 0.970.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30715212
Train loss (w/o reg) on all data: 0.29262763
Test loss (w/o reg) on all data: 0.23234065
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 4.354177e-06
Norm of the params: 17.043756
                Loss: fixed 407 labels. Loss 0.23234. Accuracy 0.919.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47197402
Train loss (w/o reg) on all data: 0.46695185
Test loss (w/o reg) on all data: 0.2990575
Train acc on all data:  0.7787503039144177
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 3.270608e-05
Norm of the params: 10.022133
              Random: fixed 114 labels. Loss 0.29906. Accuracy 0.959.
### Flips: 1230, rs: 27, checks: 615
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429921
Train loss (w/o reg) on all data: 0.33112073
Test loss (w/o reg) on all data: 0.20754384
Train acc on all data:  0.8424507658643327
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.0599418e-05
Norm of the params: 15.408669
     Influence (LOO): fixed 392 labels. Loss 0.20754. Accuracy 0.970.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21417649
Train loss (w/o reg) on all data: 0.19576797
Test loss (w/o reg) on all data: 0.17884544
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.846308e-06
Norm of the params: 19.187765
                Loss: fixed 608 labels. Loss 0.17885. Accuracy 0.938.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45751655
Train loss (w/o reg) on all data: 0.4522983
Test loss (w/o reg) on all data: 0.28121018
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.3295311e-05
Norm of the params: 10.215906
              Random: fixed 168 labels. Loss 0.28121. Accuracy 0.960.
### Flips: 1230, rs: 27, checks: 820
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3046902
Train loss (w/o reg) on all data: 0.292634
Test loss (w/o reg) on all data: 0.17548281
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.4651028e-05
Norm of the params: 15.528162
     Influence (LOO): fixed 486 labels. Loss 0.17548. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12226589
Train loss (w/o reg) on all data: 0.10285473
Test loss (w/o reg) on all data: 0.1129435
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 6.21435e-06
Norm of the params: 19.703382
                Loss: fixed 809 labels. Loss 0.11294. Accuracy 0.959.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4473365
Train loss (w/o reg) on all data: 0.4422339
Test loss (w/o reg) on all data: 0.26289955
Train acc on all data:  0.7986870897155361
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 3.797733e-05
Norm of the params: 10.1020775
              Random: fixed 209 labels. Loss 0.26290. Accuracy 0.974.
### Flips: 1230, rs: 27, checks: 1025
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27222013
Train loss (w/o reg) on all data: 0.2608692
Test loss (w/o reg) on all data: 0.15157883
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.482285e-06
Norm of the params: 15.067142
     Influence (LOO): fixed 561 labels. Loss 0.15158. Accuracy 0.987.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04951642
Train loss (w/o reg) on all data: 0.034308944
Test loss (w/o reg) on all data: 0.03304334
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.669324e-06
Norm of the params: 17.439884
                Loss: fixed 960 labels. Loss 0.03304. Accuracy 0.990.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4306203
Train loss (w/o reg) on all data: 0.42539582
Test loss (w/o reg) on all data: 0.2473686
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.977895e-05
Norm of the params: 10.222029
              Random: fixed 264 labels. Loss 0.24737. Accuracy 0.978.
### Flips: 1230, rs: 27, checks: 1230
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23921122
Train loss (w/o reg) on all data: 0.22825444
Test loss (w/o reg) on all data: 0.12730113
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.107679e-06
Norm of the params: 14.803227
     Influence (LOO): fixed 637 labels. Loss 0.12730. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02811399
Train loss (w/o reg) on all data: 0.01686486
Test loss (w/o reg) on all data: 0.016768368
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2424733e-06
Norm of the params: 14.999419
                Loss: fixed 1006 labels. Loss 0.01677. Accuracy 0.994.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41703182
Train loss (w/o reg) on all data: 0.41184926
Test loss (w/o reg) on all data: 0.23072436
Train acc on all data:  0.8220277169948942
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4961268e-05
Norm of the params: 10.180927
              Random: fixed 312 labels. Loss 0.23072. Accuracy 0.981.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49779424
Train loss (w/o reg) on all data: 0.4932338
Test loss (w/o reg) on all data: 0.3392464
Train acc on all data:  0.75103330902018
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 5.5529643e-05
Norm of the params: 9.550336
Flipped loss: 0.33925. Accuracy: 0.927
### Flips: 1230, rs: 28, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42687702
Train loss (w/o reg) on all data: 0.41710675
Test loss (w/o reg) on all data: 0.28328982
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 3.566712e-05
Norm of the params: 13.97876
     Influence (LOO): fixed 166 labels. Loss 0.28329. Accuracy 0.937.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39680466
Train loss (w/o reg) on all data: 0.38655102
Test loss (w/o reg) on all data: 0.28736892
Train acc on all data:  0.8020909311937758
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 2.8060535e-05
Norm of the params: 14.320371
                Loss: fixed 203 labels. Loss 0.28737. Accuracy 0.913.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48339623
Train loss (w/o reg) on all data: 0.47842807
Test loss (w/o reg) on all data: 0.3171355
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.4591759e-05
Norm of the params: 9.968112
              Random: fixed  59 labels. Loss 0.31714. Accuracy 0.934.
### Flips: 1230, rs: 28, checks: 410
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3870569
Train loss (w/o reg) on all data: 0.37554276
Test loss (w/o reg) on all data: 0.24652429
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.5441201e-05
Norm of the params: 15.175073
     Influence (LOO): fixed 275 labels. Loss 0.24652. Accuracy 0.950.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30588734
Train loss (w/o reg) on all data: 0.29073712
Test loss (w/o reg) on all data: 0.23527591
Train acc on all data:  0.8541210795040116
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 4.867589e-05
Norm of the params: 17.407028
                Loss: fixed 406 labels. Loss 0.23528. Accuracy 0.915.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47213185
Train loss (w/o reg) on all data: 0.46704605
Test loss (w/o reg) on all data: 0.29956213
Train acc on all data:  0.7780209093119378
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 2.618602e-05
Norm of the params: 10.085435
              Random: fixed 109 labels. Loss 0.29956. Accuracy 0.948.
### Flips: 1230, rs: 28, checks: 615
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3485126
Train loss (w/o reg) on all data: 0.3361665
Test loss (w/o reg) on all data: 0.20828527
Train acc on all data:  0.838317529783613
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.733112e-05
Norm of the params: 15.713745
     Influence (LOO): fixed 379 labels. Loss 0.20829. Accuracy 0.967.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21620809
Train loss (w/o reg) on all data: 0.19696419
Test loss (w/o reg) on all data: 0.17282309
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 1.0800641e-05
Norm of the params: 19.618303
                Loss: fixed 605 labels. Loss 0.17282. Accuracy 0.939.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4576745
Train loss (w/o reg) on all data: 0.45240915
Test loss (w/o reg) on all data: 0.27570832
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.1949689e-05
Norm of the params: 10.261921
              Random: fixed 169 labels. Loss 0.27571. Accuracy 0.957.
### Flips: 1230, rs: 28, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3057645
Train loss (w/o reg) on all data: 0.29253796
Test loss (w/o reg) on all data: 0.17953974
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.082606e-05
Norm of the params: 16.264406
     Influence (LOO): fixed 481 labels. Loss 0.17954. Accuracy 0.971.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12869684
Train loss (w/o reg) on all data: 0.10748447
Test loss (w/o reg) on all data: 0.11876505
Train acc on all data:  0.9552637977145636
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.697893e-06
Norm of the params: 20.597271
                Loss: fixed 791 labels. Loss 0.11877. Accuracy 0.958.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44383198
Train loss (w/o reg) on all data: 0.4387181
Test loss (w/o reg) on all data: 0.25590363
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.5877758e-05
Norm of the params: 10.11324
              Random: fixed 224 labels. Loss 0.25590. Accuracy 0.968.
### Flips: 1230, rs: 28, checks: 1025
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27052823
Train loss (w/o reg) on all data: 0.25799832
Test loss (w/o reg) on all data: 0.15437292
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.2578782e-05
Norm of the params: 15.830287
     Influence (LOO): fixed 567 labels. Loss 0.15437. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06853402
Train loss (w/o reg) on all data: 0.049613737
Test loss (w/o reg) on all data: 0.07146304
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.7721977e-06
Norm of the params: 19.45265
                Loss: fixed 928 labels. Loss 0.07146. Accuracy 0.978.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43129224
Train loss (w/o reg) on all data: 0.42607236
Test loss (w/o reg) on all data: 0.23994097
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 4.247952e-05
Norm of the params: 10.217508
              Random: fixed 268 labels. Loss 0.23994. Accuracy 0.971.
### Flips: 1230, rs: 28, checks: 1230
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23311518
Train loss (w/o reg) on all data: 0.22036533
Test loss (w/o reg) on all data: 0.12709539
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1325838e-05
Norm of the params: 15.968629
     Influence (LOO): fixed 648 labels. Loss 0.12710. Accuracy 0.986.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045498464
Train loss (w/o reg) on all data: 0.030575486
Test loss (w/o reg) on all data: 0.039035138
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.064691e-06
Norm of the params: 17.275982
                Loss: fixed 984 labels. Loss 0.03904. Accuracy 0.985.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41597304
Train loss (w/o reg) on all data: 0.4106334
Test loss (w/o reg) on all data: 0.2240509
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.1905372e-05
Norm of the params: 10.33407
              Random: fixed 322 labels. Loss 0.22405. Accuracy 0.976.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4937567
Train loss (w/o reg) on all data: 0.4890252
Test loss (w/o reg) on all data: 0.34426504
Train acc on all data:  0.7551665451008995
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.3989449e-05
Norm of the params: 9.727813
Flipped loss: 0.34427. Accuracy: 0.943
### Flips: 1230, rs: 29, checks: 205
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4258653
Train loss (w/o reg) on all data: 0.4157815
Test loss (w/o reg) on all data: 0.29430634
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.859785e-05
Norm of the params: 14.20126
     Influence (LOO): fixed 161 labels. Loss 0.29431. Accuracy 0.947.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3916729
Train loss (w/o reg) on all data: 0.37990192
Test loss (w/o reg) on all data: 0.29536963
Train acc on all data:  0.8091417456844152
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 3.185314e-05
Norm of the params: 15.343389
                Loss: fixed 203 labels. Loss 0.29537. Accuracy 0.919.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4836088
Train loss (w/o reg) on all data: 0.4783953
Test loss (w/o reg) on all data: 0.3299656
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 2.4955662e-05
Norm of the params: 10.211276
              Random: fixed  45 labels. Loss 0.32997. Accuracy 0.949.
### Flips: 1230, rs: 29, checks: 410
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37838733
Train loss (w/o reg) on all data: 0.3662819
Test loss (w/o reg) on all data: 0.254913
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 3.4495086e-05
Norm of the params: 15.559841
     Influence (LOO): fixed 287 labels. Loss 0.25491. Accuracy 0.948.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30454898
Train loss (w/o reg) on all data: 0.28764847
Test loss (w/o reg) on all data: 0.24646401
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 6.3144767e-06
Norm of the params: 18.38506
                Loss: fixed 406 labels. Loss 0.24646. Accuracy 0.924.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4722303
Train loss (w/o reg) on all data: 0.46699843
Test loss (w/o reg) on all data: 0.3098675
Train acc on all data:  0.7758327255044979
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.477176e-05
Norm of the params: 10.22922
              Random: fixed  98 labels. Loss 0.30987. Accuracy 0.952.
### Flips: 1230, rs: 29, checks: 615
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33540043
Train loss (w/o reg) on all data: 0.32229543
Test loss (w/o reg) on all data: 0.21997859
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.8940154e-05
Norm of the params: 16.189505
     Influence (LOO): fixed 391 labels. Loss 0.21998. Accuracy 0.955.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21221733
Train loss (w/o reg) on all data: 0.19174762
Test loss (w/o reg) on all data: 0.18850823
Train acc on all data:  0.912472647702407
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 4.431731e-06
Norm of the params: 20.233496
                Loss: fixed 609 labels. Loss 0.18851. Accuracy 0.941.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45948502
Train loss (w/o reg) on all data: 0.45423248
Test loss (w/o reg) on all data: 0.2915837
Train acc on all data:  0.7855579868708972
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.3885877e-05
Norm of the params: 10.249417
              Random: fixed 157 labels. Loss 0.29158. Accuracy 0.953.
### Flips: 1230, rs: 29, checks: 820
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29963627
Train loss (w/o reg) on all data: 0.28602052
Test loss (w/o reg) on all data: 0.18077289
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4286533e-05
Norm of the params: 16.50198
     Influence (LOO): fixed 486 labels. Loss 0.18077. Accuracy 0.979.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12777653
Train loss (w/o reg) on all data: 0.10667431
Test loss (w/o reg) on all data: 0.109175935
Train acc on all data:  0.9542912715779237
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.10813135e-05
Norm of the params: 20.54372
                Loss: fixed 795 labels. Loss 0.10918. Accuracy 0.956.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44417134
Train loss (w/o reg) on all data: 0.43898913
Test loss (w/o reg) on all data: 0.2654327
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 6.416596e-05
Norm of the params: 10.180569
              Random: fixed 219 labels. Loss 0.26543. Accuracy 0.964.
### Flips: 1230, rs: 29, checks: 1025
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25949436
Train loss (w/o reg) on all data: 0.24617544
Test loss (w/o reg) on all data: 0.1468683
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.8000002e-05
Norm of the params: 16.321098
     Influence (LOO): fixed 584 labels. Loss 0.14687. Accuracy 0.986.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058181856
Train loss (w/o reg) on all data: 0.041795615
Test loss (w/o reg) on all data: 0.050128616
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.6888722e-06
Norm of the params: 18.103172
                Loss: fixed 946 labels. Loss 0.05013. Accuracy 0.982.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42849743
Train loss (w/o reg) on all data: 0.42315197
Test loss (w/o reg) on all data: 0.2496211
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.0957587e-05
Norm of the params: 10.339687
              Random: fixed 275 labels. Loss 0.24962. Accuracy 0.969.
### Flips: 1230, rs: 29, checks: 1230
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22486594
Train loss (w/o reg) on all data: 0.21219562
Test loss (w/o reg) on all data: 0.122301154
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.046022e-06
Norm of the params: 15.918748
     Influence (LOO): fixed 663 labels. Loss 0.12230. Accuracy 0.991.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031791523
Train loss (w/o reg) on all data: 0.01877751
Test loss (w/o reg) on all data: 0.032501757
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.302823e-06
Norm of the params: 16.133204
                Loss: fixed 998 labels. Loss 0.03250. Accuracy 0.991.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40956977
Train loss (w/o reg) on all data: 0.40363258
Test loss (w/o reg) on all data: 0.23391981
Train acc on all data:  0.8261609530756139
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.1284012e-05
Norm of the params: 10.896961
              Random: fixed 327 labels. Loss 0.23392. Accuracy 0.967.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4995212
Train loss (w/o reg) on all data: 0.49544656
Test loss (w/o reg) on all data: 0.33927613
Train acc on all data:  0.75079017748602
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 3.2295702e-05
Norm of the params: 9.027328
Flipped loss: 0.33928. Accuracy: 0.940
### Flips: 1230, rs: 30, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4367519
Train loss (w/o reg) on all data: 0.42746592
Test loss (w/o reg) on all data: 0.29569155
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 3.0034895e-05
Norm of the params: 13.6279
     Influence (LOO): fixed 158 labels. Loss 0.29569. Accuracy 0.937.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40052038
Train loss (w/o reg) on all data: 0.39044714
Test loss (w/o reg) on all data: 0.29485002
Train acc on all data:  0.8011184050571359
Test acc on all data:   0.9037900874635568
Norm of the mean of gradients: 3.2142376e-05
Norm of the params: 14.193843
                Loss: fixed 205 labels. Loss 0.29485. Accuracy 0.904.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48710585
Train loss (w/o reg) on all data: 0.48269048
Test loss (w/o reg) on all data: 0.31629243
Train acc on all data:  0.7644055433989788
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.880533e-05
Norm of the params: 9.397195
              Random: fixed  56 labels. Loss 0.31629. Accuracy 0.954.
### Flips: 1230, rs: 30, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39054674
Train loss (w/o reg) on all data: 0.3801371
Test loss (w/o reg) on all data: 0.2529475
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.0563387e-05
Norm of the params: 14.428894
     Influence (LOO): fixed 281 labels. Loss 0.25295. Accuracy 0.948.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30557334
Train loss (w/o reg) on all data: 0.29068354
Test loss (w/o reg) on all data: 0.24936411
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9067055393586005
Norm of the mean of gradients: 2.336057e-05
Norm of the params: 17.256763
                Loss: fixed 406 labels. Loss 0.24936. Accuracy 0.907.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47362438
Train loss (w/o reg) on all data: 0.46916777
Test loss (w/o reg) on all data: 0.29666156
Train acc on all data:  0.7758327255044979
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 9.616377e-06
Norm of the params: 9.440988
              Random: fixed 112 labels. Loss 0.29666. Accuracy 0.964.
### Flips: 1230, rs: 30, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3448
Train loss (w/o reg) on all data: 0.33333528
Test loss (w/o reg) on all data: 0.21297781
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.0302642e-05
Norm of the params: 15.142475
     Influence (LOO): fixed 393 labels. Loss 0.21298. Accuracy 0.971.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2112486
Train loss (w/o reg) on all data: 0.19283128
Test loss (w/o reg) on all data: 0.20357037
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 9.181721e-06
Norm of the params: 19.192354
                Loss: fixed 607 labels. Loss 0.20357. Accuracy 0.930.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46036163
Train loss (w/o reg) on all data: 0.45561326
Test loss (w/o reg) on all data: 0.28045446
Train acc on all data:  0.787989302212497
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 7.441597e-05
Norm of the params: 9.745138
              Random: fixed 161 labels. Loss 0.28045. Accuracy 0.962.
### Flips: 1230, rs: 30, checks: 820
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3047061
Train loss (w/o reg) on all data: 0.29322547
Test loss (w/o reg) on all data: 0.17778072
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.1705451e-05
Norm of the params: 15.152972
     Influence (LOO): fixed 495 labels. Loss 0.17778. Accuracy 0.978.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1280759
Train loss (w/o reg) on all data: 0.10831608
Test loss (w/o reg) on all data: 0.12485453
Train acc on all data:  0.9533187454412837
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 5.160275e-06
Norm of the params: 19.879549
                Loss: fixed 795 labels. Loss 0.12485. Accuracy 0.952.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44821468
Train loss (w/o reg) on all data: 0.44337
Test loss (w/o reg) on all data: 0.26479554
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 4.030803e-05
Norm of the params: 9.843454
              Random: fixed 205 labels. Loss 0.26480. Accuracy 0.968.
### Flips: 1230, rs: 30, checks: 1025
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26891458
Train loss (w/o reg) on all data: 0.25690138
Test loss (w/o reg) on all data: 0.1558916
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.847232e-06
Norm of the params: 15.500457
     Influence (LOO): fixed 574 labels. Loss 0.15589. Accuracy 0.982.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06441425
Train loss (w/o reg) on all data: 0.047689483
Test loss (w/o reg) on all data: 0.06070624
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.713037e-06
Norm of the params: 18.289215
                Loss: fixed 937 labels. Loss 0.06071. Accuracy 0.981.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43259242
Train loss (w/o reg) on all data: 0.42762494
Test loss (w/o reg) on all data: 0.24202543
Train acc on all data:  0.812059324094335
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.6046094e-05
Norm of the params: 9.967417
              Random: fixed 262 labels. Loss 0.24203. Accuracy 0.981.
### Flips: 1230, rs: 30, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23049888
Train loss (w/o reg) on all data: 0.21856464
Test loss (w/o reg) on all data: 0.12886399
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.0409905e-05
Norm of the params: 15.449424
     Influence (LOO): fixed 657 labels. Loss 0.12886. Accuracy 0.983.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03672892
Train loss (w/o reg) on all data: 0.025025254
Test loss (w/o reg) on all data: 0.02856893
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.5212346e-07
Norm of the params: 15.299456
                Loss: fixed 1003 labels. Loss 0.02857. Accuracy 0.994.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41240984
Train loss (w/o reg) on all data: 0.406654
Test loss (w/o reg) on all data: 0.22689527
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.31486495e-05
Norm of the params: 10.729264
              Random: fixed 323 labels. Loss 0.22690. Accuracy 0.978.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5023365
Train loss (w/o reg) on all data: 0.49731272
Test loss (w/o reg) on all data: 0.338991
Train acc on all data:  0.7473863360077803
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 7.765179e-05
Norm of the params: 10.023762
Flipped loss: 0.33899. Accuracy: 0.935
### Flips: 1230, rs: 31, checks: 205
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44051266
Train loss (w/o reg) on all data: 0.43048295
Test loss (w/o reg) on all data: 0.2848076
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.3737362e-05
Norm of the params: 14.163131
     Influence (LOO): fixed 161 labels. Loss 0.28481. Accuracy 0.948.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4065127
Train loss (w/o reg) on all data: 0.39529634
Test loss (w/o reg) on all data: 0.28422514
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9105928085519922
Norm of the mean of gradients: 1.7261424e-05
Norm of the params: 14.9775715
                Loss: fixed 204 labels. Loss 0.28423. Accuracy 0.911.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49271017
Train loss (w/o reg) on all data: 0.48755106
Test loss (w/o reg) on all data: 0.32294917
Train acc on all data:  0.762217359591539
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.4689318e-05
Norm of the params: 10.157852
              Random: fixed  49 labels. Loss 0.32295. Accuracy 0.946.
### Flips: 1230, rs: 31, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39341283
Train loss (w/o reg) on all data: 0.38249934
Test loss (w/o reg) on all data: 0.24199493
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.5503252e-05
Norm of the params: 14.773963
     Influence (LOO): fixed 287 labels. Loss 0.24199. Accuracy 0.962.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31447035
Train loss (w/o reg) on all data: 0.2992842
Test loss (w/o reg) on all data: 0.22142364
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 2.0859474e-05
Norm of the params: 17.427662
                Loss: fixed 409 labels. Loss 0.22142. Accuracy 0.923.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.478429
Train loss (w/o reg) on all data: 0.47304615
Test loss (w/o reg) on all data: 0.3034792
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 3.518546e-05
Norm of the params: 10.3757715
              Random: fixed 113 labels. Loss 0.30348. Accuracy 0.950.
### Flips: 1230, rs: 31, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35200936
Train loss (w/o reg) on all data: 0.34058893
Test loss (w/o reg) on all data: 0.20603973
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.799545e-05
Norm of the params: 15.113194
     Influence (LOO): fixed 392 labels. Loss 0.20604. Accuracy 0.969.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22350708
Train loss (w/o reg) on all data: 0.20439573
Test loss (w/o reg) on all data: 0.17099112
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 8.205286e-06
Norm of the params: 19.550625
                Loss: fixed 609 labels. Loss 0.17099. Accuracy 0.936.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4654337
Train loss (w/o reg) on all data: 0.45971867
Test loss (w/o reg) on all data: 0.28684524
Train acc on all data:  0.7823972769268174
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.765101e-05
Norm of the params: 10.691141
              Random: fixed 163 labels. Loss 0.28685. Accuracy 0.957.
### Flips: 1230, rs: 31, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3132633
Train loss (w/o reg) on all data: 0.30175084
Test loss (w/o reg) on all data: 0.17937016
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4952479e-05
Norm of the params: 15.173957
     Influence (LOO): fixed 484 labels. Loss 0.17937. Accuracy 0.981.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1376935
Train loss (w/o reg) on all data: 0.116658255
Test loss (w/o reg) on all data: 0.11066254
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.891799e-06
Norm of the params: 20.511087
                Loss: fixed 796 labels. Loss 0.11066. Accuracy 0.958.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45021662
Train loss (w/o reg) on all data: 0.44453457
Test loss (w/o reg) on all data: 0.2677652
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8051923e-05
Norm of the params: 10.660258
              Random: fixed 222 labels. Loss 0.26777. Accuracy 0.967.
### Flips: 1230, rs: 31, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27488878
Train loss (w/o reg) on all data: 0.26355392
Test loss (w/o reg) on all data: 0.14939685
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.3508383e-05
Norm of the params: 15.05648
     Influence (LOO): fixed 572 labels. Loss 0.14940. Accuracy 0.991.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07214545
Train loss (w/o reg) on all data: 0.05424843
Test loss (w/o reg) on all data: 0.061428722
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.5116246e-06
Norm of the params: 18.919313
                Loss: fixed 944 labels. Loss 0.06143. Accuracy 0.978.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43062544
Train loss (w/o reg) on all data: 0.42480487
Test loss (w/o reg) on all data: 0.2473769
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 5.0528077e-05
Norm of the params: 10.789403
              Random: fixed 286 labels. Loss 0.24738. Accuracy 0.975.
### Flips: 1230, rs: 31, checks: 1230
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2414008
Train loss (w/o reg) on all data: 0.22997838
Test loss (w/o reg) on all data: 0.12886375
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.434316e-06
Norm of the params: 15.114505
     Influence (LOO): fixed 648 labels. Loss 0.12886. Accuracy 0.993.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039585926
Train loss (w/o reg) on all data: 0.026771525
Test loss (w/o reg) on all data: 0.026779672
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.4906567e-07
Norm of the params: 16.008997
                Loss: fixed 1016 labels. Loss 0.02678. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41328183
Train loss (w/o reg) on all data: 0.40732056
Test loss (w/o reg) on all data: 0.22917184
Train acc on all data:  0.8237296377340141
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.567671e-06
Norm of the params: 10.919025
              Random: fixed 343 labels. Loss 0.22917. Accuracy 0.976.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49124414
Train loss (w/o reg) on all data: 0.48634067
Test loss (w/o reg) on all data: 0.33884898
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 4.3174154e-05
Norm of the params: 9.902984
Flipped loss: 0.33885. Accuracy: 0.924
### Flips: 1230, rs: 32, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4199861
Train loss (w/o reg) on all data: 0.40938145
Test loss (w/o reg) on all data: 0.291188
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 1.4555367e-05
Norm of the params: 14.563412
     Influence (LOO): fixed 163 labels. Loss 0.29119. Accuracy 0.930.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38452983
Train loss (w/o reg) on all data: 0.373131
Test loss (w/o reg) on all data: 0.2997286
Train acc on all data:  0.8154631655725748
Test acc on all data:   0.8882410106899903
Norm of the mean of gradients: 1.0161334e-05
Norm of the params: 15.098891
                Loss: fixed 204 labels. Loss 0.29973. Accuracy 0.888.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48100248
Train loss (w/o reg) on all data: 0.4758281
Test loss (w/o reg) on all data: 0.3187852
Train acc on all data:  0.7699975686846584
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.8991714e-05
Norm of the params: 10.172879
              Random: fixed  51 labels. Loss 0.31879. Accuracy 0.937.
### Flips: 1230, rs: 32, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37484717
Train loss (w/o reg) on all data: 0.363415
Test loss (w/o reg) on all data: 0.24480727
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.052927e-05
Norm of the params: 15.120964
     Influence (LOO): fixed 283 labels. Loss 0.24481. Accuracy 0.945.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28881082
Train loss (w/o reg) on all data: 0.27235052
Test loss (w/o reg) on all data: 0.24929996
Train acc on all data:  0.8633600778020909
Test acc on all data:   0.8999028182701652
Norm of the mean of gradients: 1.0011334e-05
Norm of the params: 18.14404
                Loss: fixed 407 labels. Loss 0.24930. Accuracy 0.900.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4682342
Train loss (w/o reg) on all data: 0.4626756
Test loss (w/o reg) on all data: 0.29856187
Train acc on all data:  0.7797228300510576
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 1.1719516e-05
Norm of the params: 10.543832
              Random: fixed 107 labels. Loss 0.29856. Accuracy 0.948.
### Flips: 1230, rs: 32, checks: 615
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33109093
Train loss (w/o reg) on all data: 0.31888083
Test loss (w/o reg) on all data: 0.21282637
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 7.487102e-06
Norm of the params: 15.626971
     Influence (LOO): fixed 391 labels. Loss 0.21283. Accuracy 0.948.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1990944
Train loss (w/o reg) on all data: 0.17803141
Test loss (w/o reg) on all data: 0.18829364
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 1.0570621e-05
Norm of the params: 20.52461
                Loss: fixed 605 labels. Loss 0.18829. Accuracy 0.930.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4559131
Train loss (w/o reg) on all data: 0.45016432
Test loss (w/o reg) on all data: 0.2786817
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.7655395e-05
Norm of the params: 10.722658
              Random: fixed 161 labels. Loss 0.27868. Accuracy 0.955.
### Flips: 1230, rs: 32, checks: 820
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29769176
Train loss (w/o reg) on all data: 0.28497806
Test loss (w/o reg) on all data: 0.18540578
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 8.46646e-06
Norm of the params: 15.945969
     Influence (LOO): fixed 475 labels. Loss 0.18541. Accuracy 0.965.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12383376
Train loss (w/o reg) on all data: 0.1016701
Test loss (w/o reg) on all data: 0.12878935
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.340011e-06
Norm of the params: 21.054056
                Loss: fixed 782 labels. Loss 0.12879. Accuracy 0.964.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44261718
Train loss (w/o reg) on all data: 0.43682697
Test loss (w/o reg) on all data: 0.26062116
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.602447e-05
Norm of the params: 10.761231
              Random: fixed 217 labels. Loss 0.26062. Accuracy 0.961.
### Flips: 1230, rs: 32, checks: 1025
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25973323
Train loss (w/o reg) on all data: 0.24694988
Test loss (w/o reg) on all data: 0.15995105
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.13381575e-05
Norm of the params: 15.989585
     Influence (LOO): fixed 561 labels. Loss 0.15995. Accuracy 0.969.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078045696
Train loss (w/o reg) on all data: 0.057722654
Test loss (w/o reg) on all data: 0.07843419
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.2916407e-06
Norm of the params: 20.160873
                Loss: fixed 896 labels. Loss 0.07843. Accuracy 0.976.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42525566
Train loss (w/o reg) on all data: 0.4191081
Test loss (w/o reg) on all data: 0.24165933
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 7.122282e-05
Norm of the params: 11.088346
              Random: fixed 273 labels. Loss 0.24166. Accuracy 0.967.
### Flips: 1230, rs: 32, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22605458
Train loss (w/o reg) on all data: 0.21284454
Test loss (w/o reg) on all data: 0.13119553
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.3486265e-05
Norm of the params: 16.254255
     Influence (LOO): fixed 639 labels. Loss 0.13120. Accuracy 0.976.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050207973
Train loss (w/o reg) on all data: 0.03388216
Test loss (w/o reg) on all data: 0.042429492
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.064138e-06
Norm of the params: 18.069761
                Loss: fixed 965 labels. Loss 0.04243. Accuracy 0.987.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4123535
Train loss (w/o reg) on all data: 0.40582234
Test loss (w/o reg) on all data: 0.22645253
Train acc on all data:  0.825674690007294
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.1596687e-05
Norm of the params: 11.429053
              Random: fixed 318 labels. Loss 0.22645. Accuracy 0.973.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49388552
Train loss (w/o reg) on all data: 0.48937652
Test loss (w/o reg) on all data: 0.33003572
Train acc on all data:  0.7554096766350595
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 3.758195e-05
Norm of the params: 9.496315
Flipped loss: 0.33004. Accuracy: 0.944
### Flips: 1230, rs: 33, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4301131
Train loss (w/o reg) on all data: 0.42004097
Test loss (w/o reg) on all data: 0.27692622
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 6.508679e-05
Norm of the params: 14.193054
     Influence (LOO): fixed 158 labels. Loss 0.27693. Accuracy 0.959.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39290598
Train loss (w/o reg) on all data: 0.38137928
Test loss (w/o reg) on all data: 0.28146833
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 1.3086363e-05
Norm of the params: 15.183344
                Loss: fixed 201 labels. Loss 0.28147. Accuracy 0.917.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4832718
Train loss (w/o reg) on all data: 0.47865954
Test loss (w/o reg) on all data: 0.30866626
Train acc on all data:  0.7658643326039387
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.5198935e-05
Norm of the params: 9.604453
              Random: fixed  57 labels. Loss 0.30867. Accuracy 0.951.
### Flips: 1230, rs: 33, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3824302
Train loss (w/o reg) on all data: 0.37107423
Test loss (w/o reg) on all data: 0.23494115
Train acc on all data:  0.8198395331874544
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.4753446e-05
Norm of the params: 15.070487
     Influence (LOO): fixed 287 labels. Loss 0.23494. Accuracy 0.971.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30410868
Train loss (w/o reg) on all data: 0.2885801
Test loss (w/o reg) on all data: 0.23434065
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9203109815354713
Norm of the mean of gradients: 1.0087673e-05
Norm of the params: 17.623043
                Loss: fixed 402 labels. Loss 0.23434. Accuracy 0.920.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4738538
Train loss (w/o reg) on all data: 0.46947023
Test loss (w/o reg) on all data: 0.28997117
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7836423e-05
Norm of the params: 9.363305
              Random: fixed 112 labels. Loss 0.28997. Accuracy 0.964.
### Flips: 1230, rs: 33, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3374335
Train loss (w/o reg) on all data: 0.3256276
Test loss (w/o reg) on all data: 0.19863798
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.6374997e-05
Norm of the params: 15.366129
     Influence (LOO): fixed 398 labels. Loss 0.19864. Accuracy 0.972.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21094523
Train loss (w/o reg) on all data: 0.19149356
Test loss (w/o reg) on all data: 0.18006402
Train acc on all data:  0.911986384634087
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 5.0870085e-06
Norm of the params: 19.723936
                Loss: fixed 605 labels. Loss 0.18006. Accuracy 0.935.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46062908
Train loss (w/o reg) on all data: 0.4560617
Test loss (w/o reg) on all data: 0.2706888
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.8439764e-05
Norm of the params: 9.557609
              Random: fixed 165 labels. Loss 0.27069. Accuracy 0.971.
### Flips: 1230, rs: 33, checks: 820
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29235423
Train loss (w/o reg) on all data: 0.27984637
Test loss (w/o reg) on all data: 0.16242039
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0439265e-05
Norm of the params: 15.816363
     Influence (LOO): fixed 503 labels. Loss 0.16242. Accuracy 0.975.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12670222
Train loss (w/o reg) on all data: 0.10590188
Test loss (w/o reg) on all data: 0.109298676
Train acc on all data:  0.9535618769754437
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.3016809e-05
Norm of the params: 20.396248
                Loss: fixed 792 labels. Loss 0.10930. Accuracy 0.959.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4438829
Train loss (w/o reg) on all data: 0.43919507
Test loss (w/o reg) on all data: 0.24783702
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.36603785e-05
Norm of the params: 9.682813
              Random: fixed 227 labels. Loss 0.24784. Accuracy 0.976.
### Flips: 1230, rs: 33, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25658453
Train loss (w/o reg) on all data: 0.24414952
Test loss (w/o reg) on all data: 0.13842241
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8571436e-05
Norm of the params: 15.77022
     Influence (LOO): fixed 582 labels. Loss 0.13842. Accuracy 0.983.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06396146
Train loss (w/o reg) on all data: 0.046116393
Test loss (w/o reg) on all data: 0.043378867
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4732452e-06
Norm of the params: 18.891836
                Loss: fixed 939 labels. Loss 0.04338. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4261313
Train loss (w/o reg) on all data: 0.42131793
Test loss (w/o reg) on all data: 0.22657011
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.4416238e-05
Norm of the params: 9.811606
              Random: fixed 291 labels. Loss 0.22657. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 1230
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22633703
Train loss (w/o reg) on all data: 0.21458475
Test loss (w/o reg) on all data: 0.11886266
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6144204e-05
Norm of the params: 15.3312
     Influence (LOO): fixed 651 labels. Loss 0.11886. Accuracy 0.984.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037931584
Train loss (w/o reg) on all data: 0.025154954
Test loss (w/o reg) on all data: 0.019278672
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.081236e-06
Norm of the params: 15.985387
                Loss: fixed 994 labels. Loss 0.01928. Accuracy 0.997.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4090394
Train loss (w/o reg) on all data: 0.40410212
Test loss (w/o reg) on all data: 0.2101274
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1242416e-05
Norm of the params: 9.937081
              Random: fixed 345 labels. Loss 0.21013. Accuracy 0.985.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49958402
Train loss (w/o reg) on all data: 0.49481544
Test loss (w/o reg) on all data: 0.33550337
Train acc on all data:  0.7588135181132993
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 3.1340704e-05
Norm of the params: 9.765842
Flipped loss: 0.33550. Accuracy: 0.954
### Flips: 1230, rs: 34, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43278873
Train loss (w/o reg) on all data: 0.42327338
Test loss (w/o reg) on all data: 0.28353068
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.5605552e-05
Norm of the params: 13.795177
     Influence (LOO): fixed 163 labels. Loss 0.28353. Accuracy 0.955.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39497617
Train loss (w/o reg) on all data: 0.38208082
Test loss (w/o reg) on all data: 0.28642252
Train acc on all data:  0.8096280087527352
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 5.189327e-05
Norm of the params: 16.059483
                Loss: fixed 205 labels. Loss 0.28642. Accuracy 0.921.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48450527
Train loss (w/o reg) on all data: 0.47930133
Test loss (w/o reg) on all data: 0.30664065
Train acc on all data:  0.7729151470945782
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 4.676086e-05
Norm of the params: 10.20189
              Random: fixed  70 labels. Loss 0.30664. Accuracy 0.957.
### Flips: 1230, rs: 34, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38199496
Train loss (w/o reg) on all data: 0.370566
Test loss (w/o reg) on all data: 0.24501155
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.967632e-06
Norm of the params: 15.1188345
     Influence (LOO): fixed 291 labels. Loss 0.24501. Accuracy 0.954.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30237204
Train loss (w/o reg) on all data: 0.28472653
Test loss (w/o reg) on all data: 0.2330977
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 6.439759e-05
Norm of the params: 18.785896
                Loss: fixed 409 labels. Loss 0.23310. Accuracy 0.922.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4694229
Train loss (w/o reg) on all data: 0.4638869
Test loss (w/o reg) on all data: 0.286272
Train acc on all data:  0.7806953561876976
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.1826749e-05
Norm of the params: 10.522376
              Random: fixed 130 labels. Loss 0.28627. Accuracy 0.965.
### Flips: 1230, rs: 34, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33867013
Train loss (w/o reg) on all data: 0.32604745
Test loss (w/o reg) on all data: 0.20891052
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.4411634e-05
Norm of the params: 15.888793
     Influence (LOO): fixed 395 labels. Loss 0.20891. Accuracy 0.967.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21440385
Train loss (w/o reg) on all data: 0.19386907
Test loss (w/o reg) on all data: 0.16953687
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.746625e-05
Norm of the params: 20.265629
                Loss: fixed 607 labels. Loss 0.16954. Accuracy 0.945.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45809117
Train loss (w/o reg) on all data: 0.4526959
Test loss (w/o reg) on all data: 0.27075338
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.0710627e-05
Norm of the params: 10.38776
              Random: fixed 181 labels. Loss 0.27075. Accuracy 0.968.
### Flips: 1230, rs: 34, checks: 820
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30452704
Train loss (w/o reg) on all data: 0.29209095
Test loss (w/o reg) on all data: 0.17595914
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.305954e-05
Norm of the params: 15.77091
     Influence (LOO): fixed 483 labels. Loss 0.17596. Accuracy 0.978.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827885
Train loss (w/o reg) on all data: 0.105866164
Test loss (w/o reg) on all data: 0.1126955
Train acc on all data:  0.9557500607828835
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.170463e-06
Norm of the params: 21.172003
                Loss: fixed 795 labels. Loss 0.11270. Accuracy 0.962.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44330165
Train loss (w/o reg) on all data: 0.43777585
Test loss (w/o reg) on all data: 0.25769958
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.417212e-05
Norm of the params: 10.512646
              Random: fixed 232 labels. Loss 0.25770. Accuracy 0.968.
### Flips: 1230, rs: 34, checks: 1025
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2696461
Train loss (w/o reg) on all data: 0.257314
Test loss (w/o reg) on all data: 0.14941657
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 7.5921603e-06
Norm of the params: 15.704838
     Influence (LOO): fixed 567 labels. Loss 0.14942. Accuracy 0.981.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06611189
Train loss (w/o reg) on all data: 0.048120998
Test loss (w/o reg) on all data: 0.062326986
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3560691e-06
Norm of the params: 18.968868
                Loss: fixed 939 labels. Loss 0.06233. Accuracy 0.975.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42864582
Train loss (w/o reg) on all data: 0.4231151
Test loss (w/o reg) on all data: 0.24118157
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.110228e-05
Norm of the params: 10.517333
              Random: fixed 281 labels. Loss 0.24118. Accuracy 0.972.
### Flips: 1230, rs: 34, checks: 1230
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23688985
Train loss (w/o reg) on all data: 0.22504291
Test loss (w/o reg) on all data: 0.12863329
Train acc on all data:  0.8986141502552881
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.1086806e-06
Norm of the params: 15.392824
     Influence (LOO): fixed 635 labels. Loss 0.12863. Accuracy 0.982.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03742917
Train loss (w/o reg) on all data: 0.025057036
Test loss (w/o reg) on all data: 0.028626561
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3554786e-06
Norm of the params: 15.73031
                Loss: fixed 1008 labels. Loss 0.02863. Accuracy 0.991.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41227487
Train loss (w/o reg) on all data: 0.40616038
Test loss (w/o reg) on all data: 0.2236634
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.613304e-06
Norm of the params: 11.0584755
              Random: fixed 334 labels. Loss 0.22366. Accuracy 0.981.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.50116456
Train loss (w/o reg) on all data: 0.4956009
Test loss (w/o reg) on all data: 0.332468
Train acc on all data:  0.74981765134938
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.8364724e-05
Norm of the params: 10.548577
Flipped loss: 0.33247. Accuracy: 0.948
### Flips: 1230, rs: 35, checks: 205
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4356273
Train loss (w/o reg) on all data: 0.42541358
Test loss (w/o reg) on all data: 0.2891233
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.033923e-05
Norm of the params: 14.292479
     Influence (LOO): fixed 155 labels. Loss 0.28912. Accuracy 0.946.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.405161
Train loss (w/o reg) on all data: 0.3936957
Test loss (w/o reg) on all data: 0.28535396
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 1.9954154e-05
Norm of the params: 15.142843
                Loss: fixed 204 labels. Loss 0.28535. Accuracy 0.913.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4904836
Train loss (w/o reg) on all data: 0.4849638
Test loss (w/o reg) on all data: 0.31770393
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.1027299e-05
Norm of the params: 10.506953
              Random: fixed  51 labels. Loss 0.31770. Accuracy 0.954.
### Flips: 1230, rs: 35, checks: 410
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39145485
Train loss (w/o reg) on all data: 0.37991992
Test loss (w/o reg) on all data: 0.24563749
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.6934087e-05
Norm of the params: 15.18876
     Influence (LOO): fixed 280 labels. Loss 0.24564. Accuracy 0.957.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31105968
Train loss (w/o reg) on all data: 0.2951779
Test loss (w/o reg) on all data: 0.23734199
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9067055393586005
Norm of the mean of gradients: 8.416849e-06
Norm of the params: 17.822334
                Loss: fixed 406 labels. Loss 0.23734. Accuracy 0.907.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4790701
Train loss (w/o reg) on all data: 0.47337064
Test loss (w/o reg) on all data: 0.29921797
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.884887e-05
Norm of the params: 10.676561
              Random: fixed 104 labels. Loss 0.29922. Accuracy 0.965.
### Flips: 1230, rs: 35, checks: 615
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34542006
Train loss (w/o reg) on all data: 0.3331065
Test loss (w/o reg) on all data: 0.20164764
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.310395e-05
Norm of the params: 15.693048
     Influence (LOO): fixed 401 labels. Loss 0.20165. Accuracy 0.983.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21988058
Train loss (w/o reg) on all data: 0.20037606
Test loss (w/o reg) on all data: 0.18084247
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9329446064139941
Norm of the mean of gradients: 9.731289e-06
Norm of the params: 19.750708
                Loss: fixed 609 labels. Loss 0.18084. Accuracy 0.933.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46685788
Train loss (w/o reg) on all data: 0.46112758
Test loss (w/o reg) on all data: 0.28086397
Train acc on all data:  0.7848285922684172
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.0365656e-05
Norm of the params: 10.705409
              Random: fixed 155 labels. Loss 0.28086. Accuracy 0.970.
### Flips: 1230, rs: 35, checks: 820
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30722558
Train loss (w/o reg) on all data: 0.29468018
Test loss (w/o reg) on all data: 0.17166902
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4156241e-05
Norm of the params: 15.840073
     Influence (LOO): fixed 490 labels. Loss 0.17167. Accuracy 0.980.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13714422
Train loss (w/o reg) on all data: 0.116749994
Test loss (w/o reg) on all data: 0.111831374
Train acc on all data:  0.9508874300996839
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.3190127e-05
Norm of the params: 20.196156
                Loss: fixed 796 labels. Loss 0.11183. Accuracy 0.965.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4557319
Train loss (w/o reg) on all data: 0.45003846
Test loss (w/o reg) on all data: 0.26435995
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.040317e-05
Norm of the params: 10.670938
              Random: fixed 201 labels. Loss 0.26436. Accuracy 0.980.
### Flips: 1230, rs: 35, checks: 1025
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2676608
Train loss (w/o reg) on all data: 0.25590858
Test loss (w/o reg) on all data: 0.14364241
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3010611e-05
Norm of the params: 15.331148
     Influence (LOO): fixed 584 labels. Loss 0.14364. Accuracy 0.988.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06969115
Train loss (w/o reg) on all data: 0.052500837
Test loss (w/o reg) on all data: 0.062051345
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.0575793e-06
Norm of the params: 18.542011
                Loss: fixed 942 labels. Loss 0.06205. Accuracy 0.981.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43897724
Train loss (w/o reg) on all data: 0.43299413
Test loss (w/o reg) on all data: 0.24656658
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4347812e-05
Norm of the params: 10.93903
              Random: fixed 254 labels. Loss 0.24657. Accuracy 0.979.
### Flips: 1230, rs: 35, checks: 1230
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23824352
Train loss (w/o reg) on all data: 0.22638188
Test loss (w/o reg) on all data: 0.12541945
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7625227e-05
Norm of the params: 15.402358
     Influence (LOO): fixed 652 labels. Loss 0.12542. Accuracy 0.988.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03801173
Train loss (w/o reg) on all data: 0.025395902
Test loss (w/o reg) on all data: 0.026437035
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.769966e-07
Norm of the params: 15.884475
                Loss: fixed 1014 labels. Loss 0.02644. Accuracy 0.994.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.425228
Train loss (w/o reg) on all data: 0.41940916
Test loss (w/o reg) on all data: 0.22803074
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.066215e-05
Norm of the params: 10.787801
              Random: fixed 306 labels. Loss 0.22803. Accuracy 0.982.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48982236
Train loss (w/o reg) on all data: 0.48339087
Test loss (w/o reg) on all data: 0.35888204
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9047619047619048
Norm of the mean of gradients: 6.358005e-05
Norm of the params: 11.3415165
Flipped loss: 0.35888. Accuracy: 0.905
### Flips: 1230, rs: 36, checks: 205
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42785856
Train loss (w/o reg) on all data: 0.41672114
Test loss (w/o reg) on all data: 0.30605173
Train acc on all data:  0.7950401167031363
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 1.4103181e-05
Norm of the params: 14.924765
     Influence (LOO): fixed 152 labels. Loss 0.30605. Accuracy 0.922.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39070013
Train loss (w/o reg) on all data: 0.3776237
Test loss (w/o reg) on all data: 0.30883667
Train acc on all data:  0.8086554826160953
Test acc on all data:   0.8833819241982507
Norm of the mean of gradients: 2.8538674e-05
Norm of the params: 16.171837
                Loss: fixed 203 labels. Loss 0.30884. Accuracy 0.883.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47728267
Train loss (w/o reg) on all data: 0.47088197
Test loss (w/o reg) on all data: 0.33891168
Train acc on all data:  0.7695113056163384
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 1.9412992e-05
Norm of the params: 11.314339
              Random: fixed  61 labels. Loss 0.33891. Accuracy 0.914.
### Flips: 1230, rs: 36, checks: 410
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38542503
Train loss (w/o reg) on all data: 0.37293035
Test loss (w/o reg) on all data: 0.27063227
Train acc on all data:  0.8183807439824945
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 4.455725e-05
Norm of the params: 15.808029
     Influence (LOO): fixed 270 labels. Loss 0.27063. Accuracy 0.925.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3000181
Train loss (w/o reg) on all data: 0.28219923
Test loss (w/o reg) on all data: 0.2584367
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 2.8893708e-05
Norm of the params: 18.877966
                Loss: fixed 404 labels. Loss 0.25844. Accuracy 0.895.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4680696
Train loss (w/o reg) on all data: 0.46177366
Test loss (w/o reg) on all data: 0.31378323
Train acc on all data:  0.7792365669827377
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.1644101e-05
Norm of the params: 11.221375
              Random: fixed 112 labels. Loss 0.31378. Accuracy 0.944.
### Flips: 1230, rs: 36, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34786832
Train loss (w/o reg) on all data: 0.33461982
Test loss (w/o reg) on all data: 0.2329074
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 1.9516649e-05
Norm of the params: 16.277908
     Influence (LOO): fixed 374 labels. Loss 0.23291. Accuracy 0.942.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21559824
Train loss (w/o reg) on all data: 0.194504
Test loss (w/o reg) on all data: 0.19734369
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9076773566569485
Norm of the mean of gradients: 2.7660473e-05
Norm of the params: 20.539837
                Loss: fixed 595 labels. Loss 0.19734. Accuracy 0.908.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45458928
Train loss (w/o reg) on all data: 0.4485399
Test loss (w/o reg) on all data: 0.29055628
Train acc on all data:  0.7899343544857768
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.637391e-05
Norm of the params: 10.9994135
              Random: fixed 171 labels. Loss 0.29056. Accuracy 0.954.
### Flips: 1230, rs: 36, checks: 820
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30934623
Train loss (w/o reg) on all data: 0.2957059
Test loss (w/o reg) on all data: 0.18891068
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.940118e-06
Norm of the params: 16.516855
     Influence (LOO): fixed 478 labels. Loss 0.18891. Accuracy 0.970.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13411416
Train loss (w/o reg) on all data: 0.11076585
Test loss (w/o reg) on all data: 0.12261112
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 2.382975e-05
Norm of the params: 21.609404
                Loss: fixed 780 labels. Loss 0.12261. Accuracy 0.954.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44020218
Train loss (w/o reg) on all data: 0.43402648
Test loss (w/o reg) on all data: 0.26788118
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.3040433e-05
Norm of the params: 11.113686
              Random: fixed 230 labels. Loss 0.26788. Accuracy 0.964.
### Flips: 1230, rs: 36, checks: 1025
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27336186
Train loss (w/o reg) on all data: 0.25985852
Test loss (w/o reg) on all data: 0.15119024
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.9030021e-05
Norm of the params: 16.433718
     Influence (LOO): fixed 571 labels. Loss 0.15119. Accuracy 0.984.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08151442
Train loss (w/o reg) on all data: 0.060802467
Test loss (w/o reg) on all data: 0.06983696
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.5117763e-06
Norm of the params: 20.352861
                Loss: fixed 906 labels. Loss 0.06984. Accuracy 0.974.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42452818
Train loss (w/o reg) on all data: 0.4183037
Test loss (w/o reg) on all data: 0.24408822
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.5567115e-05
Norm of the params: 11.157489
              Random: fixed 288 labels. Loss 0.24409. Accuracy 0.977.
### Flips: 1230, rs: 36, checks: 1230
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23961914
Train loss (w/o reg) on all data: 0.22715643
Test loss (w/o reg) on all data: 0.1227308
Train acc on all data:  0.8959397033795283
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.485154e-06
Norm of the params: 15.787782
     Influence (LOO): fixed 652 labels. Loss 0.12273. Accuracy 0.993.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05246837
Train loss (w/o reg) on all data: 0.035453636
Test loss (w/o reg) on all data: 0.044427525
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.7430924e-06
Norm of the params: 18.447079
                Loss: fixed 971 labels. Loss 0.04443. Accuracy 0.983.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40956616
Train loss (w/o reg) on all data: 0.40325406
Test loss (w/o reg) on all data: 0.22584185
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.4753833e-05
Norm of the params: 11.235756
              Random: fixed 341 labels. Loss 0.22584. Accuracy 0.984.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4983947
Train loss (w/o reg) on all data: 0.4938593
Test loss (w/o reg) on all data: 0.32902798
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.0694764e-05
Norm of the params: 9.524074
Flipped loss: 0.32903. Accuracy: 0.941
### Flips: 1230, rs: 37, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4305344
Train loss (w/o reg) on all data: 0.42058128
Test loss (w/o reg) on all data: 0.27575234
Train acc on all data:  0.7916362752248967
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 6.261971e-05
Norm of the params: 14.10894
     Influence (LOO): fixed 161 labels. Loss 0.27575. Accuracy 0.958.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39975318
Train loss (w/o reg) on all data: 0.38801655
Test loss (w/o reg) on all data: 0.281313
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 5.6376928e-05
Norm of the params: 15.32099
                Loss: fixed 202 labels. Loss 0.28131. Accuracy 0.913.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48954225
Train loss (w/o reg) on all data: 0.48476723
Test loss (w/o reg) on all data: 0.31414497
Train acc on all data:  0.7661074641380987
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 3.3325712e-05
Norm of the params: 9.772434
              Random: fixed  42 labels. Loss 0.31414. Accuracy 0.953.
### Flips: 1230, rs: 37, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3829165
Train loss (w/o reg) on all data: 0.37138048
Test loss (w/o reg) on all data: 0.23345812
Train acc on all data:  0.8208120593240943
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.8429639e-05
Norm of the params: 15.189501
     Influence (LOO): fixed 287 labels. Loss 0.23346. Accuracy 0.970.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30597752
Train loss (w/o reg) on all data: 0.29012293
Test loss (w/o reg) on all data: 0.22940914
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 8.99647e-06
Norm of the params: 17.80707
                Loss: fixed 404 labels. Loss 0.22941. Accuracy 0.917.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47818083
Train loss (w/o reg) on all data: 0.47364974
Test loss (w/o reg) on all data: 0.2940376
Train acc on all data:  0.775589593970338
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.1303796e-05
Norm of the params: 9.51956
              Random: fixed  99 labels. Loss 0.29404. Accuracy 0.961.
### Flips: 1230, rs: 37, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33974978
Train loss (w/o reg) on all data: 0.32746464
Test loss (w/o reg) on all data: 0.19458841
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.0403086e-05
Norm of the params: 15.67492
     Influence (LOO): fixed 401 labels. Loss 0.19459. Accuracy 0.979.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21160507
Train loss (w/o reg) on all data: 0.19219504
Test loss (w/o reg) on all data: 0.1647182
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 6.632645e-06
Norm of the params: 19.702803
                Loss: fixed 605 labels. Loss 0.16472. Accuracy 0.943.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4677565
Train loss (w/o reg) on all data: 0.46334168
Test loss (w/o reg) on all data: 0.2755941
Train acc on all data:  0.7843423292000973
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9401108e-05
Norm of the params: 9.396623
              Random: fixed 149 labels. Loss 0.27559. Accuracy 0.966.
### Flips: 1230, rs: 37, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29909623
Train loss (w/o reg) on all data: 0.28688237
Test loss (w/o reg) on all data: 0.16740932
Train acc on all data:  0.8662776562120107
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.590358e-06
Norm of the params: 15.629361
     Influence (LOO): fixed 491 labels. Loss 0.16741. Accuracy 0.983.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12189251
Train loss (w/o reg) on all data: 0.10052915
Test loss (w/o reg) on all data: 0.1043218
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.468628e-06
Norm of the params: 20.67044
                Loss: fixed 795 labels. Loss 0.10432. Accuracy 0.958.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45407137
Train loss (w/o reg) on all data: 0.44965148
Test loss (w/o reg) on all data: 0.2592704
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.644455e-05
Norm of the params: 9.402007
              Random: fixed 200 labels. Loss 0.25927. Accuracy 0.971.
### Flips: 1230, rs: 37, checks: 1025
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2635212
Train loss (w/o reg) on all data: 0.25183803
Test loss (w/o reg) on all data: 0.13998163
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1449268e-05
Norm of the params: 15.286049
     Influence (LOO): fixed 577 labels. Loss 0.13998. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05863171
Train loss (w/o reg) on all data: 0.042140592
Test loss (w/o reg) on all data: 0.037272133
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6731358e-06
Norm of the params: 18.16101
                Loss: fixed 940 labels. Loss 0.03727. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43987277
Train loss (w/o reg) on all data: 0.43538737
Test loss (w/o reg) on all data: 0.24380527
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.0464754e-05
Norm of the params: 9.47143
              Random: fixed 250 labels. Loss 0.24381. Accuracy 0.979.
### Flips: 1230, rs: 37, checks: 1230
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2260722
Train loss (w/o reg) on all data: 0.2143842
Test loss (w/o reg) on all data: 0.11948292
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.095088e-06
Norm of the params: 15.289216
     Influence (LOO): fixed 649 labels. Loss 0.11948. Accuracy 0.991.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035784997
Train loss (w/o reg) on all data: 0.023035983
Test loss (w/o reg) on all data: 0.020292846
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.242003e-07
Norm of the params: 15.968102
                Loss: fixed 996 labels. Loss 0.02029. Accuracy 0.994.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42647234
Train loss (w/o reg) on all data: 0.42199525
Test loss (w/o reg) on all data: 0.22600108
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.9567236e-05
Norm of the params: 9.462647
              Random: fixed 298 labels. Loss 0.22600. Accuracy 0.980.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.496441
Train loss (w/o reg) on all data: 0.49121258
Test loss (w/o reg) on all data: 0.33280525
Train acc on all data:  0.75127644055434
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.7631848e-05
Norm of the params: 10.225868
Flipped loss: 0.33281. Accuracy: 0.934
### Flips: 1230, rs: 38, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43348774
Train loss (w/o reg) on all data: 0.42409012
Test loss (w/o reg) on all data: 0.27896887
Train acc on all data:  0.7904206175540968
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.2923297e-05
Norm of the params: 13.709585
     Influence (LOO): fixed 163 labels. Loss 0.27897. Accuracy 0.953.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39707905
Train loss (w/o reg) on all data: 0.38522157
Test loss (w/o reg) on all data: 0.28097427
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 2.6224388e-05
Norm of the params: 15.399658
                Loss: fixed 203 labels. Loss 0.28097. Accuracy 0.912.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48681074
Train loss (w/o reg) on all data: 0.4815133
Test loss (w/o reg) on all data: 0.3185777
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.0734224e-05
Norm of the params: 10.293157
              Random: fixed  50 labels. Loss 0.31858. Accuracy 0.943.
### Flips: 1230, rs: 38, checks: 410
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3846953
Train loss (w/o reg) on all data: 0.3743868
Test loss (w/o reg) on all data: 0.23165525
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.1782524e-05
Norm of the params: 14.35862
     Influence (LOO): fixed 293 labels. Loss 0.23166. Accuracy 0.961.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3068385
Train loss (w/o reg) on all data: 0.29080486
Test loss (w/o reg) on all data: 0.22758958
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 9.625435e-06
Norm of the params: 17.907345
                Loss: fixed 406 labels. Loss 0.22759. Accuracy 0.914.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4755021
Train loss (w/o reg) on all data: 0.47023755
Test loss (w/o reg) on all data: 0.30316082
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.9954407e-05
Norm of the params: 10.261135
              Random: fixed  99 labels. Loss 0.30316. Accuracy 0.953.
### Flips: 1230, rs: 38, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34131333
Train loss (w/o reg) on all data: 0.33006105
Test loss (w/o reg) on all data: 0.19888595
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.20427885e-05
Norm of the params: 15.001517
     Influence (LOO): fixed 398 labels. Loss 0.19889. Accuracy 0.964.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21488318
Train loss (w/o reg) on all data: 0.19502029
Test loss (w/o reg) on all data: 0.1656057
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 8.424822e-06
Norm of the params: 19.931324
                Loss: fixed 608 labels. Loss 0.16561. Accuracy 0.939.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46370867
Train loss (w/o reg) on all data: 0.45835736
Test loss (w/o reg) on all data: 0.28453392
Train acc on all data:  0.7850717238025772
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.3302766e-05
Norm of the params: 10.345352
              Random: fixed 147 labels. Loss 0.28453. Accuracy 0.955.
### Flips: 1230, rs: 38, checks: 820
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30039802
Train loss (w/o reg) on all data: 0.2888262
Test loss (w/o reg) on all data: 0.17281231
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.1831242e-05
Norm of the params: 15.213042
     Influence (LOO): fixed 497 labels. Loss 0.17281. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12904389
Train loss (w/o reg) on all data: 0.10732734
Test loss (w/o reg) on all data: 0.118251145
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.0334132e-05
Norm of the params: 20.840609
                Loss: fixed 796 labels. Loss 0.11825. Accuracy 0.956.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4510054
Train loss (w/o reg) on all data: 0.44567624
Test loss (w/o reg) on all data: 0.26804253
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.149243e-05
Norm of the params: 10.32391
              Random: fixed 199 labels. Loss 0.26804. Accuracy 0.970.
### Flips: 1230, rs: 38, checks: 1025
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25603753
Train loss (w/o reg) on all data: 0.24348696
Test loss (w/o reg) on all data: 0.14580086
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5983916e-05
Norm of the params: 15.843345
     Influence (LOO): fixed 589 labels. Loss 0.14580. Accuracy 0.976.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07366288
Train loss (w/o reg) on all data: 0.055430334
Test loss (w/o reg) on all data: 0.06277737
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.6206396e-06
Norm of the params: 19.095835
                Loss: fixed 929 labels. Loss 0.06278. Accuracy 0.982.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4386274
Train loss (w/o reg) on all data: 0.43325466
Test loss (w/o reg) on all data: 0.25047922
Train acc on all data:  0.8088986141502553
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.4923483e-05
Norm of the params: 10.366025
              Random: fixed 249 labels. Loss 0.25048. Accuracy 0.971.
### Flips: 1230, rs: 38, checks: 1230
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23068735
Train loss (w/o reg) on all data: 0.21843316
Test loss (w/o reg) on all data: 0.12775181
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.9132714e-06
Norm of the params: 15.655153
     Influence (LOO): fixed 651 labels. Loss 0.12775. Accuracy 0.983.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043865073
Train loss (w/o reg) on all data: 0.029698633
Test loss (w/o reg) on all data: 0.025150634
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0415664e-06
Norm of the params: 16.832375
                Loss: fixed 1000 labels. Loss 0.02515. Accuracy 0.993.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42670226
Train loss (w/o reg) on all data: 0.4213241
Test loss (w/o reg) on all data: 0.23395318
Train acc on all data:  0.8203257962557744
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.6158293e-05
Norm of the params: 10.371265
              Random: fixed 296 labels. Loss 0.23395. Accuracy 0.977.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49135104
Train loss (w/o reg) on all data: 0.48592773
Test loss (w/o reg) on all data: 0.32554746
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.3872873e-05
Norm of the params: 10.414711
Flipped loss: 0.32555. Accuracy: 0.944
### Flips: 1230, rs: 39, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4311805
Train loss (w/o reg) on all data: 0.42118245
Test loss (w/o reg) on all data: 0.2678537
Train acc on all data:  0.7923656698273767
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 1.4521683e-05
Norm of the params: 14.14075
     Influence (LOO): fixed 155 labels. Loss 0.26785. Accuracy 0.951.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39030755
Train loss (w/o reg) on all data: 0.3780982
Test loss (w/o reg) on all data: 0.27138308
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 1.0227705e-05
Norm of the params: 15.62648
                Loss: fixed 205 labels. Loss 0.27138. Accuracy 0.921.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48011944
Train loss (w/o reg) on all data: 0.4747212
Test loss (w/o reg) on all data: 0.3079663
Train acc on all data:  0.7680525164113785
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 2.8680024e-05
Norm of the params: 10.390627
              Random: fixed  50 labels. Loss 0.30797. Accuracy 0.950.
### Flips: 1230, rs: 39, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38141647
Train loss (w/o reg) on all data: 0.3704089
Test loss (w/o reg) on all data: 0.22189157
Train acc on all data:  0.8200826647216144
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.57155e-05
Norm of the params: 14.837508
     Influence (LOO): fixed 291 labels. Loss 0.22189. Accuracy 0.966.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3003719
Train loss (w/o reg) on all data: 0.28416187
Test loss (w/o reg) on all data: 0.22160406
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 8.379068e-06
Norm of the params: 18.005568
                Loss: fixed 406 labels. Loss 0.22160. Accuracy 0.927.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46793088
Train loss (w/o reg) on all data: 0.46249118
Test loss (w/o reg) on all data: 0.29136768
Train acc on all data:  0.7799659615852176
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.432849e-05
Norm of the params: 10.430425
              Random: fixed 105 labels. Loss 0.29137. Accuracy 0.958.
### Flips: 1230, rs: 39, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34099638
Train loss (w/o reg) on all data: 0.3285815
Test loss (w/o reg) on all data: 0.18862765
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.5144349e-05
Norm of the params: 15.757461
     Influence (LOO): fixed 392 labels. Loss 0.18863. Accuracy 0.973.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20755178
Train loss (w/o reg) on all data: 0.18778929
Test loss (w/o reg) on all data: 0.16102996
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 4.0691666e-05
Norm of the params: 19.880886
                Loss: fixed 608 labels. Loss 0.16103. Accuracy 0.946.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45869386
Train loss (w/o reg) on all data: 0.45303497
Test loss (w/o reg) on all data: 0.27958742
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.1232976e-05
Norm of the params: 10.6385
              Random: fixed 144 labels. Loss 0.27959. Accuracy 0.958.
### Flips: 1230, rs: 39, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29593137
Train loss (w/o reg) on all data: 0.28281066
Test loss (w/o reg) on all data: 0.16170003
Train acc on all data:  0.8684658400194505
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 5.4714284e-05
Norm of the params: 16.19921
     Influence (LOO): fixed 493 labels. Loss 0.16170. Accuracy 0.974.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11808233
Train loss (w/o reg) on all data: 0.09803259
Test loss (w/o reg) on all data: 0.09467609
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.5604382e-06
Norm of the params: 20.024858
                Loss: fixed 797 labels. Loss 0.09468. Accuracy 0.966.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4446972
Train loss (w/o reg) on all data: 0.43916517
Test loss (w/o reg) on all data: 0.25813603
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.7767443e-05
Norm of the params: 10.518566
              Random: fixed 204 labels. Loss 0.25814. Accuracy 0.969.
### Flips: 1230, rs: 39, checks: 1025
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2593936
Train loss (w/o reg) on all data: 0.24695668
Test loss (w/o reg) on all data: 0.13803475
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.036121e-06
Norm of the params: 15.771449
     Influence (LOO): fixed 577 labels. Loss 0.13803. Accuracy 0.984.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057989314
Train loss (w/o reg) on all data: 0.0420788
Test loss (w/o reg) on all data: 0.049998485
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.040412e-06
Norm of the params: 17.83845
                Loss: fixed 931 labels. Loss 0.05000. Accuracy 0.983.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42604384
Train loss (w/o reg) on all data: 0.42029428
Test loss (w/o reg) on all data: 0.2359904
Train acc on all data:  0.8164356917092147
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.3691332e-05
Norm of the params: 10.723376
              Random: fixed 267 labels. Loss 0.23599. Accuracy 0.974.
### Flips: 1230, rs: 39, checks: 1230
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23308335
Train loss (w/o reg) on all data: 0.22077598
Test loss (w/o reg) on all data: 0.11869938
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.370483e-05
Norm of the params: 15.689087
     Influence (LOO): fixed 642 labels. Loss 0.11870. Accuracy 0.991.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037686154
Train loss (w/o reg) on all data: 0.024281492
Test loss (w/o reg) on all data: 0.029780503
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0200449e-06
Norm of the params: 16.373554
                Loss: fixed 977 labels. Loss 0.02978. Accuracy 0.991.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40707776
Train loss (w/o reg) on all data: 0.40131584
Test loss (w/o reg) on all data: 0.21438533
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.663689e-05
Norm of the params: 10.734905
              Random: fixed 332 labels. Loss 0.21439. Accuracy 0.980.
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-15 12:18:32.506021: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-15 12:18:32.754918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 10.44GiB
2018-02-15 12:18:32.754987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[<1x121 sparse matrix of type '<type 'numpy.int64'>'
	with 10 stored elements in Compressed Sparse Row format>]
Total number of parameters: 121
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01469375e-07
Norm of the params: 9.153184
Orig loss: 0.01205. Accuracy: 0.992
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087252215
Train loss (w/o reg) on all data: 0.080029026
Test loss (w/o reg) on all data: 0.047921844
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.528693e-06
Norm of the params: 12.019307
Flipped loss: 0.04792. Accuracy: 0.992
### Flips: 52, rs: 0, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728417
Test loss (w/o reg) on all data: 0.012054368
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6297749e-06
Norm of the params: 9.153334
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728498
Test loss (w/o reg) on all data: 0.012054711
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7731013e-07
Norm of the params: 9.153326
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079653315
Train loss (w/o reg) on all data: 0.07249693
Test loss (w/o reg) on all data: 0.049043953
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7833265e-05
Norm of the params: 11.963601
              Random: fixed   2 labels. Loss 0.04904. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729316
Test loss (w/o reg) on all data: 0.012055758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6448168e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172933
Test loss (w/o reg) on all data: 0.012055802
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1205841e-07
Norm of the params: 9.153234
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07139545
Train loss (w/o reg) on all data: 0.06340553
Test loss (w/o reg) on all data: 0.04379207
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1879038e-05
Norm of the params: 12.641138
              Random: fixed   4 labels. Loss 0.04379. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021732491
Test loss (w/o reg) on all data: 0.012054615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9879626e-07
Norm of the params: 9.15289
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173244
Test loss (w/o reg) on all data: 0.012054807
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5940914e-07
Norm of the params: 9.152893
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06725767
Train loss (w/o reg) on all data: 0.058855135
Test loss (w/o reg) on all data: 0.041062485
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2284856e-05
Norm of the params: 12.963441
              Random: fixed   5 labels. Loss 0.04106. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.012055401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.412124e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012055513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2667893e-07
Norm of the params: 9.153163
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059911057
Train loss (w/o reg) on all data: 0.051428974
Test loss (w/o reg) on all data: 0.039166737
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3440341e-05
Norm of the params: 13.024658
              Random: fixed   8 labels. Loss 0.03917. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172825
Test loss (w/o reg) on all data: 0.012056464
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9265976e-07
Norm of the params: 9.153352
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728277
Test loss (w/o reg) on all data: 0.012056326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3285116e-07
Norm of the params: 9.153349
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059911054
Train loss (w/o reg) on all data: 0.051429406
Test loss (w/o reg) on all data: 0.03916658
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0874546e-06
Norm of the params: 13.024322
              Random: fixed   8 labels. Loss 0.03917. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8849947e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.0120555805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.356728e-07
Norm of the params: 9.153222
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054738425
Train loss (w/o reg) on all data: 0.046066698
Test loss (w/o reg) on all data: 0.03640074
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.23239e-06
Norm of the params: 13.169456
              Random: fixed  10 labels. Loss 0.03640. Accuracy 0.989.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07200118
Train loss (w/o reg) on all data: 0.06079712
Test loss (w/o reg) on all data: 0.047040217
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0776419e-05
Norm of the params: 14.969343
Flipped loss: 0.04704. Accuracy: 0.989
### Flips: 52, rs: 1, checks: 52
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216441
Train loss (w/o reg) on all data: 0.0055171447
Test loss (w/o reg) on all data: 0.012118511
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.671773e-07
Norm of the params: 9.6946335
     Influence (LOO): fixed  25 labels. Loss 0.01212. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.909519e-07
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07196041
Train loss (w/o reg) on all data: 0.06086532
Test loss (w/o reg) on all data: 0.047954172
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.362572e-06
Norm of the params: 14.896371
              Random: fixed   1 labels. Loss 0.04795. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055133
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5158254e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012055097
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2738839e-07
Norm of the params: 9.153162
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070276104
Train loss (w/o reg) on all data: 0.05913779
Test loss (w/o reg) on all data: 0.04740826
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4681376e-05
Norm of the params: 14.925356
              Random: fixed   2 labels. Loss 0.04741. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055837
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3578447e-07
Norm of the params: 9.153164
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.012055732
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7351856e-07
Norm of the params: 9.153164
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070276104
Train loss (w/o reg) on all data: 0.059138462
Test loss (w/o reg) on all data: 0.047419365
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9809496e-06
Norm of the params: 14.924907
              Random: fixed   2 labels. Loss 0.04742. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729125
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2409884e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172914
Test loss (w/o reg) on all data: 0.012055019
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3297036e-07
Norm of the params: 9.153256
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069444835
Train loss (w/o reg) on all data: 0.058209613
Test loss (w/o reg) on all data: 0.045141716
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9199864e-06
Norm of the params: 14.990147
              Random: fixed   3 labels. Loss 0.04514. Accuracy 0.992.
### Flips: 52, rs: 1, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7301768e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729672
Test loss (w/o reg) on all data: 0.012055086
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4093607e-07
Norm of the params: 9.153198
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0653918
Train loss (w/o reg) on all data: 0.054970503
Test loss (w/o reg) on all data: 0.044738196
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.666925e-06
Norm of the params: 14.436966
              Random: fixed   5 labels. Loss 0.04474. Accuracy 0.992.
### Flips: 52, rs: 1, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0513572e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012055259
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2468634e-07
Norm of the params: 9.153177
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060915574
Train loss (w/o reg) on all data: 0.05034953
Test loss (w/o reg) on all data: 0.042064168
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.8871375e-06
Norm of the params: 14.536881
              Random: fixed   6 labels. Loss 0.04206. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08262793
Train loss (w/o reg) on all data: 0.07450222
Test loss (w/o reg) on all data: 0.054654576
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.8964674e-06
Norm of the params: 12.748106
Flipped loss: 0.05465. Accuracy: 0.985
### Flips: 52, rs: 2, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730755
Test loss (w/o reg) on all data: 0.012054577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5419747e-07
Norm of the params: 9.153081
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730731
Test loss (w/o reg) on all data: 0.012054703
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.359184e-07
Norm of the params: 9.153082
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07762253
Train loss (w/o reg) on all data: 0.06910072
Test loss (w/o reg) on all data: 0.054545943
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.084903e-06
Norm of the params: 13.055123
              Random: fixed   1 labels. Loss 0.05455. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729951
Test loss (w/o reg) on all data: 0.012054957
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1063154e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729947
Test loss (w/o reg) on all data: 0.01205501
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3447041e-07
Norm of the params: 9.153168
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07762253
Train loss (w/o reg) on all data: 0.06910422
Test loss (w/o reg) on all data: 0.054532386
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.50812e-06
Norm of the params: 13.052444
              Random: fixed   1 labels. Loss 0.05453. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729127
Test loss (w/o reg) on all data: 0.01205571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7696559e-07
Norm of the params: 9.153258
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729176
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.574288e-07
Norm of the params: 9.153251
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07645432
Train loss (w/o reg) on all data: 0.068271324
Test loss (w/o reg) on all data: 0.05432995
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.8772005e-06
Norm of the params: 12.792962
              Random: fixed   2 labels. Loss 0.05433. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055606
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0350107e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.01205533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2073583e-07
Norm of the params: 9.153184
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730586
Train loss (w/o reg) on all data: 0.06505861
Test loss (w/o reg) on all data: 0.055133242
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.189196e-06
Norm of the params: 12.649098
              Random: fixed   3 labels. Loss 0.05513. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172985
Test loss (w/o reg) on all data: 0.012055097
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8700154e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729837
Test loss (w/o reg) on all data: 0.012055178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3180884e-07
Norm of the params: 9.153177
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062552944
Train loss (w/o reg) on all data: 0.054973707
Test loss (w/o reg) on all data: 0.048362937
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.2035863e-06
Norm of the params: 12.311974
              Random: fixed   7 labels. Loss 0.04836. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730398
Test loss (w/o reg) on all data: 0.012055213
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1463289e-06
Norm of the params: 9.153117
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173039
Test loss (w/o reg) on all data: 0.012055002
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2774442e-07
Norm of the params: 9.153118
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056279175
Train loss (w/o reg) on all data: 0.04821194
Test loss (w/o reg) on all data: 0.04006632
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.7664296e-06
Norm of the params: 12.702152
              Random: fixed   9 labels. Loss 0.04007. Accuracy 0.985.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08899167
Train loss (w/o reg) on all data: 0.07971659
Test loss (w/o reg) on all data: 0.038062196
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7982202e-06
Norm of the params: 13.619898
Flipped loss: 0.03806. Accuracy: 0.992
### Flips: 52, rs: 3, checks: 52
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728934
Test loss (w/o reg) on all data: 0.012055533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6244815e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2539567e-07
Norm of the params: 9.153262
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087101765
Train loss (w/o reg) on all data: 0.0780043
Test loss (w/o reg) on all data: 0.037810087
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0426564e-05
Norm of the params: 13.48886
              Random: fixed   1 labels. Loss 0.03781. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 104
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730382
Test loss (w/o reg) on all data: 0.012056387
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1862661e-06
Norm of the params: 9.153117
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730405
Test loss (w/o reg) on all data: 0.01205659
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6430623e-07
Norm of the params: 9.153118
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07818106
Train loss (w/o reg) on all data: 0.06931923
Test loss (w/o reg) on all data: 0.03525604
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3351124e-06
Norm of the params: 13.313024
              Random: fixed   4 labels. Loss 0.03526. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172913
Test loss (w/o reg) on all data: 0.012056338
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3531867e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012056428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4326e-07
Norm of the params: 9.153255
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07080484
Train loss (w/o reg) on all data: 0.06264634
Test loss (w/o reg) on all data: 0.030163601
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8340087e-06
Norm of the params: 12.773803
              Random: fixed   7 labels. Loss 0.03016. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.0120548075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0679587e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.012054885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.161239e-07
Norm of the params: 9.153224
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067986205
Train loss (w/o reg) on all data: 0.059627034
Test loss (w/o reg) on all data: 0.028439809
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3100262e-05
Norm of the params: 12.92994
              Random: fixed   8 labels. Loss 0.02844. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728438
Test loss (w/o reg) on all data: 0.012055403
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6596206e-07
Norm of the params: 9.153332
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728459
Test loss (w/o reg) on all data: 0.0120555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9217074e-07
Norm of the params: 9.153329
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06408659
Train loss (w/o reg) on all data: 0.055926003
Test loss (w/o reg) on all data: 0.03013481
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.45443e-06
Norm of the params: 12.775435
              Random: fixed   9 labels. Loss 0.03013. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729863
Test loss (w/o reg) on all data: 0.01205589
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0810913e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055812
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8468517e-07
Norm of the params: 9.153181
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063106924
Train loss (w/o reg) on all data: 0.055025235
Test loss (w/o reg) on all data: 0.028751351
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7912117e-06
Norm of the params: 12.713529
              Random: fixed  10 labels. Loss 0.02875. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07887534
Train loss (w/o reg) on all data: 0.07000621
Test loss (w/o reg) on all data: 0.026994174
Train acc on all data:  0.9789875835721108
Test acc on all data:   1.0
Norm of the mean of gradients: 1.627275e-05
Norm of the params: 13.318508
Flipped loss: 0.02699. Accuracy: 1.000
### Flips: 52, rs: 4, checks: 52
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729928
Test loss (w/o reg) on all data: 0.012056063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2627335e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.012056122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1410717e-07
Norm of the params: 9.153171
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07646251
Train loss (w/o reg) on all data: 0.067683294
Test loss (w/o reg) on all data: 0.025848698
Train acc on all data:  0.9799426934097422
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6064232e-05
Norm of the params: 13.250818
              Random: fixed   1 labels. Loss 0.02585. Accuracy 1.000.
### Flips: 52, rs: 4, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730207
Test loss (w/o reg) on all data: 0.012056251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4868467e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730186
Test loss (w/o reg) on all data: 0.012056364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2939655e-07
Norm of the params: 9.153141
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072589695
Train loss (w/o reg) on all data: 0.063174605
Test loss (w/o reg) on all data: 0.02843931
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9467145e-06
Norm of the params: 13.722312
              Random: fixed   2 labels. Loss 0.02844. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172897
Test loss (w/o reg) on all data: 0.012056189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.230183e-07
Norm of the params: 9.153275
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728978
Test loss (w/o reg) on all data: 0.012056101
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8847848e-07
Norm of the params: 9.153275
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0620549
Train loss (w/o reg) on all data: 0.05206943
Test loss (w/o reg) on all data: 0.024630355
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9782804e-06
Norm of the params: 14.131857
              Random: fixed   6 labels. Loss 0.02463. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730298
Test loss (w/o reg) on all data: 0.012055424
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5118513e-07
Norm of the params: 9.153129
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730203
Test loss (w/o reg) on all data: 0.012055295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.096791e-07
Norm of the params: 9.153142
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062054902
Train loss (w/o reg) on all data: 0.05206907
Test loss (w/o reg) on all data: 0.024628451
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.796274e-06
Norm of the params: 14.132113
              Random: fixed   6 labels. Loss 0.02463. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729257
Test loss (w/o reg) on all data: 0.012056011
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5627212e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729264
Test loss (w/o reg) on all data: 0.012056151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.19468e-07
Norm of the params: 9.153244
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056213032
Train loss (w/o reg) on all data: 0.04615946
Test loss (w/o reg) on all data: 0.02671504
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.959415e-06
Norm of the params: 14.179967
              Random: fixed   7 labels. Loss 0.02672. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.012056143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.107466e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012056213
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2585574e-07
Norm of the params: 9.153182
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047233
Train loss (w/o reg) on all data: 0.036704034
Test loss (w/o reg) on all data: 0.025331102
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.253397e-06
Norm of the params: 14.5113535
              Random: fixed   9 labels. Loss 0.02533. Accuracy 0.996.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08950253
Train loss (w/o reg) on all data: 0.079122566
Test loss (w/o reg) on all data: 0.03730898
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6208101e-05
Norm of the params: 14.408308
Flipped loss: 0.03731. Accuracy: 0.992
### Flips: 52, rs: 5, checks: 52
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510402
Train loss (w/o reg) on all data: 0.0043741814
Test loss (w/o reg) on all data: 0.011669545
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4811706e-07
Norm of the params: 10.135305
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510403
Train loss (w/o reg) on all data: 0.0043741814
Test loss (w/o reg) on all data: 0.0116695
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7427592e-07
Norm of the params: 10.135306
                Loss: fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08572606
Train loss (w/o reg) on all data: 0.07526135
Test loss (w/o reg) on all data: 0.036611218
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5232365e-06
Norm of the params: 14.467007
              Random: fixed   1 labels. Loss 0.03661. Accuracy 0.996.
### Flips: 52, rs: 5, checks: 104
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0095104035
Train loss (w/o reg) on all data: 0.004374116
Test loss (w/o reg) on all data: 0.011669711
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.61578e-07
Norm of the params: 10.13537
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012055103
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4045497e-07
Norm of the params: 9.153208
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07879514
Train loss (w/o reg) on all data: 0.068317935
Test loss (w/o reg) on all data: 0.032090284
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.5211513e-06
Norm of the params: 14.475639
              Random: fixed   4 labels. Loss 0.03209. Accuracy 0.996.
### Flips: 52, rs: 5, checks: 156
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510399
Train loss (w/o reg) on all data: 0.0043741344
Test loss (w/o reg) on all data: 0.01166936
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.283407e-07
Norm of the params: 10.135348
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173058
Test loss (w/o reg) on all data: 0.012055946
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7379723e-07
Norm of the params: 9.153097
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07101981
Train loss (w/o reg) on all data: 0.059840742
Test loss (w/o reg) on all data: 0.030630428
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.090819e-06
Norm of the params: 14.952637
              Random: fixed   6 labels. Loss 0.03063. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729425
Test loss (w/o reg) on all data: 0.012054602
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.651338e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012054747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3067378e-07
Norm of the params: 9.153225
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071019806
Train loss (w/o reg) on all data: 0.059840634
Test loss (w/o reg) on all data: 0.030626748
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6430446e-06
Norm of the params: 14.952707
              Random: fixed   6 labels. Loss 0.03063. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012054875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2345502e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012054773
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.210316e-07
Norm of the params: 9.153191
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067344375
Train loss (w/o reg) on all data: 0.056384034
Test loss (w/o reg) on all data: 0.027847271
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0426867e-06
Norm of the params: 14.805636
              Random: fixed   7 labels. Loss 0.02785. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730785
Test loss (w/o reg) on all data: 0.012055538
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1768744e-07
Norm of the params: 9.153077
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730715
Test loss (w/o reg) on all data: 0.0120556
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.837892e-07
Norm of the params: 9.153084
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06734438
Train loss (w/o reg) on all data: 0.056383595
Test loss (w/o reg) on all data: 0.02785189
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9168234e-06
Norm of the params: 14.805934
              Random: fixed   7 labels. Loss 0.02785. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07236478
Train loss (w/o reg) on all data: 0.06405481
Test loss (w/o reg) on all data: 0.031583335
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.0895677e-06
Norm of the params: 12.891831
Flipped loss: 0.03158. Accuracy: 0.996
### Flips: 52, rs: 6, checks: 52
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728443
Test loss (w/o reg) on all data: 0.0120559605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.720773e-07
Norm of the params: 9.153333
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172844
Test loss (w/o reg) on all data: 0.012056087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.456772e-07
Norm of the params: 9.15333
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07236478
Train loss (w/o reg) on all data: 0.06405485
Test loss (w/o reg) on all data: 0.03158427
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.652904e-06
Norm of the params: 12.891807
              Random: fixed   0 labels. Loss 0.03158. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012055749
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9814416e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.0120560685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5127748e-07
Norm of the params: 9.153192
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002458
Train loss (w/o reg) on all data: 0.061823577
Test loss (w/o reg) on all data: 0.029488236
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2600066e-05
Norm of the params: 12.80703
              Random: fixed   1 labels. Loss 0.02949. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730042
Test loss (w/o reg) on all data: 0.012056123
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.096316e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173005
Test loss (w/o reg) on all data: 0.012056023
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3343904e-07
Norm of the params: 9.153157
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002458
Train loss (w/o reg) on all data: 0.061824385
Test loss (w/o reg) on all data: 0.02948261
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.783706e-06
Norm of the params: 12.8063965
              Random: fixed   1 labels. Loss 0.02948. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055648
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.660725e-07
Norm of the params: 9.153192
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012055765
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.346467e-07
Norm of the params: 9.1531925
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002458
Train loss (w/o reg) on all data: 0.061825596
Test loss (w/o reg) on all data: 0.029485375
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2941057e-06
Norm of the params: 12.805452
              Random: fixed   1 labels. Loss 0.02949. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730382
Test loss (w/o reg) on all data: 0.01205521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.890039e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173038
Test loss (w/o reg) on all data: 0.012055333
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1386371e-07
Norm of the params: 9.153119
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519724
Train loss (w/o reg) on all data: 0.056905024
Test loss (w/o reg) on all data: 0.027820604
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.121113e-05
Norm of the params: 12.878052
              Random: fixed   2 labels. Loss 0.02782. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172866
Test loss (w/o reg) on all data: 0.012055293
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.752248e-07
Norm of the params: 9.153307
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012055336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1544925e-07
Norm of the params: 9.15327
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056067273
Train loss (w/o reg) on all data: 0.04834568
Test loss (w/o reg) on all data: 0.024240976
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8384259e-06
Norm of the params: 12.427061
              Random: fixed   5 labels. Loss 0.02424. Accuracy 0.996.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063624606
Train loss (w/o reg) on all data: 0.053844403
Test loss (w/o reg) on all data: 0.044550397
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.695972e-06
Norm of the params: 13.985851
Flipped loss: 0.04455. Accuracy: 0.985
### Flips: 52, rs: 7, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729297
Test loss (w/o reg) on all data: 0.012055298
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6305258e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012055374
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2590516e-07
Norm of the params: 9.153239
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06059411
Train loss (w/o reg) on all data: 0.050400186
Test loss (w/o reg) on all data: 0.043962486
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4786675e-06
Norm of the params: 14.278604
              Random: fixed   1 labels. Loss 0.04396. Accuracy 0.981.
### Flips: 52, rs: 7, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055213
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3777424e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729325
Test loss (w/o reg) on all data: 0.012055259
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2396826e-07
Norm of the params: 9.153236
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060594123
Train loss (w/o reg) on all data: 0.050399225
Test loss (w/o reg) on all data: 0.043968234
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.0250586e-06
Norm of the params: 14.279284
              Random: fixed   1 labels. Loss 0.04397. Accuracy 0.981.
### Flips: 52, rs: 7, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728752
Test loss (w/o reg) on all data: 0.012054836
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3554715e-07
Norm of the params: 9.153299
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728748
Test loss (w/o reg) on all data: 0.01205477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7894159e-07
Norm of the params: 9.153297
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06006226
Train loss (w/o reg) on all data: 0.05053905
Test loss (w/o reg) on all data: 0.037500616
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8867415e-06
Norm of the params: 13.800877
              Random: fixed   2 labels. Loss 0.03750. Accuracy 0.992.
### Flips: 52, rs: 7, checks: 208
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729139
Test loss (w/o reg) on all data: 0.01205611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0238357e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729143
Test loss (w/o reg) on all data: 0.012055912
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3780358e-07
Norm of the params: 9.153256
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04868926
Train loss (w/o reg) on all data: 0.039067417
Test loss (w/o reg) on all data: 0.031748343
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1246346e-06
Norm of the params: 13.872163
              Random: fixed   6 labels. Loss 0.03175. Accuracy 0.992.
### Flips: 52, rs: 7, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730424
Test loss (w/o reg) on all data: 0.012054959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4755627e-07
Norm of the params: 9.153113
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173044
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0729535e-07
Norm of the params: 9.153114
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0464482
Train loss (w/o reg) on all data: 0.036413833
Test loss (w/o reg) on all data: 0.03358934
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1634166e-06
Norm of the params: 14.166417
              Random: fixed   7 labels. Loss 0.03359. Accuracy 0.989.
### Flips: 52, rs: 7, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729877
Test loss (w/o reg) on all data: 0.012056476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6887017e-07
Norm of the params: 9.153175
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729867
Test loss (w/o reg) on all data: 0.012056313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2042547e-07
Norm of the params: 9.153176
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040759735
Train loss (w/o reg) on all data: 0.030121425
Test loss (w/o reg) on all data: 0.037064735
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2645091e-06
Norm of the params: 14.586507
              Random: fixed   8 labels. Loss 0.03706. Accuracy 0.981.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937619
Train loss (w/o reg) on all data: 0.08261063
Test loss (w/o reg) on all data: 0.062839046
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.254029e-06
Norm of the params: 14.934035
Flipped loss: 0.06284. Accuracy: 0.985
### Flips: 52, rs: 8, checks: 52
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02007905
Train loss (w/o reg) on all data: 0.013315663
Test loss (w/o reg) on all data: 0.014227046
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5650502e-06
Norm of the params: 11.6304655
     Influence (LOO): fixed  29 labels. Loss 0.01423. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345254
Train loss (w/o reg) on all data: 0.0031440926
Test loss (w/o reg) on all data: 0.011324606
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.126535e-07
Norm of the params: 10.199178
                Loss: fixed  33 labels. Loss 0.01132. Accuracy 0.996.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08875705
Train loss (w/o reg) on all data: 0.07749565
Test loss (w/o reg) on all data: 0.060326826
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.588428e-06
Norm of the params: 15.007602
              Random: fixed   2 labels. Loss 0.06033. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 104
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345253
Train loss (w/o reg) on all data: 0.003144124
Test loss (w/o reg) on all data: 0.011325727
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.052529e-07
Norm of the params: 10.199146
     Influence (LOO): fixed  33 labels. Loss 0.01133. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012055135
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1192605e-07
Norm of the params: 9.153198
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08617993
Train loss (w/o reg) on all data: 0.074891746
Test loss (w/o reg) on all data: 0.060036004
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7385217e-05
Norm of the params: 15.025435
              Random: fixed   3 labels. Loss 0.06004. Accuracy 0.985.
### Flips: 52, rs: 8, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730391
Test loss (w/o reg) on all data: 0.012055272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.989414e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730382
Test loss (w/o reg) on all data: 0.01205522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5321893e-07
Norm of the params: 9.153121
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08375781
Train loss (w/o reg) on all data: 0.0726888
Test loss (w/o reg) on all data: 0.058638964
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.012219e-06
Norm of the params: 14.878849
              Random: fixed   5 labels. Loss 0.05864. Accuracy 0.985.
### Flips: 52, rs: 8, checks: 208
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012055339
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5288716e-07
Norm of the params: 9.153238
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012055266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2359872e-07
Norm of the params: 9.153237
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074274585
Train loss (w/o reg) on all data: 0.063821524
Test loss (w/o reg) on all data: 0.05998249
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.564565e-06
Norm of the params: 14.458952
              Random: fixed   8 labels. Loss 0.05998. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729134
Test loss (w/o reg) on all data: 0.012055094
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2156251e-07
Norm of the params: 9.153256
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172916
Test loss (w/o reg) on all data: 0.012055123
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2884047e-07
Norm of the params: 9.153255
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427459
Train loss (w/o reg) on all data: 0.063820764
Test loss (w/o reg) on all data: 0.059984528
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0891348e-05
Norm of the params: 14.459479
              Random: fixed   8 labels. Loss 0.05998. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012056395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.915125e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012056318
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.84299e-07
Norm of the params: 9.153227
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069326706
Train loss (w/o reg) on all data: 0.05929284
Test loss (w/o reg) on all data: 0.054077994
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0452968e-05
Norm of the params: 14.1660595
              Random: fixed  10 labels. Loss 0.05408. Accuracy 0.989.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05737938
Train loss (w/o reg) on all data: 0.048711818
Test loss (w/o reg) on all data: 0.0405794
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.678957e-06
Norm of the params: 13.16629
Flipped loss: 0.04058. Accuracy: 0.989
### Flips: 52, rs: 9, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012054962
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0110141e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729956
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3479298e-07
Norm of the params: 9.153168
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057379376
Train loss (w/o reg) on all data: 0.04871321
Test loss (w/o reg) on all data: 0.040570825
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.2213726e-06
Norm of the params: 13.165232
              Random: fixed   0 labels. Loss 0.04057. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.0120552005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1867267e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012055204
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.756362e-07
Norm of the params: 9.153185
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05277232
Train loss (w/o reg) on all data: 0.043951686
Test loss (w/o reg) on all data: 0.03613772
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.979905e-06
Norm of the params: 13.2820425
              Random: fixed   2 labels. Loss 0.03614. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.012054673
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9577793e-07
Norm of the params: 9.153279
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172895
Test loss (w/o reg) on all data: 0.012054646
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.107296e-07
Norm of the params: 9.1532755
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049879037
Train loss (w/o reg) on all data: 0.041128997
Test loss (w/o reg) on all data: 0.034184776
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1866822e-06
Norm of the params: 13.228787
              Random: fixed   3 labels. Loss 0.03418. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173015
Test loss (w/o reg) on all data: 0.012055677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9616606e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730133
Test loss (w/o reg) on all data: 0.012055598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5235085e-07
Norm of the params: 9.153147
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044132024
Train loss (w/o reg) on all data: 0.035038408
Test loss (w/o reg) on all data: 0.03106617
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.363025e-06
Norm of the params: 13.486004
              Random: fixed   5 labels. Loss 0.03107. Accuracy 0.992.
### Flips: 52, rs: 9, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012054251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3271988e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054323
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7580262e-07
Norm of the params: 9.153182
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037788033
Train loss (w/o reg) on all data: 0.028831402
Test loss (w/o reg) on all data: 0.023091443
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7358678e-06
Norm of the params: 13.384045
              Random: fixed   7 labels. Loss 0.02309. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729695
Test loss (w/o reg) on all data: 0.012054974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.904055e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054902
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7733046e-07
Norm of the params: 9.153193
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037180386
Train loss (w/o reg) on all data: 0.028168749
Test loss (w/o reg) on all data: 0.031837862
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2429702e-06
Norm of the params: 13.425077
              Random: fixed   8 labels. Loss 0.03184. Accuracy 0.989.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097148366
Train loss (w/o reg) on all data: 0.08999147
Test loss (w/o reg) on all data: 0.039968353
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.947456e-06
Norm of the params: 11.9640255
Flipped loss: 0.03997. Accuracy: 0.992
### Flips: 52, rs: 10, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318257
Train loss (w/o reg) on all data: 0.0021986896
Test loss (w/o reg) on all data: 0.006433136
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1749e-06
Norm of the params: 9.076968
     Influence (LOO): fixed  31 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063182563
Train loss (w/o reg) on all data: 0.0021986887
Test loss (w/o reg) on all data: 0.0064331554
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2187497e-07
Norm of the params: 9.076968
                Loss: fixed  31 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0831822
Train loss (w/o reg) on all data: 0.07525891
Test loss (w/o reg) on all data: 0.034338973
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7397384e-06
Norm of the params: 12.588319
              Random: fixed   4 labels. Loss 0.03434. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 104
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012055354
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5424426e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012055193
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1672896e-07
Norm of the params: 9.153212
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0831822
Train loss (w/o reg) on all data: 0.07526032
Test loss (w/o reg) on all data: 0.034348488
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4096008e-05
Norm of the params: 12.587204
              Random: fixed   4 labels. Loss 0.03435. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 156
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729534
Test loss (w/o reg) on all data: 0.012055014
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.483535e-08
Norm of the params: 9.1532135
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012055045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3305078e-07
Norm of the params: 9.153213
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081970885
Train loss (w/o reg) on all data: 0.074251756
Test loss (w/o reg) on all data: 0.030624004
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.406485e-06
Norm of the params: 12.42508
              Random: fixed   5 labels. Loss 0.03062. Accuracy 0.996.
### Flips: 52, rs: 10, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729912
Test loss (w/o reg) on all data: 0.012055662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7182904e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012055154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4452638e-07
Norm of the params: 9.153186
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0715512
Train loss (w/o reg) on all data: 0.06397049
Test loss (w/o reg) on all data: 0.031195495
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4807456e-05
Norm of the params: 12.313169
              Random: fixed   9 labels. Loss 0.03120. Accuracy 0.989.
### Flips: 52, rs: 10, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.012055439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6036356e-07
Norm of the params: 9.153105
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730417
Test loss (w/o reg) on all data: 0.012055395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7457463e-07
Norm of the params: 9.153115
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06503577
Train loss (w/o reg) on all data: 0.057533845
Test loss (w/o reg) on all data: 0.025560038
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.932802e-06
Norm of the params: 12.249019
              Random: fixed  12 labels. Loss 0.02556. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012055713
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8217553e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729826
Test loss (w/o reg) on all data: 0.01205563
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8546751e-07
Norm of the params: 9.153181
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058665317
Train loss (w/o reg) on all data: 0.050812762
Test loss (w/o reg) on all data: 0.02084206
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.831219e-06
Norm of the params: 12.5320015
              Random: fixed  14 labels. Loss 0.02084. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09872505
Train loss (w/o reg) on all data: 0.090635896
Test loss (w/o reg) on all data: 0.05059818
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0580696e-05
Norm of the params: 12.719399
Flipped loss: 0.05060. Accuracy: 0.992
### Flips: 52, rs: 11, checks: 52
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729376
Test loss (w/o reg) on all data: 0.012055726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.682352e-07
Norm of the params: 9.153232
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729372
Test loss (w/o reg) on all data: 0.01205585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6441455e-07
Norm of the params: 9.153231
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09310423
Train loss (w/o reg) on all data: 0.084542744
Test loss (w/o reg) on all data: 0.04928636
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.336116e-06
Norm of the params: 13.08548
              Random: fixed   2 labels. Loss 0.04929. Accuracy 0.992.
### Flips: 52, rs: 11, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055242
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2130988e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012055248
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.13851e-07
Norm of the params: 9.153193
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09051882
Train loss (w/o reg) on all data: 0.08181643
Test loss (w/o reg) on all data: 0.04743528
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.985968e-06
Norm of the params: 13.192718
              Random: fixed   3 labels. Loss 0.04744. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729826
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00534436e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.012055233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5003762e-07
Norm of the params: 9.153187
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08867226
Train loss (w/o reg) on all data: 0.07977646
Test loss (w/o reg) on all data: 0.046238963
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2239902e-06
Norm of the params: 13.338516
              Random: fixed   4 labels. Loss 0.04624. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 208
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.00217294
Test loss (w/o reg) on all data: 0.012055105
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0900285e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012055148
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.04161195e-07
Norm of the params: 9.153227
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08681524
Train loss (w/o reg) on all data: 0.077776566
Test loss (w/o reg) on all data: 0.043157358
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.0474746e-06
Norm of the params: 13.445199
              Random: fixed   5 labels. Loss 0.04316. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012056046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.079359e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012055875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.062145e-07
Norm of the params: 9.153197
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08424825
Train loss (w/o reg) on all data: 0.074952655
Test loss (w/o reg) on all data: 0.040466398
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1472512e-05
Norm of the params: 13.634954
              Random: fixed   6 labels. Loss 0.04047. Accuracy 0.992.
### Flips: 52, rs: 11, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729332
Test loss (w/o reg) on all data: 0.012055288
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.444808e-07
Norm of the params: 9.153236
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012055246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8574293e-07
Norm of the params: 9.153234
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08424825
Train loss (w/o reg) on all data: 0.07495407
Test loss (w/o reg) on all data: 0.04046962
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3770794e-05
Norm of the params: 13.633914
              Random: fixed   6 labels. Loss 0.04047. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09457391
Train loss (w/o reg) on all data: 0.08659582
Test loss (w/o reg) on all data: 0.06309074
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8332777e-05
Norm of the params: 12.631776
Flipped loss: 0.06309. Accuracy: 0.981
### Flips: 52, rs: 12, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729036
Test loss (w/o reg) on all data: 0.012055726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.407679e-07
Norm of the params: 9.153268
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729027
Test loss (w/o reg) on all data: 0.012055854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9294257e-07
Norm of the params: 9.153267
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09120894
Train loss (w/o reg) on all data: 0.08303599
Test loss (w/o reg) on all data: 0.059091102
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.162749e-06
Norm of the params: 12.7851095
              Random: fixed   1 labels. Loss 0.05909. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012055154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0280079e-07
Norm of the params: 9.153236
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729367
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4267496e-07
Norm of the params: 9.153234
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08629014
Train loss (w/o reg) on all data: 0.077739194
Test loss (w/o reg) on all data: 0.058593757
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.878354e-06
Norm of the params: 13.077422
              Random: fixed   3 labels. Loss 0.05859. Accuracy 0.977.
### Flips: 52, rs: 12, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728799
Test loss (w/o reg) on all data: 0.012056512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0907042e-07
Norm of the params: 9.153294
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172883
Test loss (w/o reg) on all data: 0.012056326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3846612e-07
Norm of the params: 9.153288
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07949973
Train loss (w/o reg) on all data: 0.070571244
Test loss (w/o reg) on all data: 0.055427395
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4474733e-06
Norm of the params: 13.362996
              Random: fixed   5 labels. Loss 0.05543. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012056013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1734675e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012055944
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5718236e-07
Norm of the params: 9.153157
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07277818
Train loss (w/o reg) on all data: 0.06357604
Test loss (w/o reg) on all data: 0.049517002
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0422771e-05
Norm of the params: 13.566237
              Random: fixed   7 labels. Loss 0.04952. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729202
Test loss (w/o reg) on all data: 0.012056544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.080161e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729206
Test loss (w/o reg) on all data: 0.012056658
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.693541e-07
Norm of the params: 9.153249
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067399606
Train loss (w/o reg) on all data: 0.058198623
Test loss (w/o reg) on all data: 0.045587778
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6619188e-06
Norm of the params: 13.565383
              Random: fixed   9 labels. Loss 0.04559. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.990707e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055113
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0172822e-07
Norm of the params: 9.15322
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06235001
Train loss (w/o reg) on all data: 0.052627653
Test loss (w/o reg) on all data: 0.047796123
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.505151e-06
Norm of the params: 13.944428
              Random: fixed  11 labels. Loss 0.04780. Accuracy 0.981.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07961573
Train loss (w/o reg) on all data: 0.07099479
Test loss (w/o reg) on all data: 0.04055029
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1569987e-06
Norm of the params: 13.130837
Flipped loss: 0.04055. Accuracy: 0.992
### Flips: 52, rs: 13, checks: 52
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012056306
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0374175e-06
Norm of the params: 9.153224
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.012056134
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8306945e-07
Norm of the params: 9.153223
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079615735
Train loss (w/o reg) on all data: 0.07099385
Test loss (w/o reg) on all data: 0.040552545
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.181391e-06
Norm of the params: 13.131553
              Random: fixed   0 labels. Loss 0.04055. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055069
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1846428e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012055167
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0930895e-07
Norm of the params: 9.153219
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07772334
Train loss (w/o reg) on all data: 0.069311135
Test loss (w/o reg) on all data: 0.040163364
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.459526e-06
Norm of the params: 12.970892
              Random: fixed   1 labels. Loss 0.04016. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012054955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.846787e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012054911
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5348012e-07
Norm of the params: 9.153183
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06921672
Train loss (w/o reg) on all data: 0.060793884
Test loss (w/o reg) on all data: 0.037655953
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.280057e-06
Norm of the params: 12.97909
              Random: fixed   4 labels. Loss 0.03766. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012054904
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.11672485e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.0120547945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3503914e-07
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06835372
Train loss (w/o reg) on all data: 0.059923057
Test loss (w/o reg) on all data: 0.036779445
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0543556e-06
Norm of the params: 12.985117
              Random: fixed   5 labels. Loss 0.03678. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729122
Test loss (w/o reg) on all data: 0.012054724
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.988038e-07
Norm of the params: 9.153258
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.012054396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3798619e-07
Norm of the params: 9.153245
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06524015
Train loss (w/o reg) on all data: 0.056952562
Test loss (w/o reg) on all data: 0.03660809
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.8263993e-06
Norm of the params: 12.874465
              Random: fixed   6 labels. Loss 0.03661. Accuracy 0.996.
### Flips: 52, rs: 13, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729898
Test loss (w/o reg) on all data: 0.012055139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.05516e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729902
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5600454e-07
Norm of the params: 9.153173
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06301926
Train loss (w/o reg) on all data: 0.054870993
Test loss (w/o reg) on all data: 0.035936937
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.038369e-05
Norm of the params: 12.76579
              Random: fixed   7 labels. Loss 0.03594. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08454633
Train loss (w/o reg) on all data: 0.07576247
Test loss (w/o reg) on all data: 0.042148493
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.884468e-06
Norm of the params: 13.254322
Flipped loss: 0.04215. Accuracy: 0.992
### Flips: 52, rs: 14, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012055351
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1336983e-07
Norm of the params: 9.15321
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729558
Test loss (w/o reg) on all data: 0.012055441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4119907e-07
Norm of the params: 9.15321
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0827644
Train loss (w/o reg) on all data: 0.07400607
Test loss (w/o reg) on all data: 0.038837645
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0404599e-05
Norm of the params: 13.235052
              Random: fixed   1 labels. Loss 0.03884. Accuracy 0.992.
### Flips: 52, rs: 14, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217315
Test loss (w/o reg) on all data: 0.012055062
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.36338e-07
Norm of the params: 9.152997
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021731462
Test loss (w/o reg) on all data: 0.012055217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7637255e-07
Norm of the params: 9.153
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08276439
Train loss (w/o reg) on all data: 0.07400868
Test loss (w/o reg) on all data: 0.03885087
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0476969e-05
Norm of the params: 13.233072
              Random: fixed   1 labels. Loss 0.03885. Accuracy 0.992.
### Flips: 52, rs: 14, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172893
Test loss (w/o reg) on all data: 0.012055271
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3204222e-07
Norm of the params: 9.153278
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728962
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4582167e-07
Norm of the params: 9.153276
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07933327
Train loss (w/o reg) on all data: 0.07103209
Test loss (w/o reg) on all data: 0.034763563
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.882284e-06
Norm of the params: 12.885013
              Random: fixed   3 labels. Loss 0.03476. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.0120552685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7377641e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012055234
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.989865e-08
Norm of the params: 9.15317
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095027
Train loss (w/o reg) on all data: 0.062575586
Test loss (w/o reg) on all data: 0.03155724
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.054302e-06
Norm of the params: 12.941938
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729774
Test loss (w/o reg) on all data: 0.012054934
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2022768e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012055035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3877596e-07
Norm of the params: 9.153192
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095028
Train loss (w/o reg) on all data: 0.06257419
Test loss (w/o reg) on all data: 0.03155957
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.369855e-06
Norm of the params: 12.943016
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730708
Test loss (w/o reg) on all data: 0.012055322
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9118876e-07
Norm of the params: 9.153084
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730699
Test loss (w/o reg) on all data: 0.012055413
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.613683e-07
Norm of the params: 9.153085
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095027
Train loss (w/o reg) on all data: 0.06257609
Test loss (w/o reg) on all data: 0.031559877
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.072552e-06
Norm of the params: 12.9415455
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.996.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08138944
Train loss (w/o reg) on all data: 0.072607584
Test loss (w/o reg) on all data: 0.03604637
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6664644e-06
Norm of the params: 13.252819
Flipped loss: 0.03605. Accuracy: 0.996
### Flips: 52, rs: 15, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00740458
Train loss (w/o reg) on all data: 0.0027831646
Test loss (w/o reg) on all data: 0.013700693
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0571474e-06
Norm of the params: 9.613964
     Influence (LOO): fixed  25 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007404579
Train loss (w/o reg) on all data: 0.0027831648
Test loss (w/o reg) on all data: 0.013700554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6807535e-07
Norm of the params: 9.613963
                Loss: fixed  25 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07611614
Train loss (w/o reg) on all data: 0.067625456
Test loss (w/o reg) on all data: 0.034889538
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7006487e-05
Norm of the params: 13.031259
              Random: fixed   3 labels. Loss 0.03489. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021730205
Test loss (w/o reg) on all data: 0.01205516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.570113e-08
Norm of the params: 9.153141
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.01205519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1753576e-07
Norm of the params: 9.153141
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076116145
Train loss (w/o reg) on all data: 0.06762567
Test loss (w/o reg) on all data: 0.034888502
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7588258e-05
Norm of the params: 13.031097
              Random: fixed   3 labels. Loss 0.03489. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172918
Test loss (w/o reg) on all data: 0.012055379
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7042974e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729197
Test loss (w/o reg) on all data: 0.012055295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3055852e-07
Norm of the params: 9.15325
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07420097
Train loss (w/o reg) on all data: 0.06661635
Test loss (w/o reg) on all data: 0.033356037
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.11756e-06
Norm of the params: 12.316351
              Random: fixed   5 labels. Loss 0.03336. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055951
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.813933e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012055826
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4885632e-07
Norm of the params: 9.15318
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074200965
Train loss (w/o reg) on all data: 0.06661415
Test loss (w/o reg) on all data: 0.033353653
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7529934e-06
Norm of the params: 12.318129
              Random: fixed   5 labels. Loss 0.03335. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.0120562855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9807578e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728978
Test loss (w/o reg) on all data: 0.012056221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1792133e-07
Norm of the params: 9.153274
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069145545
Train loss (w/o reg) on all data: 0.061963048
Test loss (w/o reg) on all data: 0.031765096
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.122686e-06
Norm of the params: 11.985406
              Random: fixed   6 labels. Loss 0.03177. Accuracy 0.992.
### Flips: 52, rs: 15, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217304
Test loss (w/o reg) on all data: 0.012055761
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9195686e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730387
Test loss (w/o reg) on all data: 0.012055865
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5941753e-07
Norm of the params: 9.153118
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069145545
Train loss (w/o reg) on all data: 0.061965052
Test loss (w/o reg) on all data: 0.03176093
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0359228e-05
Norm of the params: 11.983732
              Random: fixed   6 labels. Loss 0.03176. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074852616
Train loss (w/o reg) on all data: 0.06524951
Test loss (w/o reg) on all data: 0.06526685
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.122973e-05
Norm of the params: 13.858649
Flipped loss: 0.06527. Accuracy: 0.981
### Flips: 52, rs: 16, checks: 52
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729714
Test loss (w/o reg) on all data: 0.01205525
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7159381e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4402983e-07
Norm of the params: 9.153193
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074852616
Train loss (w/o reg) on all data: 0.06525008
Test loss (w/o reg) on all data: 0.065273955
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.405452e-06
Norm of the params: 13.858241
              Random: fixed   0 labels. Loss 0.06527. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730172
Test loss (w/o reg) on all data: 0.012056225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0223874e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730177
Test loss (w/o reg) on all data: 0.012056096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1294821e-07
Norm of the params: 9.153142
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06826821
Train loss (w/o reg) on all data: 0.058782585
Test loss (w/o reg) on all data: 0.06616819
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.626359e-06
Norm of the params: 13.773618
              Random: fixed   2 labels. Loss 0.06617. Accuracy 0.973.
### Flips: 52, rs: 16, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012055064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2217018e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9850411e-07
Norm of the params: 9.153208
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0652236
Train loss (w/o reg) on all data: 0.05612008
Test loss (w/o reg) on all data: 0.057881374
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.689488e-06
Norm of the params: 13.493345
              Random: fixed   4 labels. Loss 0.05788. Accuracy 0.977.
### Flips: 52, rs: 16, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055338
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0724465e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729784
Test loss (w/o reg) on all data: 0.012055263
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4927428e-07
Norm of the params: 9.153185
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06154379
Train loss (w/o reg) on all data: 0.05184809
Test loss (w/o reg) on all data: 0.060736526
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.735008e-06
Norm of the params: 13.925299
              Random: fixed   5 labels. Loss 0.06074. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.012054671
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6264402e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.01205462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.662096e-08
Norm of the params: 9.153247
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06154378
Train loss (w/o reg) on all data: 0.051844053
Test loss (w/o reg) on all data: 0.060750257
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5247226e-05
Norm of the params: 13.928193
              Random: fixed   5 labels. Loss 0.06075. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172932
Test loss (w/o reg) on all data: 0.012054615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2528277e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729318
Test loss (w/o reg) on all data: 0.012054689
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3726914e-07
Norm of the params: 9.153237
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061543778
Train loss (w/o reg) on all data: 0.05184701
Test loss (w/o reg) on all data: 0.06073477
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.7808787e-06
Norm of the params: 13.926065
              Random: fixed   5 labels. Loss 0.06073. Accuracy 0.981.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07356674
Train loss (w/o reg) on all data: 0.06313983
Test loss (w/o reg) on all data: 0.07806685
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3143088e-05
Norm of the params: 14.440853
Flipped loss: 0.07807. Accuracy: 0.973
### Flips: 52, rs: 17, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729257
Test loss (w/o reg) on all data: 0.01205501
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2312894e-07
Norm of the params: 9.153245
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.012055068
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6384785e-07
Norm of the params: 9.153243
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0658425
Train loss (w/o reg) on all data: 0.056237463
Test loss (w/o reg) on all data: 0.06338587
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.486094e-06
Norm of the params: 13.860044
              Random: fixed   3 labels. Loss 0.06339. Accuracy 0.985.
### Flips: 52, rs: 17, checks: 104
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.01205452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4506134e-07
Norm of the params: 9.153186
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729774
Test loss (w/o reg) on all data: 0.012054499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8408939e-07
Norm of the params: 9.153185
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06275891
Train loss (w/o reg) on all data: 0.05313156
Test loss (w/o reg) on all data: 0.06444922
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5978205e-06
Norm of the params: 13.876125
              Random: fixed   4 labels. Loss 0.06445. Accuracy 0.981.
### Flips: 52, rs: 17, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012054607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.949804e-08
Norm of the params: 9.153204
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012054674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.384207e-07
Norm of the params: 9.153199
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060537204
Train loss (w/o reg) on all data: 0.050839312
Test loss (w/o reg) on all data: 0.06391444
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.6205458e-06
Norm of the params: 13.926873
              Random: fixed   5 labels. Loss 0.06391. Accuracy 0.977.
### Flips: 52, rs: 17, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012054517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1591343e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172973
Test loss (w/o reg) on all data: 0.012054598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.363412e-07
Norm of the params: 9.153191
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057854105
Train loss (w/o reg) on all data: 0.04922755
Test loss (w/o reg) on all data: 0.059951555
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.26166e-06
Norm of the params: 13.135109
              Random: fixed   7 labels. Loss 0.05995. Accuracy 0.985.
### Flips: 52, rs: 17, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012054259
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.632049e-07
Norm of the params: 9.153239
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729306
Test loss (w/o reg) on all data: 0.012054232
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5117762e-07
Norm of the params: 9.153238
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056032106
Train loss (w/o reg) on all data: 0.047596242
Test loss (w/o reg) on all data: 0.049681123
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0047908e-06
Norm of the params: 12.989121
              Random: fixed   8 labels. Loss 0.04968. Accuracy 0.992.
### Flips: 52, rs: 17, checks: 312
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729956
Test loss (w/o reg) on all data: 0.012054642
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4835956e-07
Norm of the params: 9.153169
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012054618
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4983445e-07
Norm of the params: 9.15317
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052088276
Train loss (w/o reg) on all data: 0.04335004
Test loss (w/o reg) on all data: 0.046433732
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3481708e-06
Norm of the params: 13.219861
              Random: fixed   9 labels. Loss 0.04643. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07054171
Train loss (w/o reg) on all data: 0.06260962
Test loss (w/o reg) on all data: 0.04085223
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.423804e-06
Norm of the params: 12.595308
Flipped loss: 0.04085. Accuracy: 0.989
### Flips: 52, rs: 18, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729164
Test loss (w/o reg) on all data: 0.012054679
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5538637e-07
Norm of the params: 9.1532545
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729234
Test loss (w/o reg) on all data: 0.012054939
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1939945e-07
Norm of the params: 9.153247
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057607446
Train loss (w/o reg) on all data: 0.049156796
Test loss (w/o reg) on all data: 0.033149064
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.95255e-06
Norm of the params: 13.0005
              Random: fixed   5 labels. Loss 0.03315. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173037
Test loss (w/o reg) on all data: 0.012055586
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8051658e-07
Norm of the params: 9.153121
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173035
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1965242e-07
Norm of the params: 9.153124
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05453307
Train loss (w/o reg) on all data: 0.04638497
Test loss (w/o reg) on all data: 0.029293206
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.246264e-06
Norm of the params: 12.765657
              Random: fixed   6 labels. Loss 0.02929. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729174
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6649127e-07
Norm of the params: 9.153254
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729202
Test loss (w/o reg) on all data: 0.012055044
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.490223e-07
Norm of the params: 9.153251
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05180011
Train loss (w/o reg) on all data: 0.043751325
Test loss (w/o reg) on all data: 0.02925191
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7370127e-06
Norm of the params: 12.68762
              Random: fixed   7 labels. Loss 0.02925. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730121
Test loss (w/o reg) on all data: 0.0120554315
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0804741e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730103
Test loss (w/o reg) on all data: 0.0120553905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7068992e-07
Norm of the params: 9.15315
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04282535
Train loss (w/o reg) on all data: 0.034477968
Test loss (w/o reg) on all data: 0.027095921
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.706881e-06
Norm of the params: 12.920821
              Random: fixed   9 labels. Loss 0.02710. Accuracy 0.996.
### Flips: 52, rs: 18, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021730135
Test loss (w/o reg) on all data: 0.012054977
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3108464e-07
Norm of the params: 9.15315
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730107
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.830425e-07
Norm of the params: 9.153151
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04282535
Train loss (w/o reg) on all data: 0.034478907
Test loss (w/o reg) on all data: 0.02709185
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8711878e-06
Norm of the params: 12.920093
              Random: fixed   9 labels. Loss 0.02709. Accuracy 0.996.
### Flips: 52, rs: 18, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730573
Test loss (w/o reg) on all data: 0.012055337
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.597838e-07
Norm of the params: 9.153098
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173056
Test loss (w/o reg) on all data: 0.012055247
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6944645e-07
Norm of the params: 9.1531
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04100789
Train loss (w/o reg) on all data: 0.033084296
Test loss (w/o reg) on all data: 0.023857323
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.150002e-06
Norm of the params: 12.588562
              Random: fixed  10 labels. Loss 0.02386. Accuracy 0.996.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093134485
Train loss (w/o reg) on all data: 0.08258609
Test loss (w/o reg) on all data: 0.069864936
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.132332e-06
Norm of the params: 14.524736
Flipped loss: 0.06986. Accuracy: 0.989
### Flips: 52, rs: 19, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730226
Test loss (w/o reg) on all data: 0.012055716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.741642e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012055879
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8781218e-07
Norm of the params: 9.153138
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08724585
Train loss (w/o reg) on all data: 0.07670034
Test loss (w/o reg) on all data: 0.06720492
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.484437e-06
Norm of the params: 14.52275
              Random: fixed   2 labels. Loss 0.06720. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012054565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8146149e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012054545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9137195e-08
Norm of the params: 9.153188
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07793774
Train loss (w/o reg) on all data: 0.06704027
Test loss (w/o reg) on all data: 0.062889546
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.426878e-05
Norm of the params: 14.763107
              Random: fixed   5 labels. Loss 0.06289. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729064
Test loss (w/o reg) on all data: 0.012054941
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.403642e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729108
Test loss (w/o reg) on all data: 0.012054894
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6060326e-07
Norm of the params: 9.153261
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07634512
Train loss (w/o reg) on all data: 0.065572396
Test loss (w/o reg) on all data: 0.060537487
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7179014e-05
Norm of the params: 14.67837
              Random: fixed   6 labels. Loss 0.06054. Accuracy 0.992.
### Flips: 52, rs: 19, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002173081
Test loss (w/o reg) on all data: 0.012054258
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2641338e-07
Norm of the params: 9.153071
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730796
Test loss (w/o reg) on all data: 0.012054353
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.262953e-07
Norm of the params: 9.153075
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07484379
Train loss (w/o reg) on all data: 0.064107046
Test loss (w/o reg) on all data: 0.05788198
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.1972193e-06
Norm of the params: 14.6538315
              Random: fixed   7 labels. Loss 0.05788. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729877
Test loss (w/o reg) on all data: 0.012055053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.716145e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729884
Test loss (w/o reg) on all data: 0.012055091
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3687381e-07
Norm of the params: 9.153177
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06809807
Train loss (w/o reg) on all data: 0.056939825
Test loss (w/o reg) on all data: 0.05071138
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.17401905e-05
Norm of the params: 14.938703
              Random: fixed   9 labels. Loss 0.05071. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728224
Test loss (w/o reg) on all data: 0.012054974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.583707e-07
Norm of the params: 9.153357
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172827
Test loss (w/o reg) on all data: 0.01205487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0838274e-07
Norm of the params: 9.153351
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06588433
Train loss (w/o reg) on all data: 0.055222087
Test loss (w/o reg) on all data: 0.045365237
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0920407e-05
Norm of the params: 14.602907
              Random: fixed  11 labels. Loss 0.04537. Accuracy 0.989.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073684834
Train loss (w/o reg) on all data: 0.063629776
Test loss (w/o reg) on all data: 0.05416092
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.1545486e-06
Norm of the params: 14.181014
Flipped loss: 0.05416. Accuracy: 0.989
### Flips: 52, rs: 20, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.0120547535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4772843e-07
Norm of the params: 9.153172
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.012054806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8126114e-07
Norm of the params: 9.153172
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06995129
Train loss (w/o reg) on all data: 0.059948757
Test loss (w/o reg) on all data: 0.050868392
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1371836e-06
Norm of the params: 14.143924
              Random: fixed   2 labels. Loss 0.05087. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 104
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173006
Test loss (w/o reg) on all data: 0.012054766
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.962369e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730058
Test loss (w/o reg) on all data: 0.012054739
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4326804e-07
Norm of the params: 9.153154
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06574498
Train loss (w/o reg) on all data: 0.055280913
Test loss (w/o reg) on all data: 0.051583238
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2901412e-06
Norm of the params: 14.466558
              Random: fixed   3 labels. Loss 0.05158. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012054793
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6421487e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172944
Test loss (w/o reg) on all data: 0.012054867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2273751e-07
Norm of the params: 9.153224
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056975223
Train loss (w/o reg) on all data: 0.04641182
Test loss (w/o reg) on all data: 0.053701088
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5495148e-06
Norm of the params: 14.535064
              Random: fixed   5 labels. Loss 0.05370. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012055401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0941096e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729504
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5477744e-07
Norm of the params: 9.153217
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055231288
Train loss (w/o reg) on all data: 0.044412207
Test loss (w/o reg) on all data: 0.05799617
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3811527e-06
Norm of the params: 14.709917
              Random: fixed   6 labels. Loss 0.05800. Accuracy 0.985.
### Flips: 52, rs: 20, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.0021729874
Test loss (w/o reg) on all data: 0.012054999
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.06306075e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012054945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1400788e-07
Norm of the params: 9.153181
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045950435
Train loss (w/o reg) on all data: 0.03568967
Test loss (w/o reg) on all data: 0.046912972
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8759214e-06
Norm of the params: 14.325336
              Random: fixed   9 labels. Loss 0.04691. Accuracy 0.992.
### Flips: 52, rs: 20, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1666038e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729413
Test loss (w/o reg) on all data: 0.012054899
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2741735e-07
Norm of the params: 9.153226
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04219281
Train loss (w/o reg) on all data: 0.031964134
Test loss (w/o reg) on all data: 0.048516072
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.1410788e-06
Norm of the params: 14.302921
              Random: fixed  10 labels. Loss 0.04852. Accuracy 0.985.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08157077
Train loss (w/o reg) on all data: 0.07253787
Test loss (w/o reg) on all data: 0.05421554
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0598912e-05
Norm of the params: 13.440905
Flipped loss: 0.05422. Accuracy: 0.989
### Flips: 52, rs: 21, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730254
Test loss (w/o reg) on all data: 0.012055129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5315037e-07
Norm of the params: 9.153134
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012055272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8559846e-07
Norm of the params: 9.153139
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079557754
Train loss (w/o reg) on all data: 0.07048312
Test loss (w/o reg) on all data: 0.049863137
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.540475e-06
Norm of the params: 13.471925
              Random: fixed   1 labels. Loss 0.04986. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729057
Test loss (w/o reg) on all data: 0.012055237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.084335e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729085
Test loss (w/o reg) on all data: 0.0120551195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.423095e-07
Norm of the params: 9.153264
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0790672
Train loss (w/o reg) on all data: 0.07064558
Test loss (w/o reg) on all data: 0.05010481
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.506442e-06
Norm of the params: 12.97815
              Random: fixed   2 labels. Loss 0.05010. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 156
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.0120549435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8391641e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012054918
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.057674e-08
Norm of the params: 9.153249
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069595575
Train loss (w/o reg) on all data: 0.06152273
Test loss (w/o reg) on all data: 0.04226548
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.838368e-06
Norm of the params: 12.706572
              Random: fixed   5 labels. Loss 0.04227. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012055092
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3803502e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729951
Test loss (w/o reg) on all data: 0.012055026
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.585496e-08
Norm of the params: 9.153167
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067158684
Train loss (w/o reg) on all data: 0.059244793
Test loss (w/o reg) on all data: 0.041005347
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.27742e-05
Norm of the params: 12.580851
              Random: fixed   6 labels. Loss 0.04101. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.012055424
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.015013e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012055588
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7343662e-07
Norm of the params: 9.153226
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064843416
Train loss (w/o reg) on all data: 0.057396084
Test loss (w/o reg) on all data: 0.040863685
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8426048e-05
Norm of the params: 12.204372
              Random: fixed   7 labels. Loss 0.04086. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021728852
Test loss (w/o reg) on all data: 0.012056178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.11816e-06
Norm of the params: 9.153286
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728869
Test loss (w/o reg) on all data: 0.012055972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2835934e-07
Norm of the params: 9.153284
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06484342
Train loss (w/o reg) on all data: 0.057397425
Test loss (w/o reg) on all data: 0.04086036
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.180828e-06
Norm of the params: 12.203276
              Random: fixed   7 labels. Loss 0.04086. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095989
Train loss (w/o reg) on all data: 0.06185387
Test loss (w/o reg) on all data: 0.040807445
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6419586e-06
Norm of the params: 13.495197
Flipped loss: 0.04081. Accuracy: 0.996
### Flips: 52, rs: 22, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012057735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.890658e-07
Norm of the params: 9.15314
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730196
Test loss (w/o reg) on all data: 0.012057894
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.716683e-07
Norm of the params: 9.15314
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095989
Train loss (w/o reg) on all data: 0.061852187
Test loss (w/o reg) on all data: 0.040803257
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.245269e-06
Norm of the params: 13.496445
              Random: fixed   0 labels. Loss 0.04080. Accuracy 0.996.
### Flips: 52, rs: 22, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729404
Test loss (w/o reg) on all data: 0.012055539
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.83521e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729406
Test loss (w/o reg) on all data: 0.012055484
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1058752e-07
Norm of the params: 9.153227
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06612307
Train loss (w/o reg) on all data: 0.05623387
Test loss (w/o reg) on all data: 0.04136732
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3417283e-05
Norm of the params: 14.063565
              Random: fixed   2 labels. Loss 0.04137. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012055201
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.586636e-08
Norm of the params: 9.153207
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012055115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1785411e-07
Norm of the params: 9.153206
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06417325
Train loss (w/o reg) on all data: 0.05386268
Test loss (w/o reg) on all data: 0.04160106
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5478766e-06
Norm of the params: 14.360066
              Random: fixed   3 labels. Loss 0.04160. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012056302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.389421e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729528
Test loss (w/o reg) on all data: 0.012056141
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2786539e-07
Norm of the params: 9.153213
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06164042
Train loss (w/o reg) on all data: 0.051157515
Test loss (w/o reg) on all data: 0.03730991
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.211602e-06
Norm of the params: 14.479574
              Random: fixed   5 labels. Loss 0.03731. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055218
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3697964e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729951
Test loss (w/o reg) on all data: 0.012055179
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1898117e-07
Norm of the params: 9.153169
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05393089
Train loss (w/o reg) on all data: 0.044236507
Test loss (w/o reg) on all data: 0.03918217
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.889817e-06
Norm of the params: 13.924354
              Random: fixed  10 labels. Loss 0.03918. Accuracy 0.989.
### Flips: 52, rs: 22, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7119594e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055297
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7142225e-08
Norm of the params: 9.153184
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053930882
Train loss (w/o reg) on all data: 0.044235677
Test loss (w/o reg) on all data: 0.039182473
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3149043e-06
Norm of the params: 13.924946
              Random: fixed  10 labels. Loss 0.03918. Accuracy 0.989.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070095085
Train loss (w/o reg) on all data: 0.061106443
Test loss (w/o reg) on all data: 0.035112076
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3714051e-06
Norm of the params: 13.407941
Flipped loss: 0.03511. Accuracy: 0.992
### Flips: 52, rs: 23, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012055227
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0143705e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012055417
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7803103e-07
Norm of the params: 9.153196
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070095085
Train loss (w/o reg) on all data: 0.06110669
Test loss (w/o reg) on all data: 0.03511173
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6383467e-06
Norm of the params: 13.407755
              Random: fixed   0 labels. Loss 0.03511. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217294
Test loss (w/o reg) on all data: 0.012055853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1852924e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012056072
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2222439e-07
Norm of the params: 9.153211
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061571054
Train loss (w/o reg) on all data: 0.052260604
Test loss (w/o reg) on all data: 0.03668137
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.3217026e-06
Norm of the params: 13.645842
              Random: fixed   2 labels. Loss 0.03668. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172993
Test loss (w/o reg) on all data: 0.012055444
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2488678e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055469
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.985664e-07
Norm of the params: 9.153171
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04605725
Train loss (w/o reg) on all data: 0.037045162
Test loss (w/o reg) on all data: 0.037066676
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1593603e-05
Norm of the params: 13.425415
              Random: fixed   6 labels. Loss 0.03707. Accuracy 0.985.
### Flips: 52, rs: 23, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729663
Test loss (w/o reg) on all data: 0.012055308
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3046751e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055338
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3456244e-07
Norm of the params: 9.153198
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04401062
Train loss (w/o reg) on all data: 0.03538157
Test loss (w/o reg) on all data: 0.03740413
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.551067e-06
Norm of the params: 13.13701
              Random: fixed   7 labels. Loss 0.03740. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173014
Test loss (w/o reg) on all data: 0.012055532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1496992e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730128
Test loss (w/o reg) on all data: 0.0120555535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5541647e-07
Norm of the params: 9.153151
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040821362
Train loss (w/o reg) on all data: 0.03271669
Test loss (w/o reg) on all data: 0.03468261
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.029753e-06
Norm of the params: 12.731591
              Random: fixed   8 labels. Loss 0.03468. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012055275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8542964e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012055311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4767319e-07
Norm of the params: 9.15319
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040821366
Train loss (w/o reg) on all data: 0.03271543
Test loss (w/o reg) on all data: 0.034679886
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2966365e-06
Norm of the params: 12.732585
              Random: fixed   8 labels. Loss 0.03468. Accuracy 0.989.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068577476
Train loss (w/o reg) on all data: 0.0580225
Test loss (w/o reg) on all data: 0.082311995
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4432316e-06
Norm of the params: 14.529264
Flipped loss: 0.08231. Accuracy: 0.969
### Flips: 52, rs: 24, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007247486
Train loss (w/o reg) on all data: 0.002613013
Test loss (w/o reg) on all data: 0.011079584
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4237132e-07
Norm of the params: 9.627536
     Influence (LOO): fixed  23 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172932
Test loss (w/o reg) on all data: 0.0120547945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.229246e-08
Norm of the params: 9.153235
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064915895
Train loss (w/o reg) on all data: 0.053964052
Test loss (w/o reg) on all data: 0.07995302
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.1968083e-06
Norm of the params: 14.799894
              Random: fixed   1 labels. Loss 0.07995. Accuracy 0.966.
### Flips: 52, rs: 24, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729288
Test loss (w/o reg) on all data: 0.012055273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6576876e-07
Norm of the params: 9.153242
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729292
Test loss (w/o reg) on all data: 0.012055223
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.581633e-08
Norm of the params: 9.153242
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519359
Train loss (w/o reg) on all data: 0.054826245
Test loss (w/o reg) on all data: 0.07736229
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.9746693e-06
Norm of the params: 14.399548
              Random: fixed   2 labels. Loss 0.07736. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172884
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9067652e-07
Norm of the params: 9.153288
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172886
Test loss (w/o reg) on all data: 0.012054985
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0499087e-07
Norm of the params: 9.153287
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065193586
Train loss (w/o reg) on all data: 0.054824747
Test loss (w/o reg) on all data: 0.07736678
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.9537425e-06
Norm of the params: 14.400583
              Random: fixed   2 labels. Loss 0.07737. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172902
Test loss (w/o reg) on all data: 0.012055306
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.11159e-07
Norm of the params: 9.153268
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729048
Test loss (w/o reg) on all data: 0.012055217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1196878e-07
Norm of the params: 9.153267
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519359
Train loss (w/o reg) on all data: 0.054825775
Test loss (w/o reg) on all data: 0.07736205
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0916733e-06
Norm of the params: 14.399871
              Random: fixed   2 labels. Loss 0.07736. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.012054951
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2119686e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055015
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6210731e-07
Norm of the params: 9.153159
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062051877
Train loss (w/o reg) on all data: 0.052094206
Test loss (w/o reg) on all data: 0.07005121
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.097942e-05
Norm of the params: 14.112173
              Random: fixed   3 labels. Loss 0.07005. Accuracy 0.981.
### Flips: 52, rs: 24, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172927
Test loss (w/o reg) on all data: 0.012055293
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2423321e-07
Norm of the params: 9.153243
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.012055473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00072555e-07
Norm of the params: 9.153221
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06205187
Train loss (w/o reg) on all data: 0.052091703
Test loss (w/o reg) on all data: 0.07006458
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.1587897e-06
Norm of the params: 14.113941
              Random: fixed   3 labels. Loss 0.07006. Accuracy 0.981.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09873342
Train loss (w/o reg) on all data: 0.0916478
Test loss (w/o reg) on all data: 0.04843956
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.285551e-06
Norm of the params: 11.904304
Flipped loss: 0.04844. Accuracy: 0.989
### Flips: 52, rs: 25, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729425
Test loss (w/o reg) on all data: 0.012055814
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6646168e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012055832
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2544673e-07
Norm of the params: 9.153224
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09674992
Train loss (w/o reg) on all data: 0.089637324
Test loss (w/o reg) on all data: 0.04693486
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2736016e-05
Norm of the params: 11.926938
              Random: fixed   1 labels. Loss 0.04693. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.012054957
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1331459e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055017
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0518068e-07
Norm of the params: 9.153158
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09674993
Train loss (w/o reg) on all data: 0.08963669
Test loss (w/o reg) on all data: 0.046942405
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2699177e-05
Norm of the params: 11.927482
              Random: fixed   1 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173002
Test loss (w/o reg) on all data: 0.012055063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3770822e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173001
Test loss (w/o reg) on all data: 0.012055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4420248e-07
Norm of the params: 9.153161
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09674991
Train loss (w/o reg) on all data: 0.08963655
Test loss (w/o reg) on all data: 0.046944905
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.268201e-06
Norm of the params: 11.927582
              Random: fixed   1 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729139
Test loss (w/o reg) on all data: 0.012054664
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9174139e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729136
Test loss (w/o reg) on all data: 0.012054713
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0763054e-07
Norm of the params: 9.153257
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085310675
Train loss (w/o reg) on all data: 0.07749875
Test loss (w/o reg) on all data: 0.048650738
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4736422e-06
Norm of the params: 12.499543
              Random: fixed   4 labels. Loss 0.04865. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 260
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012055407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7512407e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.012055481
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6738766e-07
Norm of the params: 9.153204
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06843868
Train loss (w/o reg) on all data: 0.060256135
Test loss (w/o reg) on all data: 0.043548997
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5429174e-06
Norm of the params: 12.792609
              Random: fixed   9 labels. Loss 0.04355. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012054828
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0944973e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054954
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.672834e-07
Norm of the params: 9.153193
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063560024
Train loss (w/o reg) on all data: 0.05551222
Test loss (w/o reg) on all data: 0.04182143
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5122163e-06
Norm of the params: 12.686849
              Random: fixed  11 labels. Loss 0.04182. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079595946
Train loss (w/o reg) on all data: 0.068792015
Test loss (w/o reg) on all data: 0.060617033
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.7892345e-06
Norm of the params: 14.699614
Flipped loss: 0.06062. Accuracy: 0.981
### Flips: 52, rs: 26, checks: 52
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012055019
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.567987e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.18316386e-07
Norm of the params: 9.153201
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079595946
Train loss (w/o reg) on all data: 0.06879127
Test loss (w/o reg) on all data: 0.060629297
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.7086281e-06
Norm of the params: 14.700119
              Random: fixed   0 labels. Loss 0.06063. Accuracy 0.981.
### Flips: 52, rs: 26, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.01205523
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6136221e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.0120552955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.262358e-07
Norm of the params: 9.153193
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226575
Train loss (w/o reg) on all data: 0.06152769
Test loss (w/o reg) on all data: 0.056635655
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.3577204e-06
Norm of the params: 14.654736
              Random: fixed   3 labels. Loss 0.05664. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.01205502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8089602e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729663
Test loss (w/o reg) on all data: 0.012054986
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5067297e-07
Norm of the params: 9.153198
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226576
Train loss (w/o reg) on all data: 0.061529152
Test loss (w/o reg) on all data: 0.05663884
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.14332e-06
Norm of the params: 14.65374
              Random: fixed   3 labels. Loss 0.05664. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.012055021
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8864604e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.012055081
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2218295e-07
Norm of the params: 9.15323
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226576
Train loss (w/o reg) on all data: 0.061526906
Test loss (w/o reg) on all data: 0.05663388
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.504125e-06
Norm of the params: 14.655274
              Random: fixed   3 labels. Loss 0.05663. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172925
Test loss (w/o reg) on all data: 0.012054833
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.513252e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729264
Test loss (w/o reg) on all data: 0.012054993
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.684222e-07
Norm of the params: 9.153242
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070223734
Train loss (w/o reg) on all data: 0.05961869
Test loss (w/o reg) on all data: 0.05621291
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.093547e-06
Norm of the params: 14.563683
              Random: fixed   4 labels. Loss 0.05621. Accuracy 0.981.
### Flips: 52, rs: 26, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728622
Test loss (w/o reg) on all data: 0.012054549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.797712e-07
Norm of the params: 9.153312
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728622
Test loss (w/o reg) on all data: 0.012054762
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7708577e-07
Norm of the params: 9.153311
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05895491
Train loss (w/o reg) on all data: 0.04827656
Test loss (w/o reg) on all data: 0.04703581
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8825871e-05
Norm of the params: 14.613934
              Random: fixed   8 labels. Loss 0.04704. Accuracy 0.977.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070414826
Train loss (w/o reg) on all data: 0.062218696
Test loss (w/o reg) on all data: 0.032471668
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.326698e-06
Norm of the params: 12.803225
Flipped loss: 0.03247. Accuracy: 0.992
### Flips: 52, rs: 27, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012055638
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.321322e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1281146e-07
Norm of the params: 9.153197
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06906162
Train loss (w/o reg) on all data: 0.061173216
Test loss (w/o reg) on all data: 0.030377308
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.06658235e-05
Norm of the params: 12.56058
              Random: fixed   1 labels. Loss 0.03038. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729148
Test loss (w/o reg) on all data: 0.012055085
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.19759e-07
Norm of the params: 9.153256
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172915
Test loss (w/o reg) on all data: 0.012055235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1859647e-07
Norm of the params: 9.153255
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06741366
Train loss (w/o reg) on all data: 0.0595335
Test loss (w/o reg) on all data: 0.028982345
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.935348e-06
Norm of the params: 12.5540085
              Random: fixed   2 labels. Loss 0.02898. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5954923e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3423606e-07
Norm of the params: 9.153184
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636372
Train loss (w/o reg) on all data: 0.055673264
Test loss (w/o reg) on all data: 0.026365906
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2557156e-06
Norm of the params: 12.620566
              Random: fixed   3 labels. Loss 0.02637. Accuracy 0.996.
### Flips: 52, rs: 27, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173058
Test loss (w/o reg) on all data: 0.0120560685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.495515e-07
Norm of the params: 9.1531
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730547
Test loss (w/o reg) on all data: 0.012056229
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1184467e-07
Norm of the params: 9.153102
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636372
Train loss (w/o reg) on all data: 0.055671662
Test loss (w/o reg) on all data: 0.02636557
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3109623e-06
Norm of the params: 12.621835
              Random: fixed   3 labels. Loss 0.02637. Accuracy 0.996.
### Flips: 52, rs: 27, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012056431
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7914326e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012056536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6588564e-07
Norm of the params: 9.153214
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055447996
Train loss (w/o reg) on all data: 0.04800437
Test loss (w/o reg) on all data: 0.027849656
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.562828e-06
Norm of the params: 12.201332
              Random: fixed   6 labels. Loss 0.02785. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012054946
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9445362e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.0120549165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2252981e-07
Norm of the params: 9.153203
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04571775
Train loss (w/o reg) on all data: 0.038146734
Test loss (w/o reg) on all data: 0.020962117
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1078743e-06
Norm of the params: 12.305297
              Random: fixed   9 labels. Loss 0.02096. Accuracy 0.996.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08328471
Train loss (w/o reg) on all data: 0.07497297
Test loss (w/o reg) on all data: 0.04283444
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4883427e-06
Norm of the params: 12.8932085
Flipped loss: 0.04283. Accuracy: 0.992
### Flips: 52, rs: 28, checks: 52
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729306
Test loss (w/o reg) on all data: 0.0120553775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3275022e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3083507e-07
Norm of the params: 9.153237
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08033608
Train loss (w/o reg) on all data: 0.071896926
Test loss (w/o reg) on all data: 0.041795846
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9198769e-05
Norm of the params: 12.9916525
              Random: fixed   1 labels. Loss 0.04180. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172949
Test loss (w/o reg) on all data: 0.012055585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8680876e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012055519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4409058e-07
Norm of the params: 9.153218
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0675814
Train loss (w/o reg) on all data: 0.059636313
Test loss (w/o reg) on all data: 0.042873647
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2016504e-05
Norm of the params: 12.60562
              Random: fixed   5 labels. Loss 0.04287. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.012055574
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2616135e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729912
Test loss (w/o reg) on all data: 0.012055037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.152669e-07
Norm of the params: 9.153172
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0675814
Train loss (w/o reg) on all data: 0.059633687
Test loss (w/o reg) on all data: 0.04287574
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.499811e-06
Norm of the params: 12.607707
              Random: fixed   5 labels. Loss 0.04288. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055632
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1405039e-07
Norm of the params: 9.153192
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2526375e-07
Norm of the params: 9.1531925
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061266266
Train loss (w/o reg) on all data: 0.05366634
Test loss (w/o reg) on all data: 0.0332432
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.703818e-06
Norm of the params: 12.328768
              Random: fixed   7 labels. Loss 0.03324. Accuracy 0.996.
### Flips: 52, rs: 28, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012055384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.771322e-07
Norm of the params: 9.153192
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012055453
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8688874e-07
Norm of the params: 9.1531925
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058186516
Train loss (w/o reg) on all data: 0.05048995
Test loss (w/o reg) on all data: 0.03406261
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9212852e-05
Norm of the params: 12.406904
              Random: fixed   8 labels. Loss 0.03406. Accuracy 0.996.
### Flips: 52, rs: 28, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172823
Test loss (w/o reg) on all data: 0.012055514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5421275e-07
Norm of the params: 9.153354
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728275
Test loss (w/o reg) on all data: 0.0120554315
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.995015e-07
Norm of the params: 9.15335
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05818651
Train loss (w/o reg) on all data: 0.05048718
Test loss (w/o reg) on all data: 0.03405475
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1312657e-05
Norm of the params: 12.409134
              Random: fixed   8 labels. Loss 0.03405. Accuracy 0.996.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07191625
Train loss (w/o reg) on all data: 0.06339826
Test loss (w/o reg) on all data: 0.036152273
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.3821923e-06
Norm of the params: 13.052197
Flipped loss: 0.03615. Accuracy: 0.996
### Flips: 52, rs: 29, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172905
Test loss (w/o reg) on all data: 0.012055499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7055027e-07
Norm of the params: 9.153265
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729073
Test loss (w/o reg) on all data: 0.012055452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4526897e-07
Norm of the params: 9.153264
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06978868
Train loss (w/o reg) on all data: 0.06100299
Test loss (w/o reg) on all data: 0.03628147
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3808205e-05
Norm of the params: 13.255707
              Random: fixed   1 labels. Loss 0.03628. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173158
Test loss (w/o reg) on all data: 0.012056547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5832218e-07
Norm of the params: 9.1529875
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731548
Test loss (w/o reg) on all data: 0.012056401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.929019e-07
Norm of the params: 9.152991
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06978868
Train loss (w/o reg) on all data: 0.061004814
Test loss (w/o reg) on all data: 0.036282368
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1383562e-05
Norm of the params: 13.25433
              Random: fixed   1 labels. Loss 0.03628. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729607
Test loss (w/o reg) on all data: 0.012055264
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.984005e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4078113e-07
Norm of the params: 9.153204
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0666966
Train loss (w/o reg) on all data: 0.05812943
Test loss (w/o reg) on all data: 0.03390046
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9520177e-06
Norm of the params: 13.089816
              Random: fixed   2 labels. Loss 0.03390. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012056127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.260683e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172944
Test loss (w/o reg) on all data: 0.012056221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6433798e-07
Norm of the params: 9.153223
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0610857
Train loss (w/o reg) on all data: 0.052693225
Test loss (w/o reg) on all data: 0.032014616
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.494992e-06
Norm of the params: 12.955677
              Random: fixed   4 labels. Loss 0.03201. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728827
Test loss (w/o reg) on all data: 0.012055417
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2117621e-07
Norm of the params: 9.153289
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728834
Test loss (w/o reg) on all data: 0.012055362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6641813e-07
Norm of the params: 9.153288
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061085694
Train loss (w/o reg) on all data: 0.052693684
Test loss (w/o reg) on all data: 0.0320127
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1834154e-06
Norm of the params: 12.955316
              Random: fixed   4 labels. Loss 0.03201. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012055358
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6367362e-07
Norm of the params: 9.1532545
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012055496
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.826919e-07
Norm of the params: 9.153228
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05434378
Train loss (w/o reg) on all data: 0.045591805
Test loss (w/o reg) on all data: 0.030985069
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8399473e-06
Norm of the params: 13.230249
              Random: fixed   6 labels. Loss 0.03099. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08841733
Train loss (w/o reg) on all data: 0.08005041
Test loss (w/o reg) on all data: 0.062706485
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.60237e-06
Norm of the params: 12.935933
Flipped loss: 0.06271. Accuracy: 0.985
### Flips: 52, rs: 30, checks: 52
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.0055395034
Test loss (w/o reg) on all data: 0.013846929
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.529778e-07
Norm of the params: 9.950006
     Influence (LOO): fixed  26 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2653047e-07
Norm of the params: 9.153196
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08841732
Train loss (w/o reg) on all data: 0.08005108
Test loss (w/o reg) on all data: 0.06269364
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5688201e-05
Norm of the params: 12.935412
              Random: fixed   0 labels. Loss 0.06269. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729663
Test loss (w/o reg) on all data: 0.012055364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.761816e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.01205532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.630032e-07
Norm of the params: 9.153197
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08841733
Train loss (w/o reg) on all data: 0.0800531
Test loss (w/o reg) on all data: 0.06268644
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.93248e-06
Norm of the params: 12.933856
              Random: fixed   0 labels. Loss 0.06269. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012056021
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3255714e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172985
Test loss (w/o reg) on all data: 0.01205595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8266083e-07
Norm of the params: 9.153177
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08704899
Train loss (w/o reg) on all data: 0.078916214
Test loss (w/o reg) on all data: 0.059604723
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6239082e-05
Norm of the params: 12.753646
              Random: fixed   1 labels. Loss 0.05960. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012055697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7334165e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729425
Test loss (w/o reg) on all data: 0.012055738
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0572807e-07
Norm of the params: 9.153226
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08704899
Train loss (w/o reg) on all data: 0.07891431
Test loss (w/o reg) on all data: 0.05958991
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.666351e-06
Norm of the params: 12.755146
              Random: fixed   1 labels. Loss 0.05959. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0911827e-07
Norm of the params: 9.153159
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.88456e-08
Norm of the params: 9.153159
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08130013
Train loss (w/o reg) on all data: 0.07291271
Test loss (w/o reg) on all data: 0.05753071
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1749902e-05
Norm of the params: 12.951776
              Random: fixed   3 labels. Loss 0.05753. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012054608
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01636054e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012054676
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.420863e-07
Norm of the params: 9.1531925
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073673055
Train loss (w/o reg) on all data: 0.065212205
Test loss (w/o reg) on all data: 0.054256335
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8396857e-05
Norm of the params: 13.008341
              Random: fixed   5 labels. Loss 0.05426. Accuracy 0.985.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669101
Train loss (w/o reg) on all data: 0.05680264
Test loss (w/o reg) on all data: 0.03975891
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.55396e-06
Norm of the params: 14.06298
Flipped loss: 0.03976. Accuracy: 0.989
### Flips: 52, rs: 31, checks: 52
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728685
Test loss (w/o reg) on all data: 0.01205519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5818145e-07
Norm of the params: 9.153306
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.00217287
Test loss (w/o reg) on all data: 0.012055036
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6045227e-07
Norm of the params: 9.153304
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669102
Train loss (w/o reg) on all data: 0.056800522
Test loss (w/o reg) on all data: 0.039752223
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.987579e-06
Norm of the params: 14.064492
              Random: fixed   0 labels. Loss 0.03975. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729828
Test loss (w/o reg) on all data: 0.012055473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3285425e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729835
Test loss (w/o reg) on all data: 0.0120553365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3924997e-07
Norm of the params: 9.153181
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065928645
Train loss (w/o reg) on all data: 0.05632784
Test loss (w/o reg) on all data: 0.035790037
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.580513e-06
Norm of the params: 13.856987
              Random: fixed   1 labels. Loss 0.03579. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729663
Test loss (w/o reg) on all data: 0.012054333
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.131373e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012054607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.633822e-07
Norm of the params: 9.153197
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06251109
Train loss (w/o reg) on all data: 0.052764047
Test loss (w/o reg) on all data: 0.035939846
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7443879e-06
Norm of the params: 13.962121
              Random: fixed   2 labels. Loss 0.03594. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172925
Test loss (w/o reg) on all data: 0.012055375
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6658175e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729257
Test loss (w/o reg) on all data: 0.012055471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0291213e-07
Norm of the params: 9.153243
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06039066
Train loss (w/o reg) on all data: 0.051224884
Test loss (w/o reg) on all data: 0.03735572
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2718565e-06
Norm of the params: 13.539406
              Random: fixed   3 labels. Loss 0.03736. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173023
Test loss (w/o reg) on all data: 0.012055365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0122519e-07
Norm of the params: 9.153137
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730224
Test loss (w/o reg) on all data: 0.012055428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5104686e-07
Norm of the params: 9.153137
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057521965
Train loss (w/o reg) on all data: 0.04842975
Test loss (w/o reg) on all data: 0.035571683
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.074459e-06
Norm of the params: 13.484966
              Random: fixed   4 labels. Loss 0.03557. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729947
Test loss (w/o reg) on all data: 0.012055385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4465571e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729954
Test loss (w/o reg) on all data: 0.012055338
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3275252e-07
Norm of the params: 9.153168
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04687595
Train loss (w/o reg) on all data: 0.037139162
Test loss (w/o reg) on all data: 0.027983867
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.436316e-06
Norm of the params: 13.954775
              Random: fixed   7 labels. Loss 0.02798. Accuracy 0.996.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068934724
Train loss (w/o reg) on all data: 0.060964357
Test loss (w/o reg) on all data: 0.03606229
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1170356e-06
Norm of the params: 12.62566
Flipped loss: 0.03606. Accuracy: 0.996
### Flips: 52, rs: 32, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021728391
Test loss (w/o reg) on all data: 0.012054093
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3244133e-06
Norm of the params: 9.153335
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728484
Test loss (w/o reg) on all data: 0.01205429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6585012e-07
Norm of the params: 9.153327
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06587608
Train loss (w/o reg) on all data: 0.057855353
Test loss (w/o reg) on all data: 0.035468355
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2708919e-06
Norm of the params: 12.66549
              Random: fixed   1 labels. Loss 0.03547. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054312
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.969821e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.01205436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8575446e-07
Norm of the params: 9.153174
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250859
Train loss (w/o reg) on all data: 0.05416823
Test loss (w/o reg) on all data: 0.037058458
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9227566e-06
Norm of the params: 12.915386
              Random: fixed   2 labels. Loss 0.03706. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.012054869
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9089637e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217299
Test loss (w/o reg) on all data: 0.012054821
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.626457e-08
Norm of the params: 9.153173
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250858
Train loss (w/o reg) on all data: 0.054167353
Test loss (w/o reg) on all data: 0.03706448
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3813285e-06
Norm of the params: 12.916062
              Random: fixed   2 labels. Loss 0.03706. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172847
Test loss (w/o reg) on all data: 0.012056266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5530865e-07
Norm of the params: 9.153328
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728484
Test loss (w/o reg) on all data: 0.0120561095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3691614e-07
Norm of the params: 9.153327
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250859
Train loss (w/o reg) on all data: 0.054167986
Test loss (w/o reg) on all data: 0.037063267
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.04189485e-05
Norm of the params: 12.915576
              Random: fixed   2 labels. Loss 0.03706. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728922
Test loss (w/o reg) on all data: 0.012055336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9177528e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728924
Test loss (w/o reg) on all data: 0.012055391
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1985778e-07
Norm of the params: 9.153278
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059900336
Train loss (w/o reg) on all data: 0.051737837
Test loss (w/o reg) on all data: 0.04028482
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.431363e-06
Norm of the params: 12.776931
              Random: fixed   3 labels. Loss 0.04028. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012054757
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8396523e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729558
Test loss (w/o reg) on all data: 0.012054674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.186901e-07
Norm of the params: 9.153213
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054237723
Train loss (w/o reg) on all data: 0.045987073
Test loss (w/o reg) on all data: 0.037325457
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.459356e-06
Norm of the params: 12.845738
              Random: fixed   5 labels. Loss 0.03733. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785226
Train loss (w/o reg) on all data: 0.06821129
Test loss (w/o reg) on all data: 0.04117853
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8386649e-05
Norm of the params: 13.885944
Flipped loss: 0.04118. Accuracy: 0.996
### Flips: 52, rs: 33, checks: 52
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3704139e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729884
Test loss (w/o reg) on all data: 0.012054805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.759196e-08
Norm of the params: 9.153175
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785225
Train loss (w/o reg) on all data: 0.06820716
Test loss (w/o reg) on all data: 0.04119095
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0513298e-05
Norm of the params: 13.888909
              Random: fixed   0 labels. Loss 0.04119. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 104
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730536
Test loss (w/o reg) on all data: 0.012054724
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.988552e-07
Norm of the params: 9.153105
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730512
Test loss (w/o reg) on all data: 0.012054835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2956614e-07
Norm of the params: 9.153106
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785225
Train loss (w/o reg) on all data: 0.06820648
Test loss (w/o reg) on all data: 0.041187495
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9896942e-06
Norm of the params: 13.889397
              Random: fixed   0 labels. Loss 0.04119. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172887
Test loss (w/o reg) on all data: 0.012054974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.525622e-07
Norm of the params: 9.153287
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728869
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8027742e-07
Norm of the params: 9.153286
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06989098
Train loss (w/o reg) on all data: 0.059433397
Test loss (w/o reg) on all data: 0.035418764
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.767012e-06
Norm of the params: 14.462078
              Random: fixed   2 labels. Loss 0.03542. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055255
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9860538e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729744
Test loss (w/o reg) on all data: 0.012055151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6756908e-07
Norm of the params: 9.153189
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06660523
Train loss (w/o reg) on all data: 0.056391124
Test loss (w/o reg) on all data: 0.030405546
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1824024e-05
Norm of the params: 14.292729
              Random: fixed   4 labels. Loss 0.03041. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730163
Test loss (w/o reg) on all data: 0.012054758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9600912e-07
Norm of the params: 9.153144
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730158
Test loss (w/o reg) on all data: 0.012054823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3914448e-07
Norm of the params: 9.153145
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0624638
Train loss (w/o reg) on all data: 0.05227501
Test loss (w/o reg) on all data: 0.027988387
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6024027e-06
Norm of the params: 14.275006
              Random: fixed   5 labels. Loss 0.02799. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 312
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.012055755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1214108e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012055674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.663559e-07
Norm of the params: 9.153184
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05298998
Train loss (w/o reg) on all data: 0.042930566
Test loss (w/o reg) on all data: 0.022484265
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3564702e-06
Norm of the params: 14.184084
              Random: fixed   8 labels. Loss 0.02248. Accuracy 0.996.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08631423
Train loss (w/o reg) on all data: 0.07769419
Test loss (w/o reg) on all data: 0.04378944
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5928139e-05
Norm of the params: 13.130148
Flipped loss: 0.04379. Accuracy: 0.992
### Flips: 52, rs: 34, checks: 52
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012055585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.048656e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012055703
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6708626e-07
Norm of the params: 9.153188
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08221688
Train loss (w/o reg) on all data: 0.07413038
Test loss (w/o reg) on all data: 0.04206466
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5343787e-05
Norm of the params: 12.717312
              Random: fixed   2 labels. Loss 0.04206. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729982
Test loss (w/o reg) on all data: 0.012054825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8859147e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172998
Test loss (w/o reg) on all data: 0.012054827
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3616101e-07
Norm of the params: 9.153165
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081814356
Train loss (w/o reg) on all data: 0.07402967
Test loss (w/o reg) on all data: 0.042477828
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.270886e-05
Norm of the params: 12.477731
              Random: fixed   3 labels. Loss 0.04248. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729888
Test loss (w/o reg) on all data: 0.012055391
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5926456e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729886
Test loss (w/o reg) on all data: 0.012055348
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0136734e-07
Norm of the params: 9.153174
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07519448
Train loss (w/o reg) on all data: 0.068193674
Test loss (w/o reg) on all data: 0.039274026
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0130595e-06
Norm of the params: 11.832841
              Random: fixed   5 labels. Loss 0.03927. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730615
Test loss (w/o reg) on all data: 0.012054846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3642075e-07
Norm of the params: 9.153095
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730599
Test loss (w/o reg) on all data: 0.012054776
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6952085e-07
Norm of the params: 9.153097
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06905203
Train loss (w/o reg) on all data: 0.062208887
Test loss (w/o reg) on all data: 0.035213523
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8601935e-06
Norm of the params: 11.698841
              Random: fixed   7 labels. Loss 0.03521. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729416
Test loss (w/o reg) on all data: 0.012055389
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5449943e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.01205545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0447566e-07
Norm of the params: 9.153226
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06905204
Train loss (w/o reg) on all data: 0.062209662
Test loss (w/o reg) on all data: 0.035209257
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3528736e-05
Norm of the params: 11.698186
              Random: fixed   7 labels. Loss 0.03521. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055173
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.912302e-08
Norm of the params: 9.153183
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012055241
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5414828e-07
Norm of the params: 9.153184
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06249782
Train loss (w/o reg) on all data: 0.055826508
Test loss (w/o reg) on all data: 0.034875575
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2884225e-06
Norm of the params: 11.551028
              Random: fixed   9 labels. Loss 0.03488. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056187835
Train loss (w/o reg) on all data: 0.04508361
Test loss (w/o reg) on all data: 0.032464318
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1765261e-05
Norm of the params: 14.9025
Flipped loss: 0.03246. Accuracy: 0.989
### Flips: 52, rs: 35, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055797
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.823617e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012055893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5994064e-07
Norm of the params: 9.1532
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05618783
Train loss (w/o reg) on all data: 0.04508311
Test loss (w/o reg) on all data: 0.032464866
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7505762e-06
Norm of the params: 14.902834
              Random: fixed   0 labels. Loss 0.03246. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731334
Test loss (w/o reg) on all data: 0.012056861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4790468e-07
Norm of the params: 9.153015
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021731304
Test loss (w/o reg) on all data: 0.012056917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.965559e-07
Norm of the params: 9.153019
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05311985
Train loss (w/o reg) on all data: 0.041741956
Test loss (w/o reg) on all data: 0.03409066
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.767619e-06
Norm of the params: 15.085021
              Random: fixed   1 labels. Loss 0.03409. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6218883e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.6582916e-08
Norm of the params: 9.153201
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05311985
Train loss (w/o reg) on all data: 0.041742146
Test loss (w/o reg) on all data: 0.034095347
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7634408e-06
Norm of the params: 15.084895
              Random: fixed   1 labels. Loss 0.03410. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055538
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.250828e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172992
Test loss (w/o reg) on all data: 0.012055552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0821624e-07
Norm of the params: 9.153172
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048900872
Train loss (w/o reg) on all data: 0.03789341
Test loss (w/o reg) on all data: 0.028592845
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.44369e-06
Norm of the params: 14.837426
              Random: fixed   2 labels. Loss 0.02859. Accuracy 0.992.
### Flips: 52, rs: 35, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012055737
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2745343e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012055784
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.195404e-07
Norm of the params: 9.153184
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048316605
Train loss (w/o reg) on all data: 0.037455227
Test loss (w/o reg) on all data: 0.024559267
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0573656e-06
Norm of the params: 14.738642
              Random: fixed   3 labels. Loss 0.02456. Accuracy 0.992.
### Flips: 52, rs: 35, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728924
Test loss (w/o reg) on all data: 0.012056165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7563814e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728969
Test loss (w/o reg) on all data: 0.0120559065
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0657196e-06
Norm of the params: 9.153275
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048316605
Train loss (w/o reg) on all data: 0.03745275
Test loss (w/o reg) on all data: 0.024558935
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.37012175e-05
Norm of the params: 14.740324
              Random: fixed   3 labels. Loss 0.02456. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10110177
Train loss (w/o reg) on all data: 0.09256748
Test loss (w/o reg) on all data: 0.0523361
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0780966e-05
Norm of the params: 13.064676
Flipped loss: 0.05234. Accuracy: 0.996
### Flips: 52, rs: 36, checks: 52
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011764765
Train loss (w/o reg) on all data: 0.006820357
Test loss (w/o reg) on all data: 0.017268822
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6058262e-07
Norm of the params: 9.944252
     Influence (LOO): fixed  31 labels. Loss 0.01727. Accuracy 0.989.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9914042e-07
Norm of the params: 9.153197
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10110176
Train loss (w/o reg) on all data: 0.09256777
Test loss (w/o reg) on all data: 0.05232328
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3729639e-05
Norm of the params: 13.064448
              Random: fixed   0 labels. Loss 0.05232. Accuracy 0.996.
### Flips: 52, rs: 36, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728731
Test loss (w/o reg) on all data: 0.012054868
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0331771e-07
Norm of the params: 9.153301
     Influence (LOO): fixed  35 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728803
Test loss (w/o reg) on all data: 0.012054891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6481458e-07
Norm of the params: 9.153293
                Loss: fixed  35 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090986274
Train loss (w/o reg) on all data: 0.08285333
Test loss (w/o reg) on all data: 0.047492653
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1786465e-05
Norm of the params: 12.75378
              Random: fixed   5 labels. Loss 0.04749. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012054983
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4648255e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  35 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012054919
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6747267e-07
Norm of the params: 9.153212
                Loss: fixed  35 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087969534
Train loss (w/o reg) on all data: 0.07980785
Test loss (w/o reg) on all data: 0.047988802
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.407751e-06
Norm of the params: 12.776296
              Random: fixed   6 labels. Loss 0.04799. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 208
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012055251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.346253e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.012055321
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0347373e-07
Norm of the params: 9.153185
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08101119
Train loss (w/o reg) on all data: 0.07258859
Test loss (w/o reg) on all data: 0.0487604
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4540494e-05
Norm of the params: 12.978904
              Random: fixed   9 labels. Loss 0.04876. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728405
Test loss (w/o reg) on all data: 0.012056719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.455757e-07
Norm of the params: 9.153337
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728438
Test loss (w/o reg) on all data: 0.012056534
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3312017e-07
Norm of the params: 9.153335
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07296604
Train loss (w/o reg) on all data: 0.06417165
Test loss (w/o reg) on all data: 0.050450694
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7368175e-06
Norm of the params: 13.262269
              Random: fixed  13 labels. Loss 0.05045. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730599
Test loss (w/o reg) on all data: 0.012055897
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3922413e-07
Norm of the params: 9.153097
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002173056
Test loss (w/o reg) on all data: 0.012055782
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6462308e-07
Norm of the params: 9.153099
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071930684
Train loss (w/o reg) on all data: 0.063264735
Test loss (w/o reg) on all data: 0.049260743
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.040998e-06
Norm of the params: 13.165065
              Random: fixed  14 labels. Loss 0.04926. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089151934
Train loss (w/o reg) on all data: 0.0808881
Test loss (w/o reg) on all data: 0.06561353
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1455846e-06
Norm of the params: 12.855996
Flipped loss: 0.06561. Accuracy: 0.981
### Flips: 52, rs: 37, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012055041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0095242e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.01205494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0388525e-07
Norm of the params: 9.153223
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08250558
Train loss (w/o reg) on all data: 0.073966235
Test loss (w/o reg) on all data: 0.06448309
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9255938e-05
Norm of the params: 13.0685425
              Random: fixed   3 labels. Loss 0.06448. Accuracy 0.981.
### Flips: 52, rs: 37, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012055407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.179656e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012055512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2563142e-07
Norm of the params: 9.153188
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08250557
Train loss (w/o reg) on all data: 0.073965855
Test loss (w/o reg) on all data: 0.06446839
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.4415794e-06
Norm of the params: 13.06883
              Random: fixed   3 labels. Loss 0.06447. Accuracy 0.981.
### Flips: 52, rs: 37, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217291
Test loss (w/o reg) on all data: 0.012055264
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0473023e-07
Norm of the params: 9.153259
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.00217291
Test loss (w/o reg) on all data: 0.01205532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2484203e-07
Norm of the params: 9.153257
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076730944
Train loss (w/o reg) on all data: 0.06831138
Test loss (w/o reg) on all data: 0.06395846
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.195529e-06
Norm of the params: 12.976569
              Random: fixed   5 labels. Loss 0.06396. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 208
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012054939
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.09624786e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012054903
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1259876e-07
Norm of the params: 9.153179
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07673093
Train loss (w/o reg) on all data: 0.068308964
Test loss (w/o reg) on all data: 0.06397004
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.975321e-06
Norm of the params: 12.978418
              Random: fixed   5 labels. Loss 0.06397. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729178
Test loss (w/o reg) on all data: 0.012055399
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.537956e-07
Norm of the params: 9.153253
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729178
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0347011e-07
Norm of the params: 9.153252
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07326784
Train loss (w/o reg) on all data: 0.064663336
Test loss (w/o reg) on all data: 0.064777374
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.5192633e-06
Norm of the params: 13.118311
              Random: fixed   6 labels. Loss 0.06478. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729963
Test loss (w/o reg) on all data: 0.012055329
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9086868e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012055378
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9581934e-07
Norm of the params: 9.153165
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06317682
Train loss (w/o reg) on all data: 0.055042017
Test loss (w/o reg) on all data: 0.051391356
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.442063e-06
Norm of the params: 12.755234
              Random: fixed  10 labels. Loss 0.05139. Accuracy 0.989.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061764084
Train loss (w/o reg) on all data: 0.054140188
Test loss (w/o reg) on all data: 0.033381075
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.93867e-06
Norm of the params: 12.348196
Flipped loss: 0.03338. Accuracy: 0.992
### Flips: 52, rs: 38, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729723
Test loss (w/o reg) on all data: 0.012054931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3115362e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012054988
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9309707e-07
Norm of the params: 9.153193
                Loss: fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057502035
Train loss (w/o reg) on all data: 0.049438402
Test loss (w/o reg) on all data: 0.034872234
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.123822e-06
Norm of the params: 12.699318
              Random: fixed   1 labels. Loss 0.03487. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173027
Test loss (w/o reg) on all data: 0.012055294
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.062928e-07
Norm of the params: 9.1531315
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730263
Test loss (w/o reg) on all data: 0.012055465
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.85157e-07
Norm of the params: 9.153132
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057502046
Train loss (w/o reg) on all data: 0.049438283
Test loss (w/o reg) on all data: 0.034871653
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5036825e-06
Norm of the params: 12.699419
              Random: fixed   1 labels. Loss 0.03487. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.012055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8279703e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012055092
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2400058e-07
Norm of the params: 9.153183
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052921623
Train loss (w/o reg) on all data: 0.044927124
Test loss (w/o reg) on all data: 0.0302892
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8935013e-06
Norm of the params: 12.64476
              Random: fixed   3 labels. Loss 0.03029. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012055337
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9424432e-07
Norm of the params: 9.153169
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.333272e-08
Norm of the params: 9.15317
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04424197
Train loss (w/o reg) on all data: 0.03682787
Test loss (w/o reg) on all data: 0.02592502
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0613307e-06
Norm of the params: 12.177109
              Random: fixed   6 labels. Loss 0.02593. Accuracy 0.996.
### Flips: 52, rs: 38, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.01205571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4114107e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.0120558385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8377555e-07
Norm of the params: 9.153243
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041106664
Train loss (w/o reg) on all data: 0.033622194
Test loss (w/o reg) on all data: 0.02347059
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1608048e-06
Norm of the params: 12.234761
              Random: fixed   7 labels. Loss 0.02347. Accuracy 0.996.
### Flips: 52, rs: 38, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729222
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2037312e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.012055023
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2524116e-07
Norm of the params: 9.153248
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04110667
Train loss (w/o reg) on all data: 0.033623464
Test loss (w/o reg) on all data: 0.02346901
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.581681e-06
Norm of the params: 12.233729
              Random: fixed   7 labels. Loss 0.02347. Accuracy 0.996.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08165273
Train loss (w/o reg) on all data: 0.07238371
Test loss (w/o reg) on all data: 0.039157115
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.365819e-05
Norm of the params: 13.615447
Flipped loss: 0.03916. Accuracy: 0.992
### Flips: 52, rs: 39, checks: 52
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00883691
Train loss (w/o reg) on all data: 0.0034140712
Test loss (w/o reg) on all data: 0.023944268
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.489001e-07
Norm of the params: 10.414258
     Influence (LOO): fixed  28 labels. Loss 0.02394. Accuracy 0.989.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008045951
Train loss (w/o reg) on all data: 0.0029450634
Test loss (w/o reg) on all data: 0.017279694
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0675751e-06
Norm of the params: 10.100384
                Loss: fixed  29 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07934717
Train loss (w/o reg) on all data: 0.07025374
Test loss (w/o reg) on all data: 0.036303964
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0752744e-06
Norm of the params: 13.485868
              Random: fixed   2 labels. Loss 0.03630. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730156
Test loss (w/o reg) on all data: 0.01205644
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7173015e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008045953
Train loss (w/o reg) on all data: 0.002945025
Test loss (w/o reg) on all data: 0.01727974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.381976e-07
Norm of the params: 10.100424
                Loss: fixed  29 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076747745
Train loss (w/o reg) on all data: 0.067546844
Test loss (w/o reg) on all data: 0.038588945
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.968369e-06
Norm of the params: 13.565323
              Random: fixed   3 labels. Loss 0.03859. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729965
Test loss (w/o reg) on all data: 0.012054938
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.426362e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729954
Test loss (w/o reg) on all data: 0.0120548885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.006215e-07
Norm of the params: 9.153166
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07469636
Train loss (w/o reg) on all data: 0.0653704
Test loss (w/o reg) on all data: 0.03994242
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4895318e-05
Norm of the params: 13.657203
              Random: fixed   4 labels. Loss 0.03994. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012055561
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9407221e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.012055726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.6663675e-07
Norm of the params: 9.153206
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07046908
Train loss (w/o reg) on all data: 0.061527826
Test loss (w/o reg) on all data: 0.0415266
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.287073e-06
Norm of the params: 13.372549
              Random: fixed   6 labels. Loss 0.04153. Accuracy 0.985.
### Flips: 52, rs: 39, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729276
Test loss (w/o reg) on all data: 0.012055159
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7525313e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729283
Test loss (w/o reg) on all data: 0.012055208
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6371106e-07
Norm of the params: 9.15324
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06867471
Train loss (w/o reg) on all data: 0.059726957
Test loss (w/o reg) on all data: 0.042260535
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5753588e-06
Norm of the params: 13.377407
              Random: fixed   7 labels. Loss 0.04226. Accuracy 0.985.
### Flips: 52, rs: 39, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.01205522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6656847e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.01205518
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6761433e-07
Norm of the params: 9.153207
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066082336
Train loss (w/o reg) on all data: 0.057348706
Test loss (w/o reg) on all data: 0.03910718
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7458746e-06
Norm of the params: 13.216375
              Random: fixed   8 labels. Loss 0.03911. Accuracy 0.985.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14590092
Train loss (w/o reg) on all data: 0.13804947
Test loss (w/o reg) on all data: 0.08505272
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.267114e-06
Norm of the params: 12.531124
Flipped loss: 0.08505. Accuracy: 0.973
### Flips: 104, rs: 0, checks: 52
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031033741
Train loss (w/o reg) on all data: 0.023287483
Test loss (w/o reg) on all data: 0.03775967
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3119936e-06
Norm of the params: 12.446894
     Influence (LOO): fixed  43 labels. Loss 0.03776. Accuracy 0.989.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010720149
Train loss (w/o reg) on all data: 0.0043412754
Test loss (w/o reg) on all data: 0.011772275
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4377412e-06
Norm of the params: 11.29502
                Loss: fixed  49 labels. Loss 0.01177. Accuracy 0.996.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13566081
Train loss (w/o reg) on all data: 0.12774646
Test loss (w/o reg) on all data: 0.07822836
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4730902e-05
Norm of the params: 12.581213
              Random: fixed   5 labels. Loss 0.07823. Accuracy 0.981.
### Flips: 104, rs: 0, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729048
Test loss (w/o reg) on all data: 0.012055031
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.943987e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729036
Test loss (w/o reg) on all data: 0.012054911
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.573435e-07
Norm of the params: 9.153267
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1246306
Train loss (w/o reg) on all data: 0.11666917
Test loss (w/o reg) on all data: 0.07541882
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6273174e-05
Norm of the params: 12.618581
              Random: fixed   9 labels. Loss 0.07542. Accuracy 0.977.
### Flips: 104, rs: 0, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729835
Test loss (w/o reg) on all data: 0.012054722
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2766444e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729842
Test loss (w/o reg) on all data: 0.012054867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6382903e-07
Norm of the params: 9.153179
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11543008
Train loss (w/o reg) on all data: 0.1077366
Test loss (w/o reg) on all data: 0.06621156
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8768903e-06
Norm of the params: 12.404416
              Random: fixed  13 labels. Loss 0.06621. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 208
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730326
Test loss (w/o reg) on all data: 0.012054819
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.144753e-07
Norm of the params: 9.153125
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.012054908
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8107834e-07
Norm of the params: 9.153127
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115430064
Train loss (w/o reg) on all data: 0.107740946
Test loss (w/o reg) on all data: 0.066208504
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8800955e-06
Norm of the params: 12.400905
              Random: fixed  13 labels. Loss 0.06621. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729534
Test loss (w/o reg) on all data: 0.012054989
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9538585e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.012055075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6386993e-07
Norm of the params: 9.153211
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11543009
Train loss (w/o reg) on all data: 0.10774322
Test loss (w/o reg) on all data: 0.066204086
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.10407e-06
Norm of the params: 12.399089
              Random: fixed  13 labels. Loss 0.06620. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.00217301
Test loss (w/o reg) on all data: 0.012055286
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8191883e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173011
Test loss (w/o reg) on all data: 0.012055157
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5310387e-07
Norm of the params: 9.15315
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11125165
Train loss (w/o reg) on all data: 0.10389103
Test loss (w/o reg) on all data: 0.06099926
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5121983e-05
Norm of the params: 12.133114
              Random: fixed  15 labels. Loss 0.06100. Accuracy 0.989.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1293398
Train loss (w/o reg) on all data: 0.12081708
Test loss (w/o reg) on all data: 0.06286078
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.008858e-06
Norm of the params: 13.055821
Flipped loss: 0.06286. Accuracy: 0.992
### Flips: 104, rs: 1, checks: 52
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035183147
Train loss (w/o reg) on all data: 0.026771303
Test loss (w/o reg) on all data: 0.01648575
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0600717e-06
Norm of the params: 12.970614
     Influence (LOO): fixed  35 labels. Loss 0.01649. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0090737585
Train loss (w/o reg) on all data: 0.0041188872
Test loss (w/o reg) on all data: 0.012110121
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.923804e-07
Norm of the params: 9.954769
                Loss: fixed  47 labels. Loss 0.01211. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122245826
Train loss (w/o reg) on all data: 0.11323005
Test loss (w/o reg) on all data: 0.06255514
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.552775e-06
Norm of the params: 13.428162
              Random: fixed   3 labels. Loss 0.06256. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009073757
Train loss (w/o reg) on all data: 0.0041190106
Test loss (w/o reg) on all data: 0.012107805
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1171917e-07
Norm of the params: 9.954644
     Influence (LOO): fixed  47 labels. Loss 0.01211. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012054598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2980787e-07
Norm of the params: 9.153165
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1213233
Train loss (w/o reg) on all data: 0.11241481
Test loss (w/o reg) on all data: 0.062167596
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.672994e-06
Norm of the params: 13.348029
              Random: fixed   4 labels. Loss 0.06217. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730717
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7052847e-06
Norm of the params: 9.153081
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730633
Test loss (w/o reg) on all data: 0.012055429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1821756e-07
Norm of the params: 9.153091
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11906983
Train loss (w/o reg) on all data: 0.110036895
Test loss (w/o reg) on all data: 0.061300445
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1071619e-05
Norm of the params: 13.440936
              Random: fixed   5 labels. Loss 0.06130. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6849053e-07
Norm of the params: 9.153222
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012054959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.048992e-08
Norm of the params: 9.153222
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114589915
Train loss (w/o reg) on all data: 0.10565538
Test loss (w/o reg) on all data: 0.059165563
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6219668e-05
Norm of the params: 13.367522
              Random: fixed   7 labels. Loss 0.05917. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0619055e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729476
Test loss (w/o reg) on all data: 0.012054968
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6610788e-07
Norm of the params: 9.153218
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11268004
Train loss (w/o reg) on all data: 0.103480704
Test loss (w/o reg) on all data: 0.062320504
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2872784e-05
Norm of the params: 13.564171
              Random: fixed   9 labels. Loss 0.06232. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729171
Test loss (w/o reg) on all data: 0.0120545905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5660083e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217292
Test loss (w/o reg) on all data: 0.012054741
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.288693e-07
Norm of the params: 9.153251
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108498715
Train loss (w/o reg) on all data: 0.0988657
Test loss (w/o reg) on all data: 0.061412737
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0529129e-05
Norm of the params: 13.880212
              Random: fixed  10 labels. Loss 0.06141. Accuracy 0.989.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1226226
Train loss (w/o reg) on all data: 0.11237436
Test loss (w/o reg) on all data: 0.067627996
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9738878e-05
Norm of the params: 14.316596
Flipped loss: 0.06763. Accuracy: 0.981
### Flips: 104, rs: 2, checks: 52
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031614233
Train loss (w/o reg) on all data: 0.024412572
Test loss (w/o reg) on all data: 0.024529014
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.193871e-05
Norm of the params: 12.001385
     Influence (LOO): fixed  38 labels. Loss 0.02453. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729078
Test loss (w/o reg) on all data: 0.012054926
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.21185e-07
Norm of the params: 9.153264
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11918792
Train loss (w/o reg) on all data: 0.108761236
Test loss (w/o reg) on all data: 0.06289127
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.4760975e-06
Norm of the params: 14.440695
              Random: fixed   1 labels. Loss 0.06289. Accuracy 0.989.
### Flips: 104, rs: 2, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729437
Test loss (w/o reg) on all data: 0.012055108
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5832735e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012055048
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0246729e-07
Norm of the params: 9.153224
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652274
Train loss (w/o reg) on all data: 0.10612116
Test loss (w/o reg) on all data: 0.05855923
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5921795e-05
Norm of the params: 14.423296
              Random: fixed   3 labels. Loss 0.05856. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012054917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7508398e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.012054873
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6699032e-07
Norm of the params: 9.153141
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652271
Train loss (w/o reg) on all data: 0.106119975
Test loss (w/o reg) on all data: 0.058570642
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3692123e-05
Norm of the params: 14.4241
              Random: fixed   3 labels. Loss 0.05857. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.012056187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7362487e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728957
Test loss (w/o reg) on all data: 0.01205604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2163842e-07
Norm of the params: 9.153276
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10913495
Train loss (w/o reg) on all data: 0.09881763
Test loss (w/o reg) on all data: 0.05486746
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.596699e-05
Norm of the params: 14.36476
              Random: fixed   6 labels. Loss 0.05487. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.012056343
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2159116e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.01205622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6576466e-07
Norm of the params: 9.153222
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10398323
Train loss (w/o reg) on all data: 0.09348545
Test loss (w/o reg) on all data: 0.05179251
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.131809e-06
Norm of the params: 14.489845
              Random: fixed   8 labels. Loss 0.05179. Accuracy 0.989.
### Flips: 104, rs: 2, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.01205516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8219352e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0459518e-06
Norm of the params: 9.153214
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09973086
Train loss (w/o reg) on all data: 0.089537084
Test loss (w/o reg) on all data: 0.04922432
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.257498e-05
Norm of the params: 14.278497
              Random: fixed  10 labels. Loss 0.04922. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13628507
Train loss (w/o reg) on all data: 0.12624988
Test loss (w/o reg) on all data: 0.103207596
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1216728e-05
Norm of the params: 14.166998
Flipped loss: 0.10321. Accuracy: 0.969
### Flips: 104, rs: 3, checks: 52
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025896499
Train loss (w/o reg) on all data: 0.018077014
Test loss (w/o reg) on all data: 0.026188774
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5735812e-06
Norm of the params: 12.505588
     Influence (LOO): fixed  43 labels. Loss 0.02619. Accuracy 0.989.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011310067
Train loss (w/o reg) on all data: 0.005112434
Test loss (w/o reg) on all data: 0.04138123
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5746066e-07
Norm of the params: 11.133403
                Loss: fixed  47 labels. Loss 0.04138. Accuracy 0.981.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1309793
Train loss (w/o reg) on all data: 0.12135293
Test loss (w/o reg) on all data: 0.08593064
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.422457e-06
Norm of the params: 13.875418
              Random: fixed   4 labels. Loss 0.08593. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 104
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007540385
Train loss (w/o reg) on all data: 0.0028312658
Test loss (w/o reg) on all data: 0.014728877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5511596e-07
Norm of the params: 9.704761
     Influence (LOO): fixed  51 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075403852
Train loss (w/o reg) on all data: 0.0028312686
Test loss (w/o reg) on all data: 0.014726831
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4793953e-07
Norm of the params: 9.704759
                Loss: fixed  51 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13097931
Train loss (w/o reg) on all data: 0.1213575
Test loss (w/o reg) on all data: 0.08588857
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5509234e-05
Norm of the params: 13.87214
              Random: fixed   4 labels. Loss 0.08589. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172918
Test loss (w/o reg) on all data: 0.012056082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6307615e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172919
Test loss (w/o reg) on all data: 0.0120562
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9133636e-07
Norm of the params: 9.15325
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12712342
Train loss (w/o reg) on all data: 0.117219046
Test loss (w/o reg) on all data: 0.08095579
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.52319735e-05
Norm of the params: 14.074348
              Random: fixed   6 labels. Loss 0.08096. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728973
Test loss (w/o reg) on all data: 0.01205629
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.814425e-07
Norm of the params: 9.153275
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728992
Test loss (w/o reg) on all data: 0.01205618
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2796037e-07
Norm of the params: 9.153272
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121271454
Train loss (w/o reg) on all data: 0.11164994
Test loss (w/o reg) on all data: 0.080135465
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.675025e-06
Norm of the params: 13.871923
              Random: fixed   9 labels. Loss 0.08014. Accuracy 0.973.
### Flips: 104, rs: 3, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729714
Test loss (w/o reg) on all data: 0.012055916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8971277e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012055811
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2908017e-07
Norm of the params: 9.153193
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10706283
Train loss (w/o reg) on all data: 0.09674885
Test loss (w/o reg) on all data: 0.07755265
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.297355e-06
Norm of the params: 14.36244
              Random: fixed  14 labels. Loss 0.07755. Accuracy 0.973.
### Flips: 104, rs: 3, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730955
Test loss (w/o reg) on all data: 0.0120563265
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.854296e-07
Norm of the params: 9.153058
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173092
Test loss (w/o reg) on all data: 0.012056235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2150805e-07
Norm of the params: 9.153061
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10180007
Train loss (w/o reg) on all data: 0.09235522
Test loss (w/o reg) on all data: 0.06731458
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.57325e-05
Norm of the params: 13.74398
              Random: fixed  18 labels. Loss 0.06731. Accuracy 0.981.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121032
Train loss (w/o reg) on all data: 0.112428956
Test loss (w/o reg) on all data: 0.066123985
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.428086e-05
Norm of the params: 13.117199
Flipped loss: 0.06612. Accuracy: 0.992
### Flips: 104, rs: 4, checks: 52
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016784286
Train loss (w/o reg) on all data: 0.010867211
Test loss (w/o reg) on all data: 0.01757117
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1222669e-06
Norm of the params: 10.878488
     Influence (LOO): fixed  37 labels. Loss 0.01757. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729886
Test loss (w/o reg) on all data: 0.0120555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2019635e-07
Norm of the params: 9.153173
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11662285
Train loss (w/o reg) on all data: 0.10825308
Test loss (w/o reg) on all data: 0.06602402
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2596576e-05
Norm of the params: 12.938139
              Random: fixed   2 labels. Loss 0.06602. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729064
Test loss (w/o reg) on all data: 0.012055007
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.704207e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.01205488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0873392e-07
Norm of the params: 9.153264
                Loss: fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11327223
Train loss (w/o reg) on all data: 0.10466685
Test loss (w/o reg) on all data: 0.06617227
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2540426e-05
Norm of the params: 13.118975
              Random: fixed   3 labels. Loss 0.06617. Accuracy 0.989.
### Flips: 104, rs: 4, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731802
Test loss (w/o reg) on all data: 0.0120559875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.567011e-07
Norm of the params: 9.152965
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731767
Test loss (w/o reg) on all data: 0.012056071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3296666e-07
Norm of the params: 9.152968
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11021973
Train loss (w/o reg) on all data: 0.101573996
Test loss (w/o reg) on all data: 0.06788875
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2099295e-05
Norm of the params: 13.149707
              Random: fixed   5 labels. Loss 0.06789. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729923
Test loss (w/o reg) on all data: 0.012055714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4035352e-07
Norm of the params: 9.153169
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.01205578
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9156718e-07
Norm of the params: 9.15317
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10748534
Train loss (w/o reg) on all data: 0.09878362
Test loss (w/o reg) on all data: 0.068156555
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1268594e-06
Norm of the params: 13.192212
              Random: fixed   6 labels. Loss 0.06816. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730203
Test loss (w/o reg) on all data: 0.012055191
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1532364e-07
Norm of the params: 9.15314
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012055236
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.100091e-07
Norm of the params: 9.153169
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098763466
Train loss (w/o reg) on all data: 0.08971063
Test loss (w/o reg) on all data: 0.06299144
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.625628e-06
Norm of the params: 13.455731
              Random: fixed  10 labels. Loss 0.06299. Accuracy 0.989.
### Flips: 104, rs: 4, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729525
Test loss (w/o reg) on all data: 0.012054461
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9970676e-08
Norm of the params: 9.1532135
     Influence (LOO): fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012054375
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3242198e-07
Norm of the params: 9.153206
                Loss: fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09349829
Train loss (w/o reg) on all data: 0.084410794
Test loss (w/o reg) on all data: 0.05903946
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.00711795e-05
Norm of the params: 13.481467
              Random: fixed  12 labels. Loss 0.05904. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13010761
Train loss (w/o reg) on all data: 0.12159444
Test loss (w/o reg) on all data: 0.07129227
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.409881e-05
Norm of the params: 13.04851
Flipped loss: 0.07129. Accuracy: 0.985
### Flips: 104, rs: 5, checks: 52
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020398378
Train loss (w/o reg) on all data: 0.013473078
Test loss (w/o reg) on all data: 0.016847257
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1615987e-06
Norm of the params: 11.768857
     Influence (LOO): fixed  39 labels. Loss 0.01685. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012054755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1305096e-07
Norm of the params: 9.153184
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12080024
Train loss (w/o reg) on all data: 0.11224709
Test loss (w/o reg) on all data: 0.06379123
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.637887e-05
Norm of the params: 13.079106
              Random: fixed   3 labels. Loss 0.06379. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173037
Test loss (w/o reg) on all data: 0.012054931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3231914e-07
Norm of the params: 9.15312
     Influence (LOO): fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173036
Test loss (w/o reg) on all data: 0.012054881
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5452275e-07
Norm of the params: 9.153123
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115329936
Train loss (w/o reg) on all data: 0.106705934
Test loss (w/o reg) on all data: 0.057059787
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3974019e-05
Norm of the params: 13.133165
              Random: fixed   5 labels. Loss 0.05706. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.305914e-08
Norm of the params: 9.153198
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.01205513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.200681e-07
Norm of the params: 9.153198
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11299543
Train loss (w/o reg) on all data: 0.10469092
Test loss (w/o reg) on all data: 0.055923328
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5864253e-06
Norm of the params: 12.887605
              Random: fixed   7 labels. Loss 0.05592. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172896
Test loss (w/o reg) on all data: 0.012055148
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.851326e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728969
Test loss (w/o reg) on all data: 0.01205522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.244026e-07
Norm of the params: 9.153275
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104981005
Train loss (w/o reg) on all data: 0.09603169
Test loss (w/o reg) on all data: 0.053365394
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.839141e-05
Norm of the params: 13.378577
              Random: fixed  10 labels. Loss 0.05337. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055433
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1122354e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012055367
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3754929e-07
Norm of the params: 9.153191
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103246905
Train loss (w/o reg) on all data: 0.094565235
Test loss (w/o reg) on all data: 0.05467339
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.488487e-05
Norm of the params: 13.177002
              Random: fixed  11 labels. Loss 0.05467. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730096
Test loss (w/o reg) on all data: 0.012054734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0614452e-06
Norm of the params: 9.1531515
     Influence (LOO): fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730082
Test loss (w/o reg) on all data: 0.012054935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1343926e-07
Norm of the params: 9.1531515
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103246905
Train loss (w/o reg) on all data: 0.094569474
Test loss (w/o reg) on all data: 0.054681145
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9542003e-05
Norm of the params: 13.173783
              Random: fixed  11 labels. Loss 0.05468. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13748072
Train loss (w/o reg) on all data: 0.12922636
Test loss (w/o reg) on all data: 0.063254625
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.831398e-06
Norm of the params: 12.848628
Flipped loss: 0.06325. Accuracy: 0.992
### Flips: 104, rs: 6, checks: 52
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039617978
Train loss (w/o reg) on all data: 0.03191362
Test loss (w/o reg) on all data: 0.028911572
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4937727e-06
Norm of the params: 12.413185
     Influence (LOO): fixed  38 labels. Loss 0.02891. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010785196
Train loss (w/o reg) on all data: 0.0043966384
Test loss (w/o reg) on all data: 0.015505411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6315146e-07
Norm of the params: 11.303591
                Loss: fixed  48 labels. Loss 0.01551. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13038142
Train loss (w/o reg) on all data: 0.122198395
Test loss (w/o reg) on all data: 0.062325165
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3291224e-05
Norm of the params: 12.792983
              Random: fixed   3 labels. Loss 0.06233. Accuracy 0.985.
### Flips: 104, rs: 6, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172973
Test loss (w/o reg) on all data: 0.012054953
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.43762e-08
Norm of the params: 9.1531925
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054939
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5640473e-08
Norm of the params: 9.153192
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1270338
Train loss (w/o reg) on all data: 0.11863737
Test loss (w/o reg) on all data: 0.060465172
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.973132e-06
Norm of the params: 12.95873
              Random: fixed   4 labels. Loss 0.06047. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730077
Test loss (w/o reg) on all data: 0.01205502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9610047e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173007
Test loss (w/o reg) on all data: 0.012054993
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.881171e-08
Norm of the params: 9.153155
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12550537
Train loss (w/o reg) on all data: 0.117238276
Test loss (w/o reg) on all data: 0.058985937
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3247909e-05
Norm of the params: 12.85854
              Random: fixed   5 labels. Loss 0.05899. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729642
Test loss (w/o reg) on all data: 0.01205523
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8602746e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.0120551875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5733731e-07
Norm of the params: 9.153199
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11972199
Train loss (w/o reg) on all data: 0.11092117
Test loss (w/o reg) on all data: 0.057546478
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.75573e-06
Norm of the params: 13.267117
              Random: fixed   7 labels. Loss 0.05755. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172849
Test loss (w/o reg) on all data: 0.01205501
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.830984e-07
Norm of the params: 9.153329
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728491
Test loss (w/o reg) on all data: 0.012055104
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9685152e-07
Norm of the params: 9.153327
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116505
Train loss (w/o reg) on all data: 0.10783857
Test loss (w/o reg) on all data: 0.05411663
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.78077e-06
Norm of the params: 13.165431
              Random: fixed   9 labels. Loss 0.05412. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012055494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7973553e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012055436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8086432e-07
Norm of the params: 9.153194
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1138168
Train loss (w/o reg) on all data: 0.10480395
Test loss (w/o reg) on all data: 0.05387492
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.374193e-06
Norm of the params: 13.42598
              Random: fixed  10 labels. Loss 0.05387. Accuracy 0.989.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13511148
Train loss (w/o reg) on all data: 0.12715182
Test loss (w/o reg) on all data: 0.08240837
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2453363e-06
Norm of the params: 12.6171875
Flipped loss: 0.08241. Accuracy: 0.989
### Flips: 104, rs: 7, checks: 52
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02812077
Train loss (w/o reg) on all data: 0.020879645
Test loss (w/o reg) on all data: 0.03894701
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.469057e-06
Norm of the params: 12.034222
     Influence (LOO): fixed  42 labels. Loss 0.03895. Accuracy 0.985.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009838759
Train loss (w/o reg) on all data: 0.0038818175
Test loss (w/o reg) on all data: 0.019095132
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2302952e-07
Norm of the params: 10.915073
                Loss: fixed  48 labels. Loss 0.01910. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13314933
Train loss (w/o reg) on all data: 0.12509613
Test loss (w/o reg) on all data: 0.08062034
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4020535e-05
Norm of the params: 12.6911
              Random: fixed   1 labels. Loss 0.08062. Accuracy 0.989.
### Flips: 104, rs: 7, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012055549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9048665e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729378
Test loss (w/o reg) on all data: 0.012055592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2882013e-07
Norm of the params: 9.15323
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12529553
Train loss (w/o reg) on all data: 0.116931945
Test loss (w/o reg) on all data: 0.076926455
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.723875e-06
Norm of the params: 12.933361
              Random: fixed   4 labels. Loss 0.07693. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172989
Test loss (w/o reg) on all data: 0.012054658
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7797294e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729881
Test loss (w/o reg) on all data: 0.012054631
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.138701e-08
Norm of the params: 9.153174
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123599485
Train loss (w/o reg) on all data: 0.115324214
Test loss (w/o reg) on all data: 0.07439324
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1723783e-06
Norm of the params: 12.86489
              Random: fixed   5 labels. Loss 0.07439. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728994
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3506457e-07
Norm of the params: 9.153273
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729069
Test loss (w/o reg) on all data: 0.012055181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8260284e-07
Norm of the params: 9.153264
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1168706
Train loss (w/o reg) on all data: 0.1081665
Test loss (w/o reg) on all data: 0.07329801
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3041879e-05
Norm of the params: 13.194014
              Random: fixed   8 labels. Loss 0.07330. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729397
Test loss (w/o reg) on all data: 0.012055881
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.690263e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.01205573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7140663e-07
Norm of the params: 9.153227
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11104182
Train loss (w/o reg) on all data: 0.10245578
Test loss (w/o reg) on all data: 0.06847629
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4848667e-05
Norm of the params: 13.104228
              Random: fixed  11 labels. Loss 0.06848. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729504
Test loss (w/o reg) on all data: 0.012054881
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6387085e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012054833
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2597792e-07
Norm of the params: 9.153215
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09846371
Train loss (w/o reg) on all data: 0.08923909
Test loss (w/o reg) on all data: 0.06904949
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8180197e-05
Norm of the params: 13.582795
              Random: fixed  16 labels. Loss 0.06905. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1383766
Train loss (w/o reg) on all data: 0.13034736
Test loss (w/o reg) on all data: 0.061621863
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3904374e-05
Norm of the params: 12.672209
Flipped loss: 0.06162. Accuracy: 0.992
### Flips: 104, rs: 8, checks: 52
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027750177
Train loss (w/o reg) on all data: 0.021270968
Test loss (w/o reg) on all data: 0.018713892
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4599758e-06
Norm of the params: 11.383505
     Influence (LOO): fixed  42 labels. Loss 0.01871. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730433
Test loss (w/o reg) on all data: 0.0120554585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.662158e-07
Norm of the params: 9.153115
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13551585
Train loss (w/o reg) on all data: 0.12732188
Test loss (w/o reg) on all data: 0.062144287
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5414989e-05
Norm of the params: 12.80154
              Random: fixed   1 labels. Loss 0.06214. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172911
Test loss (w/o reg) on all data: 0.012055395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8198934e-07
Norm of the params: 9.153258
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.012055592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.745293e-07
Norm of the params: 9.153246
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13226387
Train loss (w/o reg) on all data: 0.12431784
Test loss (w/o reg) on all data: 0.05721451
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.349489e-05
Norm of the params: 12.606373
              Random: fixed   3 labels. Loss 0.05721. Accuracy 0.996.
### Flips: 104, rs: 8, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.012054844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4007094e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172967
Test loss (w/o reg) on all data: 0.012054901
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6922205e-07
Norm of the params: 9.153197
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12260875
Train loss (w/o reg) on all data: 0.11432247
Test loss (w/o reg) on all data: 0.0567837
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3402031e-05
Norm of the params: 12.873449
              Random: fixed   6 labels. Loss 0.05678. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.012054958
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2528477e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012054934
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.501358e-08
Norm of the params: 9.153223
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120584995
Train loss (w/o reg) on all data: 0.11245867
Test loss (w/o reg) on all data: 0.056990955
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.992243e-06
Norm of the params: 12.748589
              Random: fixed   7 labels. Loss 0.05699. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012055209
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8832852e-07
Norm of the params: 9.153259
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012055156
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4314342e-07
Norm of the params: 9.153258
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11202605
Train loss (w/o reg) on all data: 0.103098504
Test loss (w/o reg) on all data: 0.053437177
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.729152e-06
Norm of the params: 13.362293
              Random: fixed  10 labels. Loss 0.05344. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.012055476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.868796e-07
Norm of the params: 9.153101
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730529
Test loss (w/o reg) on all data: 0.012055517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9726363e-07
Norm of the params: 9.153104
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100046724
Train loss (w/o reg) on all data: 0.090628974
Test loss (w/o reg) on all data: 0.051448066
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3445387e-05
Norm of the params: 13.72425
              Random: fixed  14 labels. Loss 0.05145. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15861525
Train loss (w/o reg) on all data: 0.15144713
Test loss (w/o reg) on all data: 0.08506398
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5605575e-05
Norm of the params: 11.973398
Flipped loss: 0.08506. Accuracy: 0.989
### Flips: 104, rs: 9, checks: 52
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05271879
Train loss (w/o reg) on all data: 0.045370173
Test loss (w/o reg) on all data: 0.03515191
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6363683e-06
Norm of the params: 12.123214
     Influence (LOO): fixed  44 labels. Loss 0.03515. Accuracy 0.989.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020214614
Train loss (w/o reg) on all data: 0.0112572005
Test loss (w/o reg) on all data: 0.02470882
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0238089e-05
Norm of the params: 13.384628
                Loss: fixed  52 labels. Loss 0.02471. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1567775
Train loss (w/o reg) on all data: 0.14958628
Test loss (w/o reg) on all data: 0.080970325
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.3171296e-06
Norm of the params: 11.9926815
              Random: fixed   1 labels. Loss 0.08097. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 104
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012194209
Train loss (w/o reg) on all data: 0.0068053603
Test loss (w/o reg) on all data: 0.0141430935
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.191238e-07
Norm of the params: 10.381569
     Influence (LOO): fixed  59 labels. Loss 0.01414. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173044
Test loss (w/o reg) on all data: 0.012054561
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.002048e-07
Norm of the params: 9.153113
                Loss: fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14766914
Train loss (w/o reg) on all data: 0.13975431
Test loss (w/o reg) on all data: 0.07713599
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0255502e-05
Norm of the params: 12.581589
              Random: fixed   4 labels. Loss 0.07714. Accuracy 0.985.
### Flips: 104, rs: 9, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728666
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.614578e-07
Norm of the params: 9.153308
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728668
Test loss (w/o reg) on all data: 0.012055039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.492567e-07
Norm of the params: 9.153306
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14476001
Train loss (w/o reg) on all data: 0.13683537
Test loss (w/o reg) on all data: 0.07435938
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2063994e-05
Norm of the params: 12.5893955
              Random: fixed   6 labels. Loss 0.07436. Accuracy 0.992.
### Flips: 104, rs: 9, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.01205526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5722149e-07
Norm of the params: 9.153199
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012055211
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8806698e-07
Norm of the params: 9.1532
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13843384
Train loss (w/o reg) on all data: 0.12991601
Test loss (w/o reg) on all data: 0.072263345
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5792273e-05
Norm of the params: 13.052078
              Random: fixed   9 labels. Loss 0.07226. Accuracy 0.992.
### Flips: 104, rs: 9, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.0120549565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0303192e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012054933
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.49269e-08
Norm of the params: 9.15319
                Loss: fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13702343
Train loss (w/o reg) on all data: 0.12832442
Test loss (w/o reg) on all data: 0.07160352
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9366045e-05
Norm of the params: 13.190162
              Random: fixed  10 labels. Loss 0.07160. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012054879
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.419719e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730121
Test loss (w/o reg) on all data: 0.012054745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1808028e-07
Norm of the params: 9.153149
                Loss: fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13164255
Train loss (w/o reg) on all data: 0.12336566
Test loss (w/o reg) on all data: 0.06941807
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1223339e-05
Norm of the params: 12.866146
              Random: fixed  13 labels. Loss 0.06942. Accuracy 0.985.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13397911
Train loss (w/o reg) on all data: 0.12505846
Test loss (w/o reg) on all data: 0.08177479
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.200069e-05
Norm of the params: 13.357135
Flipped loss: 0.08177. Accuracy: 0.977
### Flips: 104, rs: 10, checks: 52
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028101407
Train loss (w/o reg) on all data: 0.020739729
Test loss (w/o reg) on all data: 0.030977031
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1643051e-06
Norm of the params: 12.1339855
     Influence (LOO): fixed  43 labels. Loss 0.03098. Accuracy 0.985.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0143326
Train loss (w/o reg) on all data: 0.0061861994
Test loss (w/o reg) on all data: 0.0158407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.30543e-06
Norm of the params: 12.764326
                Loss: fixed  47 labels. Loss 0.01584. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1281621
Train loss (w/o reg) on all data: 0.11935342
Test loss (w/o reg) on all data: 0.078555085
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.8167014e-06
Norm of the params: 13.273038
              Random: fixed   3 labels. Loss 0.07856. Accuracy 0.977.
### Flips: 104, rs: 10, checks: 104
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011148351
Train loss (w/o reg) on all data: 0.005745463
Test loss (w/o reg) on all data: 0.015266025
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9532143e-07
Norm of the params: 10.395084
     Influence (LOO): fixed  51 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730633
Test loss (w/o reg) on all data: 0.012055277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.072113e-07
Norm of the params: 9.153093
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12441971
Train loss (w/o reg) on all data: 0.11556785
Test loss (w/o reg) on all data: 0.06893807
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6127593e-05
Norm of the params: 13.305534
              Random: fixed   6 labels. Loss 0.06894. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172872
Test loss (w/o reg) on all data: 0.012054741
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1456859e-07
Norm of the params: 9.153301
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172877
Test loss (w/o reg) on all data: 0.012054852
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.557477e-07
Norm of the params: 9.153296
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12441971
Train loss (w/o reg) on all data: 0.11556746
Test loss (w/o reg) on all data: 0.06894199
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.239423e-06
Norm of the params: 13.305828
              Random: fixed   6 labels. Loss 0.06894. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012054431
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.983367e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.012054435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8525724e-07
Norm of the params: 9.153246
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11814253
Train loss (w/o reg) on all data: 0.10931491
Test loss (w/o reg) on all data: 0.062010642
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.907582e-06
Norm of the params: 13.287304
              Random: fixed  10 labels. Loss 0.06201. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 260
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012054739
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8476658e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012054793
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.812864e-07
Norm of the params: 9.153195
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11255852
Train loss (w/o reg) on all data: 0.10404856
Test loss (w/o reg) on all data: 0.059637718
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.571314e-05
Norm of the params: 13.046046
              Random: fixed  13 labels. Loss 0.05964. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172815
Test loss (w/o reg) on all data: 0.012054468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0002042e-07
Norm of the params: 9.153365
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728177
Test loss (w/o reg) on all data: 0.012054617
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.431916e-07
Norm of the params: 9.153359
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10964699
Train loss (w/o reg) on all data: 0.1010628
Test loss (w/o reg) on all data: 0.057457045
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1801844e-05
Norm of the params: 13.10282
              Random: fixed  14 labels. Loss 0.05746. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13957998
Train loss (w/o reg) on all data: 0.13237025
Test loss (w/o reg) on all data: 0.05993953
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.2332305e-06
Norm of the params: 12.008112
Flipped loss: 0.05994. Accuracy: 0.996
### Flips: 104, rs: 11, checks: 52
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033772334
Train loss (w/o reg) on all data: 0.02497351
Test loss (w/o reg) on all data: 0.014381252
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2424442e-06
Norm of the params: 13.2656145
     Influence (LOO): fixed  39 labels. Loss 0.01438. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071804463
Train loss (w/o reg) on all data: 0.0026112841
Test loss (w/o reg) on all data: 0.011524881
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.292522e-08
Norm of the params: 9.559459
                Loss: fixed  49 labels. Loss 0.01152. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162783
Train loss (w/o reg) on all data: 0.124906935
Test loss (w/o reg) on all data: 0.057908308
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.639551e-05
Norm of the params: 11.593879
              Random: fixed   5 labels. Loss 0.05791. Accuracy 0.996.
### Flips: 104, rs: 11, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012054865
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.42351e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5488925e-07
Norm of the params: 9.153232
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12544383
Train loss (w/o reg) on all data: 0.118668616
Test loss (w/o reg) on all data: 0.05600034
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.641067e-06
Norm of the params: 11.640634
              Random: fixed   7 labels. Loss 0.05600. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173012
Test loss (w/o reg) on all data: 0.012054677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.793861e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730121
Test loss (w/o reg) on all data: 0.012054874
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3540258e-07
Norm of the params: 9.153148
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12041125
Train loss (w/o reg) on all data: 0.11354565
Test loss (w/o reg) on all data: 0.053678513
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.776553e-06
Norm of the params: 11.71802
              Random: fixed  10 labels. Loss 0.05368. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730012
Test loss (w/o reg) on all data: 0.012055403
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4381133e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.012055316
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5119027e-07
Norm of the params: 9.153163
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118631646
Train loss (w/o reg) on all data: 0.11165859
Test loss (w/o reg) on all data: 0.053982027
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6793014e-05
Norm of the params: 11.809369
              Random: fixed  11 labels. Loss 0.05398. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012057051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.160599e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012056913
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4807085e-07
Norm of the params: 9.15321
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11280262
Train loss (w/o reg) on all data: 0.10575071
Test loss (w/o reg) on all data: 0.051302064
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.582923e-05
Norm of the params: 11.87595
              Random: fixed  13 labels. Loss 0.05130. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729171
Test loss (w/o reg) on all data: 0.012054801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9833942e-07
Norm of the params: 9.153253
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729174
Test loss (w/o reg) on all data: 0.01205488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6521548e-07
Norm of the params: 9.153253
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103252016
Train loss (w/o reg) on all data: 0.095499024
Test loss (w/o reg) on all data: 0.046460614
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5978545e-05
Norm of the params: 12.452304
              Random: fixed  17 labels. Loss 0.04646. Accuracy 0.989.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14654757
Train loss (w/o reg) on all data: 0.13855937
Test loss (w/o reg) on all data: 0.098507084
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0754534e-05
Norm of the params: 12.639782
Flipped loss: 0.09851. Accuracy: 0.969
### Flips: 104, rs: 12, checks: 52
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04196124
Train loss (w/o reg) on all data: 0.032067627
Test loss (w/o reg) on all data: 0.059092913
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6028597e-06
Norm of the params: 14.066708
     Influence (LOO): fixed  41 labels. Loss 0.05909. Accuracy 0.985.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019682031
Train loss (w/o reg) on all data: 0.010555018
Test loss (w/o reg) on all data: 0.030312557
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.9230747e-07
Norm of the params: 13.510746
                Loss: fixed  49 labels. Loss 0.03031. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14017016
Train loss (w/o reg) on all data: 0.13236594
Test loss (w/o reg) on all data: 0.09812576
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3893449e-05
Norm of the params: 12.493376
              Random: fixed   4 labels. Loss 0.09813. Accuracy 0.973.
### Flips: 104, rs: 12, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009240154
Train loss (w/o reg) on all data: 0.0036347404
Test loss (w/o reg) on all data: 0.012737364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1048089e-07
Norm of the params: 10.5881195
     Influence (LOO): fixed  56 labels. Loss 0.01274. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075122174
Train loss (w/o reg) on all data: 0.0027579742
Test loss (w/o reg) on all data: 0.013031476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2531488e-07
Norm of the params: 9.751146
                Loss: fixed  57 labels. Loss 0.01303. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13811408
Train loss (w/o reg) on all data: 0.1306324
Test loss (w/o reg) on all data: 0.097916745
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.449349e-05
Norm of the params: 12.232486
              Random: fixed   6 labels. Loss 0.09792. Accuracy 0.977.
### Flips: 104, rs: 12, checks: 156
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008591903
Train loss (w/o reg) on all data: 0.0033475745
Test loss (w/o reg) on all data: 0.011985129
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0199152e-06
Norm of the params: 10.241415
     Influence (LOO): fixed  57 labels. Loss 0.01199. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730454
Test loss (w/o reg) on all data: 0.012055517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.146199e-07
Norm of the params: 9.153111
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13146907
Train loss (w/o reg) on all data: 0.123651646
Test loss (w/o reg) on all data: 0.09218806
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2375452e-05
Norm of the params: 12.503941
              Random: fixed   9 labels. Loss 0.09219. Accuracy 0.977.
### Flips: 104, rs: 12, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173078
Test loss (w/o reg) on all data: 0.012055364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.747712e-07
Norm of the params: 9.153076
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173078
Test loss (w/o reg) on all data: 0.012055257
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5573573e-07
Norm of the params: 9.153077
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11794386
Train loss (w/o reg) on all data: 0.110126205
Test loss (w/o reg) on all data: 0.08224142
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8953957e-05
Norm of the params: 12.504124
              Random: fixed  16 labels. Loss 0.08224. Accuracy 0.981.
### Flips: 104, rs: 12, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730778
Test loss (w/o reg) on all data: 0.012055351
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8337396e-07
Norm of the params: 9.153077
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730748
Test loss (w/o reg) on all data: 0.012055281
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7144379e-07
Norm of the params: 9.153081
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11037492
Train loss (w/o reg) on all data: 0.10283108
Test loss (w/o reg) on all data: 0.06703104
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.861121e-06
Norm of the params: 12.28319
              Random: fixed  20 labels. Loss 0.06703. Accuracy 0.985.
### Flips: 104, rs: 12, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012055181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.684915e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055241
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2727806e-07
Norm of the params: 9.153189
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10247832
Train loss (w/o reg) on all data: 0.09443678
Test loss (w/o reg) on all data: 0.06858381
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.001419e-05
Norm of the params: 12.681905
              Random: fixed  23 labels. Loss 0.06858. Accuracy 0.981.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11927919
Train loss (w/o reg) on all data: 0.11094265
Test loss (w/o reg) on all data: 0.052356277
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9357622e-05
Norm of the params: 12.912434
Flipped loss: 0.05236. Accuracy: 0.996
### Flips: 104, rs: 13, checks: 52
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018503264
Train loss (w/o reg) on all data: 0.012362906
Test loss (w/o reg) on all data: 0.014418359
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4359076e-06
Norm of the params: 11.0818405
     Influence (LOO): fixed  37 labels. Loss 0.01442. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.205535e-08
Norm of the params: 9.153203
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11927918
Train loss (w/o reg) on all data: 0.110939704
Test loss (w/o reg) on all data: 0.052358408
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5336032e-05
Norm of the params: 12.914706
              Random: fixed   0 labels. Loss 0.05236. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728824
Test loss (w/o reg) on all data: 0.012055417
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.725391e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728831
Test loss (w/o reg) on all data: 0.012055336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4192199e-07
Norm of the params: 9.153289
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115387864
Train loss (w/o reg) on all data: 0.106896296
Test loss (w/o reg) on all data: 0.050603494
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.005007e-06
Norm of the params: 13.031937
              Random: fixed   2 labels. Loss 0.05060. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729525
Test loss (w/o reg) on all data: 0.012056222
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.518241e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012056039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3446048e-07
Norm of the params: 9.1532135
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11468022
Train loss (w/o reg) on all data: 0.1061727
Test loss (w/o reg) on all data: 0.052292787
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.2673234e-06
Norm of the params: 13.044173
              Random: fixed   3 labels. Loss 0.05229. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012055712
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9486566e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729881
Test loss (w/o reg) on all data: 0.012055783
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2714909e-07
Norm of the params: 9.153174
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108033895
Train loss (w/o reg) on all data: 0.100090936
Test loss (w/o reg) on all data: 0.047083933
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0755754e-05
Norm of the params: 12.603932
              Random: fixed   6 labels. Loss 0.04708. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172894
Test loss (w/o reg) on all data: 0.012054686
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.52345e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.012054654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1261757e-07
Norm of the params: 9.153276
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10501719
Train loss (w/o reg) on all data: 0.09736152
Test loss (w/o reg) on all data: 0.04649507
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.975848e-06
Norm of the params: 12.373904
              Random: fixed   7 labels. Loss 0.04650. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.0120562585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4232664e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012056095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8980754e-07
Norm of the params: 9.153247
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098133095
Train loss (w/o reg) on all data: 0.09062785
Test loss (w/o reg) on all data: 0.043844704
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0773827e-05
Norm of the params: 12.251731
              Random: fixed   9 labels. Loss 0.04384. Accuracy 0.996.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13373648
Train loss (w/o reg) on all data: 0.12443797
Test loss (w/o reg) on all data: 0.08932045
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.812126e-06
Norm of the params: 13.637088
Flipped loss: 0.08932. Accuracy: 0.977
### Flips: 104, rs: 14, checks: 52
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024064507
Train loss (w/o reg) on all data: 0.0146788545
Test loss (w/o reg) on all data: 0.026032193
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.7161055e-07
Norm of the params: 13.700843
     Influence (LOO): fixed  44 labels. Loss 0.02603. Accuracy 0.996.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014309603
Train loss (w/o reg) on all data: 0.0066178883
Test loss (w/o reg) on all data: 0.033835225
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.998012e-07
Norm of the params: 12.402994
                Loss: fixed  47 labels. Loss 0.03384. Accuracy 0.985.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12854579
Train loss (w/o reg) on all data: 0.11903285
Test loss (w/o reg) on all data: 0.08717527
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5234878e-05
Norm of the params: 13.793432
              Random: fixed   3 labels. Loss 0.08718. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 104
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839533
Train loss (w/o reg) on all data: 0.0033497552
Test loss (w/o reg) on all data: 0.023645757
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6206538e-07
Norm of the params: 10.478337
     Influence (LOO): fixed  52 labels. Loss 0.02365. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847015
Train loss (w/o reg) on all data: 0.003039429
Test loss (w/o reg) on all data: 0.015349119
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.116218e-07
Norm of the params: 9.805698
                Loss: fixed  52 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1234743
Train loss (w/o reg) on all data: 0.11371249
Test loss (w/o reg) on all data: 0.081886895
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.6065423e-06
Norm of the params: 13.9726925
              Random: fixed   5 labels. Loss 0.08189. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012054736
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4477165e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012054784
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3816447e-07
Norm of the params: 9.153208
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119989045
Train loss (w/o reg) on all data: 0.11071295
Test loss (w/o reg) on all data: 0.07801577
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6848377e-05
Norm of the params: 13.620638
              Random: fixed   7 labels. Loss 0.07802. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729246
Test loss (w/o reg) on all data: 0.012054788
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.437512e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729239
Test loss (w/o reg) on all data: 0.012054834
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3360567e-07
Norm of the params: 9.153245
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11280658
Train loss (w/o reg) on all data: 0.103230916
Test loss (w/o reg) on all data: 0.08215152
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.982951e-06
Norm of the params: 13.838835
              Random: fixed  10 labels. Loss 0.08215. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 260
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012056007
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3124511e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012055699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.27297e-07
Norm of the params: 9.153186
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10595838
Train loss (w/o reg) on all data: 0.09624388
Test loss (w/o reg) on all data: 0.07562678
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.3026754e-06
Norm of the params: 13.938794
              Random: fixed  14 labels. Loss 0.07563. Accuracy 0.969.
### Flips: 104, rs: 14, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730717
Test loss (w/o reg) on all data: 0.012056622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.701486e-07
Norm of the params: 9.153084
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730682
Test loss (w/o reg) on all data: 0.012056708
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.516149e-07
Norm of the params: 9.153086
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098192446
Train loss (w/o reg) on all data: 0.088484116
Test loss (w/o reg) on all data: 0.07489601
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.940339e-06
Norm of the params: 13.934368
              Random: fixed  17 labels. Loss 0.07490. Accuracy 0.977.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12628537
Train loss (w/o reg) on all data: 0.11775238
Test loss (w/o reg) on all data: 0.06322673
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.002127e-05
Norm of the params: 13.063689
Flipped loss: 0.06323. Accuracy: 0.992
### Flips: 104, rs: 15, checks: 52
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027796667
Train loss (w/o reg) on all data: 0.01984915
Test loss (w/o reg) on all data: 0.022478387
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6997907e-06
Norm of the params: 12.6075535
     Influence (LOO): fixed  40 labels. Loss 0.02248. Accuracy 0.992.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011403559
Train loss (w/o reg) on all data: 0.004779428
Test loss (w/o reg) on all data: 0.010836924
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.12077e-06
Norm of the params: 11.510111
                Loss: fixed  43 labels. Loss 0.01084. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11401571
Train loss (w/o reg) on all data: 0.10523369
Test loss (w/o reg) on all data: 0.061130065
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9912225e-06
Norm of the params: 13.252941
              Random: fixed   4 labels. Loss 0.06113. Accuracy 0.985.
### Flips: 104, rs: 15, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173069
Test loss (w/o reg) on all data: 0.012055806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9991995e-07
Norm of the params: 9.153085
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730156
Test loss (w/o reg) on all data: 0.012055478
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.053483e-06
Norm of the params: 9.153143
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10933198
Train loss (w/o reg) on all data: 0.10047675
Test loss (w/o reg) on all data: 0.059979957
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.936869e-06
Norm of the params: 13.308064
              Random: fixed   7 labels. Loss 0.05998. Accuracy 0.985.
### Flips: 104, rs: 15, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.012055468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1229523e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9806554e-07
Norm of the params: 9.153181
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09730605
Train loss (w/o reg) on all data: 0.08734755
Test loss (w/o reg) on all data: 0.062347464
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.500162e-05
Norm of the params: 14.112757
              Random: fixed  11 labels. Loss 0.06235. Accuracy 0.981.
### Flips: 104, rs: 15, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729039
Test loss (w/o reg) on all data: 0.012055704
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.006188e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172906
Test loss (w/o reg) on all data: 0.012055797
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8313547e-07
Norm of the params: 9.153266
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096324995
Train loss (w/o reg) on all data: 0.0864977
Test loss (w/o reg) on all data: 0.058468148
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.402593e-05
Norm of the params: 14.019484
              Random: fixed  12 labels. Loss 0.05847. Accuracy 0.981.
### Flips: 104, rs: 15, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728931
Test loss (w/o reg) on all data: 0.012055642
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6212806e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.012055529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4487534e-07
Norm of the params: 9.153276
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08951874
Train loss (w/o reg) on all data: 0.0790604
Test loss (w/o reg) on all data: 0.050147478
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.862455e-06
Norm of the params: 14.462603
              Random: fixed  16 labels. Loss 0.05015. Accuracy 0.989.
### Flips: 104, rs: 15, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.0120538715
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2779907e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729826
Test loss (w/o reg) on all data: 0.01205403
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2626147e-07
Norm of the params: 9.153181
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086577736
Train loss (w/o reg) on all data: 0.07621307
Test loss (w/o reg) on all data: 0.047242757
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.916737e-06
Norm of the params: 14.397687
              Random: fixed  17 labels. Loss 0.04724. Accuracy 0.985.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13543594
Train loss (w/o reg) on all data: 0.12680179
Test loss (w/o reg) on all data: 0.06941795
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.711504e-06
Norm of the params: 13.140887
Flipped loss: 0.06942. Accuracy: 0.989
### Flips: 104, rs: 16, checks: 52
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023224663
Train loss (w/o reg) on all data: 0.016694888
Test loss (w/o reg) on all data: 0.014289023
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.2582324e-07
Norm of the params: 11.427839
     Influence (LOO): fixed  41 labels. Loss 0.01429. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728682
Test loss (w/o reg) on all data: 0.012056085
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.541732e-07
Norm of the params: 9.153305
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13053983
Train loss (w/o reg) on all data: 0.12157683
Test loss (w/o reg) on all data: 0.06647113
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.4445995e-06
Norm of the params: 13.3888035
              Random: fixed   2 labels. Loss 0.06647. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729036
Test loss (w/o reg) on all data: 0.012055789
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.432596e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729032
Test loss (w/o reg) on all data: 0.012055916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6149166e-07
Norm of the params: 9.153266
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12098205
Train loss (w/o reg) on all data: 0.111581825
Test loss (w/o reg) on all data: 0.061305877
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3328761e-05
Norm of the params: 13.711472
              Random: fixed   6 labels. Loss 0.06131. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728838
Test loss (w/o reg) on all data: 0.012055689
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6434054e-07
Norm of the params: 9.153287
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172887
Test loss (w/o reg) on all data: 0.012055611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3059303e-07
Norm of the params: 9.153286
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11463092
Train loss (w/o reg) on all data: 0.10581355
Test loss (w/o reg) on all data: 0.05578446
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6672234e-05
Norm of the params: 13.279592
              Random: fixed   9 labels. Loss 0.05578. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.345395e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729653
Test loss (w/o reg) on all data: 0.012054996
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2459395e-08
Norm of the params: 9.153201
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10795149
Train loss (w/o reg) on all data: 0.098358884
Test loss (w/o reg) on all data: 0.05122032
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.669035e-06
Norm of the params: 13.851069
              Random: fixed  12 labels. Loss 0.05122. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.01205525
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5749131e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.012055206
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.709963e-08
Norm of the params: 9.153195
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103048176
Train loss (w/o reg) on all data: 0.09307923
Test loss (w/o reg) on all data: 0.049218543
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.264953e-06
Norm of the params: 14.120158
              Random: fixed  14 labels. Loss 0.04922. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729572
Test loss (w/o reg) on all data: 0.012055305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.10810596e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012055226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0051393e-07
Norm of the params: 9.153205
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093945816
Train loss (w/o reg) on all data: 0.08288514
Test loss (w/o reg) on all data: 0.04961647
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.6393914e-06
Norm of the params: 14.873252
              Random: fixed  18 labels. Loss 0.04962. Accuracy 0.989.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674778
Train loss (w/o reg) on all data: 0.13723114
Test loss (w/o reg) on all data: 0.0952814
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6126083e-05
Norm of the params: 13.796112
Flipped loss: 0.09528. Accuracy: 0.977
### Flips: 104, rs: 17, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05305353
Train loss (w/o reg) on all data: 0.04141398
Test loss (w/o reg) on all data: 0.045019757
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0972225e-06
Norm of the params: 15.25749
     Influence (LOO): fixed  39 labels. Loss 0.04502. Accuracy 0.989.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019311825
Train loss (w/o reg) on all data: 0.009761999
Test loss (w/o reg) on all data: 0.06607115
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9182562e-06
Norm of the params: 13.820148
                Loss: fixed  49 labels. Loss 0.06607. Accuracy 0.981.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14454702
Train loss (w/o reg) on all data: 0.13464373
Test loss (w/o reg) on all data: 0.096514344
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1540743e-05
Norm of the params: 14.073579
              Random: fixed   1 labels. Loss 0.09651. Accuracy 0.977.
### Flips: 104, rs: 17, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009028168
Train loss (w/o reg) on all data: 0.0033739773
Test loss (w/o reg) on all data: 0.027832959
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4164431e-06
Norm of the params: 10.634088
     Influence (LOO): fixed  56 labels. Loss 0.02783. Accuracy 0.989.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839533
Train loss (w/o reg) on all data: 0.0033500374
Test loss (w/o reg) on all data: 0.023645451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1121013e-07
Norm of the params: 10.478067
                Loss: fixed  57 labels. Loss 0.02365. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14327581
Train loss (w/o reg) on all data: 0.13357168
Test loss (w/o reg) on all data: 0.093686834
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.2741816e-06
Norm of the params: 13.931353
              Random: fixed   2 labels. Loss 0.09369. Accuracy 0.977.
### Flips: 104, rs: 17, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730105
Test loss (w/o reg) on all data: 0.012054974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5484024e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0129681e-07
Norm of the params: 9.153152
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13529
Train loss (w/o reg) on all data: 0.12604699
Test loss (w/o reg) on all data: 0.08877366
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1026004e-05
Norm of the params: 13.596329
              Random: fixed   7 labels. Loss 0.08877. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012055299
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4280647e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012055295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.01583e-07
Norm of the params: 9.153215
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13232037
Train loss (w/o reg) on all data: 0.123108596
Test loss (w/o reg) on all data: 0.08931236
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2615053e-05
Norm of the params: 13.57334
              Random: fixed   8 labels. Loss 0.08931. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012055677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9633602e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.012055777
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6160131e-07
Norm of the params: 9.153181
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486532
Train loss (w/o reg) on all data: 0.11520027
Test loss (w/o reg) on all data: 0.08968903
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2908741e-05
Norm of the params: 13.903275
              Random: fixed  11 labels. Loss 0.08969. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173064
Test loss (w/o reg) on all data: 0.01205634
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.5514387e-07
Norm of the params: 9.153091
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730638
Test loss (w/o reg) on all data: 0.012056499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8017305e-07
Norm of the params: 9.153092
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11735141
Train loss (w/o reg) on all data: 0.108331
Test loss (w/o reg) on all data: 0.08579517
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.842009e-06
Norm of the params: 13.431613
              Random: fixed  16 labels. Loss 0.08580. Accuracy 0.981.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12506832
Train loss (w/o reg) on all data: 0.11533307
Test loss (w/o reg) on all data: 0.06427295
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.970661e-06
Norm of the params: 13.953669
Flipped loss: 0.06427. Accuracy: 0.989
### Flips: 104, rs: 18, checks: 52
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018981857
Train loss (w/o reg) on all data: 0.011007839
Test loss (w/o reg) on all data: 0.016060045
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5086205e-06
Norm of the params: 12.628554
     Influence (LOO): fixed  40 labels. Loss 0.01606. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012058621
Train loss (w/o reg) on all data: 0.0053685578
Test loss (w/o reg) on all data: 0.009086815
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.909825e-07
Norm of the params: 11.56725
                Loss: fixed  44 labels. Loss 0.00909. Accuracy 0.996.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1216639
Train loss (w/o reg) on all data: 0.111961216
Test loss (w/o reg) on all data: 0.062555864
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.69142e-06
Norm of the params: 13.930315
              Random: fixed   2 labels. Loss 0.06256. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008034245
Train loss (w/o reg) on all data: 0.0030515098
Test loss (w/o reg) on all data: 0.008116472
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.263122e-07
Norm of the params: 9.982719
     Influence (LOO): fixed  46 labels. Loss 0.00812. Accuracy 0.996.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729239
Test loss (w/o reg) on all data: 0.012055379
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8634624e-07
Norm of the params: 9.153246
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1216639
Train loss (w/o reg) on all data: 0.11196309
Test loss (w/o reg) on all data: 0.06254933
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2683867e-05
Norm of the params: 13.9289665
              Random: fixed   2 labels. Loss 0.06255. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 156
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012055528
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6616599e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012055258
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.843171e-07
Norm of the params: 9.153212
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11620463
Train loss (w/o reg) on all data: 0.10697082
Test loss (w/o reg) on all data: 0.060200654
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0400173e-06
Norm of the params: 13.5895605
              Random: fixed   4 labels. Loss 0.06020. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729856
Test loss (w/o reg) on all data: 0.012055031
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2545164e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012055147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7302292e-07
Norm of the params: 9.153179
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11403088
Train loss (w/o reg) on all data: 0.10495577
Test loss (w/o reg) on all data: 0.060497086
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.589938e-05
Norm of the params: 13.472276
              Random: fixed   5 labels. Loss 0.06050. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730007
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6784697e-07
Norm of the params: 9.153159
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730007
Test loss (w/o reg) on all data: 0.012054972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.722288e-07
Norm of the params: 9.153161
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106450126
Train loss (w/o reg) on all data: 0.09757444
Test loss (w/o reg) on all data: 0.05576717
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5737216e-05
Norm of the params: 13.323423
              Random: fixed  10 labels. Loss 0.05577. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012055528
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5596246e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012055443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2953686e-07
Norm of the params: 9.153189
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104591906
Train loss (w/o reg) on all data: 0.095766254
Test loss (w/o reg) on all data: 0.055313457
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2697844e-05
Norm of the params: 13.285823
              Random: fixed  11 labels. Loss 0.05531. Accuracy 0.989.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11767775
Train loss (w/o reg) on all data: 0.108179435
Test loss (w/o reg) on all data: 0.057842936
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1637096e-05
Norm of the params: 13.782823
Flipped loss: 0.05784. Accuracy: 0.989
### Flips: 104, rs: 19, checks: 52
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014649479
Train loss (w/o reg) on all data: 0.008820194
Test loss (w/o reg) on all data: 0.01459915
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1439425e-06
Norm of the params: 10.797485
     Influence (LOO): fixed  37 labels. Loss 0.01460. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973835
Train loss (w/o reg) on all data: 0.0033668543
Test loss (w/o reg) on all data: 0.013190099
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.282265e-06
Norm of the params: 10.5896
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115819775
Train loss (w/o reg) on all data: 0.10644327
Test loss (w/o reg) on all data: 0.058833145
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0119984e-05
Norm of the params: 13.694163
              Random: fixed   1 labels. Loss 0.05883. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172905
Test loss (w/o reg) on all data: 0.012055115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7620528e-07
Norm of the params: 9.153265
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973834
Train loss (w/o reg) on all data: 0.003366731
Test loss (w/o reg) on all data: 0.013190251
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.797746e-07
Norm of the params: 10.589715
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112996265
Train loss (w/o reg) on all data: 0.10363911
Test loss (w/o reg) on all data: 0.056656204
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5964273e-06
Norm of the params: 13.680027
              Random: fixed   3 labels. Loss 0.05666. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012055966
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.700124e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973834
Train loss (w/o reg) on all data: 0.0033667728
Test loss (w/o reg) on all data: 0.013192336
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.4693467e-07
Norm of the params: 10.589676
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10556004
Train loss (w/o reg) on all data: 0.09650521
Test loss (w/o reg) on all data: 0.05990905
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0946763e-05
Norm of the params: 13.457212
              Random: fixed   7 labels. Loss 0.05991. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012055252
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4312766e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012055051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.109209e-07
Norm of the params: 9.153216
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09757054
Train loss (w/o reg) on all data: 0.08808575
Test loss (w/o reg) on all data: 0.05251323
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.206238e-06
Norm of the params: 13.773009
              Random: fixed  10 labels. Loss 0.05251. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172907
Test loss (w/o reg) on all data: 0.012055974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4040486e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172909
Test loss (w/o reg) on all data: 0.012055893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.043089e-07
Norm of the params: 9.15326
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08926334
Train loss (w/o reg) on all data: 0.08026259
Test loss (w/o reg) on all data: 0.049397293
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5543245e-06
Norm of the params: 13.416971
              Random: fixed  14 labels. Loss 0.04940. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.012055236
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.943143e-08
Norm of the params: 9.153206
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012055265
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3779038e-07
Norm of the params: 9.153206
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08065118
Train loss (w/o reg) on all data: 0.07221338
Test loss (w/o reg) on all data: 0.047436304
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.088124e-06
Norm of the params: 12.990614
              Random: fixed  17 labels. Loss 0.04744. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14146334
Train loss (w/o reg) on all data: 0.13264409
Test loss (w/o reg) on all data: 0.07078503
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.478073e-05
Norm of the params: 13.281002
Flipped loss: 0.07079. Accuracy: 0.992
### Flips: 104, rs: 20, checks: 52
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03958009
Train loss (w/o reg) on all data: 0.032520436
Test loss (w/o reg) on all data: 0.021836534
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2553123e-06
Norm of the params: 11.88247
     Influence (LOO): fixed  41 labels. Loss 0.02184. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0132966675
Train loss (w/o reg) on all data: 0.0060035363
Test loss (w/o reg) on all data: 0.016769959
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.906092e-07
Norm of the params: 12.077359
                Loss: fixed  50 labels. Loss 0.01677. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442047
Train loss (w/o reg) on all data: 0.12566844
Test loss (w/o reg) on all data: 0.063727476
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0166841e-05
Norm of the params: 13.230293
              Random: fixed   4 labels. Loss 0.06373. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 104
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224471
Train loss (w/o reg) on all data: 0.006245323
Test loss (w/o reg) on all data: 0.012818021
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3853048e-06
Norm of the params: 9.979126
     Influence (LOO): fixed  52 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217293
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0917021e-07
Norm of the params: 9.153239
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442048
Train loss (w/o reg) on all data: 0.12566921
Test loss (w/o reg) on all data: 0.06372798
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3913767e-05
Norm of the params: 13.229713
              Random: fixed   4 labels. Loss 0.06373. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012055008
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7498526e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729304
Test loss (w/o reg) on all data: 0.012054956
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.54778e-07
Norm of the params: 9.153238
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13266289
Train loss (w/o reg) on all data: 0.12401257
Test loss (w/o reg) on all data: 0.062447976
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3790138e-05
Norm of the params: 13.153193
              Random: fixed   5 labels. Loss 0.06245. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217301
Test loss (w/o reg) on all data: 0.012055174
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8941238e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.012055228
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1986303e-07
Norm of the params: 9.1531515
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1276506
Train loss (w/o reg) on all data: 0.1187392
Test loss (w/o reg) on all data: 0.06089714
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3319937e-05
Norm of the params: 13.350205
              Random: fixed   7 labels. Loss 0.06090. Accuracy 0.996.
### Flips: 104, rs: 20, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.012054062
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2861544e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.012054246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2013523e-07
Norm of the params: 9.153204
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12277889
Train loss (w/o reg) on all data: 0.11369023
Test loss (w/o reg) on all data: 0.059318077
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3268505e-05
Norm of the params: 13.482334
              Random: fixed   9 labels. Loss 0.05932. Accuracy 0.996.
### Flips: 104, rs: 20, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012054936
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.496225e-08
Norm of the params: 9.153197
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012054929
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0501066e-07
Norm of the params: 9.153197
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117628686
Train loss (w/o reg) on all data: 0.10855694
Test loss (w/o reg) on all data: 0.057680685
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.026402e-06
Norm of the params: 13.469777
              Random: fixed  12 labels. Loss 0.05768. Accuracy 0.996.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13253564
Train loss (w/o reg) on all data: 0.12499554
Test loss (w/o reg) on all data: 0.07342323
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.125614e-06
Norm of the params: 12.280148
Flipped loss: 0.07342. Accuracy: 0.992
### Flips: 104, rs: 21, checks: 52
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027249068
Train loss (w/o reg) on all data: 0.019016817
Test loss (w/o reg) on all data: 0.025129516
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.708912e-06
Norm of the params: 12.831407
     Influence (LOO): fixed  40 labels. Loss 0.02513. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075473925
Train loss (w/o reg) on all data: 0.0029019448
Test loss (w/o reg) on all data: 0.014047616
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2882548e-07
Norm of the params: 9.638929
                Loss: fixed  46 labels. Loss 0.01405. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12422012
Train loss (w/o reg) on all data: 0.116469435
Test loss (w/o reg) on all data: 0.06773697
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0764037e-05
Norm of the params: 12.450446
              Random: fixed   3 labels. Loss 0.06774. Accuracy 0.992.
### Flips: 104, rs: 21, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.012055102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3753292e-07
Norm of the params: 9.153245
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172924
Test loss (w/o reg) on all data: 0.012055185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7036744e-07
Norm of the params: 9.153246
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10899209
Train loss (w/o reg) on all data: 0.1013099
Test loss (w/o reg) on all data: 0.05886233
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1612846e-05
Norm of the params: 12.395315
              Random: fixed   9 labels. Loss 0.05886. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 156
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.012054942
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1172931e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012054803
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2239813e-07
Norm of the params: 9.153227
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10550343
Train loss (w/o reg) on all data: 0.09790258
Test loss (w/o reg) on all data: 0.060249496
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.675573e-06
Norm of the params: 12.329518
              Random: fixed  11 labels. Loss 0.06025. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 208
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012054747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4159844e-07
Norm of the params: 9.153271
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728997
Test loss (w/o reg) on all data: 0.0120546715
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5307198e-07
Norm of the params: 9.153271
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09829446
Train loss (w/o reg) on all data: 0.09071592
Test loss (w/o reg) on all data: 0.05477829
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4832467e-05
Norm of the params: 12.311408
              Random: fixed  14 labels. Loss 0.05478. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012056621
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4403416e-07
Norm of the params: 9.153139
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012056658
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1852988e-07
Norm of the params: 9.15314
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09002437
Train loss (w/o reg) on all data: 0.082848765
Test loss (w/o reg) on all data: 0.048917472
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1827773e-05
Norm of the params: 11.979653
              Random: fixed  18 labels. Loss 0.04892. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.012055613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.624732e-07
Norm of the params: 9.153126
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173031
Test loss (w/o reg) on all data: 0.012055755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8631322e-07
Norm of the params: 9.153128
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08755438
Train loss (w/o reg) on all data: 0.08131045
Test loss (w/o reg) on all data: 0.041624825
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2771857e-05
Norm of the params: 11.174907
              Random: fixed  20 labels. Loss 0.04162. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13846013
Train loss (w/o reg) on all data: 0.12964046
Test loss (w/o reg) on all data: 0.0845867
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7706856e-05
Norm of the params: 13.281313
Flipped loss: 0.08459. Accuracy: 0.989
### Flips: 104, rs: 22, checks: 52
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040309317
Train loss (w/o reg) on all data: 0.031259798
Test loss (w/o reg) on all data: 0.045366216
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.628679e-06
Norm of the params: 13.453267
     Influence (LOO): fixed  38 labels. Loss 0.04537. Accuracy 0.973.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016966853
Train loss (w/o reg) on all data: 0.008050301
Test loss (w/o reg) on all data: 0.025999878
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9960705e-06
Norm of the params: 13.354065
                Loss: fixed  48 labels. Loss 0.02600. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13780583
Train loss (w/o reg) on all data: 0.12903158
Test loss (w/o reg) on all data: 0.08288244
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.680833e-06
Norm of the params: 13.247078
              Random: fixed   1 labels. Loss 0.08288. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 104
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009081588
Train loss (w/o reg) on all data: 0.00366735
Test loss (w/o reg) on all data: 0.011724322
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2553595e-07
Norm of the params: 10.405996
     Influence (LOO): fixed  54 labels. Loss 0.01172. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833827
Train loss (w/o reg) on all data: 0.0028982926
Test loss (w/o reg) on all data: 0.012001723
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3092765e-07
Norm of the params: 9.884422
                Loss: fixed  54 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12725942
Train loss (w/o reg) on all data: 0.11819088
Test loss (w/o reg) on all data: 0.07395386
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3858653e-05
Norm of the params: 13.467403
              Random: fixed   6 labels. Loss 0.07395. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012055476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.343791e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012055533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9549975e-07
Norm of the params: 9.153221
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119267836
Train loss (w/o reg) on all data: 0.10997261
Test loss (w/o reg) on all data: 0.07114688
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3791763e-05
Norm of the params: 13.634681
              Random: fixed   8 labels. Loss 0.07115. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173021
Test loss (w/o reg) on all data: 0.012055389
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6034323e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.012055468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7720068e-07
Norm of the params: 9.153138
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11360186
Train loss (w/o reg) on all data: 0.104056984
Test loss (w/o reg) on all data: 0.066783935
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.652047e-06
Norm of the params: 13.816568
              Random: fixed  12 labels. Loss 0.06678. Accuracy 0.985.
### Flips: 104, rs: 22, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012055319
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7950094e-07
Norm of the params: 9.153232
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729365
Test loss (w/o reg) on all data: 0.012055273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2654495e-07
Norm of the params: 9.153232
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110420056
Train loss (w/o reg) on all data: 0.10082833
Test loss (w/o reg) on all data: 0.06515844
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1693055e-05
Norm of the params: 13.850435
              Random: fixed  14 labels. Loss 0.06516. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012055735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9267317e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172937
Test loss (w/o reg) on all data: 0.01205565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.549331e-07
Norm of the params: 9.153231
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11103687
Train loss (w/o reg) on all data: 0.101653725
Test loss (w/o reg) on all data: 0.06373729
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.872137e-06
Norm of the params: 13.699008
              Random: fixed  15 labels. Loss 0.06374. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13234998
Train loss (w/o reg) on all data: 0.1233849
Test loss (w/o reg) on all data: 0.06909291
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.966646e-06
Norm of the params: 13.390355
Flipped loss: 0.06909. Accuracy: 0.989
### Flips: 104, rs: 23, checks: 52
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03434482
Train loss (w/o reg) on all data: 0.024010753
Test loss (w/o reg) on all data: 0.027179545
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2721326e-06
Norm of the params: 14.376415
     Influence (LOO): fixed  40 labels. Loss 0.02718. Accuracy 0.985.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019978482
Train loss (w/o reg) on all data: 0.010271498
Test loss (w/o reg) on all data: 0.018627953
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3913187e-06
Norm of the params: 13.933402
                Loss: fixed  45 labels. Loss 0.01863. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12123775
Train loss (w/o reg) on all data: 0.11274413
Test loss (w/o reg) on all data: 0.06362608
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.273926e-05
Norm of the params: 13.033508
              Random: fixed   5 labels. Loss 0.06363. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 104
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014170967
Train loss (w/o reg) on all data: 0.0072801383
Test loss (w/o reg) on all data: 0.008347181
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8402033e-06
Norm of the params: 11.73953
     Influence (LOO): fixed  52 labels. Loss 0.00835. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008739539
Train loss (w/o reg) on all data: 0.0032572094
Test loss (w/o reg) on all data: 0.009238491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0999905e-06
Norm of the params: 10.471228
                Loss: fixed  53 labels. Loss 0.00924. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120636016
Train loss (w/o reg) on all data: 0.11239751
Test loss (w/o reg) on all data: 0.061406516
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.233735e-06
Norm of the params: 12.836281
              Random: fixed   7 labels. Loss 0.06141. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007623193
Train loss (w/o reg) on all data: 0.0027306837
Test loss (w/o reg) on all data: 0.00957811
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.472995e-07
Norm of the params: 9.891925
     Influence (LOO): fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00873954
Train loss (w/o reg) on all data: 0.0032569128
Test loss (w/o reg) on all data: 0.009238135
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3952576e-07
Norm of the params: 10.471511
                Loss: fixed  53 labels. Loss 0.00924. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11916101
Train loss (w/o reg) on all data: 0.11099311
Test loss (w/o reg) on all data: 0.0611494
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.0385096e-06
Norm of the params: 12.781156
              Random: fixed   8 labels. Loss 0.06115. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730135
Test loss (w/o reg) on all data: 0.012054968
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7457637e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231947
Train loss (w/o reg) on all data: 0.0027306008
Test loss (w/o reg) on all data: 0.00957818
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.959657e-07
Norm of the params: 9.892011
                Loss: fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11037028
Train loss (w/o reg) on all data: 0.10257996
Test loss (w/o reg) on all data: 0.052550104
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1198906e-05
Norm of the params: 12.482241
              Random: fixed  15 labels. Loss 0.05255. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012055239
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.212935e-08
Norm of the params: 9.153195
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231956
Train loss (w/o reg) on all data: 0.0027305342
Test loss (w/o reg) on all data: 0.009577288
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0021207e-06
Norm of the params: 9.892079
                Loss: fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10395784
Train loss (w/o reg) on all data: 0.09574542
Test loss (w/o reg) on all data: 0.043929353
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1204605e-05
Norm of the params: 12.81594
              Random: fixed  18 labels. Loss 0.04393. Accuracy 0.989.
### Flips: 104, rs: 23, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173031
Test loss (w/o reg) on all data: 0.012055838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7498298e-07
Norm of the params: 9.153128
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217303
Test loss (w/o reg) on all data: 0.012055745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1762093e-07
Norm of the params: 9.153129
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101262264
Train loss (w/o reg) on all data: 0.09303834
Test loss (w/o reg) on all data: 0.043155897
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3590263e-06
Norm of the params: 12.824915
              Random: fixed  19 labels. Loss 0.04316. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13817266
Train loss (w/o reg) on all data: 0.12973498
Test loss (w/o reg) on all data: 0.08456784
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4922066e-05
Norm of the params: 12.9905205
Flipped loss: 0.08457. Accuracy: 0.985
### Flips: 104, rs: 24, checks: 52
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050856665
Train loss (w/o reg) on all data: 0.042148434
Test loss (w/o reg) on all data: 0.04028679
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.895961e-06
Norm of the params: 13.197146
     Influence (LOO): fixed  36 labels. Loss 0.04029. Accuracy 0.981.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013151578
Train loss (w/o reg) on all data: 0.006112259
Test loss (w/o reg) on all data: 0.023733094
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.114999e-07
Norm of the params: 11.865343
                Loss: fixed  48 labels. Loss 0.02373. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13268788
Train loss (w/o reg) on all data: 0.12428546
Test loss (w/o reg) on all data: 0.077542886
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.381073e-05
Norm of the params: 12.9633465
              Random: fixed   3 labels. Loss 0.07754. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 104
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013634219
Train loss (w/o reg) on all data: 0.007406791
Test loss (w/o reg) on all data: 0.010798721
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0762444e-06
Norm of the params: 11.160133
     Influence (LOO): fixed  51 labels. Loss 0.01080. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729467
Test loss (w/o reg) on all data: 0.012054979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.151925e-07
Norm of the params: 9.15322
                Loss: fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13184172
Train loss (w/o reg) on all data: 0.12366671
Test loss (w/o reg) on all data: 0.0768882
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.413378e-06
Norm of the params: 12.7867155
              Random: fixed   4 labels. Loss 0.07689. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172961
Test loss (w/o reg) on all data: 0.012055182
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.729197e-08
Norm of the params: 9.153204
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012055206
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7125686e-07
Norm of the params: 9.153204
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13016377
Train loss (w/o reg) on all data: 0.122117236
Test loss (w/o reg) on all data: 0.07574685
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.3807435e-06
Norm of the params: 12.685846
              Random: fixed   5 labels. Loss 0.07575. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.012055225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.684871e-07
Norm of the params: 9.153155
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730063
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.484857e-07
Norm of the params: 9.153156
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486279
Train loss (w/o reg) on all data: 0.116519205
Test loss (w/o reg) on all data: 0.07427969
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3752095e-05
Norm of the params: 12.91788
              Random: fixed   7 labels. Loss 0.07428. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731143
Test loss (w/o reg) on all data: 0.012055229
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.906096e-07
Norm of the params: 9.153036
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731087
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.891339e-07
Norm of the params: 9.153042
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12170122
Train loss (w/o reg) on all data: 0.113355614
Test loss (w/o reg) on all data: 0.07417114
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8975799e-05
Norm of the params: 12.919446
              Random: fixed   9 labels. Loss 0.07417. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012055762
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7301441e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012055806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7045929e-07
Norm of the params: 9.153195
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120776996
Train loss (w/o reg) on all data: 0.112454824
Test loss (w/o reg) on all data: 0.07361873
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.53518e-06
Norm of the params: 12.901295
              Random: fixed  10 labels. Loss 0.07362. Accuracy 0.989.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13880987
Train loss (w/o reg) on all data: 0.13013588
Test loss (w/o reg) on all data: 0.06986063
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2177983e-05
Norm of the params: 13.171182
Flipped loss: 0.06986. Accuracy: 0.989
### Flips: 104, rs: 25, checks: 52
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034547605
Train loss (w/o reg) on all data: 0.026619136
Test loss (w/o reg) on all data: 0.025286084
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4529734e-06
Norm of the params: 12.592433
     Influence (LOO): fixed  44 labels. Loss 0.02529. Accuracy 0.989.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013811534
Train loss (w/o reg) on all data: 0.006689824
Test loss (w/o reg) on all data: 0.017936066
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9411866e-07
Norm of the params: 11.93458
                Loss: fixed  49 labels. Loss 0.01794. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13536821
Train loss (w/o reg) on all data: 0.12628506
Test loss (w/o reg) on all data: 0.067645624
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1976941e-05
Norm of the params: 13.478241
              Random: fixed   2 labels. Loss 0.06765. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 104
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011929339
Train loss (w/o reg) on all data: 0.005835691
Test loss (w/o reg) on all data: 0.01953331
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.413636e-07
Norm of the params: 11.039609
     Influence (LOO): fixed  53 labels. Loss 0.01953. Accuracy 0.989.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.0120549975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0440615e-07
Norm of the params: 9.153204
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12940598
Train loss (w/o reg) on all data: 0.12024456
Test loss (w/o reg) on all data: 0.06650397
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4974823e-05
Norm of the params: 13.53618
              Random: fixed   5 labels. Loss 0.06650. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.01205474
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8384295e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217292
Test loss (w/o reg) on all data: 0.012054687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.787439e-08
Norm of the params: 9.153251
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12271401
Train loss (w/o reg) on all data: 0.11357358
Test loss (w/o reg) on all data: 0.060649887
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1808201e-05
Norm of the params: 13.520676
              Random: fixed   9 labels. Loss 0.06065. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012054731
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.358754e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.012054691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0576806e-07
Norm of the params: 9.153205
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1171612
Train loss (w/o reg) on all data: 0.1075146
Test loss (w/o reg) on all data: 0.053062283
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5412235e-05
Norm of the params: 13.8899975
              Random: fixed  11 labels. Loss 0.05306. Accuracy 0.989.
### Flips: 104, rs: 25, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730354
Test loss (w/o reg) on all data: 0.012055056
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.98196e-07
Norm of the params: 9.153123
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.012055126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8338382e-07
Norm of the params: 9.153126
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09506187
Train loss (w/o reg) on all data: 0.084327534
Test loss (w/o reg) on all data: 0.05057775
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8194612e-05
Norm of the params: 14.652192
              Random: fixed  19 labels. Loss 0.05058. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729881
Test loss (w/o reg) on all data: 0.012054671
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.667231e-08
Norm of the params: 9.153177
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012054927
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4625596e-08
Norm of the params: 9.153186
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089846075
Train loss (w/o reg) on all data: 0.07897325
Test loss (w/o reg) on all data: 0.050799932
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7272137e-05
Norm of the params: 14.746409
              Random: fixed  21 labels. Loss 0.05080. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1442918
Train loss (w/o reg) on all data: 0.13600533
Test loss (w/o reg) on all data: 0.07084585
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2473723e-05
Norm of the params: 12.873603
Flipped loss: 0.07085. Accuracy: 0.981
### Flips: 104, rs: 26, checks: 52
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04056885
Train loss (w/o reg) on all data: 0.03242275
Test loss (w/o reg) on all data: 0.02205611
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5011515e-06
Norm of the params: 12.76409
     Influence (LOO): fixed  42 labels. Loss 0.02206. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012880969
Train loss (w/o reg) on all data: 0.005923512
Test loss (w/o reg) on all data: 0.014362457
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.642736e-07
Norm of the params: 11.79615
                Loss: fixed  52 labels. Loss 0.01436. Accuracy 0.996.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13966829
Train loss (w/o reg) on all data: 0.13113298
Test loss (w/o reg) on all data: 0.06771548
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6676615e-05
Norm of the params: 13.065463
              Random: fixed   3 labels. Loss 0.06772. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 104
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021108823
Train loss (w/o reg) on all data: 0.015563544
Test loss (w/o reg) on all data: 0.01483194
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7516843e-06
Norm of the params: 10.531173
     Influence (LOO): fixed  52 labels. Loss 0.01483. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.012055235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.94962e-07
Norm of the params: 9.153191
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13231772
Train loss (w/o reg) on all data: 0.12373811
Test loss (w/o reg) on all data: 0.06517686
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.153526e-06
Norm of the params: 13.099328
              Random: fixed   6 labels. Loss 0.06518. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172927
Test loss (w/o reg) on all data: 0.012054288
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.724576e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012054364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9716184e-07
Norm of the params: 9.15324
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12989973
Train loss (w/o reg) on all data: 0.12149348
Test loss (w/o reg) on all data: 0.058923755
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9284285e-05
Norm of the params: 12.966295
              Random: fixed   9 labels. Loss 0.05892. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012054865
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3521397e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012054937
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0182899e-07
Norm of the params: 9.153218
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12599152
Train loss (w/o reg) on all data: 0.11697555
Test loss (w/o reg) on all data: 0.058424298
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.748155e-06
Norm of the params: 13.428301
              Random: fixed  11 labels. Loss 0.05842. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012055938
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0828472e-06
Norm of the params: 9.153194
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172971
Test loss (w/o reg) on all data: 0.012056129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5133177e-07
Norm of the params: 9.153194
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11763934
Train loss (w/o reg) on all data: 0.108227745
Test loss (w/o reg) on all data: 0.056747496
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2667212e-05
Norm of the params: 13.719763
              Random: fixed  14 labels. Loss 0.05675. Accuracy 0.989.
### Flips: 104, rs: 26, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.01205493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.740136e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012055013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6414721e-07
Norm of the params: 9.153196
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110371016
Train loss (w/o reg) on all data: 0.10082373
Test loss (w/o reg) on all data: 0.058170926
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.960496e-06
Norm of the params: 13.818311
              Random: fixed  18 labels. Loss 0.05817. Accuracy 0.985.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116745934
Train loss (w/o reg) on all data: 0.108834766
Test loss (w/o reg) on all data: 0.06812804
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.882524e-06
Norm of the params: 12.578688
Flipped loss: 0.06813. Accuracy: 0.985
### Flips: 104, rs: 27, checks: 52
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02553416
Train loss (w/o reg) on all data: 0.017934024
Test loss (w/o reg) on all data: 0.023716602
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5142566e-06
Norm of the params: 12.3289385
     Influence (LOO): fixed  37 labels. Loss 0.02372. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009880508
Train loss (w/o reg) on all data: 0.004103894
Test loss (w/o reg) on all data: 0.016360871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4384454e-07
Norm of the params: 10.748594
                Loss: fixed  43 labels. Loss 0.01636. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11559224
Train loss (w/o reg) on all data: 0.10745846
Test loss (w/o reg) on all data: 0.06916149
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.577357e-06
Norm of the params: 12.754436
              Random: fixed   1 labels. Loss 0.06916. Accuracy 0.981.
### Flips: 104, rs: 27, checks: 104
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728487
Test loss (w/o reg) on all data: 0.012056385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0978489e-06
Norm of the params: 9.153326
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728512
Test loss (w/o reg) on all data: 0.012056196
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6837174e-07
Norm of the params: 9.153324
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11331358
Train loss (w/o reg) on all data: 0.10566316
Test loss (w/o reg) on all data: 0.06367326
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7246588e-05
Norm of the params: 12.369657
              Random: fixed   4 labels. Loss 0.06367. Accuracy 0.992.
### Flips: 104, rs: 27, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012054726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1116231e-06
Norm of the params: 9.153147
     Influence (LOO): fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730133
Test loss (w/o reg) on all data: 0.012054931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3910862e-07
Norm of the params: 9.153147
                Loss: fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109554835
Train loss (w/o reg) on all data: 0.10182822
Test loss (w/o reg) on all data: 0.05846433
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6859136e-06
Norm of the params: 12.4311075
              Random: fixed   6 labels. Loss 0.05846. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012054805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.754971e-07
Norm of the params: 9.153202
     Influence (LOO): fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729632
Test loss (w/o reg) on all data: 0.012054856
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8088816e-07
Norm of the params: 9.153202
                Loss: fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1049378
Train loss (w/o reg) on all data: 0.09726838
Test loss (w/o reg) on all data: 0.05235339
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.883235e-06
Norm of the params: 12.385008
              Random: fixed   8 labels. Loss 0.05235. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8357215e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729642
Test loss (w/o reg) on all data: 0.01205513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5957726e-07
Norm of the params: 9.153204
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09414585
Train loss (w/o reg) on all data: 0.08570052
Test loss (w/o reg) on all data: 0.053613275
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0930227e-05
Norm of the params: 12.99641
              Random: fixed  12 labels. Loss 0.05361. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729632
Test loss (w/o reg) on all data: 0.012055139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.6229684e-08
Norm of the params: 9.153203
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.201945e-07
Norm of the params: 9.153203
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09414585
Train loss (w/o reg) on all data: 0.085698515
Test loss (w/o reg) on all data: 0.053633247
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1145442e-05
Norm of the params: 12.997951
              Random: fixed  12 labels. Loss 0.05363. Accuracy 0.989.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14608032
Train loss (w/o reg) on all data: 0.13737191
Test loss (w/o reg) on all data: 0.085544705
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.941618e-06
Norm of the params: 13.197275
Flipped loss: 0.08554. Accuracy: 0.985
### Flips: 104, rs: 28, checks: 52
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040700458
Train loss (w/o reg) on all data: 0.031273067
Test loss (w/o reg) on all data: 0.036280002
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3200626e-05
Norm of the params: 13.731274
     Influence (LOO): fixed  41 labels. Loss 0.03628. Accuracy 0.989.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016321989
Train loss (w/o reg) on all data: 0.009176234
Test loss (w/o reg) on all data: 0.015802866
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.500614e-07
Norm of the params: 11.954711
                Loss: fixed  49 labels. Loss 0.01580. Accuracy 0.996.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14430057
Train loss (w/o reg) on all data: 0.13537876
Test loss (w/o reg) on all data: 0.08423482
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0723941e-05
Norm of the params: 13.358
              Random: fixed   1 labels. Loss 0.08423. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 104
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018661937
Train loss (w/o reg) on all data: 0.011595486
Test loss (w/o reg) on all data: 0.013788771
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.758149e-07
Norm of the params: 11.888187
     Influence (LOO): fixed  51 labels. Loss 0.01379. Accuracy 0.996.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028170035
Test loss (w/o reg) on all data: 0.012755741
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.595197e-07
Norm of the params: 9.683644
                Loss: fixed  54 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13933681
Train loss (w/o reg) on all data: 0.13007398
Test loss (w/o reg) on all data: 0.084651195
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.055561e-06
Norm of the params: 13.610905
              Random: fixed   3 labels. Loss 0.08465. Accuracy 0.989.
### Flips: 104, rs: 28, checks: 156
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013159689
Train loss (w/o reg) on all data: 0.00751512
Test loss (w/o reg) on all data: 0.013905625
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2862098e-06
Norm of the params: 10.625036
     Influence (LOO): fixed  53 labels. Loss 0.01391. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012055183
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3221262e-07
Norm of the params: 9.153226
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12805429
Train loss (w/o reg) on all data: 0.11815391
Test loss (w/o reg) on all data: 0.087827206
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6605842e-05
Norm of the params: 14.071516
              Random: fixed   6 labels. Loss 0.08783. Accuracy 0.981.
### Flips: 104, rs: 28, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012056352
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.574684e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.012056494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8239708e-07
Norm of the params: 9.153203
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121664
Train loss (w/o reg) on all data: 0.111593544
Test loss (w/o reg) on all data: 0.08334841
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.8938045e-06
Norm of the params: 14.191873
              Random: fixed   9 labels. Loss 0.08335. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730303
Test loss (w/o reg) on all data: 0.012055253
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6171411e-07
Norm of the params: 9.153127
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217303
Test loss (w/o reg) on all data: 0.012055313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9650267e-07
Norm of the params: 9.153129
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12166399
Train loss (w/o reg) on all data: 0.11159196
Test loss (w/o reg) on all data: 0.08336031
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.547845e-06
Norm of the params: 14.19298
              Random: fixed   9 labels. Loss 0.08336. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012055581
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4925848e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729984
Test loss (w/o reg) on all data: 0.012055529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0759932e-07
Norm of the params: 9.153161
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118844695
Train loss (w/o reg) on all data: 0.10904807
Test loss (w/o reg) on all data: 0.072863534
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.642883e-06
Norm of the params: 13.99759
              Random: fixed  11 labels. Loss 0.07286. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137922
Train loss (w/o reg) on all data: 0.12962061
Test loss (w/o reg) on all data: 0.08737912
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1594948e-05
Norm of the params: 12.885175
Flipped loss: 0.08738. Accuracy: 0.981
### Flips: 104, rs: 29, checks: 52
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030474849
Train loss (w/o reg) on all data: 0.022413561
Test loss (w/o reg) on all data: 0.01808796
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5111674e-06
Norm of the params: 12.697472
     Influence (LOO): fixed  42 labels. Loss 0.01809. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010875925
Train loss (w/o reg) on all data: 0.0047141546
Test loss (w/o reg) on all data: 0.014647734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.884296e-07
Norm of the params: 11.101144
                Loss: fixed  47 labels. Loss 0.01465. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13284947
Train loss (w/o reg) on all data: 0.12405483
Test loss (w/o reg) on all data: 0.089149356
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.41025e-05
Norm of the params: 13.262455
              Random: fixed   2 labels. Loss 0.08915. Accuracy 0.981.
### Flips: 104, rs: 29, checks: 104
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217309
Test loss (w/o reg) on all data: 0.012055753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5429153e-07
Norm of the params: 9.153063
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173083
Test loss (w/o reg) on all data: 0.0120555535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.90956e-07
Norm of the params: 9.1530695
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12815979
Train loss (w/o reg) on all data: 0.11961598
Test loss (w/o reg) on all data: 0.08015569
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1659624e-05
Norm of the params: 13.071963
              Random: fixed   4 labels. Loss 0.08016. Accuracy 0.977.
### Flips: 104, rs: 29, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730312
Test loss (w/o reg) on all data: 0.012056025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.523292e-07
Norm of the params: 9.153125
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730333
Test loss (w/o reg) on all data: 0.012056138
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9347004e-07
Norm of the params: 9.153126
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12142527
Train loss (w/o reg) on all data: 0.11257045
Test loss (w/o reg) on all data: 0.07692569
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5183193e-05
Norm of the params: 13.307761
              Random: fixed   8 labels. Loss 0.07693. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730652
Test loss (w/o reg) on all data: 0.012055534
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.663346e-07
Norm of the params: 9.1530905
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173064
Test loss (w/o reg) on all data: 0.012055613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6747105e-07
Norm of the params: 9.153091
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11357416
Train loss (w/o reg) on all data: 0.10480298
Test loss (w/o reg) on all data: 0.074817725
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.316211e-05
Norm of the params: 13.2447605
              Random: fixed  11 labels. Loss 0.07482. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730168
Test loss (w/o reg) on all data: 0.012055257
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3577e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173015
Test loss (w/o reg) on all data: 0.012055196
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5638934e-07
Norm of the params: 9.153145
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102479696
Train loss (w/o reg) on all data: 0.09310759
Test loss (w/o reg) on all data: 0.05803788
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4012532e-05
Norm of the params: 13.690951
              Random: fixed  17 labels. Loss 0.05804. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.0120562455
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.677731e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729616
Test loss (w/o reg) on all data: 0.012056105
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2200705e-07
Norm of the params: 9.153205
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096528955
Train loss (w/o reg) on all data: 0.087368
Test loss (w/o reg) on all data: 0.05139007
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.3584564e-06
Norm of the params: 13.535849
              Random: fixed  20 labels. Loss 0.05139. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13960387
Train loss (w/o reg) on all data: 0.13058612
Test loss (w/o reg) on all data: 0.082770646
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.748137e-05
Norm of the params: 13.429635
Flipped loss: 0.08277. Accuracy: 0.981
### Flips: 104, rs: 30, checks: 52
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03238816
Train loss (w/o reg) on all data: 0.024601683
Test loss (w/o reg) on all data: 0.033967696
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.349248e-07
Norm of the params: 12.479162
     Influence (LOO): fixed  40 labels. Loss 0.03397. Accuracy 0.981.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012503052
Train loss (w/o reg) on all data: 0.0052776188
Test loss (w/o reg) on all data: 0.026514063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6655046e-07
Norm of the params: 12.021175
                Loss: fixed  47 labels. Loss 0.02651. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13455674
Train loss (w/o reg) on all data: 0.12581754
Test loss (w/o reg) on all data: 0.08047387
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.261401e-06
Norm of the params: 13.220592
              Random: fixed   3 labels. Loss 0.08047. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012053559
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5293572e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.012053455
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9566363e-07
Norm of the params: 9.1531925
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12731545
Train loss (w/o reg) on all data: 0.11851853
Test loss (w/o reg) on all data: 0.07558359
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3538283e-06
Norm of the params: 13.2641735
              Random: fixed   6 labels. Loss 0.07558. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.012054572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2925515e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.01205451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5597928e-07
Norm of the params: 9.153171
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113960035
Train loss (w/o reg) on all data: 0.10509761
Test loss (w/o reg) on all data: 0.05597348
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.5727196e-06
Norm of the params: 13.313472
              Random: fixed  12 labels. Loss 0.05597. Accuracy 0.989.
### Flips: 104, rs: 30, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012053829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1916235e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730051
Test loss (w/o reg) on all data: 0.012053755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.853857e-07
Norm of the params: 9.153156
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1083818
Train loss (w/o reg) on all data: 0.09865216
Test loss (w/o reg) on all data: 0.054494232
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9105255e-06
Norm of the params: 13.949653
              Random: fixed  13 labels. Loss 0.05449. Accuracy 0.992.
### Flips: 104, rs: 30, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.01205445
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.07981464e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012054485
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1476449e-07
Norm of the params: 9.153194
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10014425
Train loss (w/o reg) on all data: 0.08983435
Test loss (w/o reg) on all data: 0.05516652
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5348864e-06
Norm of the params: 14.359602
              Random: fixed  17 labels. Loss 0.05517. Accuracy 0.992.
### Flips: 104, rs: 30, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012055639
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4878505e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729528
Test loss (w/o reg) on all data: 0.012055662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.232212e-07
Norm of the params: 9.1532135
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090871625
Train loss (w/o reg) on all data: 0.08068571
Test loss (w/o reg) on all data: 0.05528559
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2077268e-05
Norm of the params: 14.272989
              Random: fixed  21 labels. Loss 0.05529. Accuracy 0.985.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1325873
Train loss (w/o reg) on all data: 0.123617865
Test loss (w/o reg) on all data: 0.07704506
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.586876e-05
Norm of the params: 13.393608
Flipped loss: 0.07705. Accuracy: 0.981
### Flips: 104, rs: 31, checks: 52
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03260589
Train loss (w/o reg) on all data: 0.024923144
Test loss (w/o reg) on all data: 0.034992173
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.9160976e-06
Norm of the params: 12.395763
     Influence (LOO): fixed  39 labels. Loss 0.03499. Accuracy 0.981.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833736
Train loss (w/o reg) on all data: 0.0026668895
Test loss (w/o reg) on all data: 0.021343838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.890186e-08
Norm of the params: 9.712347
                Loss: fixed  47 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120105736
Train loss (w/o reg) on all data: 0.11114073
Test loss (w/o reg) on all data: 0.0703665
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2652457e-05
Norm of the params: 13.390299
              Random: fixed   5 labels. Loss 0.07037. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.0120543875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4245883e-07
Norm of the params: 9.153144
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.012054494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0097873e-07
Norm of the params: 9.153144
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11792894
Train loss (w/o reg) on all data: 0.10901688
Test loss (w/o reg) on all data: 0.069840305
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.1963235e-06
Norm of the params: 13.350697
              Random: fixed   6 labels. Loss 0.06984. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.012054601
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2637756e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012054577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.08672e-08
Norm of the params: 9.153211
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11142021
Train loss (w/o reg) on all data: 0.102146015
Test loss (w/o reg) on all data: 0.06532057
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.001783e-06
Norm of the params: 13.619247
              Random: fixed   8 labels. Loss 0.06532. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012056149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6353567e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012056158
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.481568e-07
Norm of the params: 9.153206
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10909672
Train loss (w/o reg) on all data: 0.09965431
Test loss (w/o reg) on all data: 0.06418503
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2895679e-05
Norm of the params: 13.742207
              Random: fixed   9 labels. Loss 0.06419. Accuracy 0.989.
### Flips: 104, rs: 31, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012055255
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.299071e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729737
Test loss (w/o reg) on all data: 0.012055345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3110061e-07
Norm of the params: 9.153191
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10909669
Train loss (w/o reg) on all data: 0.09965335
Test loss (w/o reg) on all data: 0.06418117
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9865864e-05
Norm of the params: 13.742886
              Random: fixed   9 labels. Loss 0.06418. Accuracy 0.989.
### Flips: 104, rs: 31, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172996
Test loss (w/o reg) on all data: 0.012054897
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1065709e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.012054856
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6906818e-07
Norm of the params: 9.153168
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10909669
Train loss (w/o reg) on all data: 0.09965377
Test loss (w/o reg) on all data: 0.0641907
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7848537e-05
Norm of the params: 13.742575
              Random: fixed   9 labels. Loss 0.06419. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15588246
Train loss (w/o reg) on all data: 0.14883298
Test loss (w/o reg) on all data: 0.072500326
Train acc on all data:  0.9474689589302769
Test acc on all data:   1.0
Norm of the mean of gradients: 3.793983e-05
Norm of the params: 11.873916
Flipped loss: 0.07250. Accuracy: 1.000
### Flips: 104, rs: 32, checks: 52
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053050414
Train loss (w/o reg) on all data: 0.044924635
Test loss (w/o reg) on all data: 0.0411371
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8665543e-06
Norm of the params: 12.74816
     Influence (LOO): fixed  42 labels. Loss 0.04114. Accuracy 0.989.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018982196
Train loss (w/o reg) on all data: 0.010182822
Test loss (w/o reg) on all data: 0.013297514
Train acc on all data:  0.9980897803247374
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9857846e-07
Norm of the params: 13.266027
                Loss: fixed  52 labels. Loss 0.01330. Accuracy 1.000.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14833789
Train loss (w/o reg) on all data: 0.14132008
Test loss (w/o reg) on all data: 0.07540501
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.459009e-06
Norm of the params: 11.847198
              Random: fixed   4 labels. Loss 0.07541. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 104
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013749358
Train loss (w/o reg) on all data: 0.008246637
Test loss (w/o reg) on all data: 0.010743446
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0267419e-06
Norm of the params: 10.490684
     Influence (LOO): fixed  59 labels. Loss 0.01074. Accuracy 0.996.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009718804
Train loss (w/o reg) on all data: 0.0038490258
Test loss (w/o reg) on all data: 0.012001578
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.334876e-08
Norm of the params: 10.834923
                Loss: fixed  59 labels. Loss 0.01200. Accuracy 0.996.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14456077
Train loss (w/o reg) on all data: 0.13763447
Test loss (w/o reg) on all data: 0.073729955
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5888509e-05
Norm of the params: 11.769707
              Random: fixed   6 labels. Loss 0.07373. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173043
Test loss (w/o reg) on all data: 0.012055838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1250515e-07
Norm of the params: 9.153113
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074045793
Train loss (w/o reg) on all data: 0.002783318
Test loss (w/o reg) on all data: 0.013697866
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4799448e-07
Norm of the params: 9.613804
                Loss: fixed  60 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13837536
Train loss (w/o reg) on all data: 0.13177744
Test loss (w/o reg) on all data: 0.07100863
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0345419e-05
Norm of the params: 11.487318
              Random: fixed  10 labels. Loss 0.07101. Accuracy 0.992.
### Flips: 104, rs: 32, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012056038
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6010417e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012055921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7579128e-07
Norm of the params: 9.153188
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13514914
Train loss (w/o reg) on all data: 0.12848043
Test loss (w/o reg) on all data: 0.06573498
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6932987e-05
Norm of the params: 11.548768
              Random: fixed  13 labels. Loss 0.06573. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730326
Test loss (w/o reg) on all data: 0.012055175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8520856e-07
Norm of the params: 9.153126
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730328
Test loss (w/o reg) on all data: 0.012055128
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.864916e-08
Norm of the params: 9.153127
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12884292
Train loss (w/o reg) on all data: 0.12184917
Test loss (w/o reg) on all data: 0.06235095
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.761876e-05
Norm of the params: 11.826872
              Random: fixed  16 labels. Loss 0.06235. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728603
Test loss (w/o reg) on all data: 0.012055139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3834486e-07
Norm of the params: 9.1533165
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728608
Test loss (w/o reg) on all data: 0.01205526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.837949e-07
Norm of the params: 9.1533165
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11609432
Train loss (w/o reg) on all data: 0.109004
Test loss (w/o reg) on all data: 0.058458257
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3890894e-05
Norm of the params: 11.908251
              Random: fixed  21 labels. Loss 0.05846. Accuracy 0.996.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11541682
Train loss (w/o reg) on all data: 0.107102506
Test loss (w/o reg) on all data: 0.06910969
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4328334e-06
Norm of the params: 12.895203
Flipped loss: 0.06911. Accuracy: 0.992
### Flips: 104, rs: 33, checks: 52
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017913006
Train loss (w/o reg) on all data: 0.012042679
Test loss (w/o reg) on all data: 0.01749755
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.505645e-07
Norm of the params: 10.835432
     Influence (LOO): fixed  38 labels. Loss 0.01750. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007380169
Train loss (w/o reg) on all data: 0.002746552
Test loss (w/o reg) on all data: 0.013937118
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.442193e-06
Norm of the params: 9.626647
                Loss: fixed  40 labels. Loss 0.01394. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113608554
Train loss (w/o reg) on all data: 0.10541666
Test loss (w/o reg) on all data: 0.0689906
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.249696e-06
Norm of the params: 12.799914
              Random: fixed   1 labels. Loss 0.06899. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012054714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0954984e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021730058
Test loss (w/o reg) on all data: 0.0120546585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6114917e-07
Norm of the params: 9.153159
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10948463
Train loss (w/o reg) on all data: 0.10124543
Test loss (w/o reg) on all data: 0.06596608
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.9123503e-06
Norm of the params: 12.836817
              Random: fixed   3 labels. Loss 0.06597. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012054558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.516316e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729395
Test loss (w/o reg) on all data: 0.012054618
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3543195e-07
Norm of the params: 9.153227
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10948463
Train loss (w/o reg) on all data: 0.1012418
Test loss (w/o reg) on all data: 0.0659728
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.8604784e-06
Norm of the params: 12.83965
              Random: fixed   3 labels. Loss 0.06597. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012054522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2997848e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729355
Test loss (w/o reg) on all data: 0.012054443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5422384e-07
Norm of the params: 9.153234
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10137649
Train loss (w/o reg) on all data: 0.09331699
Test loss (w/o reg) on all data: 0.06410551
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.3515185e-05
Norm of the params: 12.696064
              Random: fixed   6 labels. Loss 0.06411. Accuracy 0.985.
### Flips: 104, rs: 33, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729334
Test loss (w/o reg) on all data: 0.012054743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1912629e-06
Norm of the params: 9.153234
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5992366e-07
Norm of the params: 9.153233
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09841187
Train loss (w/o reg) on all data: 0.09058885
Test loss (w/o reg) on all data: 0.054223917
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.524607e-06
Norm of the params: 12.5084095
              Random: fixed   9 labels. Loss 0.05422. Accuracy 0.992.
### Flips: 104, rs: 33, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729157
Test loss (w/o reg) on all data: 0.012054025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3058385e-07
Norm of the params: 9.1532545
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172916
Test loss (w/o reg) on all data: 0.012053909
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1781917e-07
Norm of the params: 9.153254
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093664266
Train loss (w/o reg) on all data: 0.086049326
Test loss (w/o reg) on all data: 0.05270478
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6639442e-05
Norm of the params: 12.340942
              Random: fixed  11 labels. Loss 0.05270. Accuracy 0.989.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12758242
Train loss (w/o reg) on all data: 0.11891563
Test loss (w/o reg) on all data: 0.08707571
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.729445e-06
Norm of the params: 13.165697
Flipped loss: 0.08708. Accuracy: 0.981
### Flips: 104, rs: 34, checks: 52
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027994135
Train loss (w/o reg) on all data: 0.02042938
Test loss (w/o reg) on all data: 0.02827438
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6237553e-06
Norm of the params: 12.300207
     Influence (LOO): fixed  38 labels. Loss 0.02827. Accuracy 0.996.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009473183
Train loss (w/o reg) on all data: 0.004380859
Test loss (w/o reg) on all data: 0.009146139
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7337632e-07
Norm of the params: 10.091903
                Loss: fixed  45 labels. Loss 0.00915. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1275824
Train loss (w/o reg) on all data: 0.11891619
Test loss (w/o reg) on all data: 0.087073326
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.2178034e-06
Norm of the params: 13.165269
              Random: fixed   0 labels. Loss 0.08707. Accuracy 0.981.
### Flips: 104, rs: 34, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729448
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1557186e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012055193
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.151449e-07
Norm of the params: 9.153217
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12516795
Train loss (w/o reg) on all data: 0.11621725
Test loss (w/o reg) on all data: 0.08748145
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.040987e-05
Norm of the params: 13.379612
              Random: fixed   1 labels. Loss 0.08748. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730151
Test loss (w/o reg) on all data: 0.012056774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4217069e-06
Norm of the params: 9.153146
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.01205651
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6946955e-07
Norm of the params: 9.153149
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112383574
Train loss (w/o reg) on all data: 0.103573374
Test loss (w/o reg) on all data: 0.08102796
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3300241e-05
Norm of the params: 13.274185
              Random: fixed   7 labels. Loss 0.08103. Accuracy 0.973.
### Flips: 104, rs: 34, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729881
Test loss (w/o reg) on all data: 0.012055778
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.631294e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.002172985
Test loss (w/o reg) on all data: 0.01205587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1357486e-07
Norm of the params: 9.153174
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10802826
Train loss (w/o reg) on all data: 0.0992692
Test loss (w/o reg) on all data: 0.07413618
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.108774e-06
Norm of the params: 13.235607
              Random: fixed  10 labels. Loss 0.07414. Accuracy 0.977.
### Flips: 104, rs: 34, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729665
Test loss (w/o reg) on all data: 0.012055345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7629668e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.902319e-08
Norm of the params: 9.153201
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09972504
Train loss (w/o reg) on all data: 0.09073399
Test loss (w/o reg) on all data: 0.0698974
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5150933e-05
Norm of the params: 13.40973
              Random: fixed  13 labels. Loss 0.06990. Accuracy 0.977.
### Flips: 104, rs: 34, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730745
Test loss (w/o reg) on all data: 0.01205535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.896538e-07
Norm of the params: 9.15308
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730564
Test loss (w/o reg) on all data: 0.012055317
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5972616e-07
Norm of the params: 9.153098
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091145985
Train loss (w/o reg) on all data: 0.08142901
Test loss (w/o reg) on all data: 0.06926417
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.792032e-06
Norm of the params: 13.94057
              Random: fixed  16 labels. Loss 0.06926. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14201878
Train loss (w/o reg) on all data: 0.13409936
Test loss (w/o reg) on all data: 0.08634575
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2127607e-05
Norm of the params: 12.585246
Flipped loss: 0.08635. Accuracy: 0.989
### Flips: 104, rs: 35, checks: 52
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04250336
Train loss (w/o reg) on all data: 0.033606436
Test loss (w/o reg) on all data: 0.04807835
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.338028e-07
Norm of the params: 13.339358
     Influence (LOO): fixed  38 labels. Loss 0.04808. Accuracy 0.985.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0105005875
Train loss (w/o reg) on all data: 0.0042595495
Test loss (w/o reg) on all data: 0.008185238
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.812856e-07
Norm of the params: 11.172321
                Loss: fixed  51 labels. Loss 0.00819. Accuracy 0.996.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13623777
Train loss (w/o reg) on all data: 0.12771921
Test loss (w/o reg) on all data: 0.086944856
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.631847e-05
Norm of the params: 13.052629
              Random: fixed   1 labels. Loss 0.08694. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 104
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015878158
Train loss (w/o reg) on all data: 0.010455557
Test loss (w/o reg) on all data: 0.012674213
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.209211e-07
Norm of the params: 10.41403
     Influence (LOO): fixed  50 labels. Loss 0.01267. Accuracy 0.996.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008034243
Train loss (w/o reg) on all data: 0.003051638
Test loss (w/o reg) on all data: 0.008116325
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6102823e-07
Norm of the params: 9.982591
                Loss: fixed  52 labels. Loss 0.00812. Accuracy 0.996.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13249098
Train loss (w/o reg) on all data: 0.12378408
Test loss (w/o reg) on all data: 0.08371559
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.9292737e-06
Norm of the params: 13.1961355
              Random: fixed   4 labels. Loss 0.08372. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.012054991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2925694e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055043
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9545765e-08
Norm of the params: 9.153158
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13249098
Train loss (w/o reg) on all data: 0.12377951
Test loss (w/o reg) on all data: 0.08370939
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2115074e-05
Norm of the params: 13.199599
              Random: fixed   4 labels. Loss 0.08371. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729965
Test loss (w/o reg) on all data: 0.012056877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.516682e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729947
Test loss (w/o reg) on all data: 0.012056423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.646932e-07
Norm of the params: 9.153169
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1257613
Train loss (w/o reg) on all data: 0.11678328
Test loss (w/o reg) on all data: 0.0798569
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.511584e-05
Norm of the params: 13.400008
              Random: fixed   8 labels. Loss 0.07986. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730063
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2500006e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.012055557
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9682346e-07
Norm of the params: 9.153157
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105027884
Train loss (w/o reg) on all data: 0.0964173
Test loss (w/o reg) on all data: 0.076759264
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.007121e-06
Norm of the params: 13.122948
              Random: fixed  15 labels. Loss 0.07676. Accuracy 0.969.
### Flips: 104, rs: 35, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012055965
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7965683e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.01205582
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3172182e-07
Norm of the params: 9.153218
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1002555
Train loss (w/o reg) on all data: 0.09119453
Test loss (w/o reg) on all data: 0.07708876
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.5449043e-06
Norm of the params: 13.461772
              Random: fixed  17 labels. Loss 0.07709. Accuracy 0.973.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14465529
Train loss (w/o reg) on all data: 0.13706765
Test loss (w/o reg) on all data: 0.071406126
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.548173e-05
Norm of the params: 12.318793
Flipped loss: 0.07141. Accuracy: 0.989
### Flips: 104, rs: 36, checks: 52
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027654976
Train loss (w/o reg) on all data: 0.020273378
Test loss (w/o reg) on all data: 0.017049555
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0152761e-06
Norm of the params: 12.15039
     Influence (LOO): fixed  44 labels. Loss 0.01705. Accuracy 0.996.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011417238
Train loss (w/o reg) on all data: 0.004780094
Test loss (w/o reg) on all data: 0.013669575
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.131535e-07
Norm of the params: 11.521409
                Loss: fixed  49 labels. Loss 0.01367. Accuracy 0.996.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14059895
Train loss (w/o reg) on all data: 0.13289398
Test loss (w/o reg) on all data: 0.07244427
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7339536e-05
Norm of the params: 12.413678
              Random: fixed   1 labels. Loss 0.07244. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971536
Train loss (w/o reg) on all data: 0.0025171882
Test loss (w/o reg) on all data: 0.011187921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5640995e-07
Norm of the params: 9.438588
     Influence (LOO): fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971536
Train loss (w/o reg) on all data: 0.002517189
Test loss (w/o reg) on all data: 0.011187879
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.16707945e-07
Norm of the params: 9.438588
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14059892
Train loss (w/o reg) on all data: 0.13289584
Test loss (w/o reg) on all data: 0.07243062
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9597213e-05
Norm of the params: 12.412153
              Random: fixed   1 labels. Loss 0.07243. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715353
Train loss (w/o reg) on all data: 0.0025172366
Test loss (w/o reg) on all data: 0.0111881085
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8299052e-07
Norm of the params: 9.438538
     Influence (LOO): fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971536
Train loss (w/o reg) on all data: 0.0025172324
Test loss (w/o reg) on all data: 0.011188139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.939039e-07
Norm of the params: 9.438542
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13449214
Train loss (w/o reg) on all data: 0.12714398
Test loss (w/o reg) on all data: 0.06774461
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5479738e-05
Norm of the params: 12.122841
              Random: fixed   4 labels. Loss 0.06774. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728578
Test loss (w/o reg) on all data: 0.012054435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0939054e-07
Norm of the params: 9.1533165
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715343
Train loss (w/o reg) on all data: 0.0025172243
Test loss (w/o reg) on all data: 0.011187215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9149468e-07
Norm of the params: 9.438549
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12394377
Train loss (w/o reg) on all data: 0.11659972
Test loss (w/o reg) on all data: 0.06685043
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.565169e-06
Norm of the params: 12.11944
              Random: fixed   8 labels. Loss 0.06685. Accuracy 0.985.
### Flips: 104, rs: 36, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.012054812
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7060794e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715334
Train loss (w/o reg) on all data: 0.0025171123
Test loss (w/o reg) on all data: 0.011185719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7285705e-06
Norm of the params: 9.438667
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12118868
Train loss (w/o reg) on all data: 0.11391595
Test loss (w/o reg) on all data: 0.06314457
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4299192e-05
Norm of the params: 12.060457
              Random: fixed  10 labels. Loss 0.06314. Accuracy 0.985.
### Flips: 104, rs: 36, checks: 312
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.400882e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715343
Train loss (w/o reg) on all data: 0.0025171938
Test loss (w/o reg) on all data: 0.01118757
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1399264e-07
Norm of the params: 9.438581
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12011518
Train loss (w/o reg) on all data: 0.1129945
Test loss (w/o reg) on all data: 0.061219238
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1017673e-05
Norm of the params: 11.9337225
              Random: fixed  12 labels. Loss 0.06122. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101031065
Train loss (w/o reg) on all data: 0.092530526
Test loss (w/o reg) on all data: 0.06342424
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7710568e-05
Norm of the params: 13.038818
Flipped loss: 0.06342. Accuracy: 0.977
### Flips: 104, rs: 37, checks: 52
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009596841
Train loss (w/o reg) on all data: 0.003968128
Test loss (w/o reg) on all data: 0.015909709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5948702e-07
Norm of the params: 10.610103
     Influence (LOO): fixed  35 labels. Loss 0.01591. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009789285
Train loss (w/o reg) on all data: 0.00376721
Test loss (w/o reg) on all data: 0.018436758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.328636e-07
Norm of the params: 10.974585
                Loss: fixed  34 labels. Loss 0.01844. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099677436
Train loss (w/o reg) on all data: 0.091054425
Test loss (w/o reg) on all data: 0.060333103
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.678022e-06
Norm of the params: 13.132412
              Random: fixed   1 labels. Loss 0.06033. Accuracy 0.985.
### Flips: 104, rs: 37, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173043
Test loss (w/o reg) on all data: 0.012056254
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.974342e-07
Norm of the params: 9.153115
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077022435
Train loss (w/o reg) on all data: 0.0027690914
Test loss (w/o reg) on all data: 0.012365596
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5136135e-07
Norm of the params: 9.932927
                Loss: fixed  36 labels. Loss 0.01237. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096876755
Train loss (w/o reg) on all data: 0.088299595
Test loss (w/o reg) on all data: 0.05636631
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.0246053e-06
Norm of the params: 13.097453
              Random: fixed   3 labels. Loss 0.05637. Accuracy 0.985.
### Flips: 104, rs: 37, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0875733e-07
Norm of the params: 9.153202
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5682723e-07
Norm of the params: 9.153202
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094722345
Train loss (w/o reg) on all data: 0.08648846
Test loss (w/o reg) on all data: 0.05751497
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.676852e-06
Norm of the params: 12.832681
              Random: fixed   4 labels. Loss 0.05751. Accuracy 0.977.
### Flips: 104, rs: 37, checks: 208
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729404
Test loss (w/o reg) on all data: 0.0120552005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6578864e-08
Norm of the params: 9.153227
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4280592e-07
Norm of the params: 9.153223
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0879325
Train loss (w/o reg) on all data: 0.07977492
Test loss (w/o reg) on all data: 0.053331025
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.0162287e-06
Norm of the params: 12.773074
              Random: fixed   7 labels. Loss 0.05333. Accuracy 0.989.
### Flips: 104, rs: 37, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3444638e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.675726e-08
Norm of the params: 9.153187
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08025715
Train loss (w/o reg) on all data: 0.07166132
Test loss (w/o reg) on all data: 0.044683535
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.461291e-06
Norm of the params: 13.111693
              Random: fixed   9 labels. Loss 0.04468. Accuracy 0.989.
### Flips: 104, rs: 37, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728768
Test loss (w/o reg) on all data: 0.012055136
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.537952e-07
Norm of the params: 9.153296
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728799
Test loss (w/o reg) on all data: 0.012055237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9798036e-07
Norm of the params: 9.153295
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075464204
Train loss (w/o reg) on all data: 0.06578613
Test loss (w/o reg) on all data: 0.043034296
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.5358726e-06
Norm of the params: 13.912636
              Random: fixed  10 labels. Loss 0.04303. Accuracy 0.989.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12960416
Train loss (w/o reg) on all data: 0.122009516
Test loss (w/o reg) on all data: 0.06798598
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4508707e-06
Norm of the params: 12.32448
Flipped loss: 0.06799. Accuracy: 0.989
### Flips: 104, rs: 38, checks: 52
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025478233
Train loss (w/o reg) on all data: 0.017808052
Test loss (w/o reg) on all data: 0.019436514
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3177628e-06
Norm of the params: 12.385622
     Influence (LOO): fixed  41 labels. Loss 0.01944. Accuracy 0.996.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010861204
Train loss (w/o reg) on all data: 0.004541236
Test loss (w/o reg) on all data: 0.014587352
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.336646e-07
Norm of the params: 11.242747
                Loss: fixed  44 labels. Loss 0.01459. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124116234
Train loss (w/o reg) on all data: 0.11622404
Test loss (w/o reg) on all data: 0.06450186
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1610335e-05
Norm of the params: 12.56359
              Random: fixed   2 labels. Loss 0.06450. Accuracy 0.989.
### Flips: 104, rs: 38, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729835
Test loss (w/o reg) on all data: 0.012054809
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5315725e-08
Norm of the params: 9.153182
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8222848e-07
Norm of the params: 9.153184
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11818332
Train loss (w/o reg) on all data: 0.110429145
Test loss (w/o reg) on all data: 0.060257483
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1745471e-05
Norm of the params: 12.453255
              Random: fixed   5 labels. Loss 0.06026. Accuracy 0.989.
### Flips: 104, rs: 38, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4564533e-07
Norm of the params: 9.153273
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729094
Test loss (w/o reg) on all data: 0.012055412
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.262834e-07
Norm of the params: 9.15326
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11280761
Train loss (w/o reg) on all data: 0.10508984
Test loss (w/o reg) on all data: 0.060239434
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6987745e-05
Norm of the params: 12.423981
              Random: fixed   8 labels. Loss 0.06024. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173007
Test loss (w/o reg) on all data: 0.012055074
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2737268e-07
Norm of the params: 9.153153
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173007
Test loss (w/o reg) on all data: 0.012055136
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1331647e-07
Norm of the params: 9.153154
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107296646
Train loss (w/o reg) on all data: 0.099302106
Test loss (w/o reg) on all data: 0.060580753
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.410458e-06
Norm of the params: 12.644792
              Random: fixed  10 labels. Loss 0.06058. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729919
Test loss (w/o reg) on all data: 0.012054888
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3151088e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729926
Test loss (w/o reg) on all data: 0.012054962
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1411628e-07
Norm of the params: 9.15317
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09839444
Train loss (w/o reg) on all data: 0.09069169
Test loss (w/o reg) on all data: 0.060266346
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4603819e-05
Norm of the params: 12.411889
              Random: fixed  14 labels. Loss 0.06027. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729243
Test loss (w/o reg) on all data: 0.012055146
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9415653e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172924
Test loss (w/o reg) on all data: 0.012055054
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2196666e-07
Norm of the params: 9.153245
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09222511
Train loss (w/o reg) on all data: 0.0847385
Test loss (w/o reg) on all data: 0.060631774
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3326956e-05
Norm of the params: 12.236512
              Random: fixed  17 labels. Loss 0.06063. Accuracy 0.981.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11586003
Train loss (w/o reg) on all data: 0.10806009
Test loss (w/o reg) on all data: 0.07757134
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7587987e-05
Norm of the params: 12.489945
Flipped loss: 0.07757. Accuracy: 0.977
### Flips: 104, rs: 39, checks: 52
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01659909
Train loss (w/o reg) on all data: 0.010987925
Test loss (w/o reg) on all data: 0.016169768
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.4007594e-07
Norm of the params: 10.593551
     Influence (LOO): fixed  38 labels. Loss 0.01617. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011979142
Train loss (w/o reg) on all data: 0.0052503906
Test loss (w/o reg) on all data: 0.019830018
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.650651e-07
Norm of the params: 11.600649
                Loss: fixed  38 labels. Loss 0.01983. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11472028
Train loss (w/o reg) on all data: 0.107050315
Test loss (w/o reg) on all data: 0.077683225
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.991901e-06
Norm of the params: 12.385443
              Random: fixed   1 labels. Loss 0.07768. Accuracy 0.977.
### Flips: 104, rs: 39, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00897926
Train loss (w/o reg) on all data: 0.0037582708
Test loss (w/o reg) on all data: 0.013043316
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4397718e-07
Norm of the params: 10.2186
     Influence (LOO): fixed  40 labels. Loss 0.01304. Accuracy 0.996.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730307
Test loss (w/o reg) on all data: 0.012056356
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1443638e-06
Norm of the params: 9.153128
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11360637
Train loss (w/o reg) on all data: 0.10623163
Test loss (w/o reg) on all data: 0.07564217
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0018186e-05
Norm of the params: 12.144743
              Random: fixed   3 labels. Loss 0.07564. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.0120547395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2645627e-07
Norm of the params: 9.153135
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173023
Test loss (w/o reg) on all data: 0.012054811
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3558436e-07
Norm of the params: 9.153136
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10895489
Train loss (w/o reg) on all data: 0.10150492
Test loss (w/o reg) on all data: 0.073351495
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1274473e-05
Norm of the params: 12.206529
              Random: fixed   4 labels. Loss 0.07335. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012053768
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7747188e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.012054018
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4681643e-07
Norm of the params: 9.153221
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10245096
Train loss (w/o reg) on all data: 0.09493747
Test loss (w/o reg) on all data: 0.07056184
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2876245e-06
Norm of the params: 12.258454
              Random: fixed   7 labels. Loss 0.07056. Accuracy 0.985.
### Flips: 104, rs: 39, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729318
Test loss (w/o reg) on all data: 0.012054391
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9709887e-07
Norm of the params: 9.153235
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012054552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1101703e-07
Norm of the params: 9.153235
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09211655
Train loss (w/o reg) on all data: 0.08436935
Test loss (w/o reg) on all data: 0.06522779
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0591512e-05
Norm of the params: 12.44765
              Random: fixed  11 labels. Loss 0.06523. Accuracy 0.977.
### Flips: 104, rs: 39, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012055937
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.744146e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012055981
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5264235e-07
Norm of the params: 9.153195
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09130482
Train loss (w/o reg) on all data: 0.08374826
Test loss (w/o reg) on all data: 0.060255002
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.6744234e-06
Norm of the params: 12.29354
              Random: fixed  12 labels. Loss 0.06026. Accuracy 0.985.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17835598
Train loss (w/o reg) on all data: 0.17243056
Test loss (w/o reg) on all data: 0.11581246
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.070165e-05
Norm of the params: 10.886154
Flipped loss: 0.11581. Accuracy: 0.973
### Flips: 156, rs: 0, checks: 52
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07595442
Train loss (w/o reg) on all data: 0.06769152
Test loss (w/o reg) on all data: 0.07994032
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8863186e-05
Norm of the params: 12.855273
     Influence (LOO): fixed  43 labels. Loss 0.07994. Accuracy 0.962.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038306978
Train loss (w/o reg) on all data: 0.0268011
Test loss (w/o reg) on all data: 0.048834313
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4341093e-06
Norm of the params: 15.169626
                Loss: fixed  52 labels. Loss 0.04883. Accuracy 0.977.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17057797
Train loss (w/o reg) on all data: 0.16447896
Test loss (w/o reg) on all data: 0.11188651
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.268366e-05
Norm of the params: 11.044472
              Random: fixed   5 labels. Loss 0.11189. Accuracy 0.973.
### Flips: 156, rs: 0, checks: 104
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02933461
Train loss (w/o reg) on all data: 0.022091191
Test loss (w/o reg) on all data: 0.037114043
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8220434e-06
Norm of the params: 12.036128
     Influence (LOO): fixed  65 labels. Loss 0.03711. Accuracy 0.981.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715325
Train loss (w/o reg) on all data: 0.0025173111
Test loss (w/o reg) on all data: 0.011187513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4330643e-07
Norm of the params: 9.438455
                Loss: fixed  74 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15291591
Train loss (w/o reg) on all data: 0.1465251
Test loss (w/o reg) on all data: 0.11010588
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.9872335e-06
Norm of the params: 11.305588
              Random: fixed  14 labels. Loss 0.11011. Accuracy 0.962.
### Flips: 156, rs: 0, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5670073e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055243
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3595051e-07
Norm of the params: 9.15318
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14595827
Train loss (w/o reg) on all data: 0.13871431
Test loss (w/o reg) on all data: 0.10764578
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2255073e-05
Norm of the params: 12.036573
              Random: fixed  17 labels. Loss 0.10765. Accuracy 0.958.
### Flips: 156, rs: 0, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.012055076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.10933605e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012055131
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1071764e-07
Norm of the params: 9.153219
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13949351
Train loss (w/o reg) on all data: 0.13212164
Test loss (w/o reg) on all data: 0.099666685
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.468771e-06
Norm of the params: 12.142388
              Random: fixed  21 labels. Loss 0.09967. Accuracy 0.969.
### Flips: 156, rs: 0, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054933
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1867785e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.012055191
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1998541e-07
Norm of the params: 9.153223
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13612594
Train loss (w/o reg) on all data: 0.12885025
Test loss (w/o reg) on all data: 0.09787325
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.5279958e-05
Norm of the params: 12.062904
              Random: fixed  22 labels. Loss 0.09787. Accuracy 0.973.
### Flips: 156, rs: 0, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730429
Test loss (w/o reg) on all data: 0.012056662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.315303e-07
Norm of the params: 9.153115
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730412
Test loss (w/o reg) on all data: 0.012056471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.864381e-07
Norm of the params: 9.153116
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13612595
Train loss (w/o reg) on all data: 0.12884691
Test loss (w/o reg) on all data: 0.097867504
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4119159e-05
Norm of the params: 12.065688
              Random: fixed  22 labels. Loss 0.09787. Accuracy 0.973.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17488225
Train loss (w/o reg) on all data: 0.16710474
Test loss (w/o reg) on all data: 0.07697325
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.464719e-05
Norm of the params: 12.471973
Flipped loss: 0.07697. Accuracy: 0.992
### Flips: 156, rs: 1, checks: 52
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07612992
Train loss (w/o reg) on all data: 0.06672019
Test loss (w/o reg) on all data: 0.035305075
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.810666e-06
Norm of the params: 13.718404
     Influence (LOO): fixed  44 labels. Loss 0.03531. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040485248
Train loss (w/o reg) on all data: 0.0276978
Test loss (w/o reg) on all data: 0.038411785
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1881385e-06
Norm of the params: 15.992154
                Loss: fixed  52 labels. Loss 0.03841. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17177568
Train loss (w/o reg) on all data: 0.16395485
Test loss (w/o reg) on all data: 0.072703384
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.358941e-05
Norm of the params: 12.506667
              Random: fixed   2 labels. Loss 0.07270. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 104
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034939844
Train loss (w/o reg) on all data: 0.027462129
Test loss (w/o reg) on all data: 0.01803198
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.50585e-06
Norm of the params: 12.229239
     Influence (LOO): fixed  62 labels. Loss 0.01803. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008888102
Train loss (w/o reg) on all data: 0.0035020087
Test loss (w/o reg) on all data: 0.01281464
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4042976e-07
Norm of the params: 10.378914
                Loss: fixed  71 labels. Loss 0.01281. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16919686
Train loss (w/o reg) on all data: 0.16141216
Test loss (w/o reg) on all data: 0.07076073
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1831942e-05
Norm of the params: 12.47774
              Random: fixed   4 labels. Loss 0.07076. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728978
Test loss (w/o reg) on all data: 0.012055064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0462116e-07
Norm of the params: 9.153274
     Influence (LOO): fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012055007
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.887014e-07
Norm of the params: 9.153212
                Loss: fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1593173
Train loss (w/o reg) on all data: 0.15111214
Test loss (w/o reg) on all data: 0.066665076
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9933519e-05
Norm of the params: 12.810283
              Random: fixed   9 labels. Loss 0.06667. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172967
Test loss (w/o reg) on all data: 0.012054613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.03111745e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012054753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.541752e-07
Norm of the params: 9.153197
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15130052
Train loss (w/o reg) on all data: 0.143069
Test loss (w/o reg) on all data: 0.06521059
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2088843e-06
Norm of the params: 12.830838
              Random: fixed  14 labels. Loss 0.06521. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012054869
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1218113e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.012054857
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.422509e-08
Norm of the params: 9.153203
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14385194
Train loss (w/o reg) on all data: 0.13557968
Test loss (w/o reg) on all data: 0.063320175
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1507152e-05
Norm of the params: 12.862553
              Random: fixed  17 labels. Loss 0.06332. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012054758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.885133e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012054903
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1346058e-07
Norm of the params: 9.153227
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13066672
Train loss (w/o reg) on all data: 0.12244747
Test loss (w/o reg) on all data: 0.054482367
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.964758e-06
Norm of the params: 12.821271
              Random: fixed  24 labels. Loss 0.05448. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16469681
Train loss (w/o reg) on all data: 0.15678231
Test loss (w/o reg) on all data: 0.08393541
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.565492e-05
Norm of the params: 12.581335
Flipped loss: 0.08394. Accuracy: 0.977
### Flips: 156, rs: 2, checks: 52
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071691
Train loss (w/o reg) on all data: 0.06261343
Test loss (w/o reg) on all data: 0.05002158
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.1595493e-05
Norm of the params: 13.474104
     Influence (LOO): fixed  42 labels. Loss 0.05002. Accuracy 0.977.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034745414
Train loss (w/o reg) on all data: 0.023518397
Test loss (w/o reg) on all data: 0.04013553
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.0594815e-06
Norm of the params: 14.984672
                Loss: fixed  52 labels. Loss 0.04014. Accuracy 0.981.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1574073
Train loss (w/o reg) on all data: 0.14952677
Test loss (w/o reg) on all data: 0.075780906
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6580415e-05
Norm of the params: 12.554302
              Random: fixed   5 labels. Loss 0.07578. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 104
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024695724
Train loss (w/o reg) on all data: 0.015784731
Test loss (w/o reg) on all data: 0.024139518
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6723858e-06
Norm of the params: 13.3499
     Influence (LOO): fixed  63 labels. Loss 0.02414. Accuracy 0.989.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833817
Train loss (w/o reg) on all data: 0.0028981986
Test loss (w/o reg) on all data: 0.012001576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3533724e-07
Norm of the params: 9.884517
                Loss: fixed  70 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15091434
Train loss (w/o reg) on all data: 0.14284076
Test loss (w/o reg) on all data: 0.07570899
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6470778e-05
Norm of the params: 12.70715
              Random: fixed   9 labels. Loss 0.07571. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 156
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01197348
Train loss (w/o reg) on all data: 0.0064862235
Test loss (w/o reg) on all data: 0.01335649
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7441384e-07
Norm of the params: 10.475931
     Influence (LOO): fixed  69 labels. Loss 0.01336. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730606
Test loss (w/o reg) on all data: 0.012054846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.404061e-07
Norm of the params: 9.153095
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14270742
Train loss (w/o reg) on all data: 0.13501656
Test loss (w/o reg) on all data: 0.07292218
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1011732e-05
Norm of the params: 12.402314
              Random: fixed  14 labels. Loss 0.07292. Accuracy 0.981.
### Flips: 156, rs: 2, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.0120552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2297574e-07
Norm of the params: 9.1532135
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729528
Test loss (w/o reg) on all data: 0.012055156
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0040666e-07
Norm of the params: 9.1532135
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13980992
Train loss (w/o reg) on all data: 0.13191718
Test loss (w/o reg) on all data: 0.06718483
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2090015e-05
Norm of the params: 12.5640335
              Random: fixed  16 labels. Loss 0.06718. Accuracy 0.989.
### Flips: 156, rs: 2, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.012054794
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1585414e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.012054785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3359657e-07
Norm of the params: 9.15322
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12329301
Train loss (w/o reg) on all data: 0.115351416
Test loss (w/o reg) on all data: 0.05980776
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0388594e-05
Norm of the params: 12.60285
              Random: fixed  24 labels. Loss 0.05981. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012055286
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5243434e-07
Norm of the params: 9.153236
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729297
Test loss (w/o reg) on all data: 0.012055231
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5591559e-07
Norm of the params: 9.153236
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11791285
Train loss (w/o reg) on all data: 0.1099249
Test loss (w/o reg) on all data: 0.055101838
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5443471e-05
Norm of the params: 12.639584
              Random: fixed  27 labels. Loss 0.05510. Accuracy 0.989.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18960547
Train loss (w/o reg) on all data: 0.1818108
Test loss (w/o reg) on all data: 0.12294589
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.1417832e-05
Norm of the params: 12.48573
Flipped loss: 0.12295. Accuracy: 0.973
### Flips: 156, rs: 3, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1002512
Train loss (w/o reg) on all data: 0.09047914
Test loss (w/o reg) on all data: 0.08104921
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.298647e-06
Norm of the params: 13.980027
     Influence (LOO): fixed  41 labels. Loss 0.08105. Accuracy 0.973.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065891735
Train loss (w/o reg) on all data: 0.052881353
Test loss (w/o reg) on all data: 0.07793523
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1213075e-06
Norm of the params: 16.130953
                Loss: fixed  51 labels. Loss 0.07794. Accuracy 0.977.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18581545
Train loss (w/o reg) on all data: 0.17790942
Test loss (w/o reg) on all data: 0.1173354
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.799818e-06
Norm of the params: 12.574611
              Random: fixed   3 labels. Loss 0.11734. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 104
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05065917
Train loss (w/o reg) on all data: 0.042689748
Test loss (w/o reg) on all data: 0.028662864
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.045042e-06
Norm of the params: 12.624911
     Influence (LOO): fixed  66 labels. Loss 0.02866. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009946374
Train loss (w/o reg) on all data: 0.004128933
Test loss (w/o reg) on all data: 0.025591725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.453029e-07
Norm of the params: 10.786511
                Loss: fixed  79 labels. Loss 0.02559. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1805477
Train loss (w/o reg) on all data: 0.17325406
Test loss (w/o reg) on all data: 0.11155986
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1248087e-05
Norm of the params: 12.07778
              Random: fixed   8 labels. Loss 0.11156. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 156
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016517136
Train loss (w/o reg) on all data: 0.010257147
Test loss (w/o reg) on all data: 0.009906965
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.548125e-07
Norm of the params: 11.189272
     Influence (LOO): fixed  80 labels. Loss 0.00991. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008392006
Train loss (w/o reg) on all data: 0.0033220605
Test loss (w/o reg) on all data: 0.014456848
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.457408e-07
Norm of the params: 10.069703
                Loss: fixed  81 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17042607
Train loss (w/o reg) on all data: 0.16309285
Test loss (w/o reg) on all data: 0.102666505
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0436625e-05
Norm of the params: 12.110505
              Random: fixed  14 labels. Loss 0.10267. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4846643e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172964
Test loss (w/o reg) on all data: 0.012055226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3557488e-07
Norm of the params: 9.153201
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16477622
Train loss (w/o reg) on all data: 0.15735035
Test loss (w/o reg) on all data: 0.09994873
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7466724e-05
Norm of the params: 12.18677
              Random: fixed  18 labels. Loss 0.09995. Accuracy 0.977.
### Flips: 156, rs: 3, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729444
Test loss (w/o reg) on all data: 0.012054819
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.36681e-08
Norm of the params: 9.153224
     Influence (LOO): fixed  83 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012054729
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.150984e-07
Norm of the params: 9.1532135
                Loss: fixed  83 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1573412
Train loss (w/o reg) on all data: 0.15005697
Test loss (w/o reg) on all data: 0.09575639
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9703812e-05
Norm of the params: 12.069978
              Random: fixed  22 labels. Loss 0.09576. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172712
Test loss (w/o reg) on all data: 0.012055722
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.751069e-07
Norm of the params: 9.153478
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021727148
Test loss (w/o reg) on all data: 0.012055547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1987182e-07
Norm of the params: 9.153475
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14532264
Train loss (w/o reg) on all data: 0.13796106
Test loss (w/o reg) on all data: 0.08038076
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6019286e-05
Norm of the params: 12.133897
              Random: fixed  29 labels. Loss 0.08038. Accuracy 0.985.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15575309
Train loss (w/o reg) on all data: 0.14831388
Test loss (w/o reg) on all data: 0.067827664
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3685166e-05
Norm of the params: 12.197709
Flipped loss: 0.06783. Accuracy: 0.996
### Flips: 156, rs: 4, checks: 52
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062258415
Train loss (w/o reg) on all data: 0.054088354
Test loss (w/o reg) on all data: 0.035863638
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6091238e-06
Norm of the params: 12.782851
     Influence (LOO): fixed  42 labels. Loss 0.03586. Accuracy 0.996.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01914627
Train loss (w/o reg) on all data: 0.010526188
Test loss (w/o reg) on all data: 0.014769438
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4699563e-06
Norm of the params: 13.13018
                Loss: fixed  51 labels. Loss 0.01477. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15028867
Train loss (w/o reg) on all data: 0.14214362
Test loss (w/o reg) on all data: 0.0635826
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6450207e-05
Norm of the params: 12.763272
              Random: fixed   3 labels. Loss 0.06358. Accuracy 0.996.
### Flips: 156, rs: 4, checks: 104
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011234205
Train loss (w/o reg) on all data: 0.005528465
Test loss (w/o reg) on all data: 0.014959235
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.777989e-07
Norm of the params: 10.682454
     Influence (LOO): fixed  60 labels. Loss 0.01496. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00750565
Train loss (w/o reg) on all data: 0.002817139
Test loss (w/o reg) on all data: 0.012754763
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.917142e-07
Norm of the params: 9.683503
                Loss: fixed  61 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1472038
Train loss (w/o reg) on all data: 0.13907616
Test loss (w/o reg) on all data: 0.063915335
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.198369e-05
Norm of the params: 12.749625
              Random: fixed   5 labels. Loss 0.06392. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012055166
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.748671e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729844
Test loss (w/o reg) on all data: 0.012055106
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7747362e-07
Norm of the params: 9.153179
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14666106
Train loss (w/o reg) on all data: 0.1382692
Test loss (w/o reg) on all data: 0.06805878
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9693347e-05
Norm of the params: 12.955194
              Random: fixed   6 labels. Loss 0.06806. Accuracy 0.989.
### Flips: 156, rs: 4, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729462
Test loss (w/o reg) on all data: 0.012055478
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1518332e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729472
Test loss (w/o reg) on all data: 0.012055432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.990361e-08
Norm of the params: 9.153221
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13333437
Train loss (w/o reg) on all data: 0.12476825
Test loss (w/o reg) on all data: 0.063949324
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3172927e-06
Norm of the params: 13.089022
              Random: fixed  12 labels. Loss 0.06395. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021727923
Test loss (w/o reg) on all data: 0.0120558245
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5432425e-07
Norm of the params: 9.15339
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172825
Test loss (w/o reg) on all data: 0.01205531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.127276e-07
Norm of the params: 9.153353
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1333344
Train loss (w/o reg) on all data: 0.1247694
Test loss (w/o reg) on all data: 0.06396474
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1005635e-05
Norm of the params: 13.088168
              Random: fixed  12 labels. Loss 0.06396. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730687
Test loss (w/o reg) on all data: 0.012055752
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.306166e-07
Norm of the params: 9.153087
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730661
Test loss (w/o reg) on all data: 0.0120556075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5133676e-07
Norm of the params: 9.153089
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12859419
Train loss (w/o reg) on all data: 0.119796306
Test loss (w/o reg) on all data: 0.06115845
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4064004e-05
Norm of the params: 13.264909
              Random: fixed  15 labels. Loss 0.06116. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18540363
Train loss (w/o reg) on all data: 0.17707229
Test loss (w/o reg) on all data: 0.11299758
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.73301e-06
Norm of the params: 12.908407
Flipped loss: 0.11300. Accuracy: 0.981
### Flips: 156, rs: 5, checks: 52
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09678234
Train loss (w/o reg) on all data: 0.08559182
Test loss (w/o reg) on all data: 0.06371383
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.990413e-05
Norm of the params: 14.960291
     Influence (LOO): fixed  40 labels. Loss 0.06371. Accuracy 0.981.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052349556
Train loss (w/o reg) on all data: 0.03867631
Test loss (w/o reg) on all data: 0.07436999
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5208677e-05
Norm of the params: 16.536774
                Loss: fixed  51 labels. Loss 0.07437. Accuracy 0.969.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17790571
Train loss (w/o reg) on all data: 0.17000538
Test loss (w/o reg) on all data: 0.10783898
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5695625e-05
Norm of the params: 12.5700655
              Random: fixed   5 labels. Loss 0.10784. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 104
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039997038
Train loss (w/o reg) on all data: 0.031348124
Test loss (w/o reg) on all data: 0.02675399
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5585347e-06
Norm of the params: 13.152121
     Influence (LOO): fixed  68 labels. Loss 0.02675. Accuracy 0.989.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008422848
Train loss (w/o reg) on all data: 0.0032732661
Test loss (w/o reg) on all data: 0.019715004
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1276265e-07
Norm of the params: 10.14848
                Loss: fixed  79 labels. Loss 0.01972. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1662629
Train loss (w/o reg) on all data: 0.15838492
Test loss (w/o reg) on all data: 0.104342036
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.355206e-05
Norm of the params: 12.552269
              Random: fixed  11 labels. Loss 0.10434. Accuracy 0.981.
### Flips: 156, rs: 5, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729842
Test loss (w/o reg) on all data: 0.012054599
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0311006e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833745
Train loss (w/o reg) on all data: 0.0026670168
Test loss (w/o reg) on all data: 0.021343352
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.916164e-07
Norm of the params: 9.712216
                Loss: fixed  80 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1585031
Train loss (w/o reg) on all data: 0.15066811
Test loss (w/o reg) on all data: 0.09378358
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4606645e-05
Norm of the params: 12.517979
              Random: fixed  16 labels. Loss 0.09378. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730328
Test loss (w/o reg) on all data: 0.012054659
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.940889e-07
Norm of the params: 9.153126
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833745
Train loss (w/o reg) on all data: 0.0026668783
Test loss (w/o reg) on all data: 0.021342415
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0666314e-07
Norm of the params: 9.712359
                Loss: fixed  80 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15685397
Train loss (w/o reg) on all data: 0.14918457
Test loss (w/o reg) on all data: 0.08093225
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.91578e-05
Norm of the params: 12.3849945
              Random: fixed  19 labels. Loss 0.08093. Accuracy 0.992.
### Flips: 156, rs: 5, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012054433
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8043984e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.01205437
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4446161e-07
Norm of the params: 9.153249
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15130952
Train loss (w/o reg) on all data: 0.14389408
Test loss (w/o reg) on all data: 0.081711926
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.789478e-06
Norm of the params: 12.178211
              Random: fixed  21 labels. Loss 0.08171. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.012054558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0919255e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012054615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6423057e-07
Norm of the params: 9.153223
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14795497
Train loss (w/o reg) on all data: 0.14096104
Test loss (w/o reg) on all data: 0.0738519
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6562608e-05
Norm of the params: 11.827034
              Random: fixed  25 labels. Loss 0.07385. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17814073
Train loss (w/o reg) on all data: 0.16936009
Test loss (w/o reg) on all data: 0.084305234
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8703831e-05
Norm of the params: 13.251902
Flipped loss: 0.08431. Accuracy: 0.989
### Flips: 156, rs: 6, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0966118
Train loss (w/o reg) on all data: 0.08501183
Test loss (w/o reg) on all data: 0.05424493
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.876476e-06
Norm of the params: 15.231523
     Influence (LOO): fixed  35 labels. Loss 0.05424. Accuracy 0.989.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04822512
Train loss (w/o reg) on all data: 0.03324871
Test loss (w/o reg) on all data: 0.030141415
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6682035e-06
Norm of the params: 17.306885
                Loss: fixed  51 labels. Loss 0.03014. Accuracy 0.996.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17415747
Train loss (w/o reg) on all data: 0.16580173
Test loss (w/o reg) on all data: 0.081449635
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2372583e-05
Norm of the params: 12.927284
              Random: fixed   4 labels. Loss 0.08145. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 104
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029401977
Train loss (w/o reg) on all data: 0.021105632
Test loss (w/o reg) on all data: 0.017843138
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3309742e-06
Norm of the params: 12.881261
     Influence (LOO): fixed  67 labels. Loss 0.01784. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009749219
Train loss (w/o reg) on all data: 0.004092385
Test loss (w/o reg) on all data: 0.01158189
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0110699e-06
Norm of the params: 10.636573
                Loss: fixed  75 labels. Loss 0.01158. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16992499
Train loss (w/o reg) on all data: 0.16166359
Test loss (w/o reg) on all data: 0.07461531
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5687561e-05
Norm of the params: 12.854105
              Random: fixed   7 labels. Loss 0.07462. Accuracy 0.992.
### Flips: 156, rs: 6, checks: 156
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010633551
Train loss (w/o reg) on all data: 0.0051619834
Test loss (w/o reg) on all data: 0.011486153
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1955732e-06
Norm of the params: 10.460944
     Influence (LOO): fixed  75 labels. Loss 0.01149. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563416
Train loss (w/o reg) on all data: 0.0027568738
Test loss (w/o reg) on all data: 0.010965025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1967444e-07
Norm of the params: 9.797416
                Loss: fixed  76 labels. Loss 0.01097. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15881582
Train loss (w/o reg) on all data: 0.15034713
Test loss (w/o reg) on all data: 0.06779839
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7130715e-05
Norm of the params: 13.014364
              Random: fixed  13 labels. Loss 0.06780. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012054761
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4346298e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563435
Train loss (w/o reg) on all data: 0.0027567972
Test loss (w/o reg) on all data: 0.010965241
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.758762e-08
Norm of the params: 9.797497
                Loss: fixed  76 labels. Loss 0.01097. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15191974
Train loss (w/o reg) on all data: 0.14305176
Test loss (w/o reg) on all data: 0.06535969
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7021035e-05
Norm of the params: 13.317645
              Random: fixed  16 labels. Loss 0.06536. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.01205515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4435565e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007556343
Train loss (w/o reg) on all data: 0.0027567758
Test loss (w/o reg) on all data: 0.010965078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.72945e-08
Norm of the params: 9.797518
                Loss: fixed  76 labels. Loss 0.01097. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14416435
Train loss (w/o reg) on all data: 0.13531049
Test loss (w/o reg) on all data: 0.063201815
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1605836e-05
Norm of the params: 13.307036
              Random: fixed  21 labels. Loss 0.06320. Accuracy 0.996.
### Flips: 156, rs: 6, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728897
Test loss (w/o reg) on all data: 0.0120541435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6751416e-07
Norm of the params: 9.153282
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728906
Test loss (w/o reg) on all data: 0.012054237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2970902e-07
Norm of the params: 9.153281
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14215463
Train loss (w/o reg) on all data: 0.13337497
Test loss (w/o reg) on all data: 0.061317697
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.020113e-06
Norm of the params: 13.251155
              Random: fixed  22 labels. Loss 0.06132. Accuracy 0.996.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18527336
Train loss (w/o reg) on all data: 0.17907313
Test loss (w/o reg) on all data: 0.12737375
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5441364e-05
Norm of the params: 11.135742
Flipped loss: 0.12737. Accuracy: 0.992
### Flips: 156, rs: 7, checks: 52
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08953513
Train loss (w/o reg) on all data: 0.07991109
Test loss (w/o reg) on all data: 0.08591526
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7863562e-05
Norm of the params: 13.873746
     Influence (LOO): fixed  39 labels. Loss 0.08592. Accuracy 0.973.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04794927
Train loss (w/o reg) on all data: 0.034637067
Test loss (w/o reg) on all data: 0.06679605
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.158031e-06
Norm of the params: 16.316986
                Loss: fixed  52 labels. Loss 0.06680. Accuracy 0.981.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17451803
Train loss (w/o reg) on all data: 0.16817725
Test loss (w/o reg) on all data: 0.11470595
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.4036616e-05
Norm of the params: 11.261254
              Random: fixed   7 labels. Loss 0.11471. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 104
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045688782
Train loss (w/o reg) on all data: 0.037552763
Test loss (w/o reg) on all data: 0.039369605
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2004723e-06
Norm of the params: 12.756189
     Influence (LOO): fixed  61 labels. Loss 0.03937. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012055281
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2980772e-07
Norm of the params: 9.153261
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1695138
Train loss (w/o reg) on all data: 0.16319853
Test loss (w/o reg) on all data: 0.109868884
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3654705e-05
Norm of the params: 11.238574
              Random: fixed  10 labels. Loss 0.10987. Accuracy 0.989.
### Flips: 156, rs: 7, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013341177
Train loss (w/o reg) on all data: 0.007108418
Test loss (w/o reg) on all data: 0.013268012
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6256044e-06
Norm of the params: 11.164909
     Influence (LOO): fixed  75 labels. Loss 0.01327. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012055893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2935577e-07
Norm of the params: 9.153187
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16485682
Train loss (w/o reg) on all data: 0.15842116
Test loss (w/o reg) on all data: 0.10538199
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8422013e-05
Norm of the params: 11.34518
              Random: fixed  13 labels. Loss 0.10538. Accuracy 0.989.
### Flips: 156, rs: 7, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.01205566
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5441275e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055733
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1688148e-07
Norm of the params: 9.153181
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15876688
Train loss (w/o reg) on all data: 0.15221547
Test loss (w/o reg) on all data: 0.106563985
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9041916e-05
Norm of the params: 11.446765
              Random: fixed  17 labels. Loss 0.10656. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.012054961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3720535e-08
Norm of the params: 9.153196
     Influence (LOO): fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.01205497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5459116e-07
Norm of the params: 9.153194
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15349898
Train loss (w/o reg) on all data: 0.14700796
Test loss (w/o reg) on all data: 0.09975769
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.739569e-06
Norm of the params: 11.393871
              Random: fixed  20 labels. Loss 0.09976. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728722
Test loss (w/o reg) on all data: 0.012055021
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.269515e-07
Norm of the params: 9.153301
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728917
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9888475e-07
Norm of the params: 9.15328
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14998445
Train loss (w/o reg) on all data: 0.1436738
Test loss (w/o reg) on all data: 0.09239546
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.3382766e-05
Norm of the params: 11.234446
              Random: fixed  23 labels. Loss 0.09240. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18594533
Train loss (w/o reg) on all data: 0.17637742
Test loss (w/o reg) on all data: 0.09877405
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2110107e-05
Norm of the params: 13.833231
Flipped loss: 0.09877. Accuracy: 0.985
### Flips: 156, rs: 8, checks: 52
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1009882
Train loss (w/o reg) on all data: 0.09033409
Test loss (w/o reg) on all data: 0.06533078
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.5434057e-06
Norm of the params: 14.597338
     Influence (LOO): fixed  41 labels. Loss 0.06533. Accuracy 0.989.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059439622
Train loss (w/o reg) on all data: 0.04418403
Test loss (w/o reg) on all data: 0.044428963
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9143514e-06
Norm of the params: 17.467451
                Loss: fixed  51 labels. Loss 0.04443. Accuracy 0.989.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18448246
Train loss (w/o reg) on all data: 0.17487289
Test loss (w/o reg) on all data: 0.0951288
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2630695e-05
Norm of the params: 13.863309
              Random: fixed   2 labels. Loss 0.09513. Accuracy 0.989.
### Flips: 156, rs: 8, checks: 104
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042142227
Train loss (w/o reg) on all data: 0.03232861
Test loss (w/o reg) on all data: 0.03405603
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7308041e-06
Norm of the params: 14.009724
     Influence (LOO): fixed  66 labels. Loss 0.03406. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783382
Train loss (w/o reg) on all data: 0.0028982342
Test loss (w/o reg) on all data: 0.01200115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8450684e-07
Norm of the params: 9.88448
                Loss: fixed  78 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17651927
Train loss (w/o reg) on all data: 0.16732676
Test loss (w/o reg) on all data: 0.09027547
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0856644e-05
Norm of the params: 13.559143
              Random: fixed   8 labels. Loss 0.09028. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 156
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013553419
Train loss (w/o reg) on all data: 0.0077900453
Test loss (w/o reg) on all data: 0.014715864
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3377426e-07
Norm of the params: 10.736269
     Influence (LOO): fixed  77 labels. Loss 0.01472. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783384
Train loss (w/o reg) on all data: 0.0028982805
Test loss (w/o reg) on all data: 0.012001973
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4784915e-07
Norm of the params: 9.884436
                Loss: fixed  78 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17515734
Train loss (w/o reg) on all data: 0.16610548
Test loss (w/o reg) on all data: 0.0887261
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9517915e-05
Norm of the params: 13.455002
              Random: fixed   9 labels. Loss 0.08873. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021727975
Test loss (w/o reg) on all data: 0.012056302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4661952e-06
Norm of the params: 9.153383
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172809
Test loss (w/o reg) on all data: 0.012055896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4194292e-07
Norm of the params: 9.153371
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16406277
Train loss (w/o reg) on all data: 0.15434743
Test loss (w/o reg) on all data: 0.08618045
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.376247e-06
Norm of the params: 13.939388
              Random: fixed  14 labels. Loss 0.08618. Accuracy 0.989.
### Flips: 156, rs: 8, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729392
Test loss (w/o reg) on all data: 0.012054807
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4215386e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729406
Test loss (w/o reg) on all data: 0.012054735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.229535e-08
Norm of the params: 9.15323
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15518089
Train loss (w/o reg) on all data: 0.14519739
Test loss (w/o reg) on all data: 0.07738373
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6972856e-05
Norm of the params: 14.130462
              Random: fixed  19 labels. Loss 0.07738. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730023
Test loss (w/o reg) on all data: 0.012055035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.788795e-07
Norm of the params: 9.153159
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.012055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.640798e-07
Norm of the params: 9.153159
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15058555
Train loss (w/o reg) on all data: 0.14070675
Test loss (w/o reg) on all data: 0.073746614
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.859512e-06
Norm of the params: 14.056175
              Random: fixed  21 labels. Loss 0.07375. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16755088
Train loss (w/o reg) on all data: 0.15825325
Test loss (w/o reg) on all data: 0.10549836
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.22016e-05
Norm of the params: 13.636443
Flipped loss: 0.10550. Accuracy: 0.989
### Flips: 156, rs: 9, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07927944
Train loss (w/o reg) on all data: 0.06941549
Test loss (w/o reg) on all data: 0.049253892
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2792331e-05
Norm of the params: 14.045607
     Influence (LOO): fixed  41 labels. Loss 0.04925. Accuracy 0.985.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04196901
Train loss (w/o reg) on all data: 0.02934318
Test loss (w/o reg) on all data: 0.055301014
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.669119e-06
Norm of the params: 15.890769
                Loss: fixed  50 labels. Loss 0.05530. Accuracy 0.981.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16520666
Train loss (w/o reg) on all data: 0.15593968
Test loss (w/o reg) on all data: 0.10268064
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3515502e-05
Norm of the params: 13.613942
              Random: fixed   2 labels. Loss 0.10268. Accuracy 0.989.
### Flips: 156, rs: 9, checks: 104
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040549576
Train loss (w/o reg) on all data: 0.031508904
Test loss (w/o reg) on all data: 0.031948883
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8568927e-06
Norm of the params: 13.446688
     Influence (LOO): fixed  59 labels. Loss 0.03195. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505651
Train loss (w/o reg) on all data: 0.0028171479
Test loss (w/o reg) on all data: 0.012755481
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.384607e-07
Norm of the params: 9.683495
                Loss: fixed  70 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1580139
Train loss (w/o reg) on all data: 0.14779402
Test loss (w/o reg) on all data: 0.09680508
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9521334e-05
Norm of the params: 14.296762
              Random: fixed   6 labels. Loss 0.09681. Accuracy 0.981.
### Flips: 156, rs: 9, checks: 156
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013526845
Train loss (w/o reg) on all data: 0.007772798
Test loss (w/o reg) on all data: 0.010996602
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0395702e-07
Norm of the params: 10.727578
     Influence (LOO): fixed  69 labels. Loss 0.01100. Accuracy 0.996.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173018
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9630227e-07
Norm of the params: 9.153144
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15494788
Train loss (w/o reg) on all data: 0.14431491
Test loss (w/o reg) on all data: 0.09653678
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.310952e-05
Norm of the params: 14.582838
              Random: fixed   8 labels. Loss 0.09654. Accuracy 0.985.
### Flips: 156, rs: 9, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.0120552415
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6323976e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729423
Test loss (w/o reg) on all data: 0.01205509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.054957e-07
Norm of the params: 9.153224
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15074712
Train loss (w/o reg) on all data: 0.14072825
Test loss (w/o reg) on all data: 0.09189128
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6808342e-05
Norm of the params: 14.155473
              Random: fixed  11 labels. Loss 0.09189. Accuracy 0.989.
### Flips: 156, rs: 9, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730014
Test loss (w/o reg) on all data: 0.012055955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5301775e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012055775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3921886e-07
Norm of the params: 9.153182
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14063644
Train loss (w/o reg) on all data: 0.13110118
Test loss (w/o reg) on all data: 0.085752994
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.040219e-06
Norm of the params: 13.809612
              Random: fixed  17 labels. Loss 0.08575. Accuracy 0.992.
### Flips: 156, rs: 9, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172987
Test loss (w/o reg) on all data: 0.012055914
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5809005e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729828
Test loss (w/o reg) on all data: 0.012055955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7342778e-07
Norm of the params: 9.153178
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13497108
Train loss (w/o reg) on all data: 0.12545261
Test loss (w/o reg) on all data: 0.077397324
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2079072e-05
Norm of the params: 13.797441
              Random: fixed  22 labels. Loss 0.07740. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17049296
Train loss (w/o reg) on all data: 0.1619609
Test loss (w/o reg) on all data: 0.118057676
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1495143e-05
Norm of the params: 13.062976
Flipped loss: 0.11806. Accuracy: 0.985
### Flips: 156, rs: 10, checks: 52
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081616476
Train loss (w/o reg) on all data: 0.07159919
Test loss (w/o reg) on all data: 0.06526612
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7125358e-05
Norm of the params: 14.154354
     Influence (LOO): fixed  41 labels. Loss 0.06527. Accuracy 0.989.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037015654
Train loss (w/o reg) on all data: 0.024867052
Test loss (w/o reg) on all data: 0.045944475
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.534628e-06
Norm of the params: 15.58756
                Loss: fixed  52 labels. Loss 0.04594. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16795114
Train loss (w/o reg) on all data: 0.15933347
Test loss (w/o reg) on all data: 0.114949755
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3705268e-05
Norm of the params: 13.128345
              Random: fixed   2 labels. Loss 0.11495. Accuracy 0.981.
### Flips: 156, rs: 10, checks: 104
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021917662
Train loss (w/o reg) on all data: 0.015347992
Test loss (w/o reg) on all data: 0.019124996
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.195006e-07
Norm of the params: 11.462696
     Influence (LOO): fixed  64 labels. Loss 0.01912. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008086493
Train loss (w/o reg) on all data: 0.0030053612
Test loss (w/o reg) on all data: 0.01587698
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0368059e-07
Norm of the params: 10.080806
                Loss: fixed  69 labels. Loss 0.01588. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16172072
Train loss (w/o reg) on all data: 0.15308316
Test loss (w/o reg) on all data: 0.11234585
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.444977e-06
Norm of the params: 13.143485
              Random: fixed   6 labels. Loss 0.11235. Accuracy 0.981.
### Flips: 156, rs: 10, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002355
Train loss (w/o reg) on all data: 0.0054477644
Test loss (w/o reg) on all data: 0.012244232
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.807596e-07
Norm of the params: 9.544203
     Influence (LOO): fixed  69 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730235
Test loss (w/o reg) on all data: 0.012054925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5692515e-07
Norm of the params: 9.153136
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15438367
Train loss (w/o reg) on all data: 0.14565028
Test loss (w/o reg) on all data: 0.10093794
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9632889e-05
Norm of the params: 13.216191
              Random: fixed  10 labels. Loss 0.10094. Accuracy 0.992.
### Flips: 156, rs: 10, checks: 208
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.0054477593
Test loss (w/o reg) on all data: 0.012244867
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.3213076e-07
Norm of the params: 9.544208
     Influence (LOO): fixed  69 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1453853e-07
Norm of the params: 9.153185
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14951828
Train loss (w/o reg) on all data: 0.14015922
Test loss (w/o reg) on all data: 0.09737318
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0882799e-05
Norm of the params: 13.681425
              Random: fixed  12 labels. Loss 0.09737. Accuracy 0.989.
### Flips: 156, rs: 10, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055994
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9852632e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012056039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3950065e-07
Norm of the params: 9.153181
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14277974
Train loss (w/o reg) on all data: 0.133095
Test loss (w/o reg) on all data: 0.09237112
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5333737e-05
Norm of the params: 13.91743
              Random: fixed  15 labels. Loss 0.09237. Accuracy 0.992.
### Flips: 156, rs: 10, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729483
Test loss (w/o reg) on all data: 0.0120542
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.42149e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.0120543605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.842728e-07
Norm of the params: 9.153216
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13738503
Train loss (w/o reg) on all data: 0.12760355
Test loss (w/o reg) on all data: 0.090479754
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7966571e-05
Norm of the params: 13.986764
              Random: fixed  18 labels. Loss 0.09048. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17175522
Train loss (w/o reg) on all data: 0.16438472
Test loss (w/o reg) on all data: 0.09402359
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0621763e-05
Norm of the params: 12.141259
Flipped loss: 0.09402. Accuracy: 0.992
### Flips: 156, rs: 11, checks: 52
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07130034
Train loss (w/o reg) on all data: 0.06257631
Test loss (w/o reg) on all data: 0.04536907
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.168134e-06
Norm of the params: 13.209112
     Influence (LOO): fixed  42 labels. Loss 0.04537. Accuracy 0.985.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030315455
Train loss (w/o reg) on all data: 0.020212961
Test loss (w/o reg) on all data: 0.03183836
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4469844e-06
Norm of the params: 14.214426
                Loss: fixed  52 labels. Loss 0.03184. Accuracy 0.996.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16942658
Train loss (w/o reg) on all data: 0.16207181
Test loss (w/o reg) on all data: 0.09377459
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.183208e-06
Norm of the params: 12.128286
              Random: fixed   2 labels. Loss 0.09377. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 104
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020382077
Train loss (w/o reg) on all data: 0.014058854
Test loss (w/o reg) on all data: 0.01671965
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1683912e-06
Norm of the params: 11.24564
     Influence (LOO): fixed  63 labels. Loss 0.01672. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731404
Test loss (w/o reg) on all data: 0.012055668
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1983318e-06
Norm of the params: 9.1530075
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16379346
Train loss (w/o reg) on all data: 0.15691249
Test loss (w/o reg) on all data: 0.08895326
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2448707e-05
Norm of the params: 11.731126
              Random: fixed   5 labels. Loss 0.08895. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012285948
Train loss (w/o reg) on all data: 0.007103896
Test loss (w/o reg) on all data: 0.014617264
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2340324e-07
Norm of the params: 10.180424
     Influence (LOO): fixed  67 labels. Loss 0.01462. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729977
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.876129e-07
Norm of the params: 9.153162
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15625297
Train loss (w/o reg) on all data: 0.14932996
Test loss (w/o reg) on all data: 0.08181729
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.49663165e-05
Norm of the params: 11.766907
              Random: fixed  10 labels. Loss 0.08182. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038309
Train loss (w/o reg) on all data: 0.005286932
Test loss (w/o reg) on all data: 0.012296101
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6383593e-06
Norm of the params: 9.748207
     Influence (LOO): fixed  68 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728436
Test loss (w/o reg) on all data: 0.012055182
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.630071e-07
Norm of the params: 9.153334
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14600903
Train loss (w/o reg) on all data: 0.13898441
Test loss (w/o reg) on all data: 0.075146236
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7407832e-05
Norm of the params: 11.85295
              Random: fixed  15 labels. Loss 0.07515. Accuracy 0.992.
### Flips: 156, rs: 11, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730787
Test loss (w/o reg) on all data: 0.012055441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1459282e-07
Norm of the params: 9.153076
     Influence (LOO): fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173077
Test loss (w/o reg) on all data: 0.012055502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.200109e-07
Norm of the params: 9.153077
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13693324
Train loss (w/o reg) on all data: 0.13006064
Test loss (w/o reg) on all data: 0.068339095
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4739597e-06
Norm of the params: 11.72399
              Random: fixed  19 labels. Loss 0.06834. Accuracy 0.992.
### Flips: 156, rs: 11, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021731334
Test loss (w/o reg) on all data: 0.012055719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0786665e-07
Norm of the params: 9.153017
     Influence (LOO): fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731302
Test loss (w/o reg) on all data: 0.012055867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2907587e-07
Norm of the params: 9.153019
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13167833
Train loss (w/o reg) on all data: 0.12491467
Test loss (w/o reg) on all data: 0.066037
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1653127e-05
Norm of the params: 11.630695
              Random: fixed  21 labels. Loss 0.06604. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1788469
Train loss (w/o reg) on all data: 0.1717997
Test loss (w/o reg) on all data: 0.09858284
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1157473e-05
Norm of the params: 11.87197
Flipped loss: 0.09858. Accuracy: 0.992
### Flips: 156, rs: 12, checks: 52
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09324868
Train loss (w/o reg) on all data: 0.084092975
Test loss (w/o reg) on all data: 0.04989724
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3584609e-05
Norm of the params: 13.531964
     Influence (LOO): fixed  39 labels. Loss 0.04990. Accuracy 0.989.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039826572
Train loss (w/o reg) on all data: 0.02637321
Test loss (w/o reg) on all data: 0.059393138
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.2797045e-06
Norm of the params: 16.40327
                Loss: fixed  52 labels. Loss 0.05939. Accuracy 0.977.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17666863
Train loss (w/o reg) on all data: 0.16968867
Test loss (w/o reg) on all data: 0.09623217
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6106698e-05
Norm of the params: 11.815204
              Random: fixed   1 labels. Loss 0.09623. Accuracy 0.992.
### Flips: 156, rs: 12, checks: 104
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035311807
Train loss (w/o reg) on all data: 0.02723419
Test loss (w/o reg) on all data: 0.02175879
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9012058e-06
Norm of the params: 12.710326
     Influence (LOO): fixed  63 labels. Loss 0.02176. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070519317
Train loss (w/o reg) on all data: 0.00246466
Test loss (w/o reg) on all data: 0.017304443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2293634e-06
Norm of the params: 9.578384
                Loss: fixed  74 labels. Loss 0.01730. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16796416
Train loss (w/o reg) on all data: 0.16066776
Test loss (w/o reg) on all data: 0.093583904
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4200917e-05
Norm of the params: 12.080067
              Random: fixed   6 labels. Loss 0.09358. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 156
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016112357
Train loss (w/o reg) on all data: 0.010083454
Test loss (w/o reg) on all data: 0.01492542
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.173275e-06
Norm of the params: 10.980805
     Influence (LOO): fixed  72 labels. Loss 0.01493. Accuracy 0.989.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729944
Test loss (w/o reg) on all data: 0.012053343
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.927682e-07
Norm of the params: 9.153168
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16240661
Train loss (w/o reg) on all data: 0.15517835
Test loss (w/o reg) on all data: 0.08795177
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.9324064e-06
Norm of the params: 12.023523
              Random: fixed   9 labels. Loss 0.08795. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012055848
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0635999e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.01205571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8377462e-07
Norm of the params: 9.1531925
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15893982
Train loss (w/o reg) on all data: 0.15180773
Test loss (w/o reg) on all data: 0.08863611
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5057186e-05
Norm of the params: 11.943284
              Random: fixed  11 labels. Loss 0.08864. Accuracy 0.981.
### Flips: 156, rs: 12, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012055462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3407016e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.012055784
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.942287e-07
Norm of the params: 9.153184
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15098275
Train loss (w/o reg) on all data: 0.14385314
Test loss (w/o reg) on all data: 0.08674182
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2707396e-05
Norm of the params: 11.9412
              Random: fixed  17 labels. Loss 0.08674. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729928
Test loss (w/o reg) on all data: 0.012055829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4641447e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.01205572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1523964e-07
Norm of the params: 9.153171
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14220144
Train loss (w/o reg) on all data: 0.13481598
Test loss (w/o reg) on all data: 0.08201263
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.116308e-05
Norm of the params: 12.153574
              Random: fixed  22 labels. Loss 0.08201. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17042172
Train loss (w/o reg) on all data: 0.16195229
Test loss (w/o reg) on all data: 0.089186236
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.5519403e-06
Norm of the params: 13.014944
Flipped loss: 0.08919. Accuracy: 0.996
### Flips: 156, rs: 13, checks: 52
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06785492
Train loss (w/o reg) on all data: 0.059361175
Test loss (w/o reg) on all data: 0.04535239
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4561992e-05
Norm of the params: 13.033605
     Influence (LOO): fixed  45 labels. Loss 0.04535. Accuracy 0.981.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036805857
Train loss (w/o reg) on all data: 0.0246766
Test loss (w/o reg) on all data: 0.028546907
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7479732e-06
Norm of the params: 15.575145
                Loss: fixed  52 labels. Loss 0.02855. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16530348
Train loss (w/o reg) on all data: 0.15714118
Test loss (w/o reg) on all data: 0.08154593
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.63662e-06
Norm of the params: 12.776775
              Random: fixed   3 labels. Loss 0.08155. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 104
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032399096
Train loss (w/o reg) on all data: 0.025057497
Test loss (w/o reg) on all data: 0.020955384
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.335031e-06
Norm of the params: 12.117424
     Influence (LOO): fixed  61 labels. Loss 0.02096. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715343
Train loss (w/o reg) on all data: 0.0025172385
Test loss (w/o reg) on all data: 0.011187605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5855422e-07
Norm of the params: 9.438535
                Loss: fixed  69 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161756
Train loss (w/o reg) on all data: 0.15332118
Test loss (w/o reg) on all data: 0.07967254
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.767891e-05
Norm of the params: 12.988317
              Random: fixed   5 labels. Loss 0.07967. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 156
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038309
Train loss (w/o reg) on all data: 0.0052870107
Test loss (w/o reg) on all data: 0.01229646
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3721141e-06
Norm of the params: 9.748127
     Influence (LOO): fixed  69 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728433
Test loss (w/o reg) on all data: 0.012055182
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6530887e-07
Norm of the params: 9.153334
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15123273
Train loss (w/o reg) on all data: 0.14328845
Test loss (w/o reg) on all data: 0.06903449
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.76697e-06
Norm of the params: 12.604992
              Random: fixed  12 labels. Loss 0.06903. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 208
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038312
Train loss (w/o reg) on all data: 0.0052866004
Test loss (w/o reg) on all data: 0.012295287
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3134604e-06
Norm of the params: 9.748549
     Influence (LOO): fixed  69 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729188
Test loss (w/o reg) on all data: 0.01205508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7029436e-07
Norm of the params: 9.153252
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13802212
Train loss (w/o reg) on all data: 0.13012438
Test loss (w/o reg) on all data: 0.06356515
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.663046e-06
Norm of the params: 12.568014
              Random: fixed  17 labels. Loss 0.06357. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.01205502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6945071e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730002
Test loss (w/o reg) on all data: 0.012054904
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.472273e-07
Norm of the params: 9.153162
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13658808
Train loss (w/o reg) on all data: 0.12865216
Test loss (w/o reg) on all data: 0.062012345
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0635107e-05
Norm of the params: 12.598359
              Random: fixed  19 labels. Loss 0.06201. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729476
Test loss (w/o reg) on all data: 0.012054452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7756385e-07
Norm of the params: 9.153222
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729448
Test loss (w/o reg) on all data: 0.012054552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5390346e-07
Norm of the params: 9.153221
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13021113
Train loss (w/o reg) on all data: 0.122303
Test loss (w/o reg) on all data: 0.057300605
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.056045e-05
Norm of the params: 12.576272
              Random: fixed  22 labels. Loss 0.05730. Accuracy 0.996.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18877435
Train loss (w/o reg) on all data: 0.1811396
Test loss (w/o reg) on all data: 0.12369418
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7005426e-05
Norm of the params: 12.356977
Flipped loss: 0.12369. Accuracy: 0.977
### Flips: 156, rs: 14, checks: 52
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101501256
Train loss (w/o reg) on all data: 0.090277575
Test loss (w/o reg) on all data: 0.07408221
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4891811e-05
Norm of the params: 14.982442
     Influence (LOO): fixed  39 labels. Loss 0.07408. Accuracy 0.985.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05622708
Train loss (w/o reg) on all data: 0.044440955
Test loss (w/o reg) on all data: 0.075833976
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.5445003e-06
Norm of the params: 15.353255
                Loss: fixed  52 labels. Loss 0.07583. Accuracy 0.969.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18515943
Train loss (w/o reg) on all data: 0.17735656
Test loss (w/o reg) on all data: 0.11992728
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1003464e-05
Norm of the params: 12.492293
              Random: fixed   1 labels. Loss 0.11993. Accuracy 0.973.
### Flips: 156, rs: 14, checks: 104
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05000121
Train loss (w/o reg) on all data: 0.039214965
Test loss (w/o reg) on all data: 0.03806416
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2449926e-06
Norm of the params: 14.687579
     Influence (LOO): fixed  62 labels. Loss 0.03806. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345252
Train loss (w/o reg) on all data: 0.0031439872
Test loss (w/o reg) on all data: 0.011325041
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5200032e-07
Norm of the params: 10.199279
                Loss: fixed  78 labels. Loss 0.01133. Accuracy 0.996.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18113163
Train loss (w/o reg) on all data: 0.17310087
Test loss (w/o reg) on all data: 0.114817634
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.997781e-05
Norm of the params: 12.673409
              Random: fixed   4 labels. Loss 0.11482. Accuracy 0.985.
### Flips: 156, rs: 14, checks: 156
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01843791
Train loss (w/o reg) on all data: 0.012403064
Test loss (w/o reg) on all data: 0.01623785
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.4755635e-07
Norm of the params: 10.986217
     Influence (LOO): fixed  75 labels. Loss 0.01624. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345252
Train loss (w/o reg) on all data: 0.0031440638
Test loss (w/o reg) on all data: 0.011324752
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7973926e-07
Norm of the params: 10.199204
                Loss: fixed  78 labels. Loss 0.01132. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17246716
Train loss (w/o reg) on all data: 0.16457124
Test loss (w/o reg) on all data: 0.10344615
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.508969e-05
Norm of the params: 12.566555
              Random: fixed   9 labels. Loss 0.10345. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 208
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016410844
Train loss (w/o reg) on all data: 0.011156095
Test loss (w/o reg) on all data: 0.016495628
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.689253e-07
Norm of the params: 10.251585
     Influence (LOO): fixed  76 labels. Loss 0.01650. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012055527
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7532104e-07
Norm of the params: 9.15322
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16820857
Train loss (w/o reg) on all data: 0.1601682
Test loss (w/o reg) on all data: 0.10157209
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.142163e-05
Norm of the params: 12.6809845
              Random: fixed  11 labels. Loss 0.10157. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8886578e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012054997
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4979351e-07
Norm of the params: 9.153229
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16123784
Train loss (w/o reg) on all data: 0.15344365
Test loss (w/o reg) on all data: 0.09618756
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6325517e-05
Norm of the params: 12.485344
              Random: fixed  15 labels. Loss 0.09619. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729483
Test loss (w/o reg) on all data: 0.012054883
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4898544e-08
Norm of the params: 9.15322
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012054867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.443332e-08
Norm of the params: 9.153218
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16011044
Train loss (w/o reg) on all data: 0.15262152
Test loss (w/o reg) on all data: 0.09120134
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6400232e-05
Norm of the params: 12.238405
              Random: fixed  17 labels. Loss 0.09120. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19191223
Train loss (w/o reg) on all data: 0.18472162
Test loss (w/o reg) on all data: 0.11316235
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1653756e-05
Norm of the params: 11.992181
Flipped loss: 0.11316. Accuracy: 0.985
### Flips: 156, rs: 15, checks: 52
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08954846
Train loss (w/o reg) on all data: 0.080394715
Test loss (w/o reg) on all data: 0.067845345
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3045064e-05
Norm of the params: 13.53052
     Influence (LOO): fixed  43 labels. Loss 0.06785. Accuracy 0.989.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050530385
Train loss (w/o reg) on all data: 0.037254345
Test loss (w/o reg) on all data: 0.06861411
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.041299e-06
Norm of the params: 16.294811
                Loss: fixed  52 labels. Loss 0.06861. Accuracy 0.969.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18628244
Train loss (w/o reg) on all data: 0.17893617
Test loss (w/o reg) on all data: 0.104818
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4506853e-05
Norm of the params: 12.121279
              Random: fixed   4 labels. Loss 0.10482. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 104
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040081695
Train loss (w/o reg) on all data: 0.032545656
Test loss (w/o reg) on all data: 0.028727224
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3006454e-06
Norm of the params: 12.27684
     Influence (LOO): fixed  65 labels. Loss 0.02873. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076085236
Train loss (w/o reg) on all data: 0.0028445572
Test loss (w/o reg) on all data: 0.014218187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.156118e-07
Norm of the params: 9.761112
                Loss: fixed  76 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18207236
Train loss (w/o reg) on all data: 0.1747032
Test loss (w/o reg) on all data: 0.10458241
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.4573782e-05
Norm of the params: 12.140147
              Random: fixed   6 labels. Loss 0.10458. Accuracy 0.981.
### Flips: 156, rs: 15, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008985575
Train loss (w/o reg) on all data: 0.0042103413
Test loss (w/o reg) on all data: 0.014261742
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1203176e-07
Norm of the params: 9.77265
     Influence (LOO): fixed  76 labels. Loss 0.01426. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012055205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4576016e-07
Norm of the params: 9.15326
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17271842
Train loss (w/o reg) on all data: 0.16514343
Test loss (w/o reg) on all data: 0.10336608
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.380497e-05
Norm of the params: 12.308521
              Random: fixed  11 labels. Loss 0.10337. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730158
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0206862e-06
Norm of the params: 9.153145
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173013
Test loss (w/o reg) on all data: 0.012054691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7238207e-07
Norm of the params: 9.153146
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17190538
Train loss (w/o reg) on all data: 0.16426143
Test loss (w/o reg) on all data: 0.101773724
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7685934e-05
Norm of the params: 12.36443
              Random: fixed  12 labels. Loss 0.10177. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055197
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.036819e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0454873e-07
Norm of the params: 9.153181
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16279219
Train loss (w/o reg) on all data: 0.15481651
Test loss (w/o reg) on all data: 0.092672154
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.2477374e-06
Norm of the params: 12.629874
              Random: fixed  17 labels. Loss 0.09267. Accuracy 0.981.
### Flips: 156, rs: 15, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012055161
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3120078e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012055127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1452189e-07
Norm of the params: 9.15318
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16168769
Train loss (w/o reg) on all data: 0.1538495
Test loss (w/o reg) on all data: 0.09075698
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.716348e-05
Norm of the params: 12.520536
              Random: fixed  18 labels. Loss 0.09076. Accuracy 0.985.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17177056
Train loss (w/o reg) on all data: 0.16302493
Test loss (w/o reg) on all data: 0.13661297
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.658727e-05
Norm of the params: 13.225446
Flipped loss: 0.13661. Accuracy: 0.962
### Flips: 156, rs: 16, checks: 52
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09821265
Train loss (w/o reg) on all data: 0.088713154
Test loss (w/o reg) on all data: 0.091465704
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5115842e-05
Norm of the params: 13.783687
     Influence (LOO): fixed  39 labels. Loss 0.09147. Accuracy 0.977.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04896827
Train loss (w/o reg) on all data: 0.0350243
Test loss (w/o reg) on all data: 0.077825256
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.337652e-06
Norm of the params: 16.699684
                Loss: fixed  52 labels. Loss 0.07783. Accuracy 0.977.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16292052
Train loss (w/o reg) on all data: 0.15418318
Test loss (w/o reg) on all data: 0.12998417
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.2656288e-05
Norm of the params: 13.21918
              Random: fixed   5 labels. Loss 0.12998. Accuracy 0.966.
### Flips: 156, rs: 16, checks: 104
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04645422
Train loss (w/o reg) on all data: 0.038716547
Test loss (w/o reg) on all data: 0.03501683
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.959454e-06
Norm of the params: 12.439995
     Influence (LOO): fixed  64 labels. Loss 0.03502. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013514563
Train loss (w/o reg) on all data: 0.0059752753
Test loss (w/o reg) on all data: 0.03441827
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7309743e-07
Norm of the params: 12.279486
                Loss: fixed  71 labels. Loss 0.03442. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15934709
Train loss (w/o reg) on all data: 0.15055096
Test loss (w/o reg) on all data: 0.12825842
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4048874e-05
Norm of the params: 13.263582
              Random: fixed   8 labels. Loss 0.12826. Accuracy 0.969.
### Flips: 156, rs: 16, checks: 156
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013431542
Train loss (w/o reg) on all data: 0.00851759
Test loss (w/o reg) on all data: 0.012955602
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.641948e-07
Norm of the params: 9.913578
     Influence (LOO): fixed  76 labels. Loss 0.01296. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008026177
Train loss (w/o reg) on all data: 0.0029984354
Test loss (w/o reg) on all data: 0.015570404
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.285935e-07
Norm of the params: 10.027703
                Loss: fixed  76 labels. Loss 0.01557. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14452885
Train loss (w/o reg) on all data: 0.13596286
Test loss (w/o reg) on all data: 0.12399243
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4100976e-05
Norm of the params: 13.0889225
              Random: fixed  17 labels. Loss 0.12399. Accuracy 0.973.
### Flips: 156, rs: 16, checks: 208
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009260062
Train loss (w/o reg) on all data: 0.0044021397
Test loss (w/o reg) on all data: 0.012441299
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6237938e-07
Norm of the params: 9.856899
     Influence (LOO): fixed  77 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007404577
Train loss (w/o reg) on all data: 0.0027833127
Test loss (w/o reg) on all data: 0.013698198
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3328669e-07
Norm of the params: 9.613809
                Loss: fixed  77 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14374389
Train loss (w/o reg) on all data: 0.13516966
Test loss (w/o reg) on all data: 0.1237836
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.9600272e-05
Norm of the params: 13.095213
              Random: fixed  18 labels. Loss 0.12378. Accuracy 0.973.
### Flips: 156, rs: 16, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728103
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.482909e-07
Norm of the params: 9.15337
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007404579
Train loss (w/o reg) on all data: 0.0027833718
Test loss (w/o reg) on all data: 0.013698046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8665235e-07
Norm of the params: 9.613748
                Loss: fixed  77 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13678956
Train loss (w/o reg) on all data: 0.12779778
Test loss (w/o reg) on all data: 0.122359514
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.760194e-05
Norm of the params: 13.410277
              Random: fixed  21 labels. Loss 0.12236. Accuracy 0.969.
### Flips: 156, rs: 16, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729467
Test loss (w/o reg) on all data: 0.012055605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.839217e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.012055436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.204236e-07
Norm of the params: 9.1532135
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1298243
Train loss (w/o reg) on all data: 0.12046896
Test loss (w/o reg) on all data: 0.11780482
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.220859e-05
Norm of the params: 13.678702
              Random: fixed  25 labels. Loss 0.11780. Accuracy 0.969.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1848051
Train loss (w/o reg) on all data: 0.17720987
Test loss (w/o reg) on all data: 0.09749944
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3105671e-05
Norm of the params: 12.324956
Flipped loss: 0.09750. Accuracy: 0.985
### Flips: 156, rs: 17, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10502027
Train loss (w/o reg) on all data: 0.095101066
Test loss (w/o reg) on all data: 0.067770116
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5795951e-05
Norm of the params: 14.084887
     Influence (LOO): fixed  38 labels. Loss 0.06777. Accuracy 0.985.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049765363
Train loss (w/o reg) on all data: 0.035397455
Test loss (w/o reg) on all data: 0.02742199
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.983918e-06
Norm of the params: 16.951643
                Loss: fixed  52 labels. Loss 0.02742. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17573965
Train loss (w/o reg) on all data: 0.16852735
Test loss (w/o reg) on all data: 0.085808255
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.178772e-06
Norm of the params: 12.010243
              Random: fixed   8 labels. Loss 0.08581. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 104
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040555507
Train loss (w/o reg) on all data: 0.03100294
Test loss (w/o reg) on all data: 0.030488309
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6621549e-05
Norm of the params: 13.822131
     Influence (LOO): fixed  67 labels. Loss 0.03049. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0098998705
Train loss (w/o reg) on all data: 0.0039960314
Test loss (w/o reg) on all data: 0.02051658
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1254106e-07
Norm of the params: 10.866315
                Loss: fixed  80 labels. Loss 0.02052. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16345254
Train loss (w/o reg) on all data: 0.15549894
Test loss (w/o reg) on all data: 0.089133374
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4167265e-05
Norm of the params: 12.612369
              Random: fixed  15 labels. Loss 0.08913. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 156
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012896199
Train loss (w/o reg) on all data: 0.006909315
Test loss (w/o reg) on all data: 0.016698774
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.312818e-06
Norm of the params: 10.942472
     Influence (LOO): fixed  79 labels. Loss 0.01670. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729167
Test loss (w/o reg) on all data: 0.012055214
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6066212e-07
Norm of the params: 9.153255
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16046067
Train loss (w/o reg) on all data: 0.15275612
Test loss (w/o reg) on all data: 0.08561757
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.988008e-06
Norm of the params: 12.413336
              Random: fixed  17 labels. Loss 0.08562. Accuracy 0.985.
### Flips: 156, rs: 17, checks: 208
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01184174
Train loss (w/o reg) on all data: 0.0059789964
Test loss (w/o reg) on all data: 0.016266715
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0825592e-07
Norm of the params: 10.828429
     Influence (LOO): fixed  80 labels. Loss 0.01627. Accuracy 0.989.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729153
Test loss (w/o reg) on all data: 0.0120545635
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0872331e-07
Norm of the params: 9.153256
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14786455
Train loss (w/o reg) on all data: 0.13969198
Test loss (w/o reg) on all data: 0.08084019
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1137555e-05
Norm of the params: 12.784819
              Random: fixed  22 labels. Loss 0.08084. Accuracy 0.985.
### Flips: 156, rs: 17, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731462
Test loss (w/o reg) on all data: 0.012056597
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1747908e-06
Norm of the params: 9.153002
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731441
Test loss (w/o reg) on all data: 0.012056374
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9096998e-07
Norm of the params: 9.153004
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13956529
Train loss (w/o reg) on all data: 0.13101478
Test loss (w/o reg) on all data: 0.069931895
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0822043e-05
Norm of the params: 13.077085
              Random: fixed  28 labels. Loss 0.06993. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012054682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7315568e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012054781
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.266768e-07
Norm of the params: 9.153206
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13726635
Train loss (w/o reg) on all data: 0.12887418
Test loss (w/o reg) on all data: 0.06469922
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3894307e-05
Norm of the params: 12.955444
              Random: fixed  30 labels. Loss 0.06470. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17898612
Train loss (w/o reg) on all data: 0.17157899
Test loss (w/o reg) on all data: 0.09472213
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6034408e-05
Norm of the params: 12.171386
Flipped loss: 0.09472. Accuracy: 0.989
### Flips: 156, rs: 18, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090521365
Train loss (w/o reg) on all data: 0.082278356
Test loss (w/o reg) on all data: 0.055611465
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9299646e-06
Norm of the params: 12.83979
     Influence (LOO): fixed  42 labels. Loss 0.05561. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042771667
Train loss (w/o reg) on all data: 0.02954157
Test loss (w/o reg) on all data: 0.057661887
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0877542e-05
Norm of the params: 16.26659
                Loss: fixed  52 labels. Loss 0.05766. Accuracy 0.981.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17898615
Train loss (w/o reg) on all data: 0.17157735
Test loss (w/o reg) on all data: 0.09472593
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.8862992e-05
Norm of the params: 12.172753
              Random: fixed   0 labels. Loss 0.09473. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 104
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02947791
Train loss (w/o reg) on all data: 0.022523409
Test loss (w/o reg) on all data: 0.025938448
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0698876e-06
Norm of the params: 11.793641
     Influence (LOO): fixed  64 labels. Loss 0.02594. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971536
Train loss (w/o reg) on all data: 0.0025172073
Test loss (w/o reg) on all data: 0.011187735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1260746e-07
Norm of the params: 9.438569
                Loss: fixed  71 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16873166
Train loss (w/o reg) on all data: 0.16147372
Test loss (w/o reg) on all data: 0.09060288
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1322825e-05
Norm of the params: 12.048188
              Random: fixed   6 labels. Loss 0.09060. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 156
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0140178315
Train loss (w/o reg) on all data: 0.00783613
Test loss (w/o reg) on all data: 0.026240489
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2860464e-07
Norm of the params: 11.119083
     Influence (LOO): fixed  69 labels. Loss 0.02624. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012054575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0224623e-07
Norm of the params: 9.153226
                Loss: fixed  72 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15822704
Train loss (w/o reg) on all data: 0.15111744
Test loss (w/o reg) on all data: 0.08575117
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8007619e-05
Norm of the params: 11.924427
              Random: fixed  12 labels. Loss 0.08575. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729413
Test loss (w/o reg) on all data: 0.01205508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1270584e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012055047
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8552203e-08
Norm of the params: 9.153226
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562378
Train loss (w/o reg) on all data: 0.14857587
Test loss (w/o reg) on all data: 0.07893452
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.864193e-06
Norm of the params: 11.872578
              Random: fixed  14 labels. Loss 0.07893. Accuracy 0.992.
### Flips: 156, rs: 18, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012056425
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9022165e-06
Norm of the params: 9.153166
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729914
Test loss (w/o reg) on all data: 0.0120565705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1550854e-07
Norm of the params: 9.153171
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14973244
Train loss (w/o reg) on all data: 0.14249977
Test loss (w/o reg) on all data: 0.07723305
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.453598e-05
Norm of the params: 12.027195
              Random: fixed  17 labels. Loss 0.07723. Accuracy 0.992.
### Flips: 156, rs: 18, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.605386e-08
Norm of the params: 9.153196
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1471918e-07
Norm of the params: 9.153196
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14226621
Train loss (w/o reg) on all data: 0.13508527
Test loss (w/o reg) on all data: 0.072614916
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.61707e-06
Norm of the params: 11.984107
              Random: fixed  20 labels. Loss 0.07261. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19524634
Train loss (w/o reg) on all data: 0.18795492
Test loss (w/o reg) on all data: 0.11505738
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.8465306e-05
Norm of the params: 12.075948
Flipped loss: 0.11506. Accuracy: 0.969
### Flips: 156, rs: 19, checks: 52
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10660907
Train loss (w/o reg) on all data: 0.096690096
Test loss (w/o reg) on all data: 0.07682604
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.7529282e-06
Norm of the params: 14.084725
     Influence (LOO): fixed  42 labels. Loss 0.07683. Accuracy 0.977.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06191522
Train loss (w/o reg) on all data: 0.048302468
Test loss (w/o reg) on all data: 0.051117603
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0972248e-05
Norm of the params: 16.500153
                Loss: fixed  52 labels. Loss 0.05112. Accuracy 0.981.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18775332
Train loss (w/o reg) on all data: 0.18050861
Test loss (w/o reg) on all data: 0.110197045
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5867561e-05
Norm of the params: 12.037191
              Random: fixed   4 labels. Loss 0.11020. Accuracy 0.973.
### Flips: 156, rs: 19, checks: 104
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04865872
Train loss (w/o reg) on all data: 0.03950393
Test loss (w/o reg) on all data: 0.032513157
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.175848e-06
Norm of the params: 13.531294
     Influence (LOO): fixed  66 labels. Loss 0.03251. Accuracy 0.989.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008076042
Train loss (w/o reg) on all data: 0.0030637265
Test loss (w/o reg) on all data: 0.014257971
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7674623e-07
Norm of the params: 10.012308
                Loss: fixed  81 labels. Loss 0.01426. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18033998
Train loss (w/o reg) on all data: 0.173294
Test loss (w/o reg) on all data: 0.10142847
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.58339e-05
Norm of the params: 11.870963
              Random: fixed   8 labels. Loss 0.10143. Accuracy 0.973.
### Flips: 156, rs: 19, checks: 156
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216443
Train loss (w/o reg) on all data: 0.005517224
Test loss (w/o reg) on all data: 0.012120295
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2204977e-06
Norm of the params: 9.694554
     Influence (LOO): fixed  81 labels. Loss 0.01212. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.610646e-07
Norm of the params: 9.153206
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16985503
Train loss (w/o reg) on all data: 0.1628051
Test loss (w/o reg) on all data: 0.09935875
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4367334e-05
Norm of the params: 11.874289
              Random: fixed  13 labels. Loss 0.09936. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728256
Test loss (w/o reg) on all data: 0.012055298
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1912636e-07
Norm of the params: 9.153352
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728282
Test loss (w/o reg) on all data: 0.012055138
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3400838e-07
Norm of the params: 9.153351
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16807978
Train loss (w/o reg) on all data: 0.16128369
Test loss (w/o reg) on all data: 0.09669713
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.2839334e-06
Norm of the params: 11.658552
              Random: fixed  15 labels. Loss 0.09670. Accuracy 0.985.
### Flips: 156, rs: 19, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012055539
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2669379e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.0120556075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.732225e-07
Norm of the params: 9.153214
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15941903
Train loss (w/o reg) on all data: 0.15215896
Test loss (w/o reg) on all data: 0.09387868
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0069449e-05
Norm of the params: 12.049951
              Random: fixed  19 labels. Loss 0.09388. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729378
Test loss (w/o reg) on all data: 0.012055808
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.293744e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172939
Test loss (w/o reg) on all data: 0.012055669
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2718077e-07
Norm of the params: 9.153228
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15362942
Train loss (w/o reg) on all data: 0.14645822
Test loss (w/o reg) on all data: 0.086423054
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6577027e-05
Norm of the params: 11.975974
              Random: fixed  22 labels. Loss 0.08642. Accuracy 0.985.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15640973
Train loss (w/o reg) on all data: 0.14747556
Test loss (w/o reg) on all data: 0.08631782
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0617142e-05
Norm of the params: 13.36725
Flipped loss: 0.08632. Accuracy: 0.981
### Flips: 156, rs: 20, checks: 52
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07214559
Train loss (w/o reg) on all data: 0.060524814
Test loss (w/o reg) on all data: 0.057807025
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.963907e-05
Norm of the params: 15.24518
     Influence (LOO): fixed  41 labels. Loss 0.05781. Accuracy 0.969.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036670648
Train loss (w/o reg) on all data: 0.023655565
Test loss (w/o reg) on all data: 0.021551477
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1988341e-05
Norm of the params: 16.133865
                Loss: fixed  49 labels. Loss 0.02155. Accuracy 0.996.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14746019
Train loss (w/o reg) on all data: 0.13755095
Test loss (w/o reg) on all data: 0.07580774
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0223359e-05
Norm of the params: 14.077817
              Random: fixed   5 labels. Loss 0.07581. Accuracy 0.989.
### Flips: 156, rs: 20, checks: 104
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024786305
Train loss (w/o reg) on all data: 0.016711785
Test loss (w/o reg) on all data: 0.01667026
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1803392e-06
Norm of the params: 12.707887
     Influence (LOO): fixed  64 labels. Loss 0.01667. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007247486
Train loss (w/o reg) on all data: 0.0026130036
Test loss (w/o reg) on all data: 0.0110798115
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2298487e-07
Norm of the params: 9.627546
                Loss: fixed  70 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14500442
Train loss (w/o reg) on all data: 0.13566962
Test loss (w/o reg) on all data: 0.07481042
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.531885e-06
Norm of the params: 13.663679
              Random: fixed   8 labels. Loss 0.07481. Accuracy 0.977.
### Flips: 156, rs: 20, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012054721
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3012435e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012054801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2741558e-07
Norm of the params: 9.153185
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14063609
Train loss (w/o reg) on all data: 0.13125288
Test loss (w/o reg) on all data: 0.07367907
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.072247e-05
Norm of the params: 13.699054
              Random: fixed  10 labels. Loss 0.07368. Accuracy 0.977.
### Flips: 156, rs: 20, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729155
Test loss (w/o reg) on all data: 0.012055012
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.897565e-08
Norm of the params: 9.153255
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729157
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7022212e-07
Norm of the params: 9.1532545
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13945454
Train loss (w/o reg) on all data: 0.13014147
Test loss (w/o reg) on all data: 0.0722944
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8062297e-05
Norm of the params: 13.647767
              Random: fixed  11 labels. Loss 0.07229. Accuracy 0.985.
### Flips: 156, rs: 20, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055297
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7861076e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055093
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2616693e-07
Norm of the params: 9.153186
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12498243
Train loss (w/o reg) on all data: 0.11505135
Test loss (w/o reg) on all data: 0.06678508
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.604128e-06
Norm of the params: 14.093321
              Random: fixed  18 labels. Loss 0.06679. Accuracy 0.992.
### Flips: 156, rs: 20, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729826
Test loss (w/o reg) on all data: 0.012054782
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1861756e-08
Norm of the params: 9.153181
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012054868
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9422824e-07
Norm of the params: 9.153183
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120368525
Train loss (w/o reg) on all data: 0.1110546
Test loss (w/o reg) on all data: 0.06871384
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.8913106e-06
Norm of the params: 13.648391
              Random: fixed  22 labels. Loss 0.06871. Accuracy 0.985.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18565325
Train loss (w/o reg) on all data: 0.17792226
Test loss (w/o reg) on all data: 0.1306507
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0425728e-05
Norm of the params: 12.434624
Flipped loss: 0.13065. Accuracy: 0.977
### Flips: 156, rs: 21, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08601511
Train loss (w/o reg) on all data: 0.074419156
Test loss (w/o reg) on all data: 0.083413266
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8420369e-05
Norm of the params: 15.228893
     Influence (LOO): fixed  43 labels. Loss 0.08341. Accuracy 0.973.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05605905
Train loss (w/o reg) on all data: 0.043198623
Test loss (w/o reg) on all data: 0.084422916
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6310347e-06
Norm of the params: 16.037724
                Loss: fixed  51 labels. Loss 0.08442. Accuracy 0.962.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17384936
Train loss (w/o reg) on all data: 0.16606346
Test loss (w/o reg) on all data: 0.114482865
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3071121e-05
Norm of the params: 12.478702
              Random: fixed   8 labels. Loss 0.11448. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 104
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05212052
Train loss (w/o reg) on all data: 0.04309532
Test loss (w/o reg) on all data: 0.046950776
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.408136e-06
Norm of the params: 13.435177
     Influence (LOO): fixed  60 labels. Loss 0.04695. Accuracy 0.981.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009706305
Train loss (w/o reg) on all data: 0.0038574773
Test loss (w/o reg) on all data: 0.010544288
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.8300396e-07
Norm of the params: 10.815571
                Loss: fixed  77 labels. Loss 0.01054. Accuracy 0.996.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16600087
Train loss (w/o reg) on all data: 0.15832245
Test loss (w/o reg) on all data: 0.11130697
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.3837614e-06
Norm of the params: 12.392278
              Random: fixed  12 labels. Loss 0.11131. Accuracy 0.973.
### Flips: 156, rs: 21, checks: 156
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019521499
Train loss (w/o reg) on all data: 0.012593484
Test loss (w/o reg) on all data: 0.01592103
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2042375e-06
Norm of the params: 11.771164
     Influence (LOO): fixed  75 labels. Loss 0.01592. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00801833
Train loss (w/o reg) on all data: 0.0030734604
Test loss (w/o reg) on all data: 0.012503573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8677695e-07
Norm of the params: 9.944716
                Loss: fixed  78 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15882303
Train loss (w/o reg) on all data: 0.15096484
Test loss (w/o reg) on all data: 0.10679422
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.975061e-05
Norm of the params: 12.536498
              Random: fixed  16 labels. Loss 0.10679. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 208
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224469
Train loss (w/o reg) on all data: 0.006245334
Test loss (w/o reg) on all data: 0.012817637
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.3264416e-07
Norm of the params: 9.979115
     Influence (LOO): fixed  78 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012055604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2828034e-07
Norm of the params: 9.153184
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14670745
Train loss (w/o reg) on all data: 0.13911837
Test loss (w/o reg) on all data: 0.10331754
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5554247e-05
Norm of the params: 12.319956
              Random: fixed  22 labels. Loss 0.10332. Accuracy 0.977.
### Flips: 156, rs: 21, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172955
Test loss (w/o reg) on all data: 0.012055871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5158153e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012055818
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.215153e-07
Norm of the params: 9.153209
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1431308
Train loss (w/o reg) on all data: 0.13541315
Test loss (w/o reg) on all data: 0.100880586
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6034919e-05
Norm of the params: 12.423881
              Random: fixed  25 labels. Loss 0.10088. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173002
Test loss (w/o reg) on all data: 0.012054922
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.537284e-08
Norm of the params: 9.153159
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729995
Test loss (w/o reg) on all data: 0.0120549295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1339773e-07
Norm of the params: 9.153161
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12843122
Train loss (w/o reg) on all data: 0.12004644
Test loss (w/o reg) on all data: 0.09816345
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1355288e-05
Norm of the params: 12.949731
              Random: fixed  30 labels. Loss 0.09816. Accuracy 0.973.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1753715
Train loss (w/o reg) on all data: 0.16727813
Test loss (w/o reg) on all data: 0.10467078
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.1973714e-06
Norm of the params: 12.722715
Flipped loss: 0.10467. Accuracy: 0.977
### Flips: 156, rs: 22, checks: 52
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09378309
Train loss (w/o reg) on all data: 0.08355836
Test loss (w/o reg) on all data: 0.07426999
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3348175e-05
Norm of the params: 14.3001585
     Influence (LOO): fixed  37 labels. Loss 0.07427. Accuracy 0.977.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048782326
Train loss (w/o reg) on all data: 0.035408232
Test loss (w/o reg) on all data: 0.047687758
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.343709e-06
Norm of the params: 16.354876
                Loss: fixed  52 labels. Loss 0.04769. Accuracy 0.981.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17188543
Train loss (w/o reg) on all data: 0.16420323
Test loss (w/o reg) on all data: 0.10008428
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3555568e-05
Norm of the params: 12.395329
              Random: fixed   4 labels. Loss 0.10008. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 104
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02655035
Train loss (w/o reg) on all data: 0.018955478
Test loss (w/o reg) on all data: 0.024445254
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6890625e-06
Norm of the params: 12.324669
     Influence (LOO): fixed  66 labels. Loss 0.02445. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075972104
Train loss (w/o reg) on all data: 0.0029205661
Test loss (w/o reg) on all data: 0.0133775035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8554197e-07
Norm of the params: 9.671241
                Loss: fixed  73 labels. Loss 0.01338. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16833338
Train loss (w/o reg) on all data: 0.16065183
Test loss (w/o reg) on all data: 0.09758262
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.138902e-05
Norm of the params: 12.394795
              Random: fixed   6 labels. Loss 0.09758. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 156
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015606975
Train loss (w/o reg) on all data: 0.009940871
Test loss (w/o reg) on all data: 0.0150723895
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.844487e-07
Norm of the params: 10.645285
     Influence (LOO): fixed  71 labels. Loss 0.01507. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.010854e-07
Norm of the params: 9.153202
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16207692
Train loss (w/o reg) on all data: 0.15461849
Test loss (w/o reg) on all data: 0.089444764
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7619706e-05
Norm of the params: 12.213465
              Random: fixed  10 labels. Loss 0.08944. Accuracy 0.985.
### Flips: 156, rs: 22, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.0120548615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3015814e-08
Norm of the params: 9.153196
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012055023
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.907813e-08
Norm of the params: 9.153194
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16023673
Train loss (w/o reg) on all data: 0.15288311
Test loss (w/o reg) on all data: 0.085476875
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0023487e-05
Norm of the params: 12.127345
              Random: fixed  11 labels. Loss 0.08548. Accuracy 0.989.
### Flips: 156, rs: 22, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0784542e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730054
Test loss (w/o reg) on all data: 0.012055091
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8529137e-07
Norm of the params: 9.153157
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15012108
Train loss (w/o reg) on all data: 0.14207764
Test loss (w/o reg) on all data: 0.08129059
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2166354e-05
Norm of the params: 12.683403
              Random: fixed  15 labels. Loss 0.08129. Accuracy 0.981.
### Flips: 156, rs: 22, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.246735e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1176375e-07
Norm of the params: 9.1532
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14906733
Train loss (w/o reg) on all data: 0.1409944
Test loss (w/o reg) on all data: 0.07959346
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8910392e-05
Norm of the params: 12.706638
              Random: fixed  16 labels. Loss 0.07959. Accuracy 0.981.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17331587
Train loss (w/o reg) on all data: 0.16467787
Test loss (w/o reg) on all data: 0.1080177
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4294267e-06
Norm of the params: 13.14382
Flipped loss: 0.10802. Accuracy: 0.981
### Flips: 156, rs: 23, checks: 52
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09553839
Train loss (w/o reg) on all data: 0.08444092
Test loss (w/o reg) on all data: 0.07081964
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.8956017e-06
Norm of the params: 14.897968
     Influence (LOO): fixed  39 labels. Loss 0.07082. Accuracy 0.977.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05231257
Train loss (w/o reg) on all data: 0.03741729
Test loss (w/o reg) on all data: 0.0492387
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7371004e-06
Norm of the params: 17.259945
                Loss: fixed  48 labels. Loss 0.04924. Accuracy 0.985.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16554137
Train loss (w/o reg) on all data: 0.1566439
Test loss (w/o reg) on all data: 0.09867006
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.955383e-06
Norm of the params: 13.339768
              Random: fixed   5 labels. Loss 0.09867. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 104
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022856066
Train loss (w/o reg) on all data: 0.015791154
Test loss (w/o reg) on all data: 0.020574559
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2539673e-06
Norm of the params: 11.886895
     Influence (LOO): fixed  71 labels. Loss 0.02057. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011212135
Train loss (w/o reg) on all data: 0.005092094
Test loss (w/o reg) on all data: 0.024340495
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0113961e-06
Norm of the params: 11.06349
                Loss: fixed  72 labels. Loss 0.02434. Accuracy 0.989.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16242222
Train loss (w/o reg) on all data: 0.15366817
Test loss (w/o reg) on all data: 0.09672896
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7622484e-05
Norm of the params: 13.231828
              Random: fixed   7 labels. Loss 0.09673. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.012054697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4303998e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730089
Test loss (w/o reg) on all data: 0.012054759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4927436e-07
Norm of the params: 9.1531515
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15182069
Train loss (w/o reg) on all data: 0.14268976
Test loss (w/o reg) on all data: 0.08671211
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3890753e-05
Norm of the params: 13.51364
              Random: fixed  12 labels. Loss 0.08671. Accuracy 0.985.
### Flips: 156, rs: 23, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012054512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7520997e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730042
Test loss (w/o reg) on all data: 0.01205446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2465439e-07
Norm of the params: 9.153158
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14574794
Train loss (w/o reg) on all data: 0.13637702
Test loss (w/o reg) on all data: 0.08812654
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.2306043e-06
Norm of the params: 13.690086
              Random: fixed  15 labels. Loss 0.08813. Accuracy 0.985.
### Flips: 156, rs: 23, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.01205446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.212138e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729572
Test loss (w/o reg) on all data: 0.012054612
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.065068e-07
Norm of the params: 9.153209
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14107293
Train loss (w/o reg) on all data: 0.1316055
Test loss (w/o reg) on all data: 0.08408992
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3342537e-05
Norm of the params: 13.760399
              Random: fixed  18 labels. Loss 0.08409. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012055217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9097986e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172943
Test loss (w/o reg) on all data: 0.012056769
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8515873e-07
Norm of the params: 9.153224
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1408302
Train loss (w/o reg) on all data: 0.13168798
Test loss (w/o reg) on all data: 0.08317223
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.906057e-06
Norm of the params: 13.522002
              Random: fixed  19 labels. Loss 0.08317. Accuracy 0.985.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1892039
Train loss (w/o reg) on all data: 0.18126424
Test loss (w/o reg) on all data: 0.11791248
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3517754e-05
Norm of the params: 12.601322
Flipped loss: 0.11791. Accuracy: 0.985
### Flips: 156, rs: 24, checks: 52
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10784102
Train loss (w/o reg) on all data: 0.09671888
Test loss (w/o reg) on all data: 0.08020285
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4123088e-05
Norm of the params: 14.914517
     Influence (LOO): fixed  37 labels. Loss 0.08020. Accuracy 0.973.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066788524
Train loss (w/o reg) on all data: 0.053156152
Test loss (w/o reg) on all data: 0.05945012
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9799474e-05
Norm of the params: 16.512041
                Loss: fixed  52 labels. Loss 0.05945. Accuracy 0.973.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18792082
Train loss (w/o reg) on all data: 0.18005782
Test loss (w/o reg) on all data: 0.11760328
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0683477e-05
Norm of the params: 12.540333
              Random: fixed   2 labels. Loss 0.11760. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 104
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043309815
Train loss (w/o reg) on all data: 0.034040455
Test loss (w/o reg) on all data: 0.03670877
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.510295e-06
Norm of the params: 13.615699
     Influence (LOO): fixed  65 labels. Loss 0.03671. Accuracy 0.985.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011734415
Train loss (w/o reg) on all data: 0.0050184573
Test loss (w/o reg) on all data: 0.014092501
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4465704e-06
Norm of the params: 11.589615
                Loss: fixed  78 labels. Loss 0.01409. Accuracy 0.996.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18516466
Train loss (w/o reg) on all data: 0.17718084
Test loss (w/o reg) on all data: 0.114592075
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.260164e-05
Norm of the params: 12.636313
              Random: fixed   5 labels. Loss 0.11459. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 156
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01690799
Train loss (w/o reg) on all data: 0.010424368
Test loss (w/o reg) on all data: 0.015114496
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.079007e-06
Norm of the params: 11.387381
     Influence (LOO): fixed  78 labels. Loss 0.01511. Accuracy 0.989.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028169488
Test loss (w/o reg) on all data: 0.012754823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.603947e-07
Norm of the params: 9.683701
                Loss: fixed  81 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17632481
Train loss (w/o reg) on all data: 0.168004
Test loss (w/o reg) on all data: 0.113893494
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6195008e-05
Norm of the params: 12.900233
              Random: fixed  10 labels. Loss 0.11389. Accuracy 0.981.
### Flips: 156, rs: 24, checks: 208
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010366621
Train loss (w/o reg) on all data: 0.0053553386
Test loss (w/o reg) on all data: 0.015478092
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0989805e-06
Norm of the params: 10.011277
     Influence (LOO): fixed  81 labels. Loss 0.01548. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.01205467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4406469e-07
Norm of the params: 9.153184
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1663304
Train loss (w/o reg) on all data: 0.15731019
Test loss (w/o reg) on all data: 0.10465999
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7582044e-05
Norm of the params: 13.431468
              Random: fixed  16 labels. Loss 0.10466. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729255
Test loss (w/o reg) on all data: 0.012055067
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4988025e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.012055113
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.865968e-07
Norm of the params: 9.153242
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15572433
Train loss (w/o reg) on all data: 0.14641246
Test loss (w/o reg) on all data: 0.10387929
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.8518104e-05
Norm of the params: 13.646882
              Random: fixed  20 labels. Loss 0.10388. Accuracy 0.981.
### Flips: 156, rs: 24, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730328
Test loss (w/o reg) on all data: 0.012055824
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8985808e-07
Norm of the params: 9.153125
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730305
Test loss (w/o reg) on all data: 0.012055725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.928584e-07
Norm of the params: 9.153129
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15090774
Train loss (w/o reg) on all data: 0.14157967
Test loss (w/o reg) on all data: 0.103479326
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.774677e-06
Norm of the params: 13.658747
              Random: fixed  23 labels. Loss 0.10348. Accuracy 0.973.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17030407
Train loss (w/o reg) on all data: 0.16206557
Test loss (w/o reg) on all data: 0.085053846
Train acc on all data:  0.9369627507163324
Test acc on all data:   1.0
Norm of the mean of gradients: 3.551499e-05
Norm of the params: 12.836288
Flipped loss: 0.08505. Accuracy: 1.000
### Flips: 156, rs: 25, checks: 52
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07529503
Train loss (w/o reg) on all data: 0.0672758
Test loss (w/o reg) on all data: 0.042692784
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2179509e-05
Norm of the params: 12.664304
     Influence (LOO): fixed  42 labels. Loss 0.04269. Accuracy 0.989.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034073003
Train loss (w/o reg) on all data: 0.021499343
Test loss (w/o reg) on all data: 0.04025747
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.0026014e-06
Norm of the params: 15.857905
                Loss: fixed  52 labels. Loss 0.04026. Accuracy 0.985.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16375843
Train loss (w/o reg) on all data: 0.15553774
Test loss (w/o reg) on all data: 0.08203852
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0157484e-05
Norm of the params: 12.822395
              Random: fixed   4 labels. Loss 0.08204. Accuracy 0.996.
### Flips: 156, rs: 25, checks: 104
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029816968
Train loss (w/o reg) on all data: 0.023317613
Test loss (w/o reg) on all data: 0.019366149
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3530469e-06
Norm of the params: 11.401189
     Influence (LOO): fixed  62 labels. Loss 0.01937. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730212
Test loss (w/o reg) on all data: 0.012055139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9274985e-07
Norm of the params: 9.153138
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16187173
Train loss (w/o reg) on all data: 0.15370065
Test loss (w/o reg) on all data: 0.07830604
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.185031e-06
Norm of the params: 12.7836485
              Random: fixed   6 labels. Loss 0.07831. Accuracy 0.996.
### Flips: 156, rs: 25, checks: 156
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002352
Train loss (w/o reg) on all data: 0.0054478073
Test loss (w/o reg) on all data: 0.012244749
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.119203e-07
Norm of the params: 9.544155
     Influence (LOO): fixed  69 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730782
Test loss (w/o reg) on all data: 0.012055296
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0766725e-07
Norm of the params: 9.153075
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15757507
Train loss (w/o reg) on all data: 0.15016249
Test loss (w/o reg) on all data: 0.07674233
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0997347e-05
Norm of the params: 12.17586
              Random: fixed   9 labels. Loss 0.07674. Accuracy 0.989.
### Flips: 156, rs: 25, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729202
Test loss (w/o reg) on all data: 0.012054512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.231712e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012054522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5372155e-07
Norm of the params: 9.153248
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15256423
Train loss (w/o reg) on all data: 0.14523552
Test loss (w/o reg) on all data: 0.07458534
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1340205e-05
Norm of the params: 12.106774
              Random: fixed  12 labels. Loss 0.07459. Accuracy 0.992.
### Flips: 156, rs: 25, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729926
Test loss (w/o reg) on all data: 0.012054793
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1233473e-07
Norm of the params: 9.153172
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012054745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7615147e-07
Norm of the params: 9.153171
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13858816
Train loss (w/o reg) on all data: 0.13075736
Test loss (w/o reg) on all data: 0.06968761
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.98943e-06
Norm of the params: 12.514636
              Random: fixed  18 labels. Loss 0.06969. Accuracy 0.989.
### Flips: 156, rs: 25, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729325
Test loss (w/o reg) on all data: 0.012055025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4104937e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172932
Test loss (w/o reg) on all data: 0.012055063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8101806e-07
Norm of the params: 9.153236
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13545406
Train loss (w/o reg) on all data: 0.12798655
Test loss (w/o reg) on all data: 0.068336844
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3784287e-05
Norm of the params: 12.220886
              Random: fixed  20 labels. Loss 0.06834. Accuracy 0.985.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16943893
Train loss (w/o reg) on all data: 0.16036393
Test loss (w/o reg) on all data: 0.11207892
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7025256e-05
Norm of the params: 13.4721985
Flipped loss: 0.11208. Accuracy: 0.977
### Flips: 156, rs: 26, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0790961
Train loss (w/o reg) on all data: 0.07023169
Test loss (w/o reg) on all data: 0.055445913
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2327124e-06
Norm of the params: 13.3149605
     Influence (LOO): fixed  41 labels. Loss 0.05545. Accuracy 0.981.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035196833
Train loss (w/o reg) on all data: 0.02305801
Test loss (w/o reg) on all data: 0.06495317
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.643395e-06
Norm of the params: 15.581286
                Loss: fixed  52 labels. Loss 0.06495. Accuracy 0.973.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15718997
Train loss (w/o reg) on all data: 0.14847635
Test loss (w/o reg) on all data: 0.094035104
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4195903e-06
Norm of the params: 13.2012205
              Random: fixed   8 labels. Loss 0.09404. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 104
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02695571
Train loss (w/o reg) on all data: 0.020140398
Test loss (w/o reg) on all data: 0.016444184
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8366005e-06
Norm of the params: 11.675027
     Influence (LOO): fixed  63 labels. Loss 0.01644. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008879045
Train loss (w/o reg) on all data: 0.003420939
Test loss (w/o reg) on all data: 0.013979847
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.264179e-07
Norm of the params: 10.448069
                Loss: fixed  70 labels. Loss 0.01398. Accuracy 0.996.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15584622
Train loss (w/o reg) on all data: 0.147219
Test loss (w/o reg) on all data: 0.09131613
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.213018e-06
Norm of the params: 13.135622
              Random: fixed  10 labels. Loss 0.09132. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 156
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009260062
Train loss (w/o reg) on all data: 0.004402259
Test loss (w/o reg) on all data: 0.0124422135
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0983134e-06
Norm of the params: 9.856778
     Influence (LOO): fixed  70 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729087
Test loss (w/o reg) on all data: 0.012054877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3747471e-07
Norm of the params: 9.153262
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14466849
Train loss (w/o reg) on all data: 0.13519564
Test loss (w/o reg) on all data: 0.09296604
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.3872235e-05
Norm of the params: 13.76433
              Random: fixed  14 labels. Loss 0.09297. Accuracy 0.969.
### Flips: 156, rs: 26, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730063
Test loss (w/o reg) on all data: 0.012055412
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.496928e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730068
Test loss (w/o reg) on all data: 0.012055488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5357775e-07
Norm of the params: 9.153154
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1411788
Train loss (w/o reg) on all data: 0.1317036
Test loss (w/o reg) on all data: 0.094892666
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.067689e-06
Norm of the params: 13.7660475
              Random: fixed  17 labels. Loss 0.09489. Accuracy 0.973.
### Flips: 156, rs: 26, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.012055326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2700148e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012055359
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4765396e-07
Norm of the params: 9.153191
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13671568
Train loss (w/o reg) on all data: 0.12678958
Test loss (w/o reg) on all data: 0.09428363
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6692138e-05
Norm of the params: 14.089783
              Random: fixed  19 labels. Loss 0.09428. Accuracy 0.977.
### Flips: 156, rs: 26, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728866
Test loss (w/o reg) on all data: 0.0120555805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8634064e-07
Norm of the params: 9.153286
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728931
Test loss (w/o reg) on all data: 0.012055603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.495019e-07
Norm of the params: 9.153279
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12782548
Train loss (w/o reg) on all data: 0.11698766
Test loss (w/o reg) on all data: 0.090935394
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1921246e-05
Norm of the params: 14.722655
              Random: fixed  23 labels. Loss 0.09094. Accuracy 0.977.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18353707
Train loss (w/o reg) on all data: 0.17654216
Test loss (w/o reg) on all data: 0.10667794
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8325427e-05
Norm of the params: 11.827855
Flipped loss: 0.10668. Accuracy: 0.989
### Flips: 156, rs: 27, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10210137
Train loss (w/o reg) on all data: 0.09395902
Test loss (w/o reg) on all data: 0.073403455
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.471413e-06
Norm of the params: 12.761155
     Influence (LOO): fixed  38 labels. Loss 0.07340. Accuracy 0.977.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04750024
Train loss (w/o reg) on all data: 0.035092145
Test loss (w/o reg) on all data: 0.059426166
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0573254e-06
Norm of the params: 15.753156
                Loss: fixed  52 labels. Loss 0.05943. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17244157
Train loss (w/o reg) on all data: 0.16512686
Test loss (w/o reg) on all data: 0.0955365
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.965881e-05
Norm of the params: 12.095215
              Random: fixed   6 labels. Loss 0.09554. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 104
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03386049
Train loss (w/o reg) on all data: 0.026052557
Test loss (w/o reg) on all data: 0.046729147
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5224265e-06
Norm of the params: 12.496346
     Influence (LOO): fixed  69 labels. Loss 0.04673. Accuracy 0.985.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009203458
Train loss (w/o reg) on all data: 0.0036351874
Test loss (w/o reg) on all data: 0.011850497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0105537e-07
Norm of the params: 10.552981
                Loss: fixed  77 labels. Loss 0.01185. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16838759
Train loss (w/o reg) on all data: 0.16090761
Test loss (w/o reg) on all data: 0.09282893
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0221621e-05
Norm of the params: 12.231093
              Random: fixed   8 labels. Loss 0.09283. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012055056
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.504469e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076085227
Train loss (w/o reg) on all data: 0.002844645
Test loss (w/o reg) on all data: 0.01421833
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.630645e-07
Norm of the params: 9.761023
                Loss: fixed  78 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16248561
Train loss (w/o reg) on all data: 0.1544687
Test loss (w/o reg) on all data: 0.08986434
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0695333e-05
Norm of the params: 12.662478
              Random: fixed  12 labels. Loss 0.08986. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730221
Test loss (w/o reg) on all data: 0.012055159
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0808228e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007608525
Train loss (w/o reg) on all data: 0.002844545
Test loss (w/o reg) on all data: 0.01421823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8756713e-07
Norm of the params: 9.761127
                Loss: fixed  78 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14830378
Train loss (w/o reg) on all data: 0.14007969
Test loss (w/o reg) on all data: 0.07915421
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.878703e-05
Norm of the params: 12.825045
              Random: fixed  20 labels. Loss 0.07915. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729888
Test loss (w/o reg) on all data: 0.012055031
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1617522e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.01205499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.827582e-08
Norm of the params: 9.153174
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14465669
Train loss (w/o reg) on all data: 0.13617036
Test loss (w/o reg) on all data: 0.07651223
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2023087e-05
Norm of the params: 13.027917
              Random: fixed  22 labels. Loss 0.07651. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 312
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730333
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.531617e-07
Norm of the params: 9.153127
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730321
Test loss (w/o reg) on all data: 0.012055328
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0987116e-07
Norm of the params: 9.153128
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139312
Train loss (w/o reg) on all data: 0.13151453
Test loss (w/o reg) on all data: 0.06900104
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.500613e-06
Norm of the params: 12.487969
              Random: fixed  26 labels. Loss 0.06900. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19577163
Train loss (w/o reg) on all data: 0.18796568
Test loss (w/o reg) on all data: 0.11359976
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.912614e-06
Norm of the params: 12.494769
Flipped loss: 0.11360. Accuracy: 0.985
### Flips: 156, rs: 28, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10902779
Train loss (w/o reg) on all data: 0.09707614
Test loss (w/o reg) on all data: 0.06742908
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6186695e-05
Norm of the params: 15.460691
     Influence (LOO): fixed  39 labels. Loss 0.06743. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06892986
Train loss (w/o reg) on all data: 0.054605782
Test loss (w/o reg) on all data: 0.05280017
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1379708e-05
Norm of the params: 16.925766
                Loss: fixed  52 labels. Loss 0.05280. Accuracy 0.985.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1930444
Train loss (w/o reg) on all data: 0.18572655
Test loss (w/o reg) on all data: 0.10742638
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3765023e-05
Norm of the params: 12.097805
              Random: fixed   3 labels. Loss 0.10743. Accuracy 0.989.
### Flips: 156, rs: 28, checks: 104
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05860848
Train loss (w/o reg) on all data: 0.04711345
Test loss (w/o reg) on all data: 0.052198265
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.959561e-06
Norm of the params: 15.162475
     Influence (LOO): fixed  63 labels. Loss 0.05220. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055231
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.999555e-08
Norm of the params: 9.15316
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18842371
Train loss (w/o reg) on all data: 0.18122673
Test loss (w/o reg) on all data: 0.104787536
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1120881e-05
Norm of the params: 11.997476
              Random: fixed   6 labels. Loss 0.10479. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 156
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017325755
Train loss (w/o reg) on all data: 0.0119770765
Test loss (w/o reg) on all data: 0.014870551
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.710879e-07
Norm of the params: 10.342804
     Influence (LOO): fixed  79 labels. Loss 0.01487. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012054993
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2335747e-07
Norm of the params: 9.153196
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18510382
Train loss (w/o reg) on all data: 0.17795362
Test loss (w/o reg) on all data: 0.104120135
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.4278414e-05
Norm of the params: 11.958426
              Random: fixed   8 labels. Loss 0.10412. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 208
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038311
Train loss (w/o reg) on all data: 0.0052867546
Test loss (w/o reg) on all data: 0.01229597
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.532536e-07
Norm of the params: 9.74839
     Influence (LOO): fixed  81 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728668
Test loss (w/o reg) on all data: 0.012055153
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7062575e-07
Norm of the params: 9.153309
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17637813
Train loss (w/o reg) on all data: 0.1691441
Test loss (w/o reg) on all data: 0.10394662
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2170774e-05
Norm of the params: 12.02833
              Random: fixed  12 labels. Loss 0.10395. Accuracy 0.977.
### Flips: 156, rs: 28, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731418
Test loss (w/o reg) on all data: 0.012055697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.688459e-07
Norm of the params: 9.153007
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021731376
Test loss (w/o reg) on all data: 0.012055803
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7467206e-07
Norm of the params: 9.153009
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17394418
Train loss (w/o reg) on all data: 0.16683437
Test loss (w/o reg) on all data: 0.102028124
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.431564e-05
Norm of the params: 11.924601
              Random: fixed  13 labels. Loss 0.10203. Accuracy 0.973.
### Flips: 156, rs: 28, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021730445
Test loss (w/o reg) on all data: 0.012055969
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6015018e-07
Norm of the params: 9.153115
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730363
Test loss (w/o reg) on all data: 0.012055652
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.712742e-07
Norm of the params: 9.153122
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16708295
Train loss (w/o reg) on all data: 0.16036154
Test loss (w/o reg) on all data: 0.09546274
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.657877e-06
Norm of the params: 11.594319
              Random: fixed  18 labels. Loss 0.09546. Accuracy 0.981.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18440148
Train loss (w/o reg) on all data: 0.17740776
Test loss (w/o reg) on all data: 0.10544278
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3136924e-05
Norm of the params: 11.826854
Flipped loss: 0.10544. Accuracy: 0.989
### Flips: 156, rs: 29, checks: 52
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0886103
Train loss (w/o reg) on all data: 0.07997223
Test loss (w/o reg) on all data: 0.06566146
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9501323e-06
Norm of the params: 13.143873
     Influence (LOO): fixed  41 labels. Loss 0.06566. Accuracy 0.989.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046974897
Train loss (w/o reg) on all data: 0.034438837
Test loss (w/o reg) on all data: 0.05139964
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5633477e-06
Norm of the params: 15.834178
                Loss: fixed  52 labels. Loss 0.05140. Accuracy 0.981.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1724658
Train loss (w/o reg) on all data: 0.16542782
Test loss (w/o reg) on all data: 0.09249055
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.22689e-06
Norm of the params: 11.864223
              Random: fixed   7 labels. Loss 0.09249. Accuracy 0.992.
### Flips: 156, rs: 29, checks: 104
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052759826
Train loss (w/o reg) on all data: 0.04460043
Test loss (w/o reg) on all data: 0.033651546
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9724677e-06
Norm of the params: 12.774505
     Influence (LOO): fixed  59 labels. Loss 0.03365. Accuracy 0.996.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010589728
Train loss (w/o reg) on all data: 0.0041359654
Test loss (w/o reg) on all data: 0.012075613
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.3520243e-07
Norm of the params: 11.361129
                Loss: fixed  74 labels. Loss 0.01208. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16308126
Train loss (w/o reg) on all data: 0.15634553
Test loss (w/o reg) on all data: 0.08715615
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0408733e-05
Norm of the params: 11.606654
              Random: fixed  13 labels. Loss 0.08716. Accuracy 0.989.
### Flips: 156, rs: 29, checks: 156
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01226959
Train loss (w/o reg) on all data: 0.0068252464
Test loss (w/o reg) on all data: 0.014959991
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.5943742e-07
Norm of the params: 10.434887
     Influence (LOO): fixed  75 labels. Loss 0.01496. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231933
Train loss (w/o reg) on all data: 0.00273054
Test loss (w/o reg) on all data: 0.009577819
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3279175e-07
Norm of the params: 9.892071
                Loss: fixed  76 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15765493
Train loss (w/o reg) on all data: 0.15077272
Test loss (w/o reg) on all data: 0.08593472
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4793866e-05
Norm of the params: 11.732179
              Random: fixed  16 labels. Loss 0.08593. Accuracy 0.981.
### Flips: 156, rs: 29, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.0120555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7843093e-07
Norm of the params: 9.153229
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007623193
Train loss (w/o reg) on all data: 0.0027304683
Test loss (w/o reg) on all data: 0.009577897
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.659772e-07
Norm of the params: 9.892142
                Loss: fixed  76 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14656344
Train loss (w/o reg) on all data: 0.13892846
Test loss (w/o reg) on all data: 0.08074138
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.4646888e-05
Norm of the params: 12.357174
              Random: fixed  20 labels. Loss 0.08074. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012055741
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4751895e-07
Norm of the params: 9.153232
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012055607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.016444e-07
Norm of the params: 9.153219
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13698195
Train loss (w/o reg) on all data: 0.1290641
Test loss (w/o reg) on all data: 0.07593941
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.24310245e-05
Norm of the params: 12.5840025
              Random: fixed  25 labels. Loss 0.07594. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012055435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1062029e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055383
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1396606e-07
Norm of the params: 9.153198
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12574644
Train loss (w/o reg) on all data: 0.11805986
Test loss (w/o reg) on all data: 0.07147393
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.3001262e-05
Norm of the params: 12.398858
              Random: fixed  29 labels. Loss 0.07147. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19069692
Train loss (w/o reg) on all data: 0.18328467
Test loss (w/o reg) on all data: 0.10053357
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.632422e-05
Norm of the params: 12.175596
Flipped loss: 0.10053. Accuracy: 0.985
### Flips: 156, rs: 30, checks: 52
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1110649
Train loss (w/o reg) on all data: 0.100559205
Test loss (w/o reg) on all data: 0.062070206
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7060982e-06
Norm of the params: 14.495307
     Influence (LOO): fixed  37 labels. Loss 0.06207. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05528873
Train loss (w/o reg) on all data: 0.042920984
Test loss (w/o reg) on all data: 0.065051496
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.46177e-06
Norm of the params: 15.727521
                Loss: fixed  52 labels. Loss 0.06505. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18173395
Train loss (w/o reg) on all data: 0.17416303
Test loss (w/o reg) on all data: 0.09394055
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6269243e-05
Norm of the params: 12.305222
              Random: fixed   6 labels. Loss 0.09394. Accuracy 0.985.
### Flips: 156, rs: 30, checks: 104
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04878488
Train loss (w/o reg) on all data: 0.040691722
Test loss (w/o reg) on all data: 0.03539897
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1166288e-06
Norm of the params: 12.722548
     Influence (LOO): fixed  66 labels. Loss 0.03540. Accuracy 0.989.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009626354
Train loss (w/o reg) on all data: 0.003669532
Test loss (w/o reg) on all data: 0.013456341
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6097392e-07
Norm of the params: 10.914965
                Loss: fixed  78 labels. Loss 0.01346. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17499575
Train loss (w/o reg) on all data: 0.16708347
Test loss (w/o reg) on all data: 0.08905214
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.624493e-05
Norm of the params: 12.579566
              Random: fixed   9 labels. Loss 0.08905. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 156
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010217054
Train loss (w/o reg) on all data: 0.0050849845
Test loss (w/o reg) on all data: 0.022661226
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4923585e-07
Norm of the params: 10.131208
     Influence (LOO): fixed  80 labels. Loss 0.02266. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728573
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.957632e-07
Norm of the params: 9.153317
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16797191
Train loss (w/o reg) on all data: 0.15952101
Test loss (w/o reg) on all data: 0.08350007
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5071478e-05
Norm of the params: 13.00069
              Random: fixed  12 labels. Loss 0.08350. Accuracy 0.992.
### Flips: 156, rs: 30, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012054971
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.081316e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729525
Test loss (w/o reg) on all data: 0.012055019
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0218976e-07
Norm of the params: 9.153214
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15916833
Train loss (w/o reg) on all data: 0.1503986
Test loss (w/o reg) on all data: 0.079847686
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1480879e-05
Norm of the params: 13.243664
              Random: fixed  16 labels. Loss 0.07985. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728734
Test loss (w/o reg) on all data: 0.012055488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9032466e-07
Norm of the params: 9.153298
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172883
Test loss (w/o reg) on all data: 0.012055326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9897854e-07
Norm of the params: 9.15329
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15730385
Train loss (w/o reg) on all data: 0.1483123
Test loss (w/o reg) on all data: 0.07881951
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.164064e-05
Norm of the params: 13.410113
              Random: fixed  17 labels. Loss 0.07882. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012055544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.969216e-07
Norm of the params: 9.15322
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.01205545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1467662e-07
Norm of the params: 9.153218
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15332058
Train loss (w/o reg) on all data: 0.1444979
Test loss (w/o reg) on all data: 0.07956797
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.981001e-05
Norm of the params: 13.283581
              Random: fixed  19 labels. Loss 0.07957. Accuracy 0.989.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17769282
Train loss (w/o reg) on all data: 0.16976772
Test loss (w/o reg) on all data: 0.11287314
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.6674247e-06
Norm of the params: 12.589756
Flipped loss: 0.11287. Accuracy: 0.973
### Flips: 156, rs: 31, checks: 52
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07847091
Train loss (w/o reg) on all data: 0.06856606
Test loss (w/o reg) on all data: 0.07120372
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.0734767e-06
Norm of the params: 14.074694
     Influence (LOO): fixed  43 labels. Loss 0.07120. Accuracy 0.977.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039168306
Train loss (w/o reg) on all data: 0.026966225
Test loss (w/o reg) on all data: 0.053422544
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.659628e-06
Norm of the params: 15.621831
                Loss: fixed  52 labels. Loss 0.05342. Accuracy 0.977.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1643865
Train loss (w/o reg) on all data: 0.15634619
Test loss (w/o reg) on all data: 0.1029513
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1010513e-05
Norm of the params: 12.680932
              Random: fixed   8 labels. Loss 0.10295. Accuracy 0.981.
### Flips: 156, rs: 31, checks: 104
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032612346
Train loss (w/o reg) on all data: 0.024495563
Test loss (w/o reg) on all data: 0.03980211
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7252507e-06
Norm of the params: 12.7411
     Influence (LOO): fixed  62 labels. Loss 0.03980. Accuracy 0.985.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012716406
Train loss (w/o reg) on all data: 0.0057113953
Test loss (w/o reg) on all data: 0.019132545
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.824178e-07
Norm of the params: 11.836393
                Loss: fixed  71 labels. Loss 0.01913. Accuracy 0.989.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15661126
Train loss (w/o reg) on all data: 0.1481364
Test loss (w/o reg) on all data: 0.0967704
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5505065e-05
Norm of the params: 13.019111
              Random: fixed  12 labels. Loss 0.09677. Accuracy 0.981.
### Flips: 156, rs: 31, checks: 156
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012055468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3089835e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008336665
Train loss (w/o reg) on all data: 0.0032727392
Test loss (w/o reg) on all data: 0.016870776
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.4190466e-07
Norm of the params: 10.063723
                Loss: fixed  73 labels. Loss 0.01687. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14927475
Train loss (w/o reg) on all data: 0.14044693
Test loss (w/o reg) on all data: 0.090072155
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4529476e-06
Norm of the params: 13.287454
              Random: fixed  16 labels. Loss 0.09007. Accuracy 0.989.
### Flips: 156, rs: 31, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012056168
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.546858e-07
Norm of the params: 9.153134
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730242
Test loss (w/o reg) on all data: 0.012056225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7661346e-07
Norm of the params: 9.153137
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14093143
Train loss (w/o reg) on all data: 0.13199107
Test loss (w/o reg) on all data: 0.085029826
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.8726514e-06
Norm of the params: 13.37187
              Random: fixed  20 labels. Loss 0.08503. Accuracy 0.985.
### Flips: 156, rs: 31, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730335
Test loss (w/o reg) on all data: 0.012055386
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0099148e-07
Norm of the params: 9.153124
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012055631
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7370746e-07
Norm of the params: 9.153135
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13347773
Train loss (w/o reg) on all data: 0.12376959
Test loss (w/o reg) on all data: 0.07936814
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2329974e-05
Norm of the params: 13.934236
              Random: fixed  24 labels. Loss 0.07937. Accuracy 0.985.
### Flips: 156, rs: 31, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728955
Test loss (w/o reg) on all data: 0.012055052
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.17113984e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729053
Test loss (w/o reg) on all data: 0.012055124
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.988631e-07
Norm of the params: 9.153265
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12738158
Train loss (w/o reg) on all data: 0.11852825
Test loss (w/o reg) on all data: 0.07368134
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2005652e-05
Norm of the params: 13.306641
              Random: fixed  28 labels. Loss 0.07368. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17779735
Train loss (w/o reg) on all data: 0.17098725
Test loss (w/o reg) on all data: 0.12194612
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2717455e-05
Norm of the params: 11.670559
Flipped loss: 0.12195. Accuracy: 0.954
### Flips: 156, rs: 32, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08516874
Train loss (w/o reg) on all data: 0.07326018
Test loss (w/o reg) on all data: 0.08639795
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2643132e-05
Norm of the params: 15.432796
     Influence (LOO): fixed  38 labels. Loss 0.08640. Accuracy 0.966.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05041205
Train loss (w/o reg) on all data: 0.03772088
Test loss (w/o reg) on all data: 0.10393562
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0299875e-05
Norm of the params: 15.931837
                Loss: fixed  51 labels. Loss 0.10394. Accuracy 0.969.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17205226
Train loss (w/o reg) on all data: 0.16521552
Test loss (w/o reg) on all data: 0.10992369
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5941945e-05
Norm of the params: 11.693362
              Random: fixed   4 labels. Loss 0.10992. Accuracy 0.969.
### Flips: 156, rs: 32, checks: 104
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038406014
Train loss (w/o reg) on all data: 0.02779346
Test loss (w/o reg) on all data: 0.054655354
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.7626678e-06
Norm of the params: 14.568842
     Influence (LOO): fixed  59 labels. Loss 0.05466. Accuracy 0.981.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010115586
Train loss (w/o reg) on all data: 0.004161204
Test loss (w/o reg) on all data: 0.016151147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2331612e-06
Norm of the params: 10.912728
                Loss: fixed  72 labels. Loss 0.01615. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1674915
Train loss (w/o reg) on all data: 0.1606281
Test loss (w/o reg) on all data: 0.10573099
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4991771e-05
Norm of the params: 11.716146
              Random: fixed   8 labels. Loss 0.10573. Accuracy 0.966.
### Flips: 156, rs: 32, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055562
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5964032e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.012055487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.403827e-07
Norm of the params: 9.15317
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16051017
Train loss (w/o reg) on all data: 0.15359364
Test loss (w/o reg) on all data: 0.10445253
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0681791e-05
Norm of the params: 11.761392
              Random: fixed  12 labels. Loss 0.10445. Accuracy 0.966.
### Flips: 156, rs: 32, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729032
Test loss (w/o reg) on all data: 0.0120566115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8582582e-07
Norm of the params: 9.153268
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729053
Test loss (w/o reg) on all data: 0.012056389
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.325772e-07
Norm of the params: 9.153265
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15810417
Train loss (w/o reg) on all data: 0.15115896
Test loss (w/o reg) on all data: 0.10013193
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.7115155e-06
Norm of the params: 11.785758
              Random: fixed  14 labels. Loss 0.10013. Accuracy 0.969.
### Flips: 156, rs: 32, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728924
Test loss (w/o reg) on all data: 0.012055411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1207177e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728934
Test loss (w/o reg) on all data: 0.01205536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3646877e-07
Norm of the params: 9.153278
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14762002
Train loss (w/o reg) on all data: 0.14058696
Test loss (w/o reg) on all data: 0.087392874
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6717318e-05
Norm of the params: 11.860071
              Random: fixed  20 labels. Loss 0.08739. Accuracy 0.977.
### Flips: 156, rs: 32, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730028
Test loss (w/o reg) on all data: 0.012054834
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7027684e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730019
Test loss (w/o reg) on all data: 0.012054791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00808215e-07
Norm of the params: 9.153158
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1410818
Train loss (w/o reg) on all data: 0.13424869
Test loss (w/o reg) on all data: 0.07719423
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5539513e-05
Norm of the params: 11.690263
              Random: fixed  25 labels. Loss 0.07719. Accuracy 0.985.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18309768
Train loss (w/o reg) on all data: 0.1757437
Test loss (w/o reg) on all data: 0.12233696
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.262476e-05
Norm of the params: 12.127632
Flipped loss: 0.12234. Accuracy: 0.977
### Flips: 156, rs: 33, checks: 52
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09832767
Train loss (w/o reg) on all data: 0.08956025
Test loss (w/o reg) on all data: 0.057833884
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7649903e-05
Norm of the params: 13.241916
     Influence (LOO): fixed  41 labels. Loss 0.05783. Accuracy 0.985.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048605166
Train loss (w/o reg) on all data: 0.035146657
Test loss (w/o reg) on all data: 0.075496495
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.815659e-06
Norm of the params: 16.406406
                Loss: fixed  51 labels. Loss 0.07550. Accuracy 0.981.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17965798
Train loss (w/o reg) on all data: 0.172136
Test loss (w/o reg) on all data: 0.119369596
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1133376e-05
Norm of the params: 12.265388
              Random: fixed   3 labels. Loss 0.11937. Accuracy 0.977.
### Flips: 156, rs: 33, checks: 104
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044400457
Train loss (w/o reg) on all data: 0.036833458
Test loss (w/o reg) on all data: 0.034249645
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.36064e-06
Norm of the params: 12.302032
     Influence (LOO): fixed  67 labels. Loss 0.03425. Accuracy 0.992.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01069845
Train loss (w/o reg) on all data: 0.004447738
Test loss (w/o reg) on all data: 0.022230424
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8975446e-07
Norm of the params: 11.180976
                Loss: fixed  75 labels. Loss 0.02223. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17589894
Train loss (w/o reg) on all data: 0.16842751
Test loss (w/o reg) on all data: 0.11694378
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0169114e-05
Norm of the params: 12.224092
              Random: fixed   5 labels. Loss 0.11694. Accuracy 0.981.
### Flips: 156, rs: 33, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.0120539935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.153501e-07
Norm of the params: 9.153164
     Influence (LOO): fixed  80 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009315196
Train loss (w/o reg) on all data: 0.0038443222
Test loss (w/o reg) on all data: 0.023222972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0013995e-07
Norm of the params: 10.460281
                Loss: fixed  77 labels. Loss 0.02322. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15462303
Train loss (w/o reg) on all data: 0.14692073
Test loss (w/o reg) on all data: 0.104907215
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0612073e-05
Norm of the params: 12.411533
              Random: fixed  14 labels. Loss 0.10491. Accuracy 0.977.
### Flips: 156, rs: 33, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729588
Test loss (w/o reg) on all data: 0.01205535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5530124e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  80 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007589909
Train loss (w/o reg) on all data: 0.0028805137
Test loss (w/o reg) on all data: 0.022446785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.704182e-07
Norm of the params: 9.705046
                Loss: fixed  78 labels. Loss 0.02245. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15180089
Train loss (w/o reg) on all data: 0.1442598
Test loss (w/o reg) on all data: 0.10049521
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1810757e-05
Norm of the params: 12.280951
              Random: fixed  16 labels. Loss 0.10050. Accuracy 0.985.
### Flips: 156, rs: 33, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730028
Test loss (w/o reg) on all data: 0.01205487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4473805e-07
Norm of the params: 9.153159
     Influence (LOO): fixed  80 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065609207
Train loss (w/o reg) on all data: 0.002378139
Test loss (w/o reg) on all data: 0.0127799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6590124e-07
Norm of the params: 9.146346
                Loss: fixed  79 labels. Loss 0.01278. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14490433
Train loss (w/o reg) on all data: 0.13669883
Test loss (w/o reg) on all data: 0.09309087
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.352575e-06
Norm of the params: 12.810542
              Random: fixed  20 labels. Loss 0.09309. Accuracy 0.985.
### Flips: 156, rs: 33, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730384
Test loss (w/o reg) on all data: 0.012055504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9798028e-07
Norm of the params: 9.153121
     Influence (LOO): fixed  80 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065609226
Train loss (w/o reg) on all data: 0.0023782246
Test loss (w/o reg) on all data: 0.012779953
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.653198e-07
Norm of the params: 9.146254
                Loss: fixed  79 labels. Loss 0.01278. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1296846
Train loss (w/o reg) on all data: 0.12101258
Test loss (w/o reg) on all data: 0.080407165
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4048151e-05
Norm of the params: 13.169668
              Random: fixed  29 labels. Loss 0.08041. Accuracy 0.985.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17705019
Train loss (w/o reg) on all data: 0.16941385
Test loss (w/o reg) on all data: 0.09690721
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 0.00015655196
Norm of the params: 12.358268
Flipped loss: 0.09691. Accuracy: 0.985
### Flips: 156, rs: 34, checks: 52
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07280204
Train loss (w/o reg) on all data: 0.062131558
Test loss (w/o reg) on all data: 0.061360445
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9259684e-06
Norm of the params: 14.608542
     Influence (LOO): fixed  42 labels. Loss 0.06136. Accuracy 0.989.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03861886
Train loss (w/o reg) on all data: 0.026391253
Test loss (w/o reg) on all data: 0.04041305
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6205765e-06
Norm of the params: 15.638163
                Loss: fixed  52 labels. Loss 0.04041. Accuracy 0.985.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17012303
Train loss (w/o reg) on all data: 0.1624653
Test loss (w/o reg) on all data: 0.09165806
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.8246977e-05
Norm of the params: 12.375557
              Random: fixed   4 labels. Loss 0.09166. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 104
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028628703
Train loss (w/o reg) on all data: 0.02190451
Test loss (w/o reg) on all data: 0.027954683
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6586688e-06
Norm of the params: 11.596718
     Influence (LOO): fixed  63 labels. Loss 0.02795. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012055012
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8246683e-07
Norm of the params: 9.153189
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16467461
Train loss (w/o reg) on all data: 0.15650786
Test loss (w/o reg) on all data: 0.08676198
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6769622e-05
Norm of the params: 12.780255
              Random: fixed   6 labels. Loss 0.08676. Accuracy 0.989.
### Flips: 156, rs: 34, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.0120555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0699651e-07
Norm of the params: 9.15321
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.012055531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2393744e-07
Norm of the params: 9.15321
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15183283
Train loss (w/o reg) on all data: 0.14364518
Test loss (w/o reg) on all data: 0.07965046
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.747104e-05
Norm of the params: 12.796607
              Random: fixed  12 labels. Loss 0.07965. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172865
Test loss (w/o reg) on all data: 0.01205637
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.109726e-07
Norm of the params: 9.15331
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728869
Test loss (w/o reg) on all data: 0.012056393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.373513e-07
Norm of the params: 9.153284
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14551651
Train loss (w/o reg) on all data: 0.13703983
Test loss (w/o reg) on all data: 0.076451495
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7902995e-05
Norm of the params: 13.020516
              Random: fixed  15 labels. Loss 0.07645. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730785
Test loss (w/o reg) on all data: 0.0120558115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1379759e-07
Norm of the params: 9.153076
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730764
Test loss (w/o reg) on all data: 0.01205592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6515837e-07
Norm of the params: 9.153078
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14386801
Train loss (w/o reg) on all data: 0.13552135
Test loss (w/o reg) on all data: 0.068793036
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.536794e-06
Norm of the params: 12.920263
              Random: fixed  17 labels. Loss 0.06879. Accuracy 0.992.
### Flips: 156, rs: 34, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728796
Test loss (w/o reg) on all data: 0.012057326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.268795e-07
Norm of the params: 9.153292
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021728796
Test loss (w/o reg) on all data: 0.012057131
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3199714e-07
Norm of the params: 9.153292
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13597152
Train loss (w/o reg) on all data: 0.12723595
Test loss (w/o reg) on all data: 0.062234383
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7664508e-05
Norm of the params: 13.217843
              Random: fixed  20 labels. Loss 0.06223. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17755322
Train loss (w/o reg) on all data: 0.16834578
Test loss (w/o reg) on all data: 0.11111444
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0135816e-05
Norm of the params: 13.570145
Flipped loss: 0.11111. Accuracy: 0.985
### Flips: 156, rs: 35, checks: 52
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09572604
Train loss (w/o reg) on all data: 0.0861809
Test loss (w/o reg) on all data: 0.0864647
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7688296e-05
Norm of the params: 13.81676
     Influence (LOO): fixed  41 labels. Loss 0.08646. Accuracy 0.985.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046995793
Train loss (w/o reg) on all data: 0.033041127
Test loss (w/o reg) on all data: 0.06929427
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0004775e-06
Norm of the params: 16.706085
                Loss: fixed  52 labels. Loss 0.06929. Accuracy 0.954.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17242828
Train loss (w/o reg) on all data: 0.16309763
Test loss (w/o reg) on all data: 0.10926424
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.200258e-06
Norm of the params: 13.660634
              Random: fixed   4 labels. Loss 0.10926. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 104
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035432518
Train loss (w/o reg) on all data: 0.027951486
Test loss (w/o reg) on all data: 0.038951457
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3949387e-06
Norm of the params: 12.231953
     Influence (LOO): fixed  65 labels. Loss 0.03895. Accuracy 0.989.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010410203
Train loss (w/o reg) on all data: 0.0043646265
Test loss (w/o reg) on all data: 0.01840774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6032645e-07
Norm of the params: 10.995976
                Loss: fixed  71 labels. Loss 0.01841. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16467023
Train loss (w/o reg) on all data: 0.1557761
Test loss (w/o reg) on all data: 0.09835453
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3560683e-05
Norm of the params: 13.337259
              Random: fixed  10 labels. Loss 0.09835. Accuracy 0.981.
### Flips: 156, rs: 35, checks: 156
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012026919
Train loss (w/o reg) on all data: 0.0068831206
Test loss (w/o reg) on all data: 0.016409516
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8154403e-07
Norm of the params: 10.142778
     Influence (LOO): fixed  74 labels. Loss 0.01641. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008584276
Train loss (w/o reg) on all data: 0.0034193275
Test loss (w/o reg) on all data: 0.015039144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7345472e-07
Norm of the params: 10.16361
                Loss: fixed  74 labels. Loss 0.01504. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15809886
Train loss (w/o reg) on all data: 0.14929657
Test loss (w/o reg) on all data: 0.09920152
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2101696e-05
Norm of the params: 13.268231
              Random: fixed  13 labels. Loss 0.09920. Accuracy 0.981.
### Flips: 156, rs: 35, checks: 208
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002357
Train loss (w/o reg) on all data: 0.0054480284
Test loss (w/o reg) on all data: 0.012243327
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1325448e-06
Norm of the params: 9.543928
     Influence (LOO): fixed  76 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730857
Test loss (w/o reg) on all data: 0.012055935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5246435e-07
Norm of the params: 9.153069
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14909714
Train loss (w/o reg) on all data: 0.14066334
Test loss (w/o reg) on all data: 0.091260724
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9120223e-05
Norm of the params: 12.987542
              Random: fixed  18 labels. Loss 0.09126. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 260
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002352
Train loss (w/o reg) on all data: 0.0054477346
Test loss (w/o reg) on all data: 0.012245707
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7439273e-07
Norm of the params: 9.544232
     Influence (LOO): fixed  76 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172979
Test loss (w/o reg) on all data: 0.0120553775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6319162e-07
Norm of the params: 9.153183
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14588556
Train loss (w/o reg) on all data: 0.13796385
Test loss (w/o reg) on all data: 0.08622937
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.949926e-06
Norm of the params: 12.587066
              Random: fixed  20 labels. Loss 0.08623. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012054402
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.819899e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.01205432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.409776e-07
Norm of the params: 9.153207
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13642155
Train loss (w/o reg) on all data: 0.12847124
Test loss (w/o reg) on all data: 0.08170393
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.896329e-06
Norm of the params: 12.609764
              Random: fixed  25 labels. Loss 0.08170. Accuracy 0.985.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16911602
Train loss (w/o reg) on all data: 0.16007192
Test loss (w/o reg) on all data: 0.1169478
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.8795275e-05
Norm of the params: 13.449232
Flipped loss: 0.11695. Accuracy: 0.977
### Flips: 156, rs: 36, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07940151
Train loss (w/o reg) on all data: 0.06935067
Test loss (w/o reg) on all data: 0.08001881
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3760635e-05
Norm of the params: 14.1780405
     Influence (LOO): fixed  40 labels. Loss 0.08002. Accuracy 0.977.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037653517
Train loss (w/o reg) on all data: 0.025013393
Test loss (w/o reg) on all data: 0.05264178
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.671101e-06
Norm of the params: 15.899764
                Loss: fixed  52 labels. Loss 0.05264. Accuracy 0.985.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16288297
Train loss (w/o reg) on all data: 0.15347111
Test loss (w/o reg) on all data: 0.11251449
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.036289e-06
Norm of the params: 13.719954
              Random: fixed   3 labels. Loss 0.11251. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 104
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01909409
Train loss (w/o reg) on all data: 0.012262278
Test loss (w/o reg) on all data: 0.019695565
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0481298e-06
Norm of the params: 11.689153
     Influence (LOO): fixed  65 labels. Loss 0.01970. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011227809
Train loss (w/o reg) on all data: 0.0045670825
Test loss (w/o reg) on all data: 0.0143108815
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4542334e-07
Norm of the params: 11.541861
                Loss: fixed  68 labels. Loss 0.01431. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15500136
Train loss (w/o reg) on all data: 0.14536466
Test loss (w/o reg) on all data: 0.111489855
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.2795335e-05
Norm of the params: 13.882867
              Random: fixed   7 labels. Loss 0.11149. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.012056446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1975102e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012056521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.805098e-07
Norm of the params: 9.153192
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15003145
Train loss (w/o reg) on all data: 0.14073958
Test loss (w/o reg) on all data: 0.10072793
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.713611e-06
Norm of the params: 13.632215
              Random: fixed  11 labels. Loss 0.10073. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012055669
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3907778e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730177
Test loss (w/o reg) on all data: 0.0120555805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9952954e-07
Norm of the params: 9.153144
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14082752
Train loss (w/o reg) on all data: 0.13096379
Test loss (w/o reg) on all data: 0.09973165
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.7715856e-05
Norm of the params: 14.045449
              Random: fixed  15 labels. Loss 0.09973. Accuracy 0.973.
### Flips: 156, rs: 36, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729686
Test loss (w/o reg) on all data: 0.012055462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7011575e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012055499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3228698e-07
Norm of the params: 9.153197
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13962552
Train loss (w/o reg) on all data: 0.13005693
Test loss (w/o reg) on all data: 0.09731989
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3461322e-05
Norm of the params: 13.833717
              Random: fixed  17 labels. Loss 0.09732. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012054796
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0608328e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729653
Test loss (w/o reg) on all data: 0.0120547805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.034222e-08
Norm of the params: 9.1532
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13455312
Train loss (w/o reg) on all data: 0.124772154
Test loss (w/o reg) on all data: 0.09310141
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3782928e-05
Norm of the params: 13.986402
              Random: fixed  20 labels. Loss 0.09310. Accuracy 0.981.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17251804
Train loss (w/o reg) on all data: 0.16254933
Test loss (w/o reg) on all data: 0.09010108
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.859403e-06
Norm of the params: 14.119992
Flipped loss: 0.09010. Accuracy: 0.989
### Flips: 156, rs: 37, checks: 52
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07614469
Train loss (w/o reg) on all data: 0.06597253
Test loss (w/o reg) on all data: 0.055154037
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.669375e-06
Norm of the params: 14.2633505
     Influence (LOO): fixed  45 labels. Loss 0.05515. Accuracy 0.985.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049924485
Train loss (w/o reg) on all data: 0.035427038
Test loss (w/o reg) on all data: 0.038097065
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0961137e-06
Norm of the params: 17.027887
                Loss: fixed  51 labels. Loss 0.03810. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16633812
Train loss (w/o reg) on all data: 0.15587114
Test loss (w/o reg) on all data: 0.09471019
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4052624e-05
Norm of the params: 14.468574
              Random: fixed   4 labels. Loss 0.09471. Accuracy 0.985.
### Flips: 156, rs: 37, checks: 104
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032952465
Train loss (w/o reg) on all data: 0.023536427
Test loss (w/o reg) on all data: 0.0293434
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1779111e-06
Norm of the params: 13.723002
     Influence (LOO): fixed  64 labels. Loss 0.02934. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007512573
Train loss (w/o reg) on all data: 0.0028914427
Test loss (w/o reg) on all data: 0.0064194608
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6964764e-07
Norm of the params: 9.6136675
                Loss: fixed  75 labels. Loss 0.00642. Accuracy 0.996.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16374832
Train loss (w/o reg) on all data: 0.15313716
Test loss (w/o reg) on all data: 0.09173829
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.754138e-05
Norm of the params: 14.567884
              Random: fixed   5 labels. Loss 0.09174. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026764877
Train loss (w/o reg) on all data: 0.018330328
Test loss (w/o reg) on all data: 0.02474716
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7199043e-06
Norm of the params: 12.988111
     Influence (LOO): fixed  68 labels. Loss 0.02475. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012056776
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5022184e-07
Norm of the params: 9.153176
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16007583
Train loss (w/o reg) on all data: 0.1492377
Test loss (w/o reg) on all data: 0.09182257
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7333079e-05
Norm of the params: 14.722856
              Random: fixed   7 labels. Loss 0.09182. Accuracy 0.981.
### Flips: 156, rs: 37, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021727725
Test loss (w/o reg) on all data: 0.012055221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0091945e-07
Norm of the params: 9.15341
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172778
Test loss (w/o reg) on all data: 0.012055334
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5510603e-07
Norm of the params: 9.153406
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15467118
Train loss (w/o reg) on all data: 0.14420223
Test loss (w/o reg) on all data: 0.088845484
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.222591e-06
Norm of the params: 14.469931
              Random: fixed  11 labels. Loss 0.08885. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.012055473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.626664e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730002
Test loss (w/o reg) on all data: 0.012055339
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.091845e-07
Norm of the params: 9.15316
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14908154
Train loss (w/o reg) on all data: 0.13856341
Test loss (w/o reg) on all data: 0.08356971
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0102194e-05
Norm of the params: 14.503881
              Random: fixed  14 labels. Loss 0.08357. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728994
Test loss (w/o reg) on all data: 0.012055297
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.862655e-07
Norm of the params: 9.15327
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172902
Test loss (w/o reg) on all data: 0.012055172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.25165e-07
Norm of the params: 9.153269
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13980083
Train loss (w/o reg) on all data: 0.12967393
Test loss (w/o reg) on all data: 0.08234526
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.408624e-05
Norm of the params: 14.231586
              Random: fixed  19 labels. Loss 0.08235. Accuracy 0.977.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19463812
Train loss (w/o reg) on all data: 0.18921182
Test loss (w/o reg) on all data: 0.11126578
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8209236e-05
Norm of the params: 10.417589
Flipped loss: 0.11127. Accuracy: 0.981
### Flips: 156, rs: 38, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104697935
Train loss (w/o reg) on all data: 0.09513043
Test loss (w/o reg) on all data: 0.06430211
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.561166e-05
Norm of the params: 13.832936
     Influence (LOO): fixed  42 labels. Loss 0.06430. Accuracy 0.985.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06157613
Train loss (w/o reg) on all data: 0.05050219
Test loss (w/o reg) on all data: 0.05749944
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5654227e-06
Norm of the params: 14.882168
                Loss: fixed  51 labels. Loss 0.05750. Accuracy 0.977.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18184258
Train loss (w/o reg) on all data: 0.17643018
Test loss (w/o reg) on all data: 0.103544995
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9239204e-05
Norm of the params: 10.404224
              Random: fixed   6 labels. Loss 0.10354. Accuracy 0.981.
### Flips: 156, rs: 38, checks: 104
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044674095
Train loss (w/o reg) on all data: 0.036788598
Test loss (w/o reg) on all data: 0.032968458
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6495507e-06
Norm of the params: 12.558263
     Influence (LOO): fixed  67 labels. Loss 0.03297. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008442434
Train loss (w/o reg) on all data: 0.0031759415
Test loss (w/o reg) on all data: 0.016537275
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.250717e-07
Norm of the params: 10.263033
                Loss: fixed  80 labels. Loss 0.01654. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17793953
Train loss (w/o reg) on all data: 0.17234173
Test loss (w/o reg) on all data: 0.09980478
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0635142e-05
Norm of the params: 10.580931
              Random: fixed   8 labels. Loss 0.09980. Accuracy 0.977.
### Flips: 156, rs: 38, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011089494
Train loss (w/o reg) on all data: 0.005329049
Test loss (w/o reg) on all data: 0.015012563
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3361615e-07
Norm of the params: 10.7335415
     Influence (LOO): fixed  80 labels. Loss 0.01501. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073551745
Train loss (w/o reg) on all data: 0.0026893327
Test loss (w/o reg) on all data: 0.015942952
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.4790005e-07
Norm of the params: 9.660065
                Loss: fixed  81 labels. Loss 0.01594. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16845988
Train loss (w/o reg) on all data: 0.16267446
Test loss (w/o reg) on all data: 0.09561639
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.121104e-06
Norm of the params: 10.756782
              Random: fixed  13 labels. Loss 0.09562. Accuracy 0.985.
### Flips: 156, rs: 38, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728817
Test loss (w/o reg) on all data: 0.012055832
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.198246e-07
Norm of the params: 9.153291
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833745
Train loss (w/o reg) on all data: 0.0026668936
Test loss (w/o reg) on all data: 0.021343987
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.743836e-07
Norm of the params: 9.712344
                Loss: fixed  82 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1615393
Train loss (w/o reg) on all data: 0.15570861
Test loss (w/o reg) on all data: 0.091161594
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6153339e-05
Norm of the params: 10.798781
              Random: fixed  18 labels. Loss 0.09116. Accuracy 0.985.
### Flips: 156, rs: 38, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012055512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3236811e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.012055539
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0947124e-07
Norm of the params: 9.153164
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15831102
Train loss (w/o reg) on all data: 0.15259734
Test loss (w/o reg) on all data: 0.08748851
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9883513e-05
Norm of the params: 10.689891
              Random: fixed  21 labels. Loss 0.08749. Accuracy 0.977.
### Flips: 156, rs: 38, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730317
Test loss (w/o reg) on all data: 0.012055683
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6314182e-07
Norm of the params: 9.153128
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730307
Test loss (w/o reg) on all data: 0.012055776
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2769431e-07
Norm of the params: 9.153129
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14145091
Train loss (w/o reg) on all data: 0.13556446
Test loss (w/o reg) on all data: 0.08016145
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4527508e-05
Norm of the params: 10.850298
              Random: fixed  29 labels. Loss 0.08016. Accuracy 0.973.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17149805
Train loss (w/o reg) on all data: 0.1632262
Test loss (w/o reg) on all data: 0.13365802
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3599547e-05
Norm of the params: 12.862228
Flipped loss: 0.13366. Accuracy: 0.977
### Flips: 156, rs: 39, checks: 52
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07214415
Train loss (w/o reg) on all data: 0.061578892
Test loss (w/o reg) on all data: 0.07610402
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0374184e-06
Norm of the params: 14.536339
     Influence (LOO): fixed  40 labels. Loss 0.07610. Accuracy 0.973.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045018476
Train loss (w/o reg) on all data: 0.032621782
Test loss (w/o reg) on all data: 0.06578368
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8681561e-05
Norm of the params: 15.745916
                Loss: fixed  49 labels. Loss 0.06578. Accuracy 0.977.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16490445
Train loss (w/o reg) on all data: 0.15641522
Test loss (w/o reg) on all data: 0.12847556
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1854945e-05
Norm of the params: 13.03014
              Random: fixed   3 labels. Loss 0.12848. Accuracy 0.969.
### Flips: 156, rs: 39, checks: 104
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033858284
Train loss (w/o reg) on all data: 0.026377713
Test loss (w/o reg) on all data: 0.027803974
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.584036e-06
Norm of the params: 12.231577
     Influence (LOO): fixed  63 labels. Loss 0.02780. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012166401
Train loss (w/o reg) on all data: 0.005344553
Test loss (w/o reg) on all data: 0.018399406
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1870384e-06
Norm of the params: 11.680623
                Loss: fixed  71 labels. Loss 0.01840. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15844902
Train loss (w/o reg) on all data: 0.15001996
Test loss (w/o reg) on all data: 0.119809225
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0465659e-05
Norm of the params: 12.983888
              Random: fixed   7 labels. Loss 0.11981. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 156
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0122273
Train loss (w/o reg) on all data: 0.0065062786
Test loss (w/o reg) on all data: 0.015421926
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.277656e-06
Norm of the params: 10.696749
     Influence (LOO): fixed  72 labels. Loss 0.01542. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664981
Train loss (w/o reg) on all data: 0.003318148
Test loss (w/o reg) on all data: 0.013526499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9312952e-07
Norm of the params: 10.341019
                Loss: fixed  73 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14895645
Train loss (w/o reg) on all data: 0.14037575
Test loss (w/o reg) on all data: 0.11388394
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.144985e-06
Norm of the params: 13.100156
              Random: fixed  12 labels. Loss 0.11388. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.012054893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.14393785e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012054836
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.570535e-07
Norm of the params: 9.153238
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14283769
Train loss (w/o reg) on all data: 0.13424979
Test loss (w/o reg) on all data: 0.10522693
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.827598e-06
Norm of the params: 13.105648
              Random: fixed  16 labels. Loss 0.10523. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055754
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9781727e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730035
Test loss (w/o reg) on all data: 0.012055843
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1423355e-07
Norm of the params: 9.153158
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14283769
Train loss (w/o reg) on all data: 0.13425027
Test loss (w/o reg) on all data: 0.10521448
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3572041e-05
Norm of the params: 13.105286
              Random: fixed  16 labels. Loss 0.10521. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172934
Test loss (w/o reg) on all data: 0.0120557975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1524007e-06
Norm of the params: 9.153234
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172934
Test loss (w/o reg) on all data: 0.012055582
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.729756e-07
Norm of the params: 9.153234
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13410345
Train loss (w/o reg) on all data: 0.12554273
Test loss (w/o reg) on all data: 0.10275581
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5852372e-05
Norm of the params: 13.084895
              Random: fixed  19 labels. Loss 0.10276. Accuracy 0.985.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21264195
Train loss (w/o reg) on all data: 0.20326519
Test loss (w/o reg) on all data: 0.14672069
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.561732e-05
Norm of the params: 13.694358
Flipped loss: 0.14672. Accuracy: 0.950
### Flips: 208, rs: 0, checks: 52
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14319222
Train loss (w/o reg) on all data: 0.13157167
Test loss (w/o reg) on all data: 0.09310761
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.3487646e-05
Norm of the params: 15.245033
     Influence (LOO): fixed  36 labels. Loss 0.09311. Accuracy 0.977.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09768614
Train loss (w/o reg) on all data: 0.08029457
Test loss (w/o reg) on all data: 0.09836475
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.327208e-05
Norm of the params: 18.650238
                Loss: fixed  49 labels. Loss 0.09836. Accuracy 0.966.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21176375
Train loss (w/o reg) on all data: 0.20235324
Test loss (w/o reg) on all data: 0.14433153
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3620874e-05
Norm of the params: 13.718975
              Random: fixed   1 labels. Loss 0.14433. Accuracy 0.954.
### Flips: 208, rs: 0, checks: 104
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09149819
Train loss (w/o reg) on all data: 0.08134477
Test loss (w/o reg) on all data: 0.05164442
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4390106e-05
Norm of the params: 14.250207
     Influence (LOO): fixed  68 labels. Loss 0.05164. Accuracy 0.989.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033458017
Train loss (w/o reg) on all data: 0.022161327
Test loss (w/o reg) on all data: 0.011893526
Train acc on all data:  0.9933142311365807
Test acc on all data:   1.0
Norm of the mean of gradients: 3.667042e-06
Norm of the params: 15.031095
                Loss: fixed  89 labels. Loss 0.01189. Accuracy 1.000.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2112311
Train loss (w/o reg) on all data: 0.20187232
Test loss (w/o reg) on all data: 0.14225906
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.7803976e-05
Norm of the params: 13.68121
              Random: fixed   2 labels. Loss 0.14226. Accuracy 0.954.
### Flips: 208, rs: 0, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039370235
Train loss (w/o reg) on all data: 0.030736247
Test loss (w/o reg) on all data: 0.020443268
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7152213e-06
Norm of the params: 13.140768
     Influence (LOO): fixed  93 labels. Loss 0.02044. Accuracy 0.996.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016636666
Train loss (w/o reg) on all data: 0.007777805
Test loss (w/o reg) on all data: 0.012173904
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7188638e-06
Norm of the params: 13.310793
                Loss: fixed  99 labels. Loss 0.01217. Accuracy 0.996.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20479383
Train loss (w/o reg) on all data: 0.19587688
Test loss (w/o reg) on all data: 0.12886533
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.530846e-05
Norm of the params: 13.354355
              Random: fixed   8 labels. Loss 0.12887. Accuracy 0.962.
### Flips: 208, rs: 0, checks: 208
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016058
Train loss (w/o reg) on all data: 0.009827499
Test loss (w/o reg) on all data: 0.020569863
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.9129786e-07
Norm of the params: 11.162885
     Influence (LOO): fixed 104 labels. Loss 0.02057. Accuracy 0.989.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008249323
Train loss (w/o reg) on all data: 0.0031785795
Test loss (w/o reg) on all data: 0.011614645
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7665072e-07
Norm of the params: 10.070495
                Loss: fixed 105 labels. Loss 0.01161. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2019448
Train loss (w/o reg) on all data: 0.19310252
Test loss (w/o reg) on all data: 0.12374321
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.5853652e-05
Norm of the params: 13.298328
              Random: fixed  14 labels. Loss 0.12374. Accuracy 0.969.
### Flips: 208, rs: 0, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447788
Test loss (w/o reg) on all data: 0.012245384
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9269307e-07
Norm of the params: 9.544178
     Influence (LOO): fixed 106 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007556345
Train loss (w/o reg) on all data: 0.0027568005
Test loss (w/o reg) on all data: 0.010964703
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7149917e-07
Norm of the params: 9.797494
                Loss: fixed 106 labels. Loss 0.01096. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19546819
Train loss (w/o reg) on all data: 0.18630745
Test loss (w/o reg) on all data: 0.11496155
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.1308274e-05
Norm of the params: 13.535688
              Random: fixed  18 labels. Loss 0.11496. Accuracy 0.973.
### Flips: 208, rs: 0, checks: 312
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002352
Train loss (w/o reg) on all data: 0.0054477006
Test loss (w/o reg) on all data: 0.012244252
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7831635e-07
Norm of the params: 9.544268
     Influence (LOO): fixed 106 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729947
Test loss (w/o reg) on all data: 0.012055301
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.398477e-07
Norm of the params: 9.153168
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18903464
Train loss (w/o reg) on all data: 0.18038388
Test loss (w/o reg) on all data: 0.10973376
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.28394695e-05
Norm of the params: 13.153525
              Random: fixed  24 labels. Loss 0.10973. Accuracy 0.973.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20785025
Train loss (w/o reg) on all data: 0.1993768
Test loss (w/o reg) on all data: 0.14082377
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2173748e-05
Norm of the params: 13.018023
Flipped loss: 0.14082. Accuracy: 0.977
### Flips: 208, rs: 1, checks: 52
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14258868
Train loss (w/o reg) on all data: 0.13138111
Test loss (w/o reg) on all data: 0.08986566
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.40111e-06
Norm of the params: 14.971683
     Influence (LOO): fixed  36 labels. Loss 0.08987. Accuracy 0.981.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08795329
Train loss (w/o reg) on all data: 0.071252465
Test loss (w/o reg) on all data: 0.088217735
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9555875e-05
Norm of the params: 18.27612
                Loss: fixed  52 labels. Loss 0.08822. Accuracy 0.977.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2036515
Train loss (w/o reg) on all data: 0.19507225
Test loss (w/o reg) on all data: 0.14198346
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7553171e-05
Norm of the params: 13.099049
              Random: fixed   2 labels. Loss 0.14198. Accuracy 0.977.
### Flips: 208, rs: 1, checks: 104
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082002535
Train loss (w/o reg) on all data: 0.07099611
Test loss (w/o reg) on all data: 0.04795611
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.025354e-06
Norm of the params: 14.836727
     Influence (LOO): fixed  65 labels. Loss 0.04796. Accuracy 0.992.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017485205
Train loss (w/o reg) on all data: 0.008543629
Test loss (w/o reg) on all data: 0.027256463
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.122606e-07
Norm of the params: 13.37279
                Loss: fixed  87 labels. Loss 0.02726. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19764437
Train loss (w/o reg) on all data: 0.18876433
Test loss (w/o reg) on all data: 0.13976587
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.85243e-06
Norm of the params: 13.326685
              Random: fixed   5 labels. Loss 0.13977. Accuracy 0.969.
### Flips: 208, rs: 1, checks: 156
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019217435
Train loss (w/o reg) on all data: 0.012848268
Test loss (w/o reg) on all data: 0.018715575
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2276606e-06
Norm of the params: 11.286424
     Influence (LOO): fixed  92 labels. Loss 0.01872. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010791395
Train loss (w/o reg) on all data: 0.0045765433
Test loss (w/o reg) on all data: 0.014612129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.453784e-07
Norm of the params: 11.148857
                Loss: fixed  93 labels. Loss 0.01461. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18663323
Train loss (w/o reg) on all data: 0.17725505
Test loss (w/o reg) on all data: 0.12450697
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1874373e-05
Norm of the params: 13.69539
              Random: fixed  12 labels. Loss 0.12451. Accuracy 0.981.
### Flips: 208, rs: 1, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.0120549565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.406992e-08
Norm of the params: 9.153197
     Influence (LOO): fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054947
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5186983e-08
Norm of the params: 9.153197
                Loss: fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18044949
Train loss (w/o reg) on all data: 0.17173095
Test loss (w/o reg) on all data: 0.11369834
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3316263e-05
Norm of the params: 13.204955
              Random: fixed  19 labels. Loss 0.11370. Accuracy 0.981.
### Flips: 208, rs: 1, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012055112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6168721e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054907
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3447923e-07
Norm of the params: 9.153183
                Loss: fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17225271
Train loss (w/o reg) on all data: 0.1630378
Test loss (w/o reg) on all data: 0.10914004
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.845029e-06
Norm of the params: 13.575645
              Random: fixed  24 labels. Loss 0.10914. Accuracy 0.985.
### Flips: 208, rs: 1, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.012055224
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3176197e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012055279
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5935555e-07
Norm of the params: 9.153207
                Loss: fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16315441
Train loss (w/o reg) on all data: 0.15404683
Test loss (w/o reg) on all data: 0.102597825
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.616259e-05
Norm of the params: 13.496351
              Random: fixed  30 labels. Loss 0.10260. Accuracy 0.981.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21802008
Train loss (w/o reg) on all data: 0.21010755
Test loss (w/o reg) on all data: 0.13258983
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.154047e-05
Norm of the params: 12.579765
Flipped loss: 0.13259. Accuracy: 0.985
### Flips: 208, rs: 2, checks: 52
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12704262
Train loss (w/o reg) on all data: 0.1165953
Test loss (w/o reg) on all data: 0.08609063
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3362012e-05
Norm of the params: 14.454978
     Influence (LOO): fixed  41 labels. Loss 0.08609. Accuracy 0.977.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081421986
Train loss (w/o reg) on all data: 0.06591591
Test loss (w/o reg) on all data: 0.08251314
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8039467e-05
Norm of the params: 17.610268
                Loss: fixed  52 labels. Loss 0.08251. Accuracy 0.966.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20327783
Train loss (w/o reg) on all data: 0.19530885
Test loss (w/o reg) on all data: 0.116599955
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.474868e-05
Norm of the params: 12.624556
              Random: fixed   9 labels. Loss 0.11660. Accuracy 0.985.
### Flips: 208, rs: 2, checks: 104
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07923558
Train loss (w/o reg) on all data: 0.06971617
Test loss (w/o reg) on all data: 0.05051037
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1224414e-05
Norm of the params: 13.798124
     Influence (LOO): fixed  66 labels. Loss 0.05051. Accuracy 0.989.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018464357
Train loss (w/o reg) on all data: 0.009364388
Test loss (w/o reg) on all data: 0.028073736
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2628511e-06
Norm of the params: 13.490714
                Loss: fixed  90 labels. Loss 0.02807. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2000566
Train loss (w/o reg) on all data: 0.19194753
Test loss (w/o reg) on all data: 0.113838576
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.697984e-05
Norm of the params: 12.735043
              Random: fixed  11 labels. Loss 0.11384. Accuracy 0.981.
### Flips: 208, rs: 2, checks: 156
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024758874
Train loss (w/o reg) on all data: 0.015386139
Test loss (w/o reg) on all data: 0.026230277
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8348685e-06
Norm of the params: 13.691409
     Influence (LOO): fixed  86 labels. Loss 0.02623. Accuracy 0.989.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011919517
Train loss (w/o reg) on all data: 0.0049142004
Test loss (w/o reg) on all data: 0.019333694
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.236416e-07
Norm of the params: 11.836652
                Loss: fixed  94 labels. Loss 0.01933. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19740252
Train loss (w/o reg) on all data: 0.18922251
Test loss (w/o reg) on all data: 0.113492504
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8373546e-05
Norm of the params: 12.790623
              Random: fixed  12 labels. Loss 0.11349. Accuracy 0.981.
### Flips: 208, rs: 2, checks: 208
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012758767
Train loss (w/o reg) on all data: 0.0074357074
Test loss (w/o reg) on all data: 0.013297845
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9379745e-07
Norm of the params: 10.318004
     Influence (LOO): fixed  96 labels. Loss 0.01330. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010259982
Train loss (w/o reg) on all data: 0.0041822633
Test loss (w/o reg) on all data: 0.017418953
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0154461e-07
Norm of the params: 11.02517
                Loss: fixed  95 labels. Loss 0.01742. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19082357
Train loss (w/o reg) on all data: 0.18286602
Test loss (w/o reg) on all data: 0.10577298
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3310849e-05
Norm of the params: 12.615504
              Random: fixed  17 labels. Loss 0.10577. Accuracy 0.985.
### Flips: 208, rs: 2, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012056318
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8158274e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012056282
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.735128e-07
Norm of the params: 9.153246
                Loss: fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17582317
Train loss (w/o reg) on all data: 0.16714191
Test loss (w/o reg) on all data: 0.095991075
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7227343e-06
Norm of the params: 13.176685
              Random: fixed  24 labels. Loss 0.09599. Accuracy 0.989.
### Flips: 208, rs: 2, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012055672
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2735116e-07
Norm of the params: 9.153157
     Influence (LOO): fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730035
Test loss (w/o reg) on all data: 0.01205561
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.17858356e-07
Norm of the params: 9.153158
                Loss: fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16791916
Train loss (w/o reg) on all data: 0.15890072
Test loss (w/o reg) on all data: 0.09675551
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.378687e-05
Norm of the params: 13.430142
              Random: fixed  28 labels. Loss 0.09676. Accuracy 0.989.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23106289
Train loss (w/o reg) on all data: 0.22487995
Test loss (w/o reg) on all data: 0.1475386
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6654776e-05
Norm of the params: 11.120194
Flipped loss: 0.14754. Accuracy: 0.969
### Flips: 208, rs: 3, checks: 52
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1565394
Train loss (w/o reg) on all data: 0.147441
Test loss (w/o reg) on all data: 0.112019435
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.077297e-06
Norm of the params: 13.489547
     Influence (LOO): fixed  39 labels. Loss 0.11202. Accuracy 0.973.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11220103
Train loss (w/o reg) on all data: 0.10019933
Test loss (w/o reg) on all data: 0.09793265
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4824497e-05
Norm of the params: 15.4930315
                Loss: fixed  52 labels. Loss 0.09793. Accuracy 0.962.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21926372
Train loss (w/o reg) on all data: 0.21246482
Test loss (w/o reg) on all data: 0.13832603
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2715063e-05
Norm of the params: 11.66096
              Random: fixed   6 labels. Loss 0.13833. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 104
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10590199
Train loss (w/o reg) on all data: 0.09707718
Test loss (w/o reg) on all data: 0.08377071
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.40131015e-05
Norm of the params: 13.285181
     Influence (LOO): fixed  65 labels. Loss 0.08377. Accuracy 0.985.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019273482
Train loss (w/o reg) on all data: 0.010064571
Test loss (w/o reg) on all data: 0.015376162
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0585646e-06
Norm of the params: 13.571227
                Loss: fixed  98 labels. Loss 0.01538. Accuracy 0.996.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2118346
Train loss (w/o reg) on all data: 0.20476358
Test loss (w/o reg) on all data: 0.12676208
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2274106e-05
Norm of the params: 11.892026
              Random: fixed  13 labels. Loss 0.12676. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 156
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055807073
Train loss (w/o reg) on all data: 0.048023466
Test loss (w/o reg) on all data: 0.049890544
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4880866e-05
Norm of the params: 12.476866
     Influence (LOO): fixed  89 labels. Loss 0.04989. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729448
Test loss (w/o reg) on all data: 0.012054553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6787375e-07
Norm of the params: 9.153223
                Loss: fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20675814
Train loss (w/o reg) on all data: 0.1995189
Test loss (w/o reg) on all data: 0.12477553
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9824245e-05
Norm of the params: 12.032654
              Random: fixed  16 labels. Loss 0.12478. Accuracy 0.977.
### Flips: 208, rs: 3, checks: 208
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022821676
Train loss (w/o reg) on all data: 0.016790535
Test loss (w/o reg) on all data: 0.025176197
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2161711e-06
Norm of the params: 10.982842
     Influence (LOO): fixed 102 labels. Loss 0.02518. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055557
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6056154e-07
Norm of the params: 9.153168
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20211701
Train loss (w/o reg) on all data: 0.19475156
Test loss (w/o reg) on all data: 0.117136225
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8075205e-05
Norm of the params: 12.137088
              Random: fixed  20 labels. Loss 0.11714. Accuracy 0.985.
### Flips: 208, rs: 3, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.012054734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3359676e-07
Norm of the params: 9.153209
     Influence (LOO): fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.0120548615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.09334515e-07
Norm of the params: 9.153209
                Loss: fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18956816
Train loss (w/o reg) on all data: 0.18226926
Test loss (w/o reg) on all data: 0.115216464
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.166819e-06
Norm of the params: 12.082131
              Random: fixed  28 labels. Loss 0.11522. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729723
Test loss (w/o reg) on all data: 0.012055288
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8693602e-07
Norm of the params: 9.153194
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055256
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7685504e-08
Norm of the params: 9.153194
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18176399
Train loss (w/o reg) on all data: 0.17428102
Test loss (w/o reg) on all data: 0.10899338
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5723594e-05
Norm of the params: 12.23354
              Random: fixed  33 labels. Loss 0.10899. Accuracy 0.985.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21750441
Train loss (w/o reg) on all data: 0.21019337
Test loss (w/o reg) on all data: 0.16009499
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.677384e-05
Norm of the params: 12.092185
Flipped loss: 0.16009. Accuracy: 0.966
### Flips: 208, rs: 4, checks: 52
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14196733
Train loss (w/o reg) on all data: 0.13223776
Test loss (w/o reg) on all data: 0.11143029
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2443293e-05
Norm of the params: 13.9496
     Influence (LOO): fixed  41 labels. Loss 0.11143. Accuracy 0.958.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09736991
Train loss (w/o reg) on all data: 0.08121016
Test loss (w/o reg) on all data: 0.11316042
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.5043423e-06
Norm of the params: 17.977625
                Loss: fixed  52 labels. Loss 0.11316. Accuracy 0.958.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2063641
Train loss (w/o reg) on all data: 0.19918618
Test loss (w/o reg) on all data: 0.1412443
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.1718856e-05
Norm of the params: 11.981579
              Random: fixed   9 labels. Loss 0.14124. Accuracy 0.973.
### Flips: 208, rs: 4, checks: 104
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08280415
Train loss (w/o reg) on all data: 0.07199738
Test loss (w/o reg) on all data: 0.0761847
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2351602e-05
Norm of the params: 14.701545
     Influence (LOO): fixed  70 labels. Loss 0.07618. Accuracy 0.973.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01813592
Train loss (w/o reg) on all data: 0.0092430515
Test loss (w/o reg) on all data: 0.0236561
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5800572e-06
Norm of the params: 13.336316
                Loss: fixed  95 labels. Loss 0.02366. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19270901
Train loss (w/o reg) on all data: 0.18521158
Test loss (w/o reg) on all data: 0.13090737
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.8733444e-05
Norm of the params: 12.245346
              Random: fixed  17 labels. Loss 0.13091. Accuracy 0.981.
### Flips: 208, rs: 4, checks: 156
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04902115
Train loss (w/o reg) on all data: 0.038759794
Test loss (w/o reg) on all data: 0.057041246
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6059602e-06
Norm of the params: 14.325751
     Influence (LOO): fixed  86 labels. Loss 0.05704. Accuracy 0.981.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0114739705
Train loss (w/o reg) on all data: 0.0048043896
Test loss (w/o reg) on all data: 0.019487346
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2117026e-06
Norm of the params: 11.549529
                Loss: fixed  99 labels. Loss 0.01949. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18642795
Train loss (w/o reg) on all data: 0.17871651
Test loss (w/o reg) on all data: 0.116796315
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.20467685e-05
Norm of the params: 12.418891
              Random: fixed  23 labels. Loss 0.11680. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 208
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023862772
Train loss (w/o reg) on all data: 0.016790349
Test loss (w/o reg) on all data: 0.023921747
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5987376e-06
Norm of the params: 11.893211
     Influence (LOO): fixed  96 labels. Loss 0.02392. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028170778
Test loss (w/o reg) on all data: 0.012755284
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8193724e-07
Norm of the params: 9.683567
                Loss: fixed 102 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18620501
Train loss (w/o reg) on all data: 0.1785879
Test loss (w/o reg) on all data: 0.11565807
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2066648e-05
Norm of the params: 12.342702
              Random: fixed  24 labels. Loss 0.11566. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 260
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016447362
Train loss (w/o reg) on all data: 0.009915536
Test loss (w/o reg) on all data: 0.014804601
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7221172e-06
Norm of the params: 11.429633
     Influence (LOO): fixed  99 labels. Loss 0.01480. Accuracy 0.996.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729534
Test loss (w/o reg) on all data: 0.012054961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9128115e-08
Norm of the params: 9.153214
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17871225
Train loss (w/o reg) on all data: 0.17148463
Test loss (w/o reg) on all data: 0.10876356
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.914092e-06
Norm of the params: 12.022993
              Random: fixed  30 labels. Loss 0.10876. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730466
Test loss (w/o reg) on all data: 0.012054651
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7074395e-07
Norm of the params: 9.15311
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730454
Test loss (w/o reg) on all data: 0.012054709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4341165e-07
Norm of the params: 9.1531105
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1699796
Train loss (w/o reg) on all data: 0.16258688
Test loss (w/o reg) on all data: 0.10457531
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1617965e-05
Norm of the params: 12.159538
              Random: fixed  35 labels. Loss 0.10458. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20152637
Train loss (w/o reg) on all data: 0.19485296
Test loss (w/o reg) on all data: 0.1090069
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 0.00017250754
Norm of the params: 11.552843
Flipped loss: 0.10901. Accuracy: 0.985
### Flips: 208, rs: 5, checks: 52
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1304557
Train loss (w/o reg) on all data: 0.12172275
Test loss (w/o reg) on all data: 0.07545963
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.9572757e-06
Norm of the params: 13.2158575
     Influence (LOO): fixed  38 labels. Loss 0.07546. Accuracy 0.985.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07516462
Train loss (w/o reg) on all data: 0.059712276
Test loss (w/o reg) on all data: 0.049939778
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4065318e-05
Norm of the params: 17.579733
                Loss: fixed  51 labels. Loss 0.04994. Accuracy 0.985.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19517793
Train loss (w/o reg) on all data: 0.18835531
Test loss (w/o reg) on all data: 0.10333601
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6190439e-05
Norm of the params: 11.681281
              Random: fixed   7 labels. Loss 0.10334. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 104
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06377405
Train loss (w/o reg) on all data: 0.05268188
Test loss (w/o reg) on all data: 0.045693062
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.7320729e-06
Norm of the params: 14.894408
     Influence (LOO): fixed  68 labels. Loss 0.04569. Accuracy 0.981.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022224516
Train loss (w/o reg) on all data: 0.011921049
Test loss (w/o reg) on all data: 0.013923411
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0512864e-06
Norm of the params: 14.355116
                Loss: fixed  84 labels. Loss 0.01392. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864197
Train loss (w/o reg) on all data: 0.18144836
Test loss (w/o reg) on all data: 0.096517146
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9408606e-05
Norm of the params: 11.994669
              Random: fixed  12 labels. Loss 0.09652. Accuracy 0.989.
### Flips: 208, rs: 5, checks: 156
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027780196
Train loss (w/o reg) on all data: 0.019402666
Test loss (w/o reg) on all data: 0.016655624
Train acc on all data:  0.994269340974212
Test acc on all data:   1.0
Norm of the mean of gradients: 2.627834e-06
Norm of the params: 12.944135
     Influence (LOO): fixed  85 labels. Loss 0.01666. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0098352665
Train loss (w/o reg) on all data: 0.0038955251
Test loss (w/o reg) on all data: 0.0053872303
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.994111e-07
Norm of the params: 10.899304
                Loss: fixed  91 labels. Loss 0.00539. Accuracy 1.000.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17061175
Train loss (w/o reg) on all data: 0.16240463
Test loss (w/o reg) on all data: 0.0857402
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0489969e-05
Norm of the params: 12.811817
              Random: fixed  21 labels. Loss 0.08574. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 208
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016116392
Train loss (w/o reg) on all data: 0.010058265
Test loss (w/o reg) on all data: 0.0074711163
Train acc on all data:  0.997134670487106
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0883314e-07
Norm of the params: 11.007384
     Influence (LOO): fixed  90 labels. Loss 0.00747. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318257
Train loss (w/o reg) on all data: 0.0021985862
Test loss (w/o reg) on all data: 0.0064341626
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.414289e-07
Norm of the params: 9.077082
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16401662
Train loss (w/o reg) on all data: 0.15547357
Test loss (w/o reg) on all data: 0.081771195
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.442475e-05
Norm of the params: 13.071379
              Random: fixed  27 labels. Loss 0.08177. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 260
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002356
Train loss (w/o reg) on all data: 0.0054477383
Test loss (w/o reg) on all data: 0.012244755
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7802358e-07
Norm of the params: 9.544231
     Influence (LOO): fixed  93 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318259
Train loss (w/o reg) on all data: 0.0021986624
Test loss (w/o reg) on all data: 0.0064333337
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6473824e-07
Norm of the params: 9.077001
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15803751
Train loss (w/o reg) on all data: 0.14954071
Test loss (w/o reg) on all data: 0.074523106
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8409596e-06
Norm of the params: 13.035955
              Random: fixed  30 labels. Loss 0.07452. Accuracy 0.989.
### Flips: 208, rs: 5, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728552
Test loss (w/o reg) on all data: 0.0120548
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2335135e-07
Norm of the params: 9.153321
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318257
Train loss (w/o reg) on all data: 0.0021987257
Test loss (w/o reg) on all data: 0.0064328206
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6883433e-07
Norm of the params: 9.076929
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1462365
Train loss (w/o reg) on all data: 0.13736042
Test loss (w/o reg) on all data: 0.068639494
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0812843e-05
Norm of the params: 13.323721
              Random: fixed  36 labels. Loss 0.06864. Accuracy 0.996.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20796883
Train loss (w/o reg) on all data: 0.20052893
Test loss (w/o reg) on all data: 0.14618301
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8336992e-05
Norm of the params: 12.19828
Flipped loss: 0.14618. Accuracy: 0.962
### Flips: 208, rs: 6, checks: 52
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1365146
Train loss (w/o reg) on all data: 0.12859768
Test loss (w/o reg) on all data: 0.10348122
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3414212e-05
Norm of the params: 12.583265
     Influence (LOO): fixed  37 labels. Loss 0.10348. Accuracy 0.962.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08016886
Train loss (w/o reg) on all data: 0.06538975
Test loss (w/o reg) on all data: 0.08441337
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7193104e-05
Norm of the params: 17.192501
                Loss: fixed  52 labels. Loss 0.08441. Accuracy 0.966.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20103404
Train loss (w/o reg) on all data: 0.19372566
Test loss (w/o reg) on all data: 0.13370287
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0650232e-05
Norm of the params: 12.089983
              Random: fixed   5 labels. Loss 0.13370. Accuracy 0.966.
### Flips: 208, rs: 6, checks: 104
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08059359
Train loss (w/o reg) on all data: 0.07183963
Test loss (w/o reg) on all data: 0.068205014
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.322306e-06
Norm of the params: 13.231752
     Influence (LOO): fixed  63 labels. Loss 0.06821. Accuracy 0.977.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022595692
Train loss (w/o reg) on all data: 0.013505688
Test loss (w/o reg) on all data: 0.024849016
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.9295046e-07
Norm of the params: 13.483327
                Loss: fixed  88 labels. Loss 0.02485. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19451527
Train loss (w/o reg) on all data: 0.1872176
Test loss (w/o reg) on all data: 0.121317565
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4618084e-05
Norm of the params: 12.081122
              Random: fixed  12 labels. Loss 0.12132. Accuracy 0.966.
### Flips: 208, rs: 6, checks: 156
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03928107
Train loss (w/o reg) on all data: 0.031073643
Test loss (w/o reg) on all data: 0.04803747
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6036757e-06
Norm of the params: 12.812048
     Influence (LOO): fixed  85 labels. Loss 0.04804. Accuracy 0.985.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505652
Train loss (w/o reg) on all data: 0.0028170142
Test loss (w/o reg) on all data: 0.01275549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9062666e-07
Norm of the params: 9.683634
                Loss: fixed  97 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18566431
Train loss (w/o reg) on all data: 0.17822996
Test loss (w/o reg) on all data: 0.11367617
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.5282908e-05
Norm of the params: 12.193734
              Random: fixed  20 labels. Loss 0.11368. Accuracy 0.962.
### Flips: 208, rs: 6, checks: 208
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009377584
Train loss (w/o reg) on all data: 0.0039480035
Test loss (w/o reg) on all data: 0.016018067
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8326285e-06
Norm of the params: 10.42073
     Influence (LOO): fixed  96 labels. Loss 0.01602. Accuracy 0.996.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505652
Train loss (w/o reg) on all data: 0.0028170338
Test loss (w/o reg) on all data: 0.012755299
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0270323e-07
Norm of the params: 9.683613
                Loss: fixed  97 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17947966
Train loss (w/o reg) on all data: 0.1722206
Test loss (w/o reg) on all data: 0.10588785
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2073099e-05
Norm of the params: 12.049111
              Random: fixed  25 labels. Loss 0.10589. Accuracy 0.973.
### Flips: 208, rs: 6, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729695
Test loss (w/o reg) on all data: 0.012054955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4948312e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012054906
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7809107e-07
Norm of the params: 9.153195
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1644148
Train loss (w/o reg) on all data: 0.15627396
Test loss (w/o reg) on all data: 0.0949335
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3116586e-05
Norm of the params: 12.759969
              Random: fixed  32 labels. Loss 0.09493. Accuracy 0.973.
### Flips: 208, rs: 6, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.012054989
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0686447e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012055083
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1270868e-07
Norm of the params: 9.153204
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16059539
Train loss (w/o reg) on all data: 0.15243065
Test loss (w/o reg) on all data: 0.09233026
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0564169e-05
Norm of the params: 12.778683
              Random: fixed  34 labels. Loss 0.09233. Accuracy 0.977.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20607728
Train loss (w/o reg) on all data: 0.19951186
Test loss (w/o reg) on all data: 0.14003891
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.330033e-05
Norm of the params: 11.458986
Flipped loss: 0.14004. Accuracy: 0.969
### Flips: 208, rs: 7, checks: 52
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1359346
Train loss (w/o reg) on all data: 0.12624
Test loss (w/o reg) on all data: 0.11747874
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0255913e-05
Norm of the params: 13.924511
     Influence (LOO): fixed  35 labels. Loss 0.11748. Accuracy 0.969.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08896049
Train loss (w/o reg) on all data: 0.076808184
Test loss (w/o reg) on all data: 0.10242165
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.824709e-05
Norm of the params: 15.589936
                Loss: fixed  51 labels. Loss 0.10242. Accuracy 0.950.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2060773
Train loss (w/o reg) on all data: 0.19950943
Test loss (w/o reg) on all data: 0.14002524
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.710893e-05
Norm of the params: 11.461121
              Random: fixed   0 labels. Loss 0.14003. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 104
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07338814
Train loss (w/o reg) on all data: 0.06392099
Test loss (w/o reg) on all data: 0.07643837
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7389988e-05
Norm of the params: 13.760194
     Influence (LOO): fixed  66 labels. Loss 0.07644. Accuracy 0.973.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014932147
Train loss (w/o reg) on all data: 0.0076830937
Test loss (w/o reg) on all data: 0.011182327
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.022667e-06
Norm of the params: 12.040809
                Loss: fixed  87 labels. Loss 0.01118. Accuracy 0.996.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20112358
Train loss (w/o reg) on all data: 0.19423969
Test loss (w/o reg) on all data: 0.14159466
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3983748e-05
Norm of the params: 11.733618
              Random: fixed   3 labels. Loss 0.14159. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 156
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029488219
Train loss (w/o reg) on all data: 0.022752704
Test loss (w/o reg) on all data: 0.034439586
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.210783e-07
Norm of the params: 11.606477
     Influence (LOO): fixed  86 labels. Loss 0.03444. Accuracy 0.985.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010381507
Train loss (w/o reg) on all data: 0.004414428
Test loss (w/o reg) on all data: 0.011721
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7745176e-07
Norm of the params: 10.924358
                Loss: fixed  91 labels. Loss 0.01172. Accuracy 0.996.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18977848
Train loss (w/o reg) on all data: 0.18267809
Test loss (w/o reg) on all data: 0.13371767
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3477798e-05
Norm of the params: 11.916708
              Random: fixed   9 labels. Loss 0.13372. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 208
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014145981
Train loss (w/o reg) on all data: 0.009372396
Test loss (w/o reg) on all data: 0.014373819
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.0163227e-07
Norm of the params: 9.770962
     Influence (LOO): fixed  92 labels. Loss 0.01437. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730007
Test loss (w/o reg) on all data: 0.012055267
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.05664355e-07
Norm of the params: 9.153161
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18125072
Train loss (w/o reg) on all data: 0.17377894
Test loss (w/o reg) on all data: 0.1292464
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0434076e-05
Norm of the params: 12.224395
              Random: fixed  14 labels. Loss 0.12925. Accuracy 0.981.
### Flips: 208, rs: 7, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002353
Train loss (w/o reg) on all data: 0.0054477532
Test loss (w/o reg) on all data: 0.012246171
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9478314e-07
Norm of the params: 9.544213
     Influence (LOO): fixed  93 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730785
Test loss (w/o reg) on all data: 0.012055487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2548166e-07
Norm of the params: 9.153074
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17181015
Train loss (w/o reg) on all data: 0.16437398
Test loss (w/o reg) on all data: 0.10976652
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.705735e-05
Norm of the params: 12.195221
              Random: fixed  20 labels. Loss 0.10977. Accuracy 0.981.
### Flips: 208, rs: 7, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.01205516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9049897e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729327
Test loss (w/o reg) on all data: 0.012055211
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7288708e-07
Norm of the params: 9.153236
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1643818
Train loss (w/o reg) on all data: 0.15685405
Test loss (w/o reg) on all data: 0.10611113
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.738786e-06
Norm of the params: 12.270091
              Random: fixed  25 labels. Loss 0.10611. Accuracy 0.977.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2324906
Train loss (w/o reg) on all data: 0.22617167
Test loss (w/o reg) on all data: 0.14088544
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9618043e-05
Norm of the params: 11.241824
Flipped loss: 0.14089. Accuracy: 0.977
### Flips: 208, rs: 8, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15558457
Train loss (w/o reg) on all data: 0.14646256
Test loss (w/o reg) on all data: 0.1092689
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.0235594e-05
Norm of the params: 13.507045
     Influence (LOO): fixed  38 labels. Loss 0.10927. Accuracy 0.969.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105339035
Train loss (w/o reg) on all data: 0.09404508
Test loss (w/o reg) on all data: 0.0802302
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2549776e-05
Norm of the params: 15.029277
                Loss: fixed  52 labels. Loss 0.08023. Accuracy 0.958.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23105241
Train loss (w/o reg) on all data: 0.22468653
Test loss (w/o reg) on all data: 0.1410727
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3127409e-05
Norm of the params: 11.2835045
              Random: fixed   1 labels. Loss 0.14107. Accuracy 0.977.
### Flips: 208, rs: 8, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103137195
Train loss (w/o reg) on all data: 0.09403383
Test loss (w/o reg) on all data: 0.06639293
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3513622e-05
Norm of the params: 13.493234
     Influence (LOO): fixed  67 labels. Loss 0.06639. Accuracy 0.977.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021082368
Train loss (w/o reg) on all data: 0.011198761
Test loss (w/o reg) on all data: 0.017453263
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0767176e-06
Norm of the params: 14.059591
                Loss: fixed  97 labels. Loss 0.01745. Accuracy 0.996.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22168396
Train loss (w/o reg) on all data: 0.21534248
Test loss (w/o reg) on all data: 0.13076623
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.765367e-05
Norm of the params: 11.261878
              Random: fixed   8 labels. Loss 0.13077. Accuracy 0.985.
### Flips: 208, rs: 8, checks: 156
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050755285
Train loss (w/o reg) on all data: 0.04267141
Test loss (w/o reg) on all data: 0.032117907
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6824953e-06
Norm of the params: 12.715249
     Influence (LOO): fixed  91 labels. Loss 0.03212. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007944423
Train loss (w/o reg) on all data: 0.0030273253
Test loss (w/o reg) on all data: 0.015965423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5777006e-07
Norm of the params: 9.916751
                Loss: fixed 105 labels. Loss 0.01597. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2154397
Train loss (w/o reg) on all data: 0.20919794
Test loss (w/o reg) on all data: 0.12161319
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4712224e-05
Norm of the params: 11.172973
              Random: fixed  13 labels. Loss 0.12161. Accuracy 0.981.
### Flips: 208, rs: 8, checks: 208
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017574236
Train loss (w/o reg) on all data: 0.011582738
Test loss (w/o reg) on all data: 0.014864108
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.234934e-06
Norm of the params: 10.946687
     Influence (LOO): fixed 104 labels. Loss 0.01486. Accuracy 0.989.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012055154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9759008e-07
Norm of the params: 9.153182
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21090059
Train loss (w/o reg) on all data: 0.2048161
Test loss (w/o reg) on all data: 0.11556285
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.42237e-05
Norm of the params: 11.031304
              Random: fixed  17 labels. Loss 0.11556. Accuracy 0.989.
### Flips: 208, rs: 8, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.0055395053
Test loss (w/o reg) on all data: 0.013845285
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6004058e-06
Norm of the params: 9.950004
     Influence (LOO): fixed 106 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012054714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.05546846e-07
Norm of the params: 9.153213
                Loss: fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20357934
Train loss (w/o reg) on all data: 0.19752164
Test loss (w/o reg) on all data: 0.11161585
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4858849e-05
Norm of the params: 11.007
              Random: fixed  22 labels. Loss 0.11162. Accuracy 0.989.
### Flips: 208, rs: 8, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729346
Test loss (w/o reg) on all data: 0.012055515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.645504e-08
Norm of the params: 9.153234
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729358
Test loss (w/o reg) on all data: 0.01205554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6157765e-07
Norm of the params: 9.153233
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19352409
Train loss (w/o reg) on all data: 0.18703036
Test loss (w/o reg) on all data: 0.09815585
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3307434e-05
Norm of the params: 11.396249
              Random: fixed  29 labels. Loss 0.09816. Accuracy 0.996.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20729148
Train loss (w/o reg) on all data: 0.19963694
Test loss (w/o reg) on all data: 0.14251041
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1683604e-05
Norm of the params: 12.372988
Flipped loss: 0.14251. Accuracy: 0.966
### Flips: 208, rs: 9, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12900813
Train loss (w/o reg) on all data: 0.11927878
Test loss (w/o reg) on all data: 0.08865376
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.438719e-06
Norm of the params: 13.949441
     Influence (LOO): fixed  41 labels. Loss 0.08865. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091541894
Train loss (w/o reg) on all data: 0.077370286
Test loss (w/o reg) on all data: 0.0832169
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6236805e-05
Norm of the params: 16.835445
                Loss: fixed  52 labels. Loss 0.08322. Accuracy 0.962.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20081452
Train loss (w/o reg) on all data: 0.19337301
Test loss (w/o reg) on all data: 0.12749839
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.9805545e-05
Norm of the params: 12.19959
              Random: fixed   7 labels. Loss 0.12750. Accuracy 0.981.
### Flips: 208, rs: 9, checks: 104
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07309364
Train loss (w/o reg) on all data: 0.06426051
Test loss (w/o reg) on all data: 0.059227064
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6684873e-05
Norm of the params: 13.291446
     Influence (LOO): fixed  69 labels. Loss 0.05923. Accuracy 0.981.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01740964
Train loss (w/o reg) on all data: 0.00933164
Test loss (w/o reg) on all data: 0.023372872
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.0112736e-07
Norm of the params: 12.710626
                Loss: fixed  91 labels. Loss 0.02337. Accuracy 0.996.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19534469
Train loss (w/o reg) on all data: 0.18788865
Test loss (w/o reg) on all data: 0.12208859
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3975791e-05
Norm of the params: 12.211496
              Random: fixed  10 labels. Loss 0.12209. Accuracy 0.985.
### Flips: 208, rs: 9, checks: 156
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039475903
Train loss (w/o reg) on all data: 0.031646736
Test loss (w/o reg) on all data: 0.029928392
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9565315e-06
Norm of the params: 12.513326
     Influence (LOO): fixed  87 labels. Loss 0.02993. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015975099
Train loss (w/o reg) on all data: 0.008228977
Test loss (w/o reg) on all data: 0.023155287
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7976683e-06
Norm of the params: 12.446783
                Loss: fixed  92 labels. Loss 0.02316. Accuracy 0.996.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19100684
Train loss (w/o reg) on all data: 0.1835322
Test loss (w/o reg) on all data: 0.11586801
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4374602e-05
Norm of the params: 12.2267275
              Random: fixed  15 labels. Loss 0.11587. Accuracy 0.985.
### Flips: 208, rs: 9, checks: 208
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020687604
Train loss (w/o reg) on all data: 0.014970006
Test loss (w/o reg) on all data: 0.018746465
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.894121e-07
Norm of the params: 10.693548
     Influence (LOO): fixed  93 labels. Loss 0.01875. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008612612
Train loss (w/o reg) on all data: 0.0033304773
Test loss (w/o reg) on all data: 0.012232451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.712592e-07
Norm of the params: 10.278263
                Loss: fixed  95 labels. Loss 0.01223. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18544623
Train loss (w/o reg) on all data: 0.17750406
Test loss (w/o reg) on all data: 0.11079407
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.6757344e-05
Norm of the params: 12.60331
              Random: fixed  18 labels. Loss 0.11079. Accuracy 0.989.
### Flips: 208, rs: 9, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011973482
Train loss (w/o reg) on all data: 0.0064862906
Test loss (w/o reg) on all data: 0.01335479
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.825541e-07
Norm of the params: 10.475868
     Influence (LOO): fixed  96 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077838157
Train loss (w/o reg) on all data: 0.0029201936
Test loss (w/o reg) on all data: 0.010634301
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0180813e-06
Norm of the params: 9.8626795
                Loss: fixed  97 labels. Loss 0.01063. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16890116
Train loss (w/o reg) on all data: 0.16070847
Test loss (w/o reg) on all data: 0.09929748
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4684113e-05
Norm of the params: 12.800535
              Random: fixed  27 labels. Loss 0.09930. Accuracy 0.989.
### Flips: 208, rs: 9, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0119734835
Train loss (w/o reg) on all data: 0.0064863
Test loss (w/o reg) on all data: 0.013355477
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4037989e-06
Norm of the params: 10.475861
     Influence (LOO): fixed  96 labels. Loss 0.01336. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783817
Train loss (w/o reg) on all data: 0.0029201605
Test loss (w/o reg) on all data: 0.010634004
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9468729e-07
Norm of the params: 9.862714
                Loss: fixed  97 labels. Loss 0.01063. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16313738
Train loss (w/o reg) on all data: 0.15477352
Test loss (w/o reg) on all data: 0.09618185
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1935848e-05
Norm of the params: 12.933562
              Random: fixed  30 labels. Loss 0.09618. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19715832
Train loss (w/o reg) on all data: 0.18899772
Test loss (w/o reg) on all data: 0.12693848
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.274227e-05
Norm of the params: 12.775452
Flipped loss: 0.12694. Accuracy: 0.969
### Flips: 208, rs: 10, checks: 52
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652091
Train loss (w/o reg) on all data: 0.10642575
Test loss (w/o reg) on all data: 0.07562908
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7141505e-05
Norm of the params: 14.2092705
     Influence (LOO): fixed  38 labels. Loss 0.07563. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07235539
Train loss (w/o reg) on all data: 0.058206413
Test loss (w/o reg) on all data: 0.08445471
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.9096016e-06
Norm of the params: 16.821999
                Loss: fixed  52 labels. Loss 0.08445. Accuracy 0.973.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1923964
Train loss (w/o reg) on all data: 0.1840657
Test loss (w/o reg) on all data: 0.12497818
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3389783e-05
Norm of the params: 12.907908
              Random: fixed   2 labels. Loss 0.12498. Accuracy 0.969.
### Flips: 208, rs: 10, checks: 104
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050132804
Train loss (w/o reg) on all data: 0.03985291
Test loss (w/o reg) on all data: 0.027918695
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4886152e-05
Norm of the params: 14.338684
     Influence (LOO): fixed  68 labels. Loss 0.02792. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0091266055
Train loss (w/o reg) on all data: 0.0038527295
Test loss (w/o reg) on all data: 0.025169332
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6361783e-07
Norm of the params: 10.270225
                Loss: fixed  83 labels. Loss 0.02517. Accuracy 0.992.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19153725
Train loss (w/o reg) on all data: 0.18336967
Test loss (w/o reg) on all data: 0.12129154
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.2206422e-05
Norm of the params: 12.780912
              Random: fixed   3 labels. Loss 0.12129. Accuracy 0.973.
### Flips: 208, rs: 10, checks: 156
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03074795
Train loss (w/o reg) on all data: 0.02388974
Test loss (w/o reg) on all data: 0.018685369
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0055131e-06
Norm of the params: 11.711713
     Influence (LOO): fixed  77 labels. Loss 0.01869. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847016
Train loss (w/o reg) on all data: 0.0030394716
Test loss (w/o reg) on all data: 0.015349741
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5894328e-07
Norm of the params: 9.805656
                Loss: fixed  85 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18857479
Train loss (w/o reg) on all data: 0.18027666
Test loss (w/o reg) on all data: 0.117209725
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0404143e-05
Norm of the params: 12.882641
              Random: fixed   5 labels. Loss 0.11721. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 208
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014804026
Train loss (w/o reg) on all data: 0.009532749
Test loss (w/o reg) on all data: 0.013199444
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6957294e-07
Norm of the params: 10.267694
     Influence (LOO): fixed  84 labels. Loss 0.01320. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847014
Train loss (w/o reg) on all data: 0.0030394702
Test loss (w/o reg) on all data: 0.0153487325
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6144846e-07
Norm of the params: 9.8056555
                Loss: fixed  85 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18654697
Train loss (w/o reg) on all data: 0.17821428
Test loss (w/o reg) on all data: 0.115152486
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.104945e-06
Norm of the params: 12.909441
              Random: fixed   7 labels. Loss 0.11515. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173017
Test loss (w/o reg) on all data: 0.012054526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7055614e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730168
Test loss (w/o reg) on all data: 0.0120547125
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1665616e-07
Norm of the params: 9.153145
                Loss: fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17998047
Train loss (w/o reg) on all data: 0.17209353
Test loss (w/o reg) on all data: 0.100844525
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.028508e-05
Norm of the params: 12.559421
              Random: fixed  12 labels. Loss 0.10084. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012055159
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.437406e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  86 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729532
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0734074e-07
Norm of the params: 9.153211
                Loss: fixed  86 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1712449
Train loss (w/o reg) on all data: 0.16396749
Test loss (w/o reg) on all data: 0.09126654
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5811322e-05
Norm of the params: 12.064341
              Random: fixed  19 labels. Loss 0.09127. Accuracy 0.989.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22331132
Train loss (w/o reg) on all data: 0.21603402
Test loss (w/o reg) on all data: 0.14629091
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3266221e-05
Norm of the params: 12.064238
Flipped loss: 0.14629. Accuracy: 0.966
### Flips: 208, rs: 11, checks: 52
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1544445
Train loss (w/o reg) on all data: 0.14524105
Test loss (w/o reg) on all data: 0.0997843
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.7172405e-06
Norm of the params: 13.567199
     Influence (LOO): fixed  36 labels. Loss 0.09978. Accuracy 0.981.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10338107
Train loss (w/o reg) on all data: 0.089303344
Test loss (w/o reg) on all data: 0.10257449
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.549874e-06
Norm of the params: 16.779587
                Loss: fixed  51 labels. Loss 0.10257. Accuracy 0.958.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21822463
Train loss (w/o reg) on all data: 0.21091034
Test loss (w/o reg) on all data: 0.13999042
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.603266e-05
Norm of the params: 12.094865
              Random: fixed   6 labels. Loss 0.13999. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 104
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08828679
Train loss (w/o reg) on all data: 0.07801643
Test loss (w/o reg) on all data: 0.07014576
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.629232e-06
Norm of the params: 14.33203
     Influence (LOO): fixed  67 labels. Loss 0.07015. Accuracy 0.981.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022283038
Train loss (w/o reg) on all data: 0.012661269
Test loss (w/o reg) on all data: 0.018221106
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.4921256e-07
Norm of the params: 13.872108
                Loss: fixed  95 labels. Loss 0.01822. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21034124
Train loss (w/o reg) on all data: 0.20322783
Test loss (w/o reg) on all data: 0.13302515
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0578582e-05
Norm of the params: 11.927626
              Random: fixed  13 labels. Loss 0.13303. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 156
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04828108
Train loss (w/o reg) on all data: 0.039129384
Test loss (w/o reg) on all data: 0.040339988
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2313783e-05
Norm of the params: 13.529005
     Influence (LOO): fixed  87 labels. Loss 0.04034. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009738266
Train loss (w/o reg) on all data: 0.0038561306
Test loss (w/o reg) on all data: 0.011846608
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.498613e-07
Norm of the params: 10.846322
                Loss: fixed 103 labels. Loss 0.01185. Accuracy 0.996.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1939342
Train loss (w/o reg) on all data: 0.18659833
Test loss (w/o reg) on all data: 0.12735686
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2429954e-05
Norm of the params: 12.112698
              Random: fixed  22 labels. Loss 0.12736. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 208
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017536119
Train loss (w/o reg) on all data: 0.011365317
Test loss (w/o reg) on all data: 0.015291014
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.481993e-06
Norm of the params: 11.109277
     Influence (LOO): fixed 102 labels. Loss 0.01529. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007988211
Train loss (w/o reg) on all data: 0.0029990652
Test loss (w/o reg) on all data: 0.01252964
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.1029124e-07
Norm of the params: 9.98914
                Loss: fixed 104 labels. Loss 0.01253. Accuracy 0.996.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18723707
Train loss (w/o reg) on all data: 0.17986146
Test loss (w/o reg) on all data: 0.12291256
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8504148e-05
Norm of the params: 12.145469
              Random: fixed  26 labels. Loss 0.12291. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 260
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009858081
Train loss (w/o reg) on all data: 0.0049288077
Test loss (w/o reg) on all data: 0.013105678
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.475933e-07
Norm of the params: 9.929021
     Influence (LOO): fixed 104 labels. Loss 0.01311. Accuracy 0.996.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007988211
Train loss (w/o reg) on all data: 0.0029991665
Test loss (w/o reg) on all data: 0.012529461
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6541175e-07
Norm of the params: 9.989038
                Loss: fixed 104 labels. Loss 0.01253. Accuracy 0.996.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1745514
Train loss (w/o reg) on all data: 0.16642204
Test loss (w/o reg) on all data: 0.121478245
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2876016e-05
Norm of the params: 12.750963
              Random: fixed  31 labels. Loss 0.12148. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012055306
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5884586e-07
Norm of the params: 9.153213
     Influence (LOO): fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012055004
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.12082e-07
Norm of the params: 9.153207
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16805796
Train loss (w/o reg) on all data: 0.15976216
Test loss (w/o reg) on all data: 0.115623884
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.879606e-06
Norm of the params: 12.880837
              Random: fixed  35 labels. Loss 0.11562. Accuracy 0.977.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20838554
Train loss (w/o reg) on all data: 0.20032836
Test loss (w/o reg) on all data: 0.12729187
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8909095e-05
Norm of the params: 12.694233
Flipped loss: 0.12729. Accuracy: 0.981
### Flips: 208, rs: 12, checks: 52
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13345394
Train loss (w/o reg) on all data: 0.12296142
Test loss (w/o reg) on all data: 0.08891866
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.9566e-06
Norm of the params: 14.4862175
     Influence (LOO): fixed  38 labels. Loss 0.08892. Accuracy 0.977.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071298234
Train loss (w/o reg) on all data: 0.05501553
Test loss (w/o reg) on all data: 0.072026044
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.1002145e-06
Norm of the params: 18.045889
                Loss: fixed  52 labels. Loss 0.07203. Accuracy 0.966.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20317763
Train loss (w/o reg) on all data: 0.19506241
Test loss (w/o reg) on all data: 0.12482793
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7728049e-05
Norm of the params: 12.739878
              Random: fixed   3 labels. Loss 0.12483. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 104
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06969539
Train loss (w/o reg) on all data: 0.059328094
Test loss (w/o reg) on all data: 0.05059884
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.6864445e-06
Norm of the params: 14.399511
     Influence (LOO): fixed  68 labels. Loss 0.05060. Accuracy 0.989.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016090792
Train loss (w/o reg) on all data: 0.008579482
Test loss (w/o reg) on all data: 0.016799
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3871486e-06
Norm of the params: 12.256679
                Loss: fixed  89 labels. Loss 0.01680. Accuracy 0.996.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1979845
Train loss (w/o reg) on all data: 0.18998818
Test loss (w/o reg) on all data: 0.12100009
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1440617e-05
Norm of the params: 12.646202
              Random: fixed   7 labels. Loss 0.12100. Accuracy 0.973.
### Flips: 208, rs: 12, checks: 156
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022049952
Train loss (w/o reg) on all data: 0.013171406
Test loss (w/o reg) on all data: 0.027397752
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5490479e-06
Norm of the params: 13.325575
     Influence (LOO): fixed  87 labels. Loss 0.02740. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505651
Train loss (w/o reg) on all data: 0.00281705
Test loss (w/o reg) on all data: 0.01275537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4806868e-07
Norm of the params: 9.683596
                Loss: fixed  93 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18802343
Train loss (w/o reg) on all data: 0.17989512
Test loss (w/o reg) on all data: 0.11486616
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.8432092e-05
Norm of the params: 12.750149
              Random: fixed  13 labels. Loss 0.11487. Accuracy 0.969.
### Flips: 208, rs: 12, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2012195e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505651
Train loss (w/o reg) on all data: 0.0028170114
Test loss (w/o reg) on all data: 0.012755573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.162558e-07
Norm of the params: 9.683636
                Loss: fixed  93 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17979193
Train loss (w/o reg) on all data: 0.1713925
Test loss (w/o reg) on all data: 0.10933435
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4762488e-05
Norm of the params: 12.961036
              Random: fixed  17 labels. Loss 0.10933. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012055589
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1297205e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729714
Test loss (w/o reg) on all data: 0.012055727
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.097739e-07
Norm of the params: 9.1531925
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1741755
Train loss (w/o reg) on all data: 0.16529459
Test loss (w/o reg) on all data: 0.10524189
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.703472e-06
Norm of the params: 13.3273535
              Random: fixed  19 labels. Loss 0.10524. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730706
Test loss (w/o reg) on all data: 0.012055012
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.417022e-07
Norm of the params: 9.153086
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730615
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0264784e-07
Norm of the params: 9.153096
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16802871
Train loss (w/o reg) on all data: 0.15933384
Test loss (w/o reg) on all data: 0.10560844
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.0210546e-06
Norm of the params: 13.187022
              Random: fixed  23 labels. Loss 0.10561. Accuracy 0.977.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21599153
Train loss (w/o reg) on all data: 0.20829985
Test loss (w/o reg) on all data: 0.14618903
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8851644e-05
Norm of the params: 12.402971
Flipped loss: 0.14619. Accuracy: 0.966
### Flips: 208, rs: 13, checks: 52
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14173442
Train loss (w/o reg) on all data: 0.13213815
Test loss (w/o reg) on all data: 0.08917191
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.085375e-05
Norm of the params: 13.853715
     Influence (LOO): fixed  39 labels. Loss 0.08917. Accuracy 0.981.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09854216
Train loss (w/o reg) on all data: 0.0851275
Test loss (w/o reg) on all data: 0.09517575
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.5815823e-06
Norm of the params: 16.379658
                Loss: fixed  52 labels. Loss 0.09518. Accuracy 0.962.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21024925
Train loss (w/o reg) on all data: 0.20213225
Test loss (w/o reg) on all data: 0.14443702
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1480104e-05
Norm of the params: 12.741266
              Random: fixed   4 labels. Loss 0.14444. Accuracy 0.966.
### Flips: 208, rs: 13, checks: 104
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08915346
Train loss (w/o reg) on all data: 0.078117184
Test loss (w/o reg) on all data: 0.07075668
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7837498e-05
Norm of the params: 14.856835
     Influence (LOO): fixed  63 labels. Loss 0.07076. Accuracy 0.977.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027126383
Train loss (w/o reg) on all data: 0.016898567
Test loss (w/o reg) on all data: 0.048153695
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1289021e-06
Norm of the params: 14.302319
                Loss: fixed  91 labels. Loss 0.04815. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20545015
Train loss (w/o reg) on all data: 0.19723535
Test loss (w/o reg) on all data: 0.14197424
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1793451e-05
Norm of the params: 12.817798
              Random: fixed   8 labels. Loss 0.14197. Accuracy 0.973.
### Flips: 208, rs: 13, checks: 156
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045361646
Train loss (w/o reg) on all data: 0.03604305
Test loss (w/o reg) on all data: 0.031831175
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.250873e-06
Norm of the params: 13.651809
     Influence (LOO): fixed  87 labels. Loss 0.03183. Accuracy 0.989.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008787829
Train loss (w/o reg) on all data: 0.0035152072
Test loss (w/o reg) on all data: 0.011547371
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.3532105e-07
Norm of the params: 10.269004
                Loss: fixed 101 labels. Loss 0.01155. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20206295
Train loss (w/o reg) on all data: 0.19369397
Test loss (w/o reg) on all data: 0.13986096
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.637554e-05
Norm of the params: 12.937531
              Random: fixed  10 labels. Loss 0.13986. Accuracy 0.966.
### Flips: 208, rs: 13, checks: 208
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028128933
Train loss (w/o reg) on all data: 0.020738779
Test loss (w/o reg) on all data: 0.026556453
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.263209e-07
Norm of the params: 12.157431
     Influence (LOO): fixed  96 labels. Loss 0.02656. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056506
Train loss (w/o reg) on all data: 0.0028170098
Test loss (w/o reg) on all data: 0.012754898
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1721873e-07
Norm of the params: 9.683637
                Loss: fixed 102 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19774543
Train loss (w/o reg) on all data: 0.1895008
Test loss (w/o reg) on all data: 0.12950285
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1363656e-05
Norm of the params: 12.841052
              Random: fixed  14 labels. Loss 0.12950. Accuracy 0.977.
### Flips: 208, rs: 13, checks: 260
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016503323
Train loss (w/o reg) on all data: 0.010187873
Test loss (w/o reg) on all data: 0.018522149
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.335671e-07
Norm of the params: 11.238729
     Influence (LOO): fixed 100 labels. Loss 0.01852. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028169479
Test loss (w/o reg) on all data: 0.012754791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6656698e-07
Norm of the params: 9.6837015
                Loss: fixed 102 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1951604
Train loss (w/o reg) on all data: 0.18695211
Test loss (w/o reg) on all data: 0.12207691
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1417096e-05
Norm of the params: 12.812721
              Random: fixed  18 labels. Loss 0.12208. Accuracy 0.989.
### Flips: 208, rs: 13, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730063
Test loss (w/o reg) on all data: 0.012054595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.831567e-07
Norm of the params: 9.153153
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173008
Test loss (w/o reg) on all data: 0.0120547395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6880116e-07
Norm of the params: 9.153154
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19187173
Train loss (w/o reg) on all data: 0.18372631
Test loss (w/o reg) on all data: 0.11831999
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0527288e-05
Norm of the params: 12.763563
              Random: fixed  20 labels. Loss 0.11832. Accuracy 0.989.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22364995
Train loss (w/o reg) on all data: 0.21595095
Test loss (w/o reg) on all data: 0.14443612
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.3112843e-05
Norm of the params: 12.408866
Flipped loss: 0.14444. Accuracy: 0.977
### Flips: 208, rs: 14, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15233488
Train loss (w/o reg) on all data: 0.14321929
Test loss (w/o reg) on all data: 0.10684404
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2373507e-05
Norm of the params: 13.502286
     Influence (LOO): fixed  41 labels. Loss 0.10684. Accuracy 0.981.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10253761
Train loss (w/o reg) on all data: 0.09191678
Test loss (w/o reg) on all data: 0.10374983
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.649893e-06
Norm of the params: 14.574523
                Loss: fixed  51 labels. Loss 0.10375. Accuracy 0.954.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2117422
Train loss (w/o reg) on all data: 0.20398372
Test loss (w/o reg) on all data: 0.13639295
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.668378e-05
Norm of the params: 12.45671
              Random: fixed  10 labels. Loss 0.13639. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 104
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09045424
Train loss (w/o reg) on all data: 0.08146774
Test loss (w/o reg) on all data: 0.069743715
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.310831e-06
Norm of the params: 13.406343
     Influence (LOO): fixed  70 labels. Loss 0.06974. Accuracy 0.973.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021731678
Train loss (w/o reg) on all data: 0.011408568
Test loss (w/o reg) on all data: 0.03252796
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.456356e-07
Norm of the params: 14.368793
                Loss: fixed  96 labels. Loss 0.03253. Accuracy 0.977.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20400752
Train loss (w/o reg) on all data: 0.1959745
Test loss (w/o reg) on all data: 0.13489966
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.6577378e-05
Norm of the params: 12.675186
              Random: fixed  14 labels. Loss 0.13490. Accuracy 0.966.
### Flips: 208, rs: 14, checks: 156
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04890973
Train loss (w/o reg) on all data: 0.038974218
Test loss (w/o reg) on all data: 0.04244686
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.70569e-06
Norm of the params: 14.096462
     Influence (LOO): fixed  90 labels. Loss 0.04245. Accuracy 0.985.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012044381
Train loss (w/o reg) on all data: 0.005215053
Test loss (w/o reg) on all data: 0.013662427
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7415702e-07
Norm of the params: 11.687026
                Loss: fixed 103 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1868958
Train loss (w/o reg) on all data: 0.17929956
Test loss (w/o reg) on all data: 0.12646873
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9797067e-05
Norm of the params: 12.325778
              Random: fixed  24 labels. Loss 0.12647. Accuracy 0.977.
### Flips: 208, rs: 14, checks: 208
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028951492
Train loss (w/o reg) on all data: 0.020471442
Test loss (w/o reg) on all data: 0.02394212
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7626297e-06
Norm of the params: 13.023095
     Influence (LOO): fixed  98 labels. Loss 0.02394. Accuracy 0.989.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009881264
Train loss (w/o reg) on all data: 0.003907762
Test loss (w/o reg) on all data: 0.011755023
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.5316716e-07
Norm of the params: 10.930236
                Loss: fixed 104 labels. Loss 0.01176. Accuracy 0.996.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18150355
Train loss (w/o reg) on all data: 0.17356046
Test loss (w/o reg) on all data: 0.121096894
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.258647e-06
Norm of the params: 12.60404
              Random: fixed  28 labels. Loss 0.12110. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010055935
Train loss (w/o reg) on all data: 0.005120526
Test loss (w/o reg) on all data: 0.01473263
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4569956e-07
Norm of the params: 9.935201
     Influence (LOO): fixed 106 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009881265
Train loss (w/o reg) on all data: 0.0039080065
Test loss (w/o reg) on all data: 0.011756386
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4645398e-07
Norm of the params: 10.930013
                Loss: fixed 104 labels. Loss 0.01176. Accuracy 0.996.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16751724
Train loss (w/o reg) on all data: 0.15950975
Test loss (w/o reg) on all data: 0.10757843
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.298295e-06
Norm of the params: 12.655033
              Random: fixed  37 labels. Loss 0.10758. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729183
Test loss (w/o reg) on all data: 0.012055711
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9562911e-07
Norm of the params: 9.153251
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961924
Train loss (w/o reg) on all data: 0.003019117
Test loss (w/o reg) on all data: 0.0108755585
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.8300672e-07
Norm of the params: 9.942642
                Loss: fixed 106 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16483182
Train loss (w/o reg) on all data: 0.15747607
Test loss (w/o reg) on all data: 0.09771825
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3568009e-05
Norm of the params: 12.129099
              Random: fixed  42 labels. Loss 0.09772. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21967353
Train loss (w/o reg) on all data: 0.21376684
Test loss (w/o reg) on all data: 0.13784684
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.1444254e-05
Norm of the params: 10.868939
Flipped loss: 0.13785. Accuracy: 0.973
### Flips: 208, rs: 15, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14336441
Train loss (w/o reg) on all data: 0.13492489
Test loss (w/o reg) on all data: 0.09434453
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.2434104e-06
Norm of the params: 12.991939
     Influence (LOO): fixed  37 labels. Loss 0.09434. Accuracy 0.969.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09459931
Train loss (w/o reg) on all data: 0.082324944
Test loss (w/o reg) on all data: 0.0809214
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1819129e-05
Norm of the params: 15.668032
                Loss: fixed  52 labels. Loss 0.08092. Accuracy 0.962.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21834278
Train loss (w/o reg) on all data: 0.2126979
Test loss (w/o reg) on all data: 0.13189322
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.24032385e-05
Norm of the params: 10.625337
              Random: fixed   3 labels. Loss 0.13189. Accuracy 0.973.
### Flips: 208, rs: 15, checks: 104
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09358721
Train loss (w/o reg) on all data: 0.084518895
Test loss (w/o reg) on all data: 0.054540806
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.727998e-05
Norm of the params: 13.467234
     Influence (LOO): fixed  63 labels. Loss 0.05454. Accuracy 0.985.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027110994
Train loss (w/o reg) on all data: 0.016777603
Test loss (w/o reg) on all data: 0.012028416
Train acc on all data:  0.9952244508118434
Test acc on all data:   1.0
Norm of the mean of gradients: 1.071191e-05
Norm of the params: 14.375946
                Loss: fixed  90 labels. Loss 0.01203. Accuracy 1.000.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21357603
Train loss (w/o reg) on all data: 0.20777465
Test loss (w/o reg) on all data: 0.12697989
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4343844e-05
Norm of the params: 10.771606
              Random: fixed   6 labels. Loss 0.12698. Accuracy 0.973.
### Flips: 208, rs: 15, checks: 156
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051305763
Train loss (w/o reg) on all data: 0.04213239
Test loss (w/o reg) on all data: 0.03934068
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.346536e-06
Norm of the params: 13.545017
     Influence (LOO): fixed  86 labels. Loss 0.03934. Accuracy 0.985.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016282823
Train loss (w/o reg) on all data: 0.007956411
Test loss (w/o reg) on all data: 0.013267692
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3837226e-06
Norm of the params: 12.904582
                Loss: fixed 102 labels. Loss 0.01327. Accuracy 0.996.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20655392
Train loss (w/o reg) on all data: 0.2006792
Test loss (w/o reg) on all data: 0.11970221
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7200858e-05
Norm of the params: 10.839485
              Random: fixed  12 labels. Loss 0.11970. Accuracy 0.981.
### Flips: 208, rs: 15, checks: 208
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026892537
Train loss (w/o reg) on all data: 0.01993812
Test loss (w/o reg) on all data: 0.016967379
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3801958e-06
Norm of the params: 11.793571
     Influence (LOO): fixed  99 labels. Loss 0.01697. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013479955
Train loss (w/o reg) on all data: 0.0061365063
Test loss (w/o reg) on all data: 0.01177242
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.312708e-07
Norm of the params: 12.118951
                Loss: fixed 104 labels. Loss 0.01177. Accuracy 0.996.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20389688
Train loss (w/o reg) on all data: 0.19805585
Test loss (w/o reg) on all data: 0.11808755
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4343393e-05
Norm of the params: 10.808358
              Random: fixed  15 labels. Loss 0.11809. Accuracy 0.977.
### Flips: 208, rs: 15, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016242001
Train loss (w/o reg) on all data: 0.00979133
Test loss (w/o reg) on all data: 0.013093212
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4569567e-06
Norm of the params: 11.358406
     Influence (LOO): fixed 104 labels. Loss 0.01309. Accuracy 0.996.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010329269
Train loss (w/o reg) on all data: 0.0041695083
Test loss (w/o reg) on all data: 0.012969896
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.778034e-07
Norm of the params: 11.099335
                Loss: fixed 105 labels. Loss 0.01297. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19560027
Train loss (w/o reg) on all data: 0.18971337
Test loss (w/o reg) on all data: 0.10819837
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2026575e-05
Norm of the params: 10.8507185
              Random: fixed  21 labels. Loss 0.10820. Accuracy 0.981.
### Flips: 208, rs: 15, checks: 312
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01252746
Train loss (w/o reg) on all data: 0.007333641
Test loss (w/o reg) on all data: 0.011878187
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0079876e-06
Norm of the params: 10.191977
     Influence (LOO): fixed 106 labels. Loss 0.01188. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074117463
Train loss (w/o reg) on all data: 0.0026373675
Test loss (w/o reg) on all data: 0.011813304
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.507361e-07
Norm of the params: 9.771774
                Loss: fixed 106 labels. Loss 0.01181. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18214595
Train loss (w/o reg) on all data: 0.17575866
Test loss (w/o reg) on all data: 0.099505186
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.187885e-06
Norm of the params: 11.302471
              Random: fixed  29 labels. Loss 0.09951. Accuracy 0.985.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2040818
Train loss (w/o reg) on all data: 0.19693008
Test loss (w/o reg) on all data: 0.14483279
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8997744e-05
Norm of the params: 11.959705
Flipped loss: 0.14483. Accuracy: 0.962
### Flips: 208, rs: 16, checks: 52
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118988454
Train loss (w/o reg) on all data: 0.1071355
Test loss (w/o reg) on all data: 0.11219568
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.5949377e-05
Norm of the params: 15.396724
     Influence (LOO): fixed  39 labels. Loss 0.11220. Accuracy 0.954.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07801384
Train loss (w/o reg) on all data: 0.0627776
Test loss (w/o reg) on all data: 0.11112756
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.3111894e-05
Norm of the params: 17.456366
                Loss: fixed  51 labels. Loss 0.11113. Accuracy 0.954.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19602603
Train loss (w/o reg) on all data: 0.18918526
Test loss (w/o reg) on all data: 0.1431668
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.4593315e-05
Norm of the params: 11.696805
              Random: fixed   5 labels. Loss 0.14317. Accuracy 0.958.
### Flips: 208, rs: 16, checks: 104
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07119742
Train loss (w/o reg) on all data: 0.059851997
Test loss (w/o reg) on all data: 0.059535697
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7948929e-05
Norm of the params: 15.063481
     Influence (LOO): fixed  67 labels. Loss 0.05954. Accuracy 0.977.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018343773
Train loss (w/o reg) on all data: 0.009179248
Test loss (w/o reg) on all data: 0.028221017
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2715145e-06
Norm of the params: 13.538482
                Loss: fixed  85 labels. Loss 0.02822. Accuracy 0.989.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18876818
Train loss (w/o reg) on all data: 0.18137741
Test loss (w/o reg) on all data: 0.13159524
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.5480189e-05
Norm of the params: 12.157936
              Random: fixed  13 labels. Loss 0.13160. Accuracy 0.958.
### Flips: 208, rs: 16, checks: 156
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032145258
Train loss (w/o reg) on all data: 0.022093995
Test loss (w/o reg) on all data: 0.03142894
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8306384e-06
Norm of the params: 14.178339
     Influence (LOO): fixed  83 labels. Loss 0.03143. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012358246
Train loss (w/o reg) on all data: 0.005521939
Test loss (w/o reg) on all data: 0.02372583
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2284314e-07
Norm of the params: 11.692996
                Loss: fixed  93 labels. Loss 0.02373. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17755719
Train loss (w/o reg) on all data: 0.1699677
Test loss (w/o reg) on all data: 0.12341537
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.55816e-05
Norm of the params: 12.320299
              Random: fixed  19 labels. Loss 0.12342. Accuracy 0.962.
### Flips: 208, rs: 16, checks: 208
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021393523
Train loss (w/o reg) on all data: 0.013986553
Test loss (w/o reg) on all data: 0.019340102
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.8740437e-06
Norm of the params: 12.171253
     Influence (LOO): fixed  92 labels. Loss 0.01934. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00837733
Train loss (w/o reg) on all data: 0.0034013987
Test loss (w/o reg) on all data: 0.011192305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4464124e-07
Norm of the params: 9.975903
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16621043
Train loss (w/o reg) on all data: 0.15778589
Test loss (w/o reg) on all data: 0.11817738
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.563489e-06
Norm of the params: 12.980396
              Random: fixed  25 labels. Loss 0.11818. Accuracy 0.966.
### Flips: 208, rs: 16, checks: 260
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008377331
Train loss (w/o reg) on all data: 0.0034015593
Test loss (w/o reg) on all data: 0.011191761
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5810763e-07
Norm of the params: 9.975742
     Influence (LOO): fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00837733
Train loss (w/o reg) on all data: 0.0034015535
Test loss (w/o reg) on all data: 0.0111916745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8554704e-07
Norm of the params: 9.975747
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15964836
Train loss (w/o reg) on all data: 0.15102004
Test loss (w/o reg) on all data: 0.11645948
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.6590405e-05
Norm of the params: 13.136452
              Random: fixed  28 labels. Loss 0.11646. Accuracy 0.973.
### Flips: 208, rs: 16, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172902
Test loss (w/o reg) on all data: 0.012053559
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5387064e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00837733
Train loss (w/o reg) on all data: 0.0034014275
Test loss (w/o reg) on all data: 0.011191815
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.026235e-07
Norm of the params: 9.975874
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154817
Train loss (w/o reg) on all data: 0.14627403
Test loss (w/o reg) on all data: 0.11307614
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.1540367e-06
Norm of the params: 13.071315
              Random: fixed  31 labels. Loss 0.11308. Accuracy 0.966.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21517643
Train loss (w/o reg) on all data: 0.20856759
Test loss (w/o reg) on all data: 0.14311227
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7830823e-05
Norm of the params: 11.496823
Flipped loss: 0.14311. Accuracy: 0.966
### Flips: 208, rs: 17, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1331093
Train loss (w/o reg) on all data: 0.12299435
Test loss (w/o reg) on all data: 0.110951625
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3519558e-05
Norm of the params: 14.223188
     Influence (LOO): fixed  37 labels. Loss 0.11095. Accuracy 0.958.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089924216
Train loss (w/o reg) on all data: 0.07899499
Test loss (w/o reg) on all data: 0.090361685
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.0306716e-05
Norm of the params: 14.784608
                Loss: fixed  52 labels. Loss 0.09036. Accuracy 0.958.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20175883
Train loss (w/o reg) on all data: 0.1941987
Test loss (w/o reg) on all data: 0.13291243
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8175085e-05
Norm of the params: 12.296445
              Random: fixed   8 labels. Loss 0.13291. Accuracy 0.962.
### Flips: 208, rs: 17, checks: 104
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08358482
Train loss (w/o reg) on all data: 0.07242125
Test loss (w/o reg) on all data: 0.08178367
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2228647e-05
Norm of the params: 14.942268
     Influence (LOO): fixed  64 labels. Loss 0.08178. Accuracy 0.973.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025446117
Train loss (w/o reg) on all data: 0.014998685
Test loss (w/o reg) on all data: 0.024197692
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9678698e-06
Norm of the params: 14.455057
                Loss: fixed  87 labels. Loss 0.02420. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19013071
Train loss (w/o reg) on all data: 0.18234816
Test loss (w/o reg) on all data: 0.12328836
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1153274e-05
Norm of the params: 12.476017
              Random: fixed  14 labels. Loss 0.12329. Accuracy 0.973.
### Flips: 208, rs: 17, checks: 156
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050154053
Train loss (w/o reg) on all data: 0.040423945
Test loss (w/o reg) on all data: 0.04908649
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.19252e-06
Norm of the params: 13.94999
     Influence (LOO): fixed  83 labels. Loss 0.04909. Accuracy 0.989.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007122745
Train loss (w/o reg) on all data: 0.00256079
Test loss (w/o reg) on all data: 0.013581396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.344286e-07
Norm of the params: 9.551916
                Loss: fixed  99 labels. Loss 0.01358. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18467481
Train loss (w/o reg) on all data: 0.1768808
Test loss (w/o reg) on all data: 0.11289479
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.425185e-05
Norm of the params: 12.485198
              Random: fixed  18 labels. Loss 0.11289. Accuracy 0.981.
### Flips: 208, rs: 17, checks: 208
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022976767
Train loss (w/o reg) on all data: 0.016033946
Test loss (w/o reg) on all data: 0.021343483
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0941486e-06
Norm of the params: 11.783736
     Influence (LOO): fixed  96 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071227457
Train loss (w/o reg) on all data: 0.0025608686
Test loss (w/o reg) on all data: 0.013581682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0131476e-07
Norm of the params: 9.551835
                Loss: fixed  99 labels. Loss 0.01358. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18017723
Train loss (w/o reg) on all data: 0.17222762
Test loss (w/o reg) on all data: 0.10728856
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.3247507e-05
Norm of the params: 12.609211
              Random: fixed  22 labels. Loss 0.10729. Accuracy 0.977.
### Flips: 208, rs: 17, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729965
Test loss (w/o reg) on all data: 0.012054961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6935714e-07
Norm of the params: 9.153166
     Influence (LOO): fixed 102 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059190285
Train loss (w/o reg) on all data: 0.0018801014
Test loss (w/o reg) on all data: 0.013727217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5614093e-07
Norm of the params: 8.987688
                Loss: fixed 100 labels. Loss 0.01373. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17206895
Train loss (w/o reg) on all data: 0.16387603
Test loss (w/o reg) on all data: 0.10341668
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.4654615e-06
Norm of the params: 12.80072
              Random: fixed  26 labels. Loss 0.10342. Accuracy 0.981.
### Flips: 208, rs: 17, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728636
Test loss (w/o reg) on all data: 0.01205534
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.628179e-07
Norm of the params: 9.15331
     Influence (LOO): fixed 102 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00591903
Train loss (w/o reg) on all data: 0.0018800839
Test loss (w/o reg) on all data: 0.013728037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.930287e-07
Norm of the params: 8.987709
                Loss: fixed 100 labels. Loss 0.01373. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16599439
Train loss (w/o reg) on all data: 0.1572097
Test loss (w/o reg) on all data: 0.096846506
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2754096e-05
Norm of the params: 13.254962
              Random: fixed  31 labels. Loss 0.09685. Accuracy 0.981.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22294885
Train loss (w/o reg) on all data: 0.21575959
Test loss (w/o reg) on all data: 0.13306694
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3504494e-05
Norm of the params: 11.991047
Flipped loss: 0.13307. Accuracy: 0.973
### Flips: 208, rs: 18, checks: 52
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15309307
Train loss (w/o reg) on all data: 0.14280412
Test loss (w/o reg) on all data: 0.096384466
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.641007e-05
Norm of the params: 14.345004
     Influence (LOO): fixed  35 labels. Loss 0.09638. Accuracy 0.977.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10519254
Train loss (w/o reg) on all data: 0.09420336
Test loss (w/o reg) on all data: 0.060553227
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.547974e-06
Norm of the params: 14.825103
                Loss: fixed  52 labels. Loss 0.06055. Accuracy 0.985.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21872263
Train loss (w/o reg) on all data: 0.21152742
Test loss (w/o reg) on all data: 0.12852798
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6660513e-05
Norm of the params: 11.996003
              Random: fixed   2 labels. Loss 0.12853. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0939427
Train loss (w/o reg) on all data: 0.084551714
Test loss (w/o reg) on all data: 0.054617666
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5484036e-05
Norm of the params: 13.704738
     Influence (LOO): fixed  63 labels. Loss 0.05462. Accuracy 0.985.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02401221
Train loss (w/o reg) on all data: 0.01417849
Test loss (w/o reg) on all data: 0.014294063
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0725006e-06
Norm of the params: 14.024065
                Loss: fixed  96 labels. Loss 0.01429. Accuracy 0.996.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21325292
Train loss (w/o reg) on all data: 0.20603512
Test loss (w/o reg) on all data: 0.11976918
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.7253e-05
Norm of the params: 12.014824
              Random: fixed   7 labels. Loss 0.11977. Accuracy 0.985.
### Flips: 208, rs: 18, checks: 156
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038118545
Train loss (w/o reg) on all data: 0.030204318
Test loss (w/o reg) on all data: 0.02631096
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6771247e-06
Norm of the params: 12.581119
     Influence (LOO): fixed  91 labels. Loss 0.02631. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00759721
Train loss (w/o reg) on all data: 0.0029205582
Test loss (w/o reg) on all data: 0.013380313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5809233e-07
Norm of the params: 9.6712475
                Loss: fixed 104 labels. Loss 0.01338. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20020111
Train loss (w/o reg) on all data: 0.19273047
Test loss (w/o reg) on all data: 0.109495565
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9530234e-05
Norm of the params: 12.22345
              Random: fixed  15 labels. Loss 0.10950. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 208
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02564976
Train loss (w/o reg) on all data: 0.018467134
Test loss (w/o reg) on all data: 0.017851295
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.928433e-06
Norm of the params: 11.985513
     Influence (LOO): fixed  97 labels. Loss 0.01785. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012055282
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.244853e-07
Norm of the params: 9.1531725
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1927862
Train loss (w/o reg) on all data: 0.18539238
Test loss (w/o reg) on all data: 0.102487214
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4112874e-05
Norm of the params: 12.160449
              Random: fixed  20 labels. Loss 0.10249. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016258474
Train loss (w/o reg) on all data: 0.00977053
Test loss (w/o reg) on all data: 0.016625669
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.3700154e-07
Norm of the params: 11.391175
     Influence (LOO): fixed 101 labels. Loss 0.01663. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012055738
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0690125e-07
Norm of the params: 9.153143
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18455365
Train loss (w/o reg) on all data: 0.17665875
Test loss (w/o reg) on all data: 0.09730819
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.184149e-05
Norm of the params: 12.565752
              Random: fixed  25 labels. Loss 0.09731. Accuracy 0.985.
### Flips: 208, rs: 18, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015065134
Train loss (w/o reg) on all data: 0.009018301
Test loss (w/o reg) on all data: 0.015715506
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5227298e-06
Norm of the params: 10.99712
     Influence (LOO): fixed 102 labels. Loss 0.01572. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730321
Test loss (w/o reg) on all data: 0.01205531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8977472e-07
Norm of the params: 9.153128
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17873147
Train loss (w/o reg) on all data: 0.17093238
Test loss (w/o reg) on all data: 0.08711666
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.35109e-06
Norm of the params: 12.489268
              Random: fixed  30 labels. Loss 0.08712. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21450883
Train loss (w/o reg) on all data: 0.20701762
Test loss (w/o reg) on all data: 0.15831713
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7289518e-05
Norm of the params: 12.24028
Flipped loss: 0.15832. Accuracy: 0.966
### Flips: 208, rs: 19, checks: 52
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1431519
Train loss (w/o reg) on all data: 0.13256535
Test loss (w/o reg) on all data: 0.12056795
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.259113e-05
Norm of the params: 14.550979
     Influence (LOO): fixed  38 labels. Loss 0.12057. Accuracy 0.969.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104410626
Train loss (w/o reg) on all data: 0.09151089
Test loss (w/o reg) on all data: 0.09536893
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7836448e-05
Norm of the params: 16.062212
                Loss: fixed  49 labels. Loss 0.09537. Accuracy 0.969.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20643285
Train loss (w/o reg) on all data: 0.19884805
Test loss (w/o reg) on all data: 0.15551062
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7886416e-05
Norm of the params: 12.316492
              Random: fixed   3 labels. Loss 0.15551. Accuracy 0.943.
### Flips: 208, rs: 19, checks: 104
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103836946
Train loss (w/o reg) on all data: 0.09420561
Test loss (w/o reg) on all data: 0.080081865
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0576375e-05
Norm of the params: 13.879001
     Influence (LOO): fixed  60 labels. Loss 0.08008. Accuracy 0.981.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046152025
Train loss (w/o reg) on all data: 0.03382243
Test loss (w/o reg) on all data: 0.056071587
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.396022e-06
Norm of the params: 15.703246
                Loss: fixed  86 labels. Loss 0.05607. Accuracy 0.977.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2011186
Train loss (w/o reg) on all data: 0.19371122
Test loss (w/o reg) on all data: 0.13727474
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.26525865e-05
Norm of the params: 12.171595
              Random: fixed  10 labels. Loss 0.13727. Accuracy 0.966.
### Flips: 208, rs: 19, checks: 156
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063191235
Train loss (w/o reg) on all data: 0.053507745
Test loss (w/o reg) on all data: 0.050433777
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.8604677e-06
Norm of the params: 13.916531
     Influence (LOO): fixed  82 labels. Loss 0.05043. Accuracy 0.989.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019748323
Train loss (w/o reg) on all data: 0.011231231
Test loss (w/o reg) on all data: 0.022214968
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5931813e-06
Norm of the params: 13.051507
                Loss: fixed 102 labels. Loss 0.02221. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19872771
Train loss (w/o reg) on all data: 0.19129847
Test loss (w/o reg) on all data: 0.13318911
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1329462e-05
Norm of the params: 12.189534
              Random: fixed  13 labels. Loss 0.13319. Accuracy 0.969.
### Flips: 208, rs: 19, checks: 208
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041819602
Train loss (w/o reg) on all data: 0.033543788
Test loss (w/o reg) on all data: 0.028863057
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5219115e-06
Norm of the params: 12.8653145
     Influence (LOO): fixed  92 labels. Loss 0.02886. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010933058
Train loss (w/o reg) on all data: 0.004688221
Test loss (w/o reg) on all data: 0.025571452
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0275208e-07
Norm of the params: 11.175721
                Loss: fixed 106 labels. Loss 0.02557. Accuracy 0.989.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19052704
Train loss (w/o reg) on all data: 0.18277788
Test loss (w/o reg) on all data: 0.12447041
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.8685022e-05
Norm of the params: 12.449216
              Random: fixed  20 labels. Loss 0.12447. Accuracy 0.973.
### Flips: 208, rs: 19, checks: 260
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027552418
Train loss (w/o reg) on all data: 0.020207945
Test loss (w/o reg) on all data: 0.039914053
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0443805e-06
Norm of the params: 12.119797
     Influence (LOO): fixed 101 labels. Loss 0.03991. Accuracy 0.985.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0082307
Train loss (w/o reg) on all data: 0.0031268334
Test loss (w/o reg) on all data: 0.013310736
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5448375e-07
Norm of the params: 10.103334
                Loss: fixed 108 labels. Loss 0.01331. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18738429
Train loss (w/o reg) on all data: 0.1793934
Test loss (w/o reg) on all data: 0.12194904
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9979903e-05
Norm of the params: 12.641908
              Random: fixed  22 labels. Loss 0.12195. Accuracy 0.973.
### Flips: 208, rs: 19, checks: 312
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010366621
Train loss (w/o reg) on all data: 0.005355163
Test loss (w/o reg) on all data: 0.015477379
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1125667e-07
Norm of the params: 10.011452
     Influence (LOO): fixed 108 labels. Loss 0.01548. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012054729
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2402013e-07
Norm of the params: 9.153179
                Loss: fixed 109 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17463534
Train loss (w/o reg) on all data: 0.16648479
Test loss (w/o reg) on all data: 0.11687975
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5563583e-05
Norm of the params: 12.76757
              Random: fixed  31 labels. Loss 0.11688. Accuracy 0.966.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21663618
Train loss (w/o reg) on all data: 0.2095447
Test loss (w/o reg) on all data: 0.12185513
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9595927e-05
Norm of the params: 11.909216
Flipped loss: 0.12186. Accuracy: 0.977
### Flips: 208, rs: 20, checks: 52
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13671051
Train loss (w/o reg) on all data: 0.1251052
Test loss (w/o reg) on all data: 0.10903654
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7899876e-05
Norm of the params: 15.235032
     Influence (LOO): fixed  37 labels. Loss 0.10904. Accuracy 0.962.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092030145
Train loss (w/o reg) on all data: 0.078123964
Test loss (w/o reg) on all data: 0.07013934
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.223573e-06
Norm of the params: 16.677038
                Loss: fixed  52 labels. Loss 0.07014. Accuracy 0.969.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21383965
Train loss (w/o reg) on all data: 0.20693967
Test loss (w/o reg) on all data: 0.11601797
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.9928997e-05
Norm of the params: 11.747326
              Random: fixed   3 labels. Loss 0.11602. Accuracy 0.977.
### Flips: 208, rs: 20, checks: 104
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09095961
Train loss (w/o reg) on all data: 0.08057556
Test loss (w/o reg) on all data: 0.07919998
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.5660734e-06
Norm of the params: 14.411138
     Influence (LOO): fixed  63 labels. Loss 0.07920. Accuracy 0.973.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014905755
Train loss (w/o reg) on all data: 0.006771949
Test loss (w/o reg) on all data: 0.02117606
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1621003e-06
Norm of the params: 12.754456
                Loss: fixed  95 labels. Loss 0.02118. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20734128
Train loss (w/o reg) on all data: 0.20067371
Test loss (w/o reg) on all data: 0.1090766
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2183821e-05
Norm of the params: 11.547788
              Random: fixed   8 labels. Loss 0.10908. Accuracy 0.985.
### Flips: 208, rs: 20, checks: 156
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04450839
Train loss (w/o reg) on all data: 0.035877563
Test loss (w/o reg) on all data: 0.03224401
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6331329e-05
Norm of the params: 13.138363
     Influence (LOO): fixed  86 labels. Loss 0.03224. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01063263
Train loss (w/o reg) on all data: 0.0045497674
Test loss (w/o reg) on all data: 0.012493959
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.269794e-07
Norm of the params: 11.029835
                Loss: fixed  98 labels. Loss 0.01249. Accuracy 0.996.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20084953
Train loss (w/o reg) on all data: 0.19403487
Test loss (w/o reg) on all data: 0.102769315
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2714243e-05
Norm of the params: 11.674468
              Random: fixed  13 labels. Loss 0.10277. Accuracy 0.989.
### Flips: 208, rs: 20, checks: 208
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754285
Train loss (w/o reg) on all data: 0.004798886
Test loss (w/o reg) on all data: 0.0115443105
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.903573e-07
Norm of the params: 9.955299
     Influence (LOO): fixed  99 labels. Loss 0.01154. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664982
Train loss (w/o reg) on all data: 0.0033180485
Test loss (w/o reg) on all data: 0.013527786
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5941503e-07
Norm of the params: 10.341115
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19219224
Train loss (w/o reg) on all data: 0.18517825
Test loss (w/o reg) on all data: 0.096866116
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2491977e-05
Norm of the params: 11.843979
              Random: fixed  18 labels. Loss 0.09687. Accuracy 0.989.
### Flips: 208, rs: 20, checks: 260
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754283
Train loss (w/o reg) on all data: 0.004798768
Test loss (w/o reg) on all data: 0.0115439
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.251734e-07
Norm of the params: 9.955417
     Influence (LOO): fixed  99 labels. Loss 0.01154. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00866498
Train loss (w/o reg) on all data: 0.0033182942
Test loss (w/o reg) on all data: 0.013530001
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0927547e-07
Norm of the params: 10.340877
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19025233
Train loss (w/o reg) on all data: 0.18307029
Test loss (w/o reg) on all data: 0.09636616
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.0478527e-06
Norm of the params: 11.985025
              Random: fixed  19 labels. Loss 0.09637. Accuracy 0.981.
### Flips: 208, rs: 20, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728533
Test loss (w/o reg) on all data: 0.012055311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1835874e-06
Norm of the params: 9.153322
     Influence (LOO): fixed 100 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664979
Train loss (w/o reg) on all data: 0.0033181072
Test loss (w/o reg) on all data: 0.013526882
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.343238e-07
Norm of the params: 10.341057
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18456307
Train loss (w/o reg) on all data: 0.17738208
Test loss (w/o reg) on all data: 0.092860535
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1135969e-05
Norm of the params: 11.984148
              Random: fixed  22 labels. Loss 0.09286. Accuracy 0.985.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20157638
Train loss (w/o reg) on all data: 0.19334504
Test loss (w/o reg) on all data: 0.14164932
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.779622e-05
Norm of the params: 12.830696
Flipped loss: 0.14165. Accuracy: 0.973
### Flips: 208, rs: 21, checks: 52
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13058132
Train loss (w/o reg) on all data: 0.118429765
Test loss (w/o reg) on all data: 0.098136
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.114774e-06
Norm of the params: 15.589458
     Influence (LOO): fixed  34 labels. Loss 0.09814. Accuracy 0.985.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0791073
Train loss (w/o reg) on all data: 0.062413637
Test loss (w/o reg) on all data: 0.08189146
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1826351e-05
Norm of the params: 18.272198
                Loss: fixed  52 labels. Loss 0.08189. Accuracy 0.981.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19562992
Train loss (w/o reg) on all data: 0.18760069
Test loss (w/o reg) on all data: 0.13563019
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.8307153e-05
Norm of the params: 12.672199
              Random: fixed   5 labels. Loss 0.13563. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 104
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074831754
Train loss (w/o reg) on all data: 0.062491532
Test loss (w/o reg) on all data: 0.061577696
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1538956e-05
Norm of the params: 15.710011
     Influence (LOO): fixed  65 labels. Loss 0.06158. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01468881
Train loss (w/o reg) on all data: 0.0067320564
Test loss (w/o reg) on all data: 0.01698812
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.029099e-07
Norm of the params: 12.614875
                Loss: fixed  88 labels. Loss 0.01699. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19160086
Train loss (w/o reg) on all data: 0.18326718
Test loss (w/o reg) on all data: 0.1371924
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.463852e-05
Norm of the params: 12.910217
              Random: fixed   7 labels. Loss 0.13719. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 156
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026812367
Train loss (w/o reg) on all data: 0.019999001
Test loss (w/o reg) on all data: 0.03499181
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0311884e-06
Norm of the params: 11.673359
     Influence (LOO): fixed  84 labels. Loss 0.03499. Accuracy 0.985.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4647375e-08
Norm of the params: 9.153183
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18294974
Train loss (w/o reg) on all data: 0.17466933
Test loss (w/o reg) on all data: 0.12775779
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4516148e-05
Norm of the params: 12.868882
              Random: fixed  14 labels. Loss 0.12776. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 208
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754286
Train loss (w/o reg) on all data: 0.0047988794
Test loss (w/o reg) on all data: 0.0115435505
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.515192e-07
Norm of the params: 9.955307
     Influence (LOO): fixed  91 labels. Loss 0.01154. Accuracy 0.996.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730983
Test loss (w/o reg) on all data: 0.012055487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8886125e-07
Norm of the params: 9.153052
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17847031
Train loss (w/o reg) on all data: 0.17030619
Test loss (w/o reg) on all data: 0.11975579
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8326831e-05
Norm of the params: 12.778202
              Random: fixed  17 labels. Loss 0.11976. Accuracy 0.969.
### Flips: 208, rs: 21, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012055473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1805764e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012055428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1075732e-07
Norm of the params: 9.153214
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17615665
Train loss (w/o reg) on all data: 0.16797838
Test loss (w/o reg) on all data: 0.11884635
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4800048e-05
Norm of the params: 12.789277
              Random: fixed  19 labels. Loss 0.11885. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729346
Test loss (w/o reg) on all data: 0.012055829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0061897e-06
Norm of the params: 9.153234
     Influence (LOO): fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729344
Test loss (w/o reg) on all data: 0.012055643
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.570561e-07
Norm of the params: 9.153234
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17036255
Train loss (w/o reg) on all data: 0.16150503
Test loss (w/o reg) on all data: 0.11341957
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.936616e-06
Norm of the params: 13.30978
              Random: fixed  21 labels. Loss 0.11342. Accuracy 0.973.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21573092
Train loss (w/o reg) on all data: 0.20766157
Test loss (w/o reg) on all data: 0.1464964
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6464344e-05
Norm of the params: 12.703818
Flipped loss: 0.14650. Accuracy: 0.958
### Flips: 208, rs: 22, checks: 52
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14727862
Train loss (w/o reg) on all data: 0.13594633
Test loss (w/o reg) on all data: 0.102775075
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.5494042e-05
Norm of the params: 15.054763
     Influence (LOO): fixed  37 labels. Loss 0.10278. Accuracy 0.969.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09166694
Train loss (w/o reg) on all data: 0.07667645
Test loss (w/o reg) on all data: 0.12557775
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.9125615e-06
Norm of the params: 17.315014
                Loss: fixed  51 labels. Loss 0.12558. Accuracy 0.950.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20578322
Train loss (w/o reg) on all data: 0.19792287
Test loss (w/o reg) on all data: 0.13420038
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9217843e-05
Norm of the params: 12.538218
              Random: fixed   9 labels. Loss 0.13420. Accuracy 0.962.
### Flips: 208, rs: 22, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07777593
Train loss (w/o reg) on all data: 0.06729015
Test loss (w/o reg) on all data: 0.0612613
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.314762e-06
Norm of the params: 14.481561
     Influence (LOO): fixed  68 labels. Loss 0.06126. Accuracy 0.973.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0171842
Train loss (w/o reg) on all data: 0.009269981
Test loss (w/o reg) on all data: 0.020061947
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9354245e-06
Norm of the params: 12.581113
                Loss: fixed  92 labels. Loss 0.02006. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20139007
Train loss (w/o reg) on all data: 0.1937198
Test loss (w/o reg) on all data: 0.1235224
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.4713303e-06
Norm of the params: 12.385695
              Random: fixed  13 labels. Loss 0.12352. Accuracy 0.962.
### Flips: 208, rs: 22, checks: 156
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029426258
Train loss (w/o reg) on all data: 0.021302084
Test loss (w/o reg) on all data: 0.019268997
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9109623e-06
Norm of the params: 12.7469015
     Influence (LOO): fixed  89 labels. Loss 0.01927. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729271
Test loss (w/o reg) on all data: 0.012054354
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8556615e-07
Norm of the params: 9.153244
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19793764
Train loss (w/o reg) on all data: 0.1901278
Test loss (w/o reg) on all data: 0.122600906
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0424091e-05
Norm of the params: 12.497871
              Random: fixed  15 labels. Loss 0.12260. Accuracy 0.966.
### Flips: 208, rs: 22, checks: 208
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016183631
Train loss (w/o reg) on all data: 0.0107288025
Test loss (w/o reg) on all data: 0.015167308
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.982373e-07
Norm of the params: 10.44493
     Influence (LOO): fixed  95 labels. Loss 0.01517. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728957
Test loss (w/o reg) on all data: 0.012054927
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.395549e-07
Norm of the params: 9.1532755
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19248737
Train loss (w/o reg) on all data: 0.1851033
Test loss (w/o reg) on all data: 0.11393033
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0388254e-05
Norm of the params: 12.152432
              Random: fixed  20 labels. Loss 0.11393. Accuracy 0.977.
### Flips: 208, rs: 22, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728508
Test loss (w/o reg) on all data: 0.012055187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0629397e-07
Norm of the params: 9.153325
     Influence (LOO): fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728536
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3405143e-07
Norm of the params: 9.153323
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18943648
Train loss (w/o reg) on all data: 0.18194586
Test loss (w/o reg) on all data: 0.11284802
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.5109573e-05
Norm of the params: 12.239787
              Random: fixed  22 labels. Loss 0.11285. Accuracy 0.977.
### Flips: 208, rs: 22, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012055357
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.857074e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.01205542
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5024544e-07
Norm of the params: 9.153225
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17829436
Train loss (w/o reg) on all data: 0.17086957
Test loss (w/o reg) on all data: 0.10107769
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.947844e-06
Norm of the params: 12.1858835
              Random: fixed  28 labels. Loss 0.10108. Accuracy 0.985.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20960134
Train loss (w/o reg) on all data: 0.20149717
Test loss (w/o reg) on all data: 0.12577386
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.6077035e-06
Norm of the params: 12.731208
Flipped loss: 0.12577. Accuracy: 0.973
### Flips: 208, rs: 23, checks: 52
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12856543
Train loss (w/o reg) on all data: 0.115950614
Test loss (w/o reg) on all data: 0.10054761
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2051488e-05
Norm of the params: 15.883836
     Influence (LOO): fixed  38 labels. Loss 0.10055. Accuracy 0.962.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085610166
Train loss (w/o reg) on all data: 0.069613054
Test loss (w/o reg) on all data: 0.08299808
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.55248e-06
Norm of the params: 17.886929
                Loss: fixed  52 labels. Loss 0.08300. Accuracy 0.973.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20453398
Train loss (w/o reg) on all data: 0.19666646
Test loss (w/o reg) on all data: 0.12087177
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4094075e-05
Norm of the params: 12.543937
              Random: fixed   6 labels. Loss 0.12087. Accuracy 0.973.
### Flips: 208, rs: 23, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08387984
Train loss (w/o reg) on all data: 0.072004735
Test loss (w/o reg) on all data: 0.068648085
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.083125e-06
Norm of the params: 15.411103
     Influence (LOO): fixed  61 labels. Loss 0.06865. Accuracy 0.969.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020468201
Train loss (w/o reg) on all data: 0.011657198
Test loss (w/o reg) on all data: 0.035272326
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6393511e-06
Norm of the params: 13.274791
                Loss: fixed  92 labels. Loss 0.03527. Accuracy 0.981.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20340842
Train loss (w/o reg) on all data: 0.19561389
Test loss (w/o reg) on all data: 0.117365845
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7671524e-05
Norm of the params: 12.485616
              Random: fixed   8 labels. Loss 0.11737. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 156
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036445398
Train loss (w/o reg) on all data: 0.027173074
Test loss (w/o reg) on all data: 0.025740217
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3005413e-06
Norm of the params: 13.617873
     Influence (LOO): fixed  84 labels. Loss 0.02574. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009996068
Train loss (w/o reg) on all data: 0.004082823
Test loss (w/o reg) on all data: 0.021046944
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.085202e-07
Norm of the params: 10.874966
                Loss: fixed  97 labels. Loss 0.02105. Accuracy 0.989.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19292627
Train loss (w/o reg) on all data: 0.18519014
Test loss (w/o reg) on all data: 0.10972235
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3424209e-05
Norm of the params: 12.438758
              Random: fixed  14 labels. Loss 0.10972. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 208
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018881524
Train loss (w/o reg) on all data: 0.0111698415
Test loss (w/o reg) on all data: 0.024692059
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8250877e-06
Norm of the params: 12.419085
     Influence (LOO): fixed  94 labels. Loss 0.02469. Accuracy 0.989.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008817984
Train loss (w/o reg) on all data: 0.0034228514
Test loss (w/o reg) on all data: 0.021008119
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.078927e-07
Norm of the params: 10.387619
                Loss: fixed  98 labels. Loss 0.02101. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18978295
Train loss (w/o reg) on all data: 0.18211712
Test loss (w/o reg) on all data: 0.10427278
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4176179e-05
Norm of the params: 12.38211
              Random: fixed  17 labels. Loss 0.10427. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 260
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01688138
Train loss (w/o reg) on all data: 0.0101452265
Test loss (w/o reg) on all data: 0.024223072
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1290955e-06
Norm of the params: 11.607027
     Influence (LOO): fixed  96 labels. Loss 0.02422. Accuracy 0.989.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729784
Test loss (w/o reg) on all data: 0.012054728
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4482127e-07
Norm of the params: 9.153186
                Loss: fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18052937
Train loss (w/o reg) on all data: 0.1724953
Test loss (w/o reg) on all data: 0.1094592
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.473809e-05
Norm of the params: 12.676014
              Random: fixed  23 labels. Loss 0.10946. Accuracy 0.985.
### Flips: 208, rs: 23, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011468942
Train loss (w/o reg) on all data: 0.0059326654
Test loss (w/o reg) on all data: 0.016595766
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.924715e-06
Norm of the params: 10.52262
     Influence (LOO): fixed  99 labels. Loss 0.01660. Accuracy 0.989.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012055145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3180443e-07
Norm of the params: 9.153195
                Loss: fixed 100 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17539449
Train loss (w/o reg) on all data: 0.16721392
Test loss (w/o reg) on all data: 0.10592217
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0740291e-05
Norm of the params: 12.791071
              Random: fixed  26 labels. Loss 0.10592. Accuracy 0.985.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2153882
Train loss (w/o reg) on all data: 0.20744398
Test loss (w/o reg) on all data: 0.13765992
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9188763e-05
Norm of the params: 12.604926
Flipped loss: 0.13766. Accuracy: 0.977
### Flips: 208, rs: 24, checks: 52
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13298702
Train loss (w/o reg) on all data: 0.120170586
Test loss (w/o reg) on all data: 0.08589197
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.5202393e-06
Norm of the params: 16.010267
     Influence (LOO): fixed  42 labels. Loss 0.08589. Accuracy 0.977.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08742032
Train loss (w/o reg) on all data: 0.0721497
Test loss (w/o reg) on all data: 0.08302754
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.5464673e-06
Norm of the params: 17.476053
                Loss: fixed  51 labels. Loss 0.08303. Accuracy 0.977.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2097843
Train loss (w/o reg) on all data: 0.20219499
Test loss (w/o reg) on all data: 0.13137148
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.750566e-05
Norm of the params: 12.32016
              Random: fixed   6 labels. Loss 0.13137. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 104
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090720534
Train loss (w/o reg) on all data: 0.078151055
Test loss (w/o reg) on all data: 0.061401688
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.9089817e-06
Norm of the params: 15.8552685
     Influence (LOO): fixed  62 labels. Loss 0.06140. Accuracy 0.977.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017745035
Train loss (w/o reg) on all data: 0.008688701
Test loss (w/o reg) on all data: 0.022421924
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9826124e-07
Norm of the params: 13.458331
                Loss: fixed  93 labels. Loss 0.02242. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20644666
Train loss (w/o reg) on all data: 0.19894865
Test loss (w/o reg) on all data: 0.1264546
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9939533e-05
Norm of the params: 12.245831
              Random: fixed  11 labels. Loss 0.12645. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 156
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041392263
Train loss (w/o reg) on all data: 0.031655293
Test loss (w/o reg) on all data: 0.027405577
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.901001e-06
Norm of the params: 13.9549055
     Influence (LOO): fixed  86 labels. Loss 0.02741. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008961579
Train loss (w/o reg) on all data: 0.003416601
Test loss (w/o reg) on all data: 0.017992137
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.87759e-07
Norm of the params: 10.530886
                Loss: fixed  99 labels. Loss 0.01799. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20139867
Train loss (w/o reg) on all data: 0.19327036
Test loss (w/o reg) on all data: 0.12623943
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3125463e-05
Norm of the params: 12.750154
              Random: fixed  15 labels. Loss 0.12624. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 208
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014688467
Train loss (w/o reg) on all data: 0.00808896
Test loss (w/o reg) on all data: 0.014035786
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.511852e-07
Norm of the params: 11.488696
     Influence (LOO): fixed  98 labels. Loss 0.01404. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008288395
Train loss (w/o reg) on all data: 0.0030801431
Test loss (w/o reg) on all data: 0.011126863
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8792943e-07
Norm of the params: 10.206127
                Loss: fixed 100 labels. Loss 0.01113. Accuracy 0.996.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19637316
Train loss (w/o reg) on all data: 0.18832424
Test loss (w/o reg) on all data: 0.117815614
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0536315e-05
Norm of the params: 12.687725
              Random: fixed  20 labels. Loss 0.11782. Accuracy 0.985.
### Flips: 208, rs: 24, checks: 260
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01122447
Train loss (w/o reg) on all data: 0.006245312
Test loss (w/o reg) on all data: 0.012817383
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6549984e-07
Norm of the params: 9.979136
     Influence (LOO): fixed 101 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008288396
Train loss (w/o reg) on all data: 0.0030800034
Test loss (w/o reg) on all data: 0.011127583
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4277481e-06
Norm of the params: 10.206265
                Loss: fixed 100 labels. Loss 0.01113. Accuracy 0.996.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1897721
Train loss (w/o reg) on all data: 0.1815414
Test loss (w/o reg) on all data: 0.11323023
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4509627e-05
Norm of the params: 12.830206
              Random: fixed  24 labels. Loss 0.11323. Accuracy 0.985.
### Flips: 208, rs: 24, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012055704
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.863707e-07
Norm of the params: 9.153221
     Influence (LOO): fixed 102 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076660486
Train loss (w/o reg) on all data: 0.0028513935
Test loss (w/o reg) on all data: 0.012248188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9343874e-07
Norm of the params: 9.812905
                Loss: fixed 101 labels. Loss 0.01225. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17789814
Train loss (w/o reg) on all data: 0.16916849
Test loss (w/o reg) on all data: 0.11002765
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.971562e-05
Norm of the params: 13.213368
              Random: fixed  30 labels. Loss 0.11003. Accuracy 0.977.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23140973
Train loss (w/o reg) on all data: 0.22247854
Test loss (w/o reg) on all data: 0.16573238
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0260064e-05
Norm of the params: 13.365016
Flipped loss: 0.16573. Accuracy: 0.966
### Flips: 208, rs: 25, checks: 52
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1772107
Train loss (w/o reg) on all data: 0.16739443
Test loss (w/o reg) on all data: 0.12757123
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.090975e-05
Norm of the params: 14.011617
     Influence (LOO): fixed  35 labels. Loss 0.12757. Accuracy 0.985.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11959753
Train loss (w/o reg) on all data: 0.10373661
Test loss (w/o reg) on all data: 0.10023948
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.2019587e-05
Norm of the params: 17.810625
                Loss: fixed  52 labels. Loss 0.10024. Accuracy 0.973.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2248623
Train loss (w/o reg) on all data: 0.21585622
Test loss (w/o reg) on all data: 0.15743189
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7242039e-05
Norm of the params: 13.420939
              Random: fixed   6 labels. Loss 0.15743. Accuracy 0.973.
### Flips: 208, rs: 25, checks: 104
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114842236
Train loss (w/o reg) on all data: 0.10377753
Test loss (w/o reg) on all data: 0.092397615
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.595088e-06
Norm of the params: 14.875961
     Influence (LOO): fixed  67 labels. Loss 0.09240. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042176936
Train loss (w/o reg) on all data: 0.028173273
Test loss (w/o reg) on all data: 0.04379449
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.945688e-06
Norm of the params: 16.73539
                Loss: fixed  93 labels. Loss 0.04379. Accuracy 0.989.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22319928
Train loss (w/o reg) on all data: 0.21430135
Test loss (w/o reg) on all data: 0.15312172
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2041632e-05
Norm of the params: 13.34011
              Random: fixed   9 labels. Loss 0.15312. Accuracy 0.969.
### Flips: 208, rs: 25, checks: 156
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06066317
Train loss (w/o reg) on all data: 0.051217273
Test loss (w/o reg) on all data: 0.03499154
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.926877e-06
Norm of the params: 13.744744
     Influence (LOO): fixed  93 labels. Loss 0.03499. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012962261
Train loss (w/o reg) on all data: 0.0057717925
Test loss (w/o reg) on all data: 0.026052624
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3560181e-06
Norm of the params: 11.992054
                Loss: fixed 109 labels. Loss 0.02605. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21853083
Train loss (w/o reg) on all data: 0.20950629
Test loss (w/o reg) on all data: 0.14788444
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7270517e-05
Norm of the params: 13.434686
              Random: fixed  14 labels. Loss 0.14788. Accuracy 0.969.
### Flips: 208, rs: 25, checks: 208
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043019228
Train loss (w/o reg) on all data: 0.0357647
Test loss (w/o reg) on all data: 0.024639625
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9596766e-06
Norm of the params: 12.045351
     Influence (LOO): fixed 103 labels. Loss 0.02464. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008440813
Train loss (w/o reg) on all data: 0.0031228608
Test loss (w/o reg) on all data: 0.023275053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5028234e-07
Norm of the params: 10.313052
                Loss: fixed 113 labels. Loss 0.02328. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21444832
Train loss (w/o reg) on all data: 0.205059
Test loss (w/o reg) on all data: 0.14414291
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.1726e-05
Norm of the params: 13.703512
              Random: fixed  16 labels. Loss 0.14414. Accuracy 0.966.
### Flips: 208, rs: 25, checks: 260
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015858606
Train loss (w/o reg) on all data: 0.010131557
Test loss (w/o reg) on all data: 0.014020949
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1731478e-06
Norm of the params: 10.702381
     Influence (LOO): fixed 113 labels. Loss 0.01402. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730105
Test loss (w/o reg) on all data: 0.012054479
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1578286e-07
Norm of the params: 9.1531515
                Loss: fixed 115 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20659184
Train loss (w/o reg) on all data: 0.19791088
Test loss (w/o reg) on all data: 0.13341148
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.5768742e-05
Norm of the params: 13.176472
              Random: fixed  24 labels. Loss 0.13341. Accuracy 0.977.
### Flips: 208, rs: 25, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013471333
Train loss (w/o reg) on all data: 0.00845545
Test loss (w/o reg) on all data: 0.013947337
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.491948e-07
Norm of the params: 10.015871
     Influence (LOO): fixed 114 labels. Loss 0.01395. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729637
Test loss (w/o reg) on all data: 0.0120549565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.765018e-07
Norm of the params: 9.153203
                Loss: fixed 115 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19961625
Train loss (w/o reg) on all data: 0.19138287
Test loss (w/o reg) on all data: 0.12804104
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6676895e-05
Norm of the params: 12.832291
              Random: fixed  29 labels. Loss 0.12804. Accuracy 0.981.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20845437
Train loss (w/o reg) on all data: 0.19969724
Test loss (w/o reg) on all data: 0.13614039
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0748475e-05
Norm of the params: 13.234144
Flipped loss: 0.13614. Accuracy: 0.977
### Flips: 208, rs: 26, checks: 52
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1317846
Train loss (w/o reg) on all data: 0.11998862
Test loss (w/o reg) on all data: 0.097526416
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2273208e-05
Norm of the params: 15.359676
     Influence (LOO): fixed  39 labels. Loss 0.09753. Accuracy 0.969.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080001116
Train loss (w/o reg) on all data: 0.06350304
Test loss (w/o reg) on all data: 0.08634462
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.518871e-06
Norm of the params: 18.164843
                Loss: fixed  52 labels. Loss 0.08634. Accuracy 0.981.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20007066
Train loss (w/o reg) on all data: 0.191636
Test loss (w/o reg) on all data: 0.13557039
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.342153e-05
Norm of the params: 12.988198
              Random: fixed   6 labels. Loss 0.13557. Accuracy 0.966.
### Flips: 208, rs: 26, checks: 104
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07896561
Train loss (w/o reg) on all data: 0.06924238
Test loss (w/o reg) on all data: 0.06418935
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8276913e-05
Norm of the params: 13.945058
     Influence (LOO): fixed  70 labels. Loss 0.06419. Accuracy 0.977.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029421404
Train loss (w/o reg) on all data: 0.017178137
Test loss (w/o reg) on all data: 0.030680293
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1248785e-06
Norm of the params: 15.648174
                Loss: fixed  84 labels. Loss 0.03068. Accuracy 0.989.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19040531
Train loss (w/o reg) on all data: 0.18198223
Test loss (w/o reg) on all data: 0.12415678
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.12931275e-05
Norm of the params: 12.979273
              Random: fixed  16 labels. Loss 0.12416. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 156
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021225352
Train loss (w/o reg) on all data: 0.012581319
Test loss (w/o reg) on all data: 0.024343163
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.117729e-07
Norm of the params: 13.148409
     Influence (LOO): fixed  92 labels. Loss 0.02434. Accuracy 0.992.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013225451
Train loss (w/o reg) on all data: 0.005951167
Test loss (w/o reg) on all data: 0.017454976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8891445e-07
Norm of the params: 12.061746
                Loss: fixed  96 labels. Loss 0.01745. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18830606
Train loss (w/o reg) on all data: 0.17999719
Test loss (w/o reg) on all data: 0.123607635
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.935546e-05
Norm of the params: 12.890986
              Random: fixed  17 labels. Loss 0.12361. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 208
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010055937
Train loss (w/o reg) on all data: 0.005120472
Test loss (w/o reg) on all data: 0.014731144
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.822044e-07
Norm of the params: 9.935255
     Influence (LOO): fixed 102 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01035466
Train loss (w/o reg) on all data: 0.00428227
Test loss (w/o reg) on all data: 0.014051255
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.364386e-07
Norm of the params: 11.020335
                Loss: fixed  99 labels. Loss 0.01405. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18558027
Train loss (w/o reg) on all data: 0.17723808
Test loss (w/o reg) on all data: 0.12553684
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.14194345e-05
Norm of the params: 12.916808
              Random: fixed  19 labels. Loss 0.12554. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729332
Test loss (w/o reg) on all data: 0.012054929
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2069192e-07
Norm of the params: 9.153235
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961923
Train loss (w/o reg) on all data: 0.0030190507
Test loss (w/o reg) on all data: 0.010874844
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.290538e-07
Norm of the params: 9.942709
                Loss: fixed 102 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17617467
Train loss (w/o reg) on all data: 0.16804622
Test loss (w/o reg) on all data: 0.11474689
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1475198e-05
Norm of the params: 12.750257
              Random: fixed  26 labels. Loss 0.11475. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 312
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012054987
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3075893e-07
Norm of the params: 9.153167
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961924
Train loss (w/o reg) on all data: 0.0030191296
Test loss (w/o reg) on all data: 0.0108752875
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5307082e-07
Norm of the params: 9.94263
                Loss: fixed 102 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16419409
Train loss (w/o reg) on all data: 0.15593109
Test loss (w/o reg) on all data: 0.11086296
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3183547e-05
Norm of the params: 12.855355
              Random: fixed  32 labels. Loss 0.11086. Accuracy 0.981.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22358091
Train loss (w/o reg) on all data: 0.21602286
Test loss (w/o reg) on all data: 0.12723728
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0329224e-05
Norm of the params: 12.294757
Flipped loss: 0.12724. Accuracy: 0.989
### Flips: 208, rs: 27, checks: 52
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13765714
Train loss (w/o reg) on all data: 0.126905
Test loss (w/o reg) on all data: 0.07853734
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.40586e-06
Norm of the params: 14.664338
     Influence (LOO): fixed  42 labels. Loss 0.07854. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10210979
Train loss (w/o reg) on all data: 0.088523
Test loss (w/o reg) on all data: 0.06027782
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.561439e-06
Norm of the params: 16.48441
                Loss: fixed  52 labels. Loss 0.06028. Accuracy 0.989.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21919496
Train loss (w/o reg) on all data: 0.21109755
Test loss (w/o reg) on all data: 0.12658888
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.1813134e-05
Norm of the params: 12.725888
              Random: fixed   3 labels. Loss 0.12659. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 104
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09582838
Train loss (w/o reg) on all data: 0.08452497
Test loss (w/o reg) on all data: 0.06269679
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1309897e-05
Norm of the params: 15.035566
     Influence (LOO): fixed  62 labels. Loss 0.06270. Accuracy 0.992.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015696725
Train loss (w/o reg) on all data: 0.0072510364
Test loss (w/o reg) on all data: 0.017215464
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.599169e-07
Norm of the params: 12.996684
                Loss: fixed  96 labels. Loss 0.01722. Accuracy 0.989.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21123603
Train loss (w/o reg) on all data: 0.20285442
Test loss (w/o reg) on all data: 0.12544651
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1227657e-05
Norm of the params: 12.947276
              Random: fixed   8 labels. Loss 0.12545. Accuracy 0.989.
### Flips: 208, rs: 27, checks: 156
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047685035
Train loss (w/o reg) on all data: 0.040551167
Test loss (w/o reg) on all data: 0.031837698
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9755863e-06
Norm of the params: 11.944762
     Influence (LOO): fixed  87 labels. Loss 0.03184. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012229867
Train loss (w/o reg) on all data: 0.00532266
Test loss (w/o reg) on all data: 0.015296332
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.03246e-07
Norm of the params: 11.753474
                Loss: fixed  98 labels. Loss 0.01530. Accuracy 0.992.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20322426
Train loss (w/o reg) on all data: 0.19457865
Test loss (w/o reg) on all data: 0.11807641
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6479967e-05
Norm of the params: 13.149609
              Random: fixed  14 labels. Loss 0.11808. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 208
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020609913
Train loss (w/o reg) on all data: 0.015112801
Test loss (w/o reg) on all data: 0.02331082
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7693424e-06
Norm of the params: 10.485334
     Influence (LOO): fixed  96 labels. Loss 0.02331. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009389463
Train loss (w/o reg) on all data: 0.003666004
Test loss (w/o reg) on all data: 0.017822936
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8131974e-07
Norm of the params: 10.699027
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19750734
Train loss (w/o reg) on all data: 0.18909104
Test loss (w/o reg) on all data: 0.10998604
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4461732e-05
Norm of the params: 12.974051
              Random: fixed  19 labels. Loss 0.10999. Accuracy 0.989.
### Flips: 208, rs: 27, checks: 260
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016548716
Train loss (w/o reg) on all data: 0.011264162
Test loss (w/o reg) on all data: 0.016454617
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.954015e-06
Norm of the params: 10.280616
     Influence (LOO): fixed  97 labels. Loss 0.01645. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009389462
Train loss (w/o reg) on all data: 0.0036659015
Test loss (w/o reg) on all data: 0.017822085
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1199315e-07
Norm of the params: 10.699121
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19170745
Train loss (w/o reg) on all data: 0.1834527
Test loss (w/o reg) on all data: 0.10844673
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4985464e-05
Norm of the params: 12.848928
              Random: fixed  23 labels. Loss 0.10845. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172894
Test loss (w/o reg) on all data: 0.012054907
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3311704e-07
Norm of the params: 9.153276
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009389462
Train loss (w/o reg) on all data: 0.0036657334
Test loss (w/o reg) on all data: 0.017822862
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0697421e-06
Norm of the params: 10.69928
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18498798
Train loss (w/o reg) on all data: 0.1763508
Test loss (w/o reg) on all data: 0.100990854
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.733099e-05
Norm of the params: 13.143191
              Random: fixed  27 labels. Loss 0.10099. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20572197
Train loss (w/o reg) on all data: 0.1978057
Test loss (w/o reg) on all data: 0.12224956
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.3150306e-05
Norm of the params: 12.582739
Flipped loss: 0.12225. Accuracy: 0.981
### Flips: 208, rs: 28, checks: 52
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122041225
Train loss (w/o reg) on all data: 0.11188855
Test loss (w/o reg) on all data: 0.08583158
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0488783e-05
Norm of the params: 14.249683
     Influence (LOO): fixed  41 labels. Loss 0.08583. Accuracy 0.973.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669381
Train loss (w/o reg) on all data: 0.052850228
Test loss (w/o reg) on all data: 0.05784952
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.290247e-06
Norm of the params: 16.63946
                Loss: fixed  52 labels. Loss 0.05785. Accuracy 0.985.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19805048
Train loss (w/o reg) on all data: 0.18997188
Test loss (w/o reg) on all data: 0.116255604
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3076938e-05
Norm of the params: 12.711106
              Random: fixed   4 labels. Loss 0.11626. Accuracy 0.985.
### Flips: 208, rs: 28, checks: 104
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0590583
Train loss (w/o reg) on all data: 0.049587
Test loss (w/o reg) on all data: 0.037078053
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.605159e-06
Norm of the params: 13.763215
     Influence (LOO): fixed  68 labels. Loss 0.03708. Accuracy 0.989.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01495038
Train loss (w/o reg) on all data: 0.0070593595
Test loss (w/o reg) on all data: 0.013987138
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4099625e-06
Norm of the params: 12.562659
                Loss: fixed  82 labels. Loss 0.01399. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19390734
Train loss (w/o reg) on all data: 0.18601693
Test loss (w/o reg) on all data: 0.107279435
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.400369e-05
Norm of the params: 12.562166
              Random: fixed   9 labels. Loss 0.10728. Accuracy 0.981.
### Flips: 208, rs: 28, checks: 156
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031195235
Train loss (w/o reg) on all data: 0.023217138
Test loss (w/o reg) on all data: 0.017427053
Train acc on all data:  0.994269340974212
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4624962e-06
Norm of the params: 12.631783
     Influence (LOO): fixed  84 labels. Loss 0.01743. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013425243
Train loss (w/o reg) on all data: 0.005813372
Test loss (w/o reg) on all data: 0.010315609
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.088243e-07
Norm of the params: 12.338453
                Loss: fixed  84 labels. Loss 0.01032. Accuracy 0.996.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18588029
Train loss (w/o reg) on all data: 0.1778087
Test loss (w/o reg) on all data: 0.1032391
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.48662875e-05
Norm of the params: 12.705575
              Random: fixed  13 labels. Loss 0.10324. Accuracy 0.977.
### Flips: 208, rs: 28, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007247486
Train loss (w/o reg) on all data: 0.0026130972
Test loss (w/o reg) on all data: 0.011080453
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4018178e-06
Norm of the params: 9.627449
     Influence (LOO): fixed  93 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01002583
Train loss (w/o reg) on all data: 0.0040134043
Test loss (w/o reg) on all data: 0.010993704
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2952495e-06
Norm of the params: 10.965788
                Loss: fixed  87 labels. Loss 0.01099. Accuracy 0.996.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17295447
Train loss (w/o reg) on all data: 0.16512604
Test loss (w/o reg) on all data: 0.093011685
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.673219e-05
Norm of the params: 12.512739
              Random: fixed  20 labels. Loss 0.09301. Accuracy 0.985.
### Flips: 208, rs: 28, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730415
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5923135e-07
Norm of the params: 9.153117
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008997529
Train loss (w/o reg) on all data: 0.0035538774
Test loss (w/o reg) on all data: 0.012286925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.970977e-08
Norm of the params: 10.434224
                Loss: fixed  89 labels. Loss 0.01229. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16930239
Train loss (w/o reg) on all data: 0.16136461
Test loss (w/o reg) on all data: 0.088691756
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1371742e-05
Norm of the params: 12.599817
              Random: fixed  22 labels. Loss 0.08869. Accuracy 0.989.
### Flips: 208, rs: 28, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173089
Test loss (w/o reg) on all data: 0.012055363
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.139868e-07
Norm of the params: 9.153066
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00859732
Train loss (w/o reg) on all data: 0.0032917769
Test loss (w/o reg) on all data: 0.013553787
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0444703e-07
Norm of the params: 10.301013
                Loss: fixed  90 labels. Loss 0.01355. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15988968
Train loss (w/o reg) on all data: 0.15188234
Test loss (w/o reg) on all data: 0.083543964
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.777751e-06
Norm of the params: 12.654917
              Random: fixed  28 labels. Loss 0.08354. Accuracy 0.989.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22933921
Train loss (w/o reg) on all data: 0.22068372
Test loss (w/o reg) on all data: 0.15690784
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4038814e-05
Norm of the params: 13.157124
Flipped loss: 0.15691. Accuracy: 0.981
### Flips: 208, rs: 29, checks: 52
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16818257
Train loss (w/o reg) on all data: 0.15853654
Test loss (w/o reg) on all data: 0.12786543
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.4655823e-06
Norm of the params: 13.889589
     Influence (LOO): fixed  36 labels. Loss 0.12787. Accuracy 0.977.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11088135
Train loss (w/o reg) on all data: 0.0964571
Test loss (w/o reg) on all data: 0.10088162
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1471042e-05
Norm of the params: 16.984846
                Loss: fixed  51 labels. Loss 0.10088. Accuracy 0.950.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21914966
Train loss (w/o reg) on all data: 0.21047963
Test loss (w/o reg) on all data: 0.14391239
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4597403e-05
Norm of the params: 13.168166
              Random: fixed   9 labels. Loss 0.14391. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 104
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104435675
Train loss (w/o reg) on all data: 0.09572266
Test loss (w/o reg) on all data: 0.080852725
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0382764e-05
Norm of the params: 13.2007675
     Influence (LOO): fixed  70 labels. Loss 0.08085. Accuracy 0.985.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03062396
Train loss (w/o reg) on all data: 0.01850315
Test loss (w/o reg) on all data: 0.026683776
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8871162e-06
Norm of the params: 15.569721
                Loss: fixed  95 labels. Loss 0.02668. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21336392
Train loss (w/o reg) on all data: 0.20487115
Test loss (w/o reg) on all data: 0.13719963
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.9998154e-05
Norm of the params: 13.032861
              Random: fixed  14 labels. Loss 0.13720. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 156
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045006067
Train loss (w/o reg) on all data: 0.036939315
Test loss (w/o reg) on all data: 0.033627227
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.1689993e-06
Norm of the params: 12.701771
     Influence (LOO): fixed  97 labels. Loss 0.03363. Accuracy 0.996.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011110355
Train loss (w/o reg) on all data: 0.0049206465
Test loss (w/o reg) on all data: 0.015926294
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8872563e-07
Norm of the params: 11.126283
                Loss: fixed 107 labels. Loss 0.01593. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21185027
Train loss (w/o reg) on all data: 0.20379607
Test loss (w/o reg) on all data: 0.13645017
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.4680237e-05
Norm of the params: 12.69189
              Random: fixed  16 labels. Loss 0.13645. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020014398
Train loss (w/o reg) on all data: 0.013990356
Test loss (w/o reg) on all data: 0.01719414
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.982239e-07
Norm of the params: 10.976377
     Influence (LOO): fixed 107 labels. Loss 0.01719. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011024869
Train loss (w/o reg) on all data: 0.004807843
Test loss (w/o reg) on all data: 0.014994581
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9189194e-07
Norm of the params: 11.150808
                Loss: fixed 108 labels. Loss 0.01499. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2072547
Train loss (w/o reg) on all data: 0.19943674
Test loss (w/o reg) on all data: 0.13171978
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.3590502e-05
Norm of the params: 12.504366
              Random: fixed  21 labels. Loss 0.13172. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 260
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011766946
Train loss (w/o reg) on all data: 0.00566863
Test loss (w/o reg) on all data: 0.009211438
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.530023e-06
Norm of the params: 11.043837
     Influence (LOO): fixed 109 labels. Loss 0.00921. Accuracy 0.996.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173002
Test loss (w/o reg) on all data: 0.012055218
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4108213e-07
Norm of the params: 9.153159
                Loss: fixed 111 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200034
Train loss (w/o reg) on all data: 0.1923681
Test loss (w/o reg) on all data: 0.12466946
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1201674e-05
Norm of the params: 12.382163
              Random: fixed  26 labels. Loss 0.12467. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012054928
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.003174e-07
Norm of the params: 9.153185
     Influence (LOO): fixed 111 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729774
Test loss (w/o reg) on all data: 0.012054842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.067301e-07
Norm of the params: 9.153187
                Loss: fixed 111 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1956071
Train loss (w/o reg) on all data: 0.18786545
Test loss (w/o reg) on all data: 0.120077245
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.083073e-05
Norm of the params: 12.443181
              Random: fixed  29 labels. Loss 0.12008. Accuracy 0.981.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21515663
Train loss (w/o reg) on all data: 0.20786054
Test loss (w/o reg) on all data: 0.13623919
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.230588e-06
Norm of the params: 12.0798025
Flipped loss: 0.13624. Accuracy: 0.977
### Flips: 208, rs: 30, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13216002
Train loss (w/o reg) on all data: 0.122470535
Test loss (w/o reg) on all data: 0.09447589
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2463852e-05
Norm of the params: 13.920838
     Influence (LOO): fixed  40 labels. Loss 0.09448. Accuracy 0.985.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08590748
Train loss (w/o reg) on all data: 0.07260104
Test loss (w/o reg) on all data: 0.09124136
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.236732e-05
Norm of the params: 16.313454
                Loss: fixed  52 labels. Loss 0.09124. Accuracy 0.962.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20342897
Train loss (w/o reg) on all data: 0.19581698
Test loss (w/o reg) on all data: 0.12921123
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2603875e-05
Norm of the params: 12.338552
              Random: fixed   7 labels. Loss 0.12921. Accuracy 0.981.
### Flips: 208, rs: 30, checks: 104
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07810958
Train loss (w/o reg) on all data: 0.06704328
Test loss (w/o reg) on all data: 0.07256791
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.0211128e-06
Norm of the params: 14.877024
     Influence (LOO): fixed  62 labels. Loss 0.07257. Accuracy 0.981.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011156082
Train loss (w/o reg) on all data: 0.0047607855
Test loss (w/o reg) on all data: 0.01158005
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0331302e-07
Norm of the params: 11.309551
                Loss: fixed  91 labels. Loss 0.01158. Accuracy 0.996.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19342186
Train loss (w/o reg) on all data: 0.18590944
Test loss (w/o reg) on all data: 0.12495096
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.547904e-05
Norm of the params: 12.257581
              Random: fixed  12 labels. Loss 0.12495. Accuracy 0.977.
### Flips: 208, rs: 30, checks: 156
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031124776
Train loss (w/o reg) on all data: 0.022720635
Test loss (w/o reg) on all data: 0.022944747
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.121617e-06
Norm of the params: 12.964676
     Influence (LOO): fixed  84 labels. Loss 0.02294. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833845
Train loss (w/o reg) on all data: 0.0028984365
Test loss (w/o reg) on all data: 0.012001028
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6588804e-07
Norm of the params: 9.884279
                Loss: fixed  94 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18464634
Train loss (w/o reg) on all data: 0.17687836
Test loss (w/o reg) on all data: 0.12256244
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6364893e-05
Norm of the params: 12.464334
              Random: fixed  18 labels. Loss 0.12256. Accuracy 0.973.
### Flips: 208, rs: 30, checks: 208
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020229729
Train loss (w/o reg) on all data: 0.0141061805
Test loss (w/o reg) on all data: 0.017198335
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9028003e-07
Norm of the params: 11.066661
     Influence (LOO): fixed  90 labels. Loss 0.01720. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783382
Train loss (w/o reg) on all data: 0.002898185
Test loss (w/o reg) on all data: 0.012001955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.943969e-07
Norm of the params: 9.88453
                Loss: fixed  94 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18251398
Train loss (w/o reg) on all data: 0.17457457
Test loss (w/o reg) on all data: 0.12363394
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9319888e-05
Norm of the params: 12.601121
              Random: fixed  20 labels. Loss 0.12363. Accuracy 0.977.
### Flips: 208, rs: 30, checks: 260
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.0054476536
Test loss (w/o reg) on all data: 0.012244488
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.63268e-07
Norm of the params: 9.544319
     Influence (LOO): fixed  94 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.012055454
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8069406e-07
Norm of the params: 9.153168
                Loss: fixed  95 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17512739
Train loss (w/o reg) on all data: 0.16752973
Test loss (w/o reg) on all data: 0.11405842
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0636988e-05
Norm of the params: 12.326927
              Random: fixed  25 labels. Loss 0.11406. Accuracy 0.981.
### Flips: 208, rs: 30, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730643
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1795985e-06
Norm of the params: 9.153091
     Influence (LOO): fixed  95 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730624
Test loss (w/o reg) on all data: 0.012055072
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8215902e-07
Norm of the params: 9.153092
                Loss: fixed  95 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16514717
Train loss (w/o reg) on all data: 0.15764022
Test loss (w/o reg) on all data: 0.107940085
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1294625e-05
Norm of the params: 12.253119
              Random: fixed  30 labels. Loss 0.10794. Accuracy 0.977.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21678115
Train loss (w/o reg) on all data: 0.2087745
Test loss (w/o reg) on all data: 0.14422956
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.979187e-05
Norm of the params: 12.65436
Flipped loss: 0.14423. Accuracy: 0.973
### Flips: 208, rs: 31, checks: 52
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13678934
Train loss (w/o reg) on all data: 0.12707485
Test loss (w/o reg) on all data: 0.104118854
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5408137e-05
Norm of the params: 13.938778
     Influence (LOO): fixed  39 labels. Loss 0.10412. Accuracy 0.966.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092647776
Train loss (w/o reg) on all data: 0.077133894
Test loss (w/o reg) on all data: 0.10418411
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.7757596e-06
Norm of the params: 17.614697
                Loss: fixed  51 labels. Loss 0.10418. Accuracy 0.962.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20917517
Train loss (w/o reg) on all data: 0.20081697
Test loss (w/o reg) on all data: 0.14017789
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.483359e-05
Norm of the params: 12.929185
              Random: fixed   4 labels. Loss 0.14018. Accuracy 0.962.
### Flips: 208, rs: 31, checks: 104
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090003304
Train loss (w/o reg) on all data: 0.07978544
Test loss (w/o reg) on all data: 0.07394664
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0958142e-06
Norm of the params: 14.295363
     Influence (LOO): fixed  66 labels. Loss 0.07395. Accuracy 0.973.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023812678
Train loss (w/o reg) on all data: 0.013650586
Test loss (w/o reg) on all data: 0.027277667
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7110976e-06
Norm of the params: 14.25629
                Loss: fixed  92 labels. Loss 0.02728. Accuracy 0.989.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20006031
Train loss (w/o reg) on all data: 0.19171664
Test loss (w/o reg) on all data: 0.13113673
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8814886e-05
Norm of the params: 12.917948
              Random: fixed  10 labels. Loss 0.13114. Accuracy 0.966.
### Flips: 208, rs: 31, checks: 156
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043879107
Train loss (w/o reg) on all data: 0.034286987
Test loss (w/o reg) on all data: 0.033407554
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4577623e-06
Norm of the params: 13.8507185
     Influence (LOO): fixed  91 labels. Loss 0.03341. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014471242
Train loss (w/o reg) on all data: 0.006671182
Test loss (w/o reg) on all data: 0.014809429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2593215e-07
Norm of the params: 12.490045
                Loss: fixed 101 labels. Loss 0.01481. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1938245
Train loss (w/o reg) on all data: 0.18553318
Test loss (w/o reg) on all data: 0.117492974
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6434731e-05
Norm of the params: 12.87736
              Random: fixed  16 labels. Loss 0.11749. Accuracy 0.973.
### Flips: 208, rs: 31, checks: 208
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014138756
Train loss (w/o reg) on all data: 0.007958585
Test loss (w/o reg) on all data: 0.0133469915
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7367261e-06
Norm of the params: 11.117708
     Influence (LOO): fixed 105 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011093646
Train loss (w/o reg) on all data: 0.0045187995
Test loss (w/o reg) on all data: 0.0102124205
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.463072e-07
Norm of the params: 11.467213
                Loss: fixed 104 labels. Loss 0.01021. Accuracy 0.996.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19191074
Train loss (w/o reg) on all data: 0.18378277
Test loss (w/o reg) on all data: 0.1133174
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3770855e-05
Norm of the params: 12.749881
              Random: fixed  20 labels. Loss 0.11332. Accuracy 0.977.
### Flips: 208, rs: 31, checks: 260
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009260062
Train loss (w/o reg) on all data: 0.004402216
Test loss (w/o reg) on all data: 0.012443293
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4871781e-06
Norm of the params: 9.856822
     Influence (LOO): fixed 107 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009233543
Train loss (w/o reg) on all data: 0.0035737816
Test loss (w/o reg) on all data: 0.011998814
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.365788e-07
Norm of the params: 10.639324
                Loss: fixed 105 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18474376
Train loss (w/o reg) on all data: 0.17723185
Test loss (w/o reg) on all data: 0.095902346
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.3311782e-05
Norm of the params: 12.257172
              Random: fixed  29 labels. Loss 0.09590. Accuracy 0.981.
### Flips: 208, rs: 31, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729292
Test loss (w/o reg) on all data: 0.012056344
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1309923e-07
Norm of the params: 9.153239
     Influence (LOO): fixed 108 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007829888
Train loss (w/o reg) on all data: 0.0028235405
Test loss (w/o reg) on all data: 0.012408977
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7643804e-07
Norm of the params: 10.006345
                Loss: fixed 106 labels. Loss 0.01241. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17520112
Train loss (w/o reg) on all data: 0.1676986
Test loss (w/o reg) on all data: 0.09291818
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2340882e-05
Norm of the params: 12.249502
              Random: fixed  35 labels. Loss 0.09292. Accuracy 0.985.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21538608
Train loss (w/o reg) on all data: 0.20762087
Test loss (w/o reg) on all data: 0.13759159
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.789334e-05
Norm of the params: 12.462103
Flipped loss: 0.13759. Accuracy: 0.966
### Flips: 208, rs: 32, checks: 52
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13773714
Train loss (w/o reg) on all data: 0.12733263
Test loss (w/o reg) on all data: 0.10818356
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5383954e-05
Norm of the params: 14.425331
     Influence (LOO): fixed  39 labels. Loss 0.10818. Accuracy 0.962.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09327926
Train loss (w/o reg) on all data: 0.07891259
Test loss (w/o reg) on all data: 0.12279744
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.1910437e-06
Norm of the params: 16.950909
                Loss: fixed  52 labels. Loss 0.12280. Accuracy 0.939.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2071089
Train loss (w/o reg) on all data: 0.19901314
Test loss (w/o reg) on all data: 0.12964714
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 0.00013630411
Norm of the params: 12.724591
              Random: fixed   5 labels. Loss 0.12965. Accuracy 0.966.
### Flips: 208, rs: 32, checks: 104
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07834821
Train loss (w/o reg) on all data: 0.06761166
Test loss (w/o reg) on all data: 0.06424979
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0526218e-05
Norm of the params: 14.653706
     Influence (LOO): fixed  68 labels. Loss 0.06425. Accuracy 0.985.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021440886
Train loss (w/o reg) on all data: 0.01147155
Test loss (w/o reg) on all data: 0.0442329
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9251853e-06
Norm of the params: 14.120438
                Loss: fixed  92 labels. Loss 0.04423. Accuracy 0.989.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19687814
Train loss (w/o reg) on all data: 0.1881351
Test loss (w/o reg) on all data: 0.11945032
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.786104e-05
Norm of the params: 13.22349
              Random: fixed  13 labels. Loss 0.11945. Accuracy 0.969.
### Flips: 208, rs: 32, checks: 156
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0361585
Train loss (w/o reg) on all data: 0.026123196
Test loss (w/o reg) on all data: 0.03625217
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5094479e-06
Norm of the params: 14.167076
     Influence (LOO): fixed  91 labels. Loss 0.03625. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014449155
Train loss (w/o reg) on all data: 0.006462554
Test loss (w/o reg) on all data: 0.01928066
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4731815e-07
Norm of the params: 12.638514
                Loss: fixed  97 labels. Loss 0.01928. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18767492
Train loss (w/o reg) on all data: 0.17886338
Test loss (w/o reg) on all data: 0.114377365
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6081447e-05
Norm of the params: 13.275205
              Random: fixed  18 labels. Loss 0.11438. Accuracy 0.981.
### Flips: 208, rs: 32, checks: 208
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02629283
Train loss (w/o reg) on all data: 0.01825936
Test loss (w/o reg) on all data: 0.024313023
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3333355e-06
Norm of the params: 12.675544
     Influence (LOO): fixed  97 labels. Loss 0.02431. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012141865
Train loss (w/o reg) on all data: 0.005142481
Test loss (w/o reg) on all data: 0.012830537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1439339e-06
Norm of the params: 11.831639
                Loss: fixed 100 labels. Loss 0.01283. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18365672
Train loss (w/o reg) on all data: 0.17596424
Test loss (w/o reg) on all data: 0.10656814
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7028498e-05
Norm of the params: 12.403618
              Random: fixed  23 labels. Loss 0.10657. Accuracy 0.989.
### Flips: 208, rs: 32, checks: 260
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017210426
Train loss (w/o reg) on all data: 0.011103638
Test loss (w/o reg) on all data: 0.01919768
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.556203e-07
Norm of the params: 11.051505
     Influence (LOO): fixed 100 labels. Loss 0.01920. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961925
Train loss (w/o reg) on all data: 0.0030191727
Test loss (w/o reg) on all data: 0.010875244
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0803253e-07
Norm of the params: 9.942588
                Loss: fixed 102 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17640018
Train loss (w/o reg) on all data: 0.168561
Test loss (w/o reg) on all data: 0.10652497
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.7639733e-05
Norm of the params: 12.521333
              Random: fixed  27 labels. Loss 0.10652. Accuracy 0.973.
### Flips: 208, rs: 32, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728664
Test loss (w/o reg) on all data: 0.012055309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.805962e-07
Norm of the params: 9.153308
     Influence (LOO): fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961924
Train loss (w/o reg) on all data: 0.0030192554
Test loss (w/o reg) on all data: 0.010874889
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.21462e-07
Norm of the params: 9.942504
                Loss: fixed 102 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17640021
Train loss (w/o reg) on all data: 0.16856351
Test loss (w/o reg) on all data: 0.106487006
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0316225e-05
Norm of the params: 12.519341
              Random: fixed  27 labels. Loss 0.10649. Accuracy 0.973.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19357626
Train loss (w/o reg) on all data: 0.18515009
Test loss (w/o reg) on all data: 0.11625435
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.37958705e-05
Norm of the params: 12.981663
Flipped loss: 0.11625. Accuracy: 0.966
### Flips: 208, rs: 33, checks: 52
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10742147
Train loss (w/o reg) on all data: 0.09778555
Test loss (w/o reg) on all data: 0.088585295
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.115893e-05
Norm of the params: 13.882309
     Influence (LOO): fixed  42 labels. Loss 0.08859. Accuracy 0.962.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06663035
Train loss (w/o reg) on all data: 0.05142064
Test loss (w/o reg) on all data: 0.099099636
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.6550504e-06
Norm of the params: 17.441162
                Loss: fixed  51 labels. Loss 0.09910. Accuracy 0.958.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19175307
Train loss (w/o reg) on all data: 0.1833045
Test loss (w/o reg) on all data: 0.113896504
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.6671692e-05
Norm of the params: 12.9989
              Random: fixed   1 labels. Loss 0.11390. Accuracy 0.962.
### Flips: 208, rs: 33, checks: 104
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050653975
Train loss (w/o reg) on all data: 0.04162316
Test loss (w/o reg) on all data: 0.044808295
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.909919e-06
Norm of the params: 13.439358
     Influence (LOO): fixed  71 labels. Loss 0.04481. Accuracy 0.977.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018914517
Train loss (w/o reg) on all data: 0.009843712
Test loss (w/o reg) on all data: 0.023815257
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.4271974e-07
Norm of the params: 13.469079
                Loss: fixed  80 labels. Loss 0.02382. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18712926
Train loss (w/o reg) on all data: 0.17854613
Test loss (w/o reg) on all data: 0.10372075
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.241474e-05
Norm of the params: 13.102004
              Random: fixed   7 labels. Loss 0.10372. Accuracy 0.981.
### Flips: 208, rs: 33, checks: 156
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019139767
Train loss (w/o reg) on all data: 0.0122517375
Test loss (w/o reg) on all data: 0.01436987
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2314473e-06
Norm of the params: 11.737146
     Influence (LOO): fixed  84 labels. Loss 0.01437. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010432987
Train loss (w/o reg) on all data: 0.0044827466
Test loss (w/o reg) on all data: 0.0061428193
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.8092004e-07
Norm of the params: 10.908932
                Loss: fixed  86 labels. Loss 0.00614. Accuracy 0.996.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18340297
Train loss (w/o reg) on all data: 0.17474547
Test loss (w/o reg) on all data: 0.103650816
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.2265117e-05
Norm of the params: 13.158645
              Random: fixed   9 labels. Loss 0.10365. Accuracy 0.973.
### Flips: 208, rs: 33, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012055347
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6277527e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318257
Train loss (w/o reg) on all data: 0.0021987346
Test loss (w/o reg) on all data: 0.0064332327
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.607422e-07
Norm of the params: 9.076919
                Loss: fixed  89 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1816903
Train loss (w/o reg) on all data: 0.17299949
Test loss (w/o reg) on all data: 0.10146971
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 0.00011283026
Norm of the params: 13.183948
              Random: fixed  11 labels. Loss 0.10147. Accuracy 0.977.
### Flips: 208, rs: 33, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729297
Test loss (w/o reg) on all data: 0.012054435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.215721e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  90 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318258
Train loss (w/o reg) on all data: 0.0021986987
Test loss (w/o reg) on all data: 0.00643278
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.006931e-08
Norm of the params: 9.076959
                Loss: fixed  89 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17081429
Train loss (w/o reg) on all data: 0.16210057
Test loss (w/o reg) on all data: 0.091426596
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4904958e-05
Norm of the params: 13.2013
              Random: fixed  20 labels. Loss 0.09143. Accuracy 0.977.
### Flips: 208, rs: 33, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.698337e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172964
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6538601e-07
Norm of the params: 9.153201
                Loss: fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16581476
Train loss (w/o reg) on all data: 0.15747789
Test loss (w/o reg) on all data: 0.093483105
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3147908e-05
Norm of the params: 12.9126835
              Random: fixed  23 labels. Loss 0.09348. Accuracy 0.973.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21376723
Train loss (w/o reg) on all data: 0.20528662
Test loss (w/o reg) on all data: 0.16361701
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.33941885e-05
Norm of the params: 13.02353
Flipped loss: 0.16362. Accuracy: 0.969
### Flips: 208, rs: 34, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13790205
Train loss (w/o reg) on all data: 0.12747942
Test loss (w/o reg) on all data: 0.12094158
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5498828e-05
Norm of the params: 14.437893
     Influence (LOO): fixed  36 labels. Loss 0.12094. Accuracy 0.969.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090263866
Train loss (w/o reg) on all data: 0.07379716
Test loss (w/o reg) on all data: 0.10546538
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.050588e-05
Norm of the params: 18.147564
                Loss: fixed  52 labels. Loss 0.10547. Accuracy 0.966.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19669311
Train loss (w/o reg) on all data: 0.18757232
Test loss (w/o reg) on all data: 0.15663835
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.6333519e-05
Norm of the params: 13.506143
              Random: fixed  12 labels. Loss 0.15664. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 104
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07727299
Train loss (w/o reg) on all data: 0.065867946
Test loss (w/o reg) on all data: 0.07985387
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.883472e-05
Norm of the params: 15.103009
     Influence (LOO): fixed  66 labels. Loss 0.07985. Accuracy 0.977.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030056752
Train loss (w/o reg) on all data: 0.018632818
Test loss (w/o reg) on all data: 0.040329922
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9762413e-06
Norm of the params: 15.11551
                Loss: fixed  88 labels. Loss 0.04033. Accuracy 0.992.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1935663
Train loss (w/o reg) on all data: 0.18473455
Test loss (w/o reg) on all data: 0.15192145
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1722403e-05
Norm of the params: 13.290413
              Random: fixed  16 labels. Loss 0.15192. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 156
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0506208
Train loss (w/o reg) on all data: 0.039508168
Test loss (w/o reg) on all data: 0.063870616
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1095818e-06
Norm of the params: 14.90814
     Influence (LOO): fixed  80 labels. Loss 0.06387. Accuracy 0.977.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011841617
Train loss (w/o reg) on all data: 0.0049350685
Test loss (w/o reg) on all data: 0.021603648
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.212169e-07
Norm of the params: 11.752913
                Loss: fixed  99 labels. Loss 0.02160. Accuracy 0.989.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18758684
Train loss (w/o reg) on all data: 0.17836948
Test loss (w/o reg) on all data: 0.14532337
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.3906796e-05
Norm of the params: 13.577453
              Random: fixed  20 labels. Loss 0.14532. Accuracy 0.958.
### Flips: 208, rs: 34, checks: 208
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032839023
Train loss (w/o reg) on all data: 0.023544664
Test loss (w/o reg) on all data: 0.041225735
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.37435e-06
Norm of the params: 13.634044
     Influence (LOO): fixed  93 labels. Loss 0.04123. Accuracy 0.981.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0118416175
Train loss (w/o reg) on all data: 0.0049354113
Test loss (w/o reg) on all data: 0.021599652
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7516584e-06
Norm of the params: 11.752622
                Loss: fixed  99 labels. Loss 0.02160. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18023145
Train loss (w/o reg) on all data: 0.17097484
Test loss (w/o reg) on all data: 0.13227157
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5490039e-05
Norm of the params: 13.606332
              Random: fixed  26 labels. Loss 0.13227. Accuracy 0.962.
### Flips: 208, rs: 34, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011788502
Train loss (w/o reg) on all data: 0.0064335214
Test loss (w/o reg) on all data: 0.015157614
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9713595e-07
Norm of the params: 10.348894
     Influence (LOO): fixed 102 labels. Loss 0.01516. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011146606
Train loss (w/o reg) on all data: 0.004580625
Test loss (w/o reg) on all data: 0.02032974
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.564565e-07
Norm of the params: 11.459477
                Loss: fixed 100 labels. Loss 0.02033. Accuracy 0.989.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17175004
Train loss (w/o reg) on all data: 0.16261774
Test loss (w/o reg) on all data: 0.12932982
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.118841e-05
Norm of the params: 13.514659
              Random: fixed  33 labels. Loss 0.12933. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002355
Train loss (w/o reg) on all data: 0.0054478343
Test loss (w/o reg) on all data: 0.012245603
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.8479896e-07
Norm of the params: 9.544129
     Influence (LOO): fixed 103 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071804468
Train loss (w/o reg) on all data: 0.0026112068
Test loss (w/o reg) on all data: 0.011524379
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.668483e-07
Norm of the params: 9.55954
                Loss: fixed 103 labels. Loss 0.01152. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16595413
Train loss (w/o reg) on all data: 0.15683027
Test loss (w/o reg) on all data: 0.123741515
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.123192e-06
Norm of the params: 13.508414
              Random: fixed  37 labels. Loss 0.12374. Accuracy 0.962.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22840974
Train loss (w/o reg) on all data: 0.22106907
Test loss (w/o reg) on all data: 0.1435732
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3052972e-05
Norm of the params: 12.116662
Flipped loss: 0.14357. Accuracy: 0.973
### Flips: 208, rs: 35, checks: 52
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15272102
Train loss (w/o reg) on all data: 0.14356148
Test loss (w/o reg) on all data: 0.08877758
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.181424e-05
Norm of the params: 13.534791
     Influence (LOO): fixed  43 labels. Loss 0.08878. Accuracy 0.985.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09862733
Train loss (w/o reg) on all data: 0.08407421
Test loss (w/o reg) on all data: 0.12214772
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.8846458e-05
Norm of the params: 17.060555
                Loss: fixed  52 labels. Loss 0.12215. Accuracy 0.943.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22386351
Train loss (w/o reg) on all data: 0.21690825
Test loss (w/o reg) on all data: 0.1428264
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5918813e-05
Norm of the params: 11.794297
              Random: fixed   6 labels. Loss 0.14283. Accuracy 0.973.
### Flips: 208, rs: 35, checks: 104
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09772858
Train loss (w/o reg) on all data: 0.08924212
Test loss (w/o reg) on all data: 0.046572156
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.18864e-06
Norm of the params: 13.028014
     Influence (LOO): fixed  72 labels. Loss 0.04657. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024789972
Train loss (w/o reg) on all data: 0.013995063
Test loss (w/o reg) on all data: 0.017570881
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.657304e-06
Norm of the params: 14.693474
                Loss: fixed  95 labels. Loss 0.01757. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2158494
Train loss (w/o reg) on all data: 0.2089145
Test loss (w/o reg) on all data: 0.126628
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9761823e-05
Norm of the params: 11.777009
              Random: fixed  13 labels. Loss 0.12663. Accuracy 0.977.
### Flips: 208, rs: 35, checks: 156
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03911893
Train loss (w/o reg) on all data: 0.029549282
Test loss (w/o reg) on all data: 0.023473207
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2441593e-06
Norm of the params: 13.834485
     Influence (LOO): fixed  93 labels. Loss 0.02347. Accuracy 0.989.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010307571
Train loss (w/o reg) on all data: 0.0041070483
Test loss (w/o reg) on all data: 0.014144883
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7608556e-07
Norm of the params: 11.135998
                Loss: fixed 103 labels. Loss 0.01414. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2059872
Train loss (w/o reg) on all data: 0.19926155
Test loss (w/o reg) on all data: 0.10768355
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.42156905e-05
Norm of the params: 11.59798
              Random: fixed  19 labels. Loss 0.10768. Accuracy 0.992.
### Flips: 208, rs: 35, checks: 208
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0214835
Train loss (w/o reg) on all data: 0.01498146
Test loss (w/o reg) on all data: 0.019156577
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7406077e-06
Norm of the params: 11.403544
     Influence (LOO): fixed 101 labels. Loss 0.01916. Accuracy 0.989.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00872796
Train loss (w/o reg) on all data: 0.0033851808
Test loss (w/o reg) on all data: 0.0118346885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8828904e-07
Norm of the params: 10.337097
                Loss: fixed 104 labels. Loss 0.01183. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20236732
Train loss (w/o reg) on all data: 0.1953279
Test loss (w/o reg) on all data: 0.10295513
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9210354e-05
Norm of the params: 11.865435
              Random: fixed  21 labels. Loss 0.10296. Accuracy 0.992.
### Flips: 208, rs: 35, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292863
Train loss (w/o reg) on all data: 0.0025757717
Test loss (w/o reg) on all data: 0.011689831
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7197216e-07
Norm of the params: 9.712972
     Influence (LOO): fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292863
Train loss (w/o reg) on all data: 0.002575773
Test loss (w/o reg) on all data: 0.011689744
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.931742e-07
Norm of the params: 9.712971
                Loss: fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19325763
Train loss (w/o reg) on all data: 0.18644288
Test loss (w/o reg) on all data: 0.09172089
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3353823e-05
Norm of the params: 11.674539
              Random: fixed  28 labels. Loss 0.09172. Accuracy 0.989.
### Flips: 208, rs: 35, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292862
Train loss (w/o reg) on all data: 0.0025758056
Test loss (w/o reg) on all data: 0.011689463
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7911067e-07
Norm of the params: 9.712936
     Influence (LOO): fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072928625
Train loss (w/o reg) on all data: 0.0025758068
Test loss (w/o reg) on all data: 0.011689543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.003041e-07
Norm of the params: 9.712935
                Loss: fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17847936
Train loss (w/o reg) on all data: 0.17131396
Test loss (w/o reg) on all data: 0.08491157
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7561299e-05
Norm of the params: 11.971138
              Random: fixed  35 labels. Loss 0.08491. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22079998
Train loss (w/o reg) on all data: 0.21136765
Test loss (w/o reg) on all data: 0.14678113
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4342646e-05
Norm of the params: 13.734865
Flipped loss: 0.14678. Accuracy: 0.981
### Flips: 208, rs: 36, checks: 52
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14594063
Train loss (w/o reg) on all data: 0.13363276
Test loss (w/o reg) on all data: 0.104288936
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.072528e-06
Norm of the params: 15.689404
     Influence (LOO): fixed  39 labels. Loss 0.10429. Accuracy 0.973.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1025883
Train loss (w/o reg) on all data: 0.08588069
Test loss (w/o reg) on all data: 0.10575771
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.8040275e-06
Norm of the params: 18.27983
                Loss: fixed  52 labels. Loss 0.10576. Accuracy 0.958.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21308082
Train loss (w/o reg) on all data: 0.2034664
Test loss (w/o reg) on all data: 0.13140042
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3221204e-05
Norm of the params: 13.866814
              Random: fixed   6 labels. Loss 0.13140. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098168604
Train loss (w/o reg) on all data: 0.0865723
Test loss (w/o reg) on all data: 0.07826254
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.785361e-05
Norm of the params: 15.22912
     Influence (LOO): fixed  64 labels. Loss 0.07826. Accuracy 0.977.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029747605
Train loss (w/o reg) on all data: 0.017371586
Test loss (w/o reg) on all data: 0.030620735
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6024295e-06
Norm of the params: 15.732781
                Loss: fixed  94 labels. Loss 0.03062. Accuracy 0.992.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203917
Train loss (w/o reg) on all data: 0.1943827
Test loss (w/o reg) on all data: 0.12102521
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.8506005e-05
Norm of the params: 13.808909
              Random: fixed  13 labels. Loss 0.12103. Accuracy 0.981.
### Flips: 208, rs: 36, checks: 156
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05873735
Train loss (w/o reg) on all data: 0.04870803
Test loss (w/o reg) on all data: 0.06438214
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.513151e-06
Norm of the params: 14.162853
     Influence (LOO): fixed  83 labels. Loss 0.06438. Accuracy 0.973.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015464726
Train loss (w/o reg) on all data: 0.007003991
Test loss (w/o reg) on all data: 0.011572986
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1029357e-06
Norm of the params: 13.008255
                Loss: fixed 101 labels. Loss 0.01157. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19598722
Train loss (w/o reg) on all data: 0.18644306
Test loss (w/o reg) on all data: 0.11466926
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6200349e-05
Norm of the params: 13.816056
              Random: fixed  18 labels. Loss 0.11467. Accuracy 0.977.
### Flips: 208, rs: 36, checks: 208
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025853634
Train loss (w/o reg) on all data: 0.016903209
Test loss (w/o reg) on all data: 0.03538417
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1745968e-06
Norm of the params: 13.379407
     Influence (LOO): fixed  98 labels. Loss 0.03538. Accuracy 0.992.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012556652
Train loss (w/o reg) on all data: 0.005587484
Test loss (w/o reg) on all data: 0.012384237
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0574349e-06
Norm of the params: 11.806072
                Loss: fixed 104 labels. Loss 0.01238. Accuracy 0.996.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19454533
Train loss (w/o reg) on all data: 0.18477598
Test loss (w/o reg) on all data: 0.11531713
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1290174e-05
Norm of the params: 13.978095
              Random: fixed  19 labels. Loss 0.11532. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 260
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489635
Train loss (w/o reg) on all data: 0.0055393665
Test loss (w/o reg) on all data: 0.01384602
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.462305e-07
Norm of the params: 9.950145
     Influence (LOO): fixed 105 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009010771
Train loss (w/o reg) on all data: 0.003476791
Test loss (w/o reg) on all data: 0.012949477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2533066e-07
Norm of the params: 10.520438
                Loss: fixed 105 labels. Loss 0.01295. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19250642
Train loss (w/o reg) on all data: 0.1829814
Test loss (w/o reg) on all data: 0.110425025
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7980674e-05
Norm of the params: 13.802188
              Random: fixed  21 labels. Loss 0.11043. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.01205574
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6133974e-07
Norm of the params: 9.153157
     Influence (LOO): fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055819
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8255408e-07
Norm of the params: 9.153158
                Loss: fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18427187
Train loss (w/o reg) on all data: 0.1749742
Test loss (w/o reg) on all data: 0.101724274
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2822692e-05
Norm of the params: 13.636472
              Random: fixed  25 labels. Loss 0.10172. Accuracy 0.989.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20879799
Train loss (w/o reg) on all data: 0.200381
Test loss (w/o reg) on all data: 0.12837118
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7954808e-05
Norm of the params: 12.974591
Flipped loss: 0.12837. Accuracy: 0.977
### Flips: 208, rs: 37, checks: 52
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13123018
Train loss (w/o reg) on all data: 0.1187119
Test loss (w/o reg) on all data: 0.10093808
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.183285e-05
Norm of the params: 15.82294
     Influence (LOO): fixed  40 labels. Loss 0.10094. Accuracy 0.969.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08221001
Train loss (w/o reg) on all data: 0.065778546
Test loss (w/o reg) on all data: 0.08966118
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.3987544e-06
Norm of the params: 18.128136
                Loss: fixed  52 labels. Loss 0.08966. Accuracy 0.969.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20753367
Train loss (w/o reg) on all data: 0.19907509
Test loss (w/o reg) on all data: 0.12436827
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4763623e-05
Norm of the params: 13.006605
              Random: fixed   2 labels. Loss 0.12437. Accuracy 0.981.
### Flips: 208, rs: 37, checks: 104
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07742693
Train loss (w/o reg) on all data: 0.06479551
Test loss (w/o reg) on all data: 0.06639613
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.379686e-06
Norm of the params: 15.89429
     Influence (LOO): fixed  68 labels. Loss 0.06640. Accuracy 0.985.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017482698
Train loss (w/o reg) on all data: 0.008138122
Test loss (w/o reg) on all data: 0.024279725
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.68369e-07
Norm of the params: 13.670828
                Loss: fixed  88 labels. Loss 0.02428. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922327
Train loss (w/o reg) on all data: 0.19111894
Test loss (w/o reg) on all data: 0.11407271
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5169992e-05
Norm of the params: 12.731315
              Random: fixed  10 labels. Loss 0.11407. Accuracy 0.992.
### Flips: 208, rs: 37, checks: 156
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03991129
Train loss (w/o reg) on all data: 0.03154287
Test loss (w/o reg) on all data: 0.022348197
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.14168715e-05
Norm of the params: 12.937093
     Influence (LOO): fixed  88 labels. Loss 0.02235. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013836843
Train loss (w/o reg) on all data: 0.0062596765
Test loss (w/o reg) on all data: 0.025621392
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.926225e-07
Norm of the params: 12.310293
                Loss: fixed  92 labels. Loss 0.02562. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19689642
Train loss (w/o reg) on all data: 0.1891
Test loss (w/o reg) on all data: 0.102664016
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.6766065e-06
Norm of the params: 12.487128
              Random: fixed  13 labels. Loss 0.10266. Accuracy 0.996.
### Flips: 208, rs: 37, checks: 208
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018346258
Train loss (w/o reg) on all data: 0.011920637
Test loss (w/o reg) on all data: 0.014266024
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.443489e-06
Norm of the params: 11.33633
     Influence (LOO): fixed  95 labels. Loss 0.01427. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010595834
Train loss (w/o reg) on all data: 0.0044023073
Test loss (w/o reg) on all data: 0.023017064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7177873e-07
Norm of the params: 11.129714
                Loss: fixed  96 labels. Loss 0.02302. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18842512
Train loss (w/o reg) on all data: 0.1797367
Test loss (w/o reg) on all data: 0.10147974
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.39151e-06
Norm of the params: 13.182119
              Random: fixed  18 labels. Loss 0.10148. Accuracy 0.989.
### Flips: 208, rs: 37, checks: 260
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224469
Train loss (w/o reg) on all data: 0.0062452992
Test loss (w/o reg) on all data: 0.012817397
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.059269e-07
Norm of the params: 9.979149
     Influence (LOO): fixed  98 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729358
Test loss (w/o reg) on all data: 0.012055202
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7260165e-07
Norm of the params: 9.153232
                Loss: fixed  99 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17633703
Train loss (w/o reg) on all data: 0.16668251
Test loss (w/o reg) on all data: 0.101992056
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.678553e-06
Norm of the params: 13.895701
              Random: fixed  24 labels. Loss 0.10199. Accuracy 0.985.
### Flips: 208, rs: 37, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224469
Train loss (w/o reg) on all data: 0.0062453453
Test loss (w/o reg) on all data: 0.012817319
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0315314e-07
Norm of the params: 9.979103
     Influence (LOO): fixed  98 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.0120558515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7238866e-07
Norm of the params: 9.153173
                Loss: fixed  99 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15194371
Train loss (w/o reg) on all data: 0.14135523
Test loss (w/o reg) on all data: 0.09208989
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5550915e-05
Norm of the params: 14.552312
              Random: fixed  35 labels. Loss 0.09209. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20351774
Train loss (w/o reg) on all data: 0.19497135
Test loss (w/o reg) on all data: 0.13749492
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.6293982e-05
Norm of the params: 13.073932
Flipped loss: 0.13749. Accuracy: 0.962
### Flips: 208, rs: 38, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12817444
Train loss (w/o reg) on all data: 0.11739277
Test loss (w/o reg) on all data: 0.12048787
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.7680531e-05
Norm of the params: 14.684461
     Influence (LOO): fixed  36 labels. Loss 0.12049. Accuracy 0.973.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07510505
Train loss (w/o reg) on all data: 0.06073333
Test loss (w/o reg) on all data: 0.088984236
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1753848e-05
Norm of the params: 16.95389
                Loss: fixed  52 labels. Loss 0.08898. Accuracy 0.966.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19807507
Train loss (w/o reg) on all data: 0.18956551
Test loss (w/o reg) on all data: 0.1349629
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0625155e-05
Norm of the params: 13.045739
              Random: fixed   5 labels. Loss 0.13496. Accuracy 0.962.
### Flips: 208, rs: 38, checks: 104
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06787799
Train loss (w/o reg) on all data: 0.057503723
Test loss (w/o reg) on all data: 0.09498369
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.9365512e-06
Norm of the params: 14.404352
     Influence (LOO): fixed  68 labels. Loss 0.09498. Accuracy 0.977.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022819106
Train loss (w/o reg) on all data: 0.011622839
Test loss (w/o reg) on all data: 0.039823752
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.7405323e-06
Norm of the params: 14.964134
                Loss: fixed  85 labels. Loss 0.03982. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19091728
Train loss (w/o reg) on all data: 0.18241489
Test loss (w/o reg) on all data: 0.13011116
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.589194e-06
Norm of the params: 13.040241
              Random: fixed  10 labels. Loss 0.13011. Accuracy 0.966.
### Flips: 208, rs: 38, checks: 156
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0327548
Train loss (w/o reg) on all data: 0.023852648
Test loss (w/o reg) on all data: 0.048158586
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.776333e-06
Norm of the params: 13.343278
     Influence (LOO): fixed  86 labels. Loss 0.04816. Accuracy 0.989.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017181225
Train loss (w/o reg) on all data: 0.0080468645
Test loss (w/o reg) on all data: 0.033111636
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.033657e-06
Norm of the params: 13.516182
                Loss: fixed  91 labels. Loss 0.03311. Accuracy 0.981.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17880987
Train loss (w/o reg) on all data: 0.17012425
Test loss (w/o reg) on all data: 0.12135399
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.653839e-06
Norm of the params: 13.180002
              Random: fixed  17 labels. Loss 0.12135. Accuracy 0.969.
### Flips: 208, rs: 38, checks: 208
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019096227
Train loss (w/o reg) on all data: 0.011489565
Test loss (w/o reg) on all data: 0.036650605
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.084902e-06
Norm of the params: 12.334231
     Influence (LOO): fixed  93 labels. Loss 0.03665. Accuracy 0.985.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01269493
Train loss (w/o reg) on all data: 0.005547865
Test loss (w/o reg) on all data: 0.021084748
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6422483e-07
Norm of the params: 11.955806
                Loss: fixed  95 labels. Loss 0.02108. Accuracy 0.989.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17183487
Train loss (w/o reg) on all data: 0.16283052
Test loss (w/o reg) on all data: 0.114050366
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.925292e-05
Norm of the params: 13.419655
              Random: fixed  20 labels. Loss 0.11405. Accuracy 0.977.
### Flips: 208, rs: 38, checks: 260
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013345555
Train loss (w/o reg) on all data: 0.0069355555
Test loss (w/o reg) on all data: 0.019620769
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.737746e-07
Norm of the params: 11.322544
     Influence (LOO): fixed  97 labels. Loss 0.01962. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839531
Train loss (w/o reg) on all data: 0.0033499594
Test loss (w/o reg) on all data: 0.023645025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.277937e-07
Norm of the params: 10.47814
                Loss: fixed  99 labels. Loss 0.02365. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17028572
Train loss (w/o reg) on all data: 0.160905
Test loss (w/o reg) on all data: 0.10759201
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.613484e-05
Norm of the params: 13.697238
              Random: fixed  23 labels. Loss 0.10759. Accuracy 0.981.
### Flips: 208, rs: 38, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729907
Test loss (w/o reg) on all data: 0.012054256
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.096796e-07
Norm of the params: 9.153173
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00883953
Train loss (w/o reg) on all data: 0.0033500025
Test loss (w/o reg) on all data: 0.023644378
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7879408e-07
Norm of the params: 10.478099
                Loss: fixed  99 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1685629
Train loss (w/o reg) on all data: 0.15928683
Test loss (w/o reg) on all data: 0.10329252
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.0028233e-05
Norm of the params: 13.620631
              Random: fixed  24 labels. Loss 0.10329. Accuracy 0.989.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21850388
Train loss (w/o reg) on all data: 0.20953867
Test loss (w/o reg) on all data: 0.1489333
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.01198475e-05
Norm of the params: 13.390455
Flipped loss: 0.14893. Accuracy: 0.966
### Flips: 208, rs: 39, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13485853
Train loss (w/o reg) on all data: 0.122853056
Test loss (w/o reg) on all data: 0.115557045
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7744392e-05
Norm of the params: 15.495468
     Influence (LOO): fixed  38 labels. Loss 0.11556. Accuracy 0.966.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0894955
Train loss (w/o reg) on all data: 0.07302357
Test loss (w/o reg) on all data: 0.089778766
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.30761155e-05
Norm of the params: 18.150446
                Loss: fixed  52 labels. Loss 0.08978. Accuracy 0.966.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21666437
Train loss (w/o reg) on all data: 0.20806059
Test loss (w/o reg) on all data: 0.13528892
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5031397e-05
Norm of the params: 13.11776
              Random: fixed   3 labels. Loss 0.13529. Accuracy 0.962.
### Flips: 208, rs: 39, checks: 104
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07852102
Train loss (w/o reg) on all data: 0.067018926
Test loss (w/o reg) on all data: 0.074934795
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.1797225e-06
Norm of the params: 15.167132
     Influence (LOO): fixed  70 labels. Loss 0.07493. Accuracy 0.973.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024021901
Train loss (w/o reg) on all data: 0.012686533
Test loss (w/o reg) on all data: 0.067606755
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.050628e-06
Norm of the params: 15.056806
                Loss: fixed  89 labels. Loss 0.06761. Accuracy 0.966.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20599258
Train loss (w/o reg) on all data: 0.19757861
Test loss (w/o reg) on all data: 0.11899382
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.650454e-06
Norm of the params: 12.972254
              Random: fixed  12 labels. Loss 0.11899. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 156
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04130182
Train loss (w/o reg) on all data: 0.03242618
Test loss (w/o reg) on all data: 0.032243766
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4382973e-06
Norm of the params: 13.323394
     Influence (LOO): fixed  88 labels. Loss 0.03224. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014724087
Train loss (w/o reg) on all data: 0.0060412865
Test loss (w/o reg) on all data: 0.042683754
Train acc on all data:  1.0
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.805979e-07
Norm of the params: 13.17786
                Loss: fixed  95 labels. Loss 0.04268. Accuracy 0.977.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20445731
Train loss (w/o reg) on all data: 0.1965527
Test loss (w/o reg) on all data: 0.1184598
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4848001e-05
Norm of the params: 12.573485
              Random: fixed  15 labels. Loss 0.11846. Accuracy 0.966.
### Flips: 208, rs: 39, checks: 208
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025040869
Train loss (w/o reg) on all data: 0.01834386
Test loss (w/o reg) on all data: 0.022289103
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6504865e-06
Norm of the params: 11.573253
     Influence (LOO): fixed  95 labels. Loss 0.02229. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011871377
Train loss (w/o reg) on all data: 0.0048812632
Test loss (w/o reg) on all data: 0.020370752
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.535576e-07
Norm of the params: 11.823801
                Loss: fixed  97 labels. Loss 0.02037. Accuracy 0.989.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19910966
Train loss (w/o reg) on all data: 0.1910396
Test loss (w/o reg) on all data: 0.110365115
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6137676e-05
Norm of the params: 12.704373
              Random: fixed  18 labels. Loss 0.11037. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 260
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013405753
Train loss (w/o reg) on all data: 0.007833308
Test loss (w/o reg) on all data: 0.01572166
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1675594e-07
Norm of the params: 10.556938
     Influence (LOO): fixed  98 labels. Loss 0.01572. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010470968
Train loss (w/o reg) on all data: 0.004200926
Test loss (w/o reg) on all data: 0.021917202
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.5053343e-07
Norm of the params: 11.198252
                Loss: fixed  98 labels. Loss 0.02192. Accuracy 0.985.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19113173
Train loss (w/o reg) on all data: 0.18244496
Test loss (w/o reg) on all data: 0.10159317
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4645577e-05
Norm of the params: 13.180868
              Random: fixed  22 labels. Loss 0.10159. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 312
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011165383
Train loss (w/o reg) on all data: 0.006348251
Test loss (w/o reg) on all data: 0.0151502155
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.1642495e-07
Norm of the params: 9.815429
     Influence (LOO): fixed  99 labels. Loss 0.01515. Accuracy 0.996.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009827383
Train loss (w/o reg) on all data: 0.003915901
Test loss (w/o reg) on all data: 0.014778062
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.782321e-07
Norm of the params: 10.873345
                Loss: fixed  99 labels. Loss 0.01478. Accuracy 0.996.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18930523
Train loss (w/o reg) on all data: 0.18058884
Test loss (w/o reg) on all data: 0.10054413
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9651605e-05
Norm of the params: 13.203323
              Random: fixed  24 labels. Loss 0.10054. Accuracy 0.969.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2243057
Train loss (w/o reg) on all data: 0.21529421
Test loss (w/o reg) on all data: 0.18693323
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.9263483e-05
Norm of the params: 13.424968
Flipped loss: 0.18693. Accuracy: 0.950
### Flips: 260, rs: 0, checks: 52
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17229284
Train loss (w/o reg) on all data: 0.16133866
Test loss (w/o reg) on all data: 0.12869754
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3851859e-05
Norm of the params: 14.801481
     Influence (LOO): fixed  34 labels. Loss 0.12870. Accuracy 0.947.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115152545
Train loss (w/o reg) on all data: 0.09949843
Test loss (w/o reg) on all data: 0.15267599
Train acc on all data:  0.956064947468959
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.00836e-06
Norm of the params: 17.694134
                Loss: fixed  50 labels. Loss 0.15268. Accuracy 0.950.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21578783
Train loss (w/o reg) on all data: 0.2069497
Test loss (w/o reg) on all data: 0.17735258
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.24680955e-05
Norm of the params: 13.295212
              Random: fixed   9 labels. Loss 0.17735. Accuracy 0.943.
### Flips: 260, rs: 0, checks: 104
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12515758
Train loss (w/o reg) on all data: 0.11393334
Test loss (w/o reg) on all data: 0.07326656
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.867083e-05
Norm of the params: 14.982822
     Influence (LOO): fixed  63 labels. Loss 0.07327. Accuracy 0.977.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053520076
Train loss (w/o reg) on all data: 0.03753564
Test loss (w/o reg) on all data: 0.08521827
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.8655096e-06
Norm of the params: 17.87984
                Loss: fixed  91 labels. Loss 0.08522. Accuracy 0.969.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20686837
Train loss (w/o reg) on all data: 0.19781548
Test loss (w/o reg) on all data: 0.1739468
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.0625942e-05
Norm of the params: 13.45577
              Random: fixed  15 labels. Loss 0.17395. Accuracy 0.947.
### Flips: 260, rs: 0, checks: 156
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08039543
Train loss (w/o reg) on all data: 0.0705549
Test loss (w/o reg) on all data: 0.049390983
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6121655e-05
Norm of the params: 14.028923
     Influence (LOO): fixed  89 labels. Loss 0.04939. Accuracy 0.985.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021634966
Train loss (w/o reg) on all data: 0.011223686
Test loss (w/o reg) on all data: 0.030636901
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8503636e-06
Norm of the params: 14.430025
                Loss: fixed 106 labels. Loss 0.03064. Accuracy 0.989.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1981898
Train loss (w/o reg) on all data: 0.18896954
Test loss (w/o reg) on all data: 0.16784187
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.7763565e-06
Norm of the params: 13.579588
              Random: fixed  24 labels. Loss 0.16784. Accuracy 0.950.
### Flips: 260, rs: 0, checks: 208
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027727842
Train loss (w/o reg) on all data: 0.01896857
Test loss (w/o reg) on all data: 0.029652582
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0958372e-06
Norm of the params: 13.235766
     Influence (LOO): fixed 112 labels. Loss 0.02965. Accuracy 0.985.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018421946
Train loss (w/o reg) on all data: 0.008955063
Test loss (w/o reg) on all data: 0.01760343
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.71721e-07
Norm of the params: 13.760002
                Loss: fixed 112 labels. Loss 0.01760. Accuracy 0.989.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139138
Train loss (w/o reg) on all data: 0.1819025
Test loss (w/o reg) on all data: 0.16123609
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.6235586e-05
Norm of the params: 13.775984
              Random: fixed  30 labels. Loss 0.16124. Accuracy 0.958.
### Flips: 260, rs: 0, checks: 260
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012507213
Train loss (w/o reg) on all data: 0.0061096456
Test loss (w/o reg) on all data: 0.016259287
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.9881758e-06
Norm of the params: 11.311558
     Influence (LOO): fixed 120 labels. Loss 0.01626. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017637085
Train loss (w/o reg) on all data: 0.008535365
Test loss (w/o reg) on all data: 0.017725484
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5732791e-06
Norm of the params: 13.492013
                Loss: fixed 113 labels. Loss 0.01773. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18833365
Train loss (w/o reg) on all data: 0.1786249
Test loss (w/o reg) on all data: 0.1574653
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1717263e-05
Norm of the params: 13.9346695
              Random: fixed  33 labels. Loss 0.15747. Accuracy 0.962.
### Flips: 260, rs: 0, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009357562
Train loss (w/o reg) on all data: 0.0039550406
Test loss (w/o reg) on all data: 0.011968751
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6436477e-06
Norm of the params: 10.394731
     Influence (LOO): fixed 122 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01436249
Train loss (w/o reg) on all data: 0.006639369
Test loss (w/o reg) on all data: 0.018568382
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.5141423e-07
Norm of the params: 12.428291
                Loss: fixed 116 labels. Loss 0.01857. Accuracy 0.989.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18392861
Train loss (w/o reg) on all data: 0.1742597
Test loss (w/o reg) on all data: 0.14635745
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7958699e-05
Norm of the params: 13.906045
              Random: fixed  37 labels. Loss 0.14636. Accuracy 0.958.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23262009
Train loss (w/o reg) on all data: 0.22387964
Test loss (w/o reg) on all data: 0.15203534
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.271163e-05
Norm of the params: 13.221545
Flipped loss: 0.15204. Accuracy: 0.962
### Flips: 260, rs: 1, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17021166
Train loss (w/o reg) on all data: 0.15964831
Test loss (w/o reg) on all data: 0.110913806
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.48772e-05
Norm of the params: 14.535025
     Influence (LOO): fixed  35 labels. Loss 0.11091. Accuracy 0.973.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12198302
Train loss (w/o reg) on all data: 0.10731244
Test loss (w/o reg) on all data: 0.11471282
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2622305e-05
Norm of the params: 17.129261
                Loss: fixed  52 labels. Loss 0.11471. Accuracy 0.962.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22963177
Train loss (w/o reg) on all data: 0.22065797
Test loss (w/o reg) on all data: 0.14740679
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2426764e-05
Norm of the params: 13.396862
              Random: fixed   3 labels. Loss 0.14741. Accuracy 0.962.
### Flips: 260, rs: 1, checks: 104
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12526003
Train loss (w/o reg) on all data: 0.113099776
Test loss (w/o reg) on all data: 0.085604034
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.031158e-05
Norm of the params: 15.595034
     Influence (LOO): fixed  61 labels. Loss 0.08560. Accuracy 0.989.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036219567
Train loss (w/o reg) on all data: 0.022903372
Test loss (w/o reg) on all data: 0.029569844
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.526065e-06
Norm of the params: 16.319433
                Loss: fixed  98 labels. Loss 0.02957. Accuracy 0.992.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22336279
Train loss (w/o reg) on all data: 0.21459667
Test loss (w/o reg) on all data: 0.14095129
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.398632e-05
Norm of the params: 13.240935
              Random: fixed   8 labels. Loss 0.14095. Accuracy 0.966.
### Flips: 260, rs: 1, checks: 156
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069904566
Train loss (w/o reg) on all data: 0.057525918
Test loss (w/o reg) on all data: 0.0580009
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4132006e-06
Norm of the params: 15.73445
     Influence (LOO): fixed  88 labels. Loss 0.05800. Accuracy 0.985.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012305338
Train loss (w/o reg) on all data: 0.0055355164
Test loss (w/o reg) on all data: 0.015670653
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0301846e-06
Norm of the params: 11.635996
                Loss: fixed 113 labels. Loss 0.01567. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21782728
Train loss (w/o reg) on all data: 0.2091288
Test loss (w/o reg) on all data: 0.12677735
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.536095e-05
Norm of the params: 13.189751
              Random: fixed  14 labels. Loss 0.12678. Accuracy 0.973.
### Flips: 260, rs: 1, checks: 208
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033524785
Train loss (w/o reg) on all data: 0.023633666
Test loss (w/o reg) on all data: 0.03148875
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1753226e-06
Norm of the params: 14.064934
     Influence (LOO): fixed 105 labels. Loss 0.03149. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0080183325
Train loss (w/o reg) on all data: 0.0030734455
Test loss (w/o reg) on all data: 0.012502432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9276754e-07
Norm of the params: 9.944735
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2125974
Train loss (w/o reg) on all data: 0.20340504
Test loss (w/o reg) on all data: 0.12220144
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1275127e-05
Norm of the params: 13.559032
              Random: fixed  17 labels. Loss 0.12220. Accuracy 0.973.
### Flips: 260, rs: 1, checks: 260
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020542016
Train loss (w/o reg) on all data: 0.012886286
Test loss (w/o reg) on all data: 0.024218716
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7318924e-07
Norm of the params: 12.373949
     Influence (LOO): fixed 111 labels. Loss 0.02422. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018332
Train loss (w/o reg) on all data: 0.0030734218
Test loss (w/o reg) on all data: 0.012503137
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1741034e-07
Norm of the params: 9.944758
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20130935
Train loss (w/o reg) on all data: 0.19239211
Test loss (w/o reg) on all data: 0.1130927
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.8207774e-05
Norm of the params: 13.354581
              Random: fixed  24 labels. Loss 0.11309. Accuracy 0.977.
### Flips: 260, rs: 1, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014790937
Train loss (w/o reg) on all data: 0.009550536
Test loss (w/o reg) on all data: 0.014461292
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.4824966e-07
Norm of the params: 10.237579
     Influence (LOO): fixed 114 labels. Loss 0.01446. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018334
Train loss (w/o reg) on all data: 0.003073396
Test loss (w/o reg) on all data: 0.012501467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0064325e-07
Norm of the params: 9.944786
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19014594
Train loss (w/o reg) on all data: 0.1816611
Test loss (w/o reg) on all data: 0.10820575
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7483724e-05
Norm of the params: 13.026776
              Random: fixed  30 labels. Loss 0.10821. Accuracy 0.969.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24793646
Train loss (w/o reg) on all data: 0.24142054
Test loss (w/o reg) on all data: 0.16596156
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5714983e-05
Norm of the params: 11.41571
Flipped loss: 0.16596. Accuracy: 0.966
### Flips: 260, rs: 2, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19661571
Train loss (w/o reg) on all data: 0.18682563
Test loss (w/o reg) on all data: 0.15919201
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.1398602e-05
Norm of the params: 13.9929085
     Influence (LOO): fixed  32 labels. Loss 0.15919. Accuracy 0.958.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13019054
Train loss (w/o reg) on all data: 0.11660529
Test loss (w/o reg) on all data: 0.12442381
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.86618e-06
Norm of the params: 16.483477
                Loss: fixed  51 labels. Loss 0.12442. Accuracy 0.950.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24238686
Train loss (w/o reg) on all data: 0.23589776
Test loss (w/o reg) on all data: 0.15721451
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 0.000111917245
Norm of the params: 11.392185
              Random: fixed   6 labels. Loss 0.15721. Accuracy 0.966.
### Flips: 260, rs: 2, checks: 104
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13850075
Train loss (w/o reg) on all data: 0.12887175
Test loss (w/o reg) on all data: 0.109725356
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.9383256e-05
Norm of the params: 13.877312
     Influence (LOO): fixed  67 labels. Loss 0.10973. Accuracy 0.969.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048791964
Train loss (w/o reg) on all data: 0.03427796
Test loss (w/o reg) on all data: 0.04764206
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.552234e-06
Norm of the params: 17.037607
                Loss: fixed  96 labels. Loss 0.04764. Accuracy 0.973.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23812446
Train loss (w/o reg) on all data: 0.23156492
Test loss (w/o reg) on all data: 0.15360789
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.2685888e-05
Norm of the params: 11.453854
              Random: fixed   8 labels. Loss 0.15361. Accuracy 0.969.
### Flips: 260, rs: 2, checks: 156
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079251595
Train loss (w/o reg) on all data: 0.07003838
Test loss (w/o reg) on all data: 0.052138567
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.78197e-05
Norm of the params: 13.574399
     Influence (LOO): fixed  97 labels. Loss 0.05214. Accuracy 0.985.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024467677
Train loss (w/o reg) on all data: 0.014643167
Test loss (w/o reg) on all data: 0.026153553
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5551167e-06
Norm of the params: 14.017496
                Loss: fixed 113 labels. Loss 0.02615. Accuracy 0.985.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22851019
Train loss (w/o reg) on all data: 0.22113048
Test loss (w/o reg) on all data: 0.13379036
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.86841e-05
Norm of the params: 12.14883
              Random: fixed  15 labels. Loss 0.13379. Accuracy 0.977.
### Flips: 260, rs: 2, checks: 208
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027130555
Train loss (w/o reg) on all data: 0.020391583
Test loss (w/o reg) on all data: 0.023633204
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6992055e-06
Norm of the params: 11.609455
     Influence (LOO): fixed 122 labels. Loss 0.02363. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017553745
Train loss (w/o reg) on all data: 0.008854579
Test loss (w/o reg) on all data: 0.015684353
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.793539e-07
Norm of the params: 13.190274
                Loss: fixed 121 labels. Loss 0.01568. Accuracy 0.996.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22171254
Train loss (w/o reg) on all data: 0.21399882
Test loss (w/o reg) on all data: 0.12686956
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.684476e-05
Norm of the params: 12.420722
              Random: fixed  22 labels. Loss 0.12687. Accuracy 0.981.
### Flips: 260, rs: 2, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008916484
Train loss (w/o reg) on all data: 0.0038531562
Test loss (w/o reg) on all data: 0.012025611
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.8217355e-07
Norm of the params: 10.063128
     Influence (LOO): fixed 128 labels. Loss 0.01203. Accuracy 0.996.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015962403
Train loss (w/o reg) on all data: 0.0076372717
Test loss (w/o reg) on all data: 0.014399851
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.741435e-07
Norm of the params: 12.903589
                Loss: fixed 123 labels. Loss 0.01440. Accuracy 0.996.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20874627
Train loss (w/o reg) on all data: 0.20071256
Test loss (w/o reg) on all data: 0.12190399
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2436914e-05
Norm of the params: 12.675734
              Random: fixed  29 labels. Loss 0.12190. Accuracy 0.981.
### Flips: 260, rs: 2, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012055362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5989524e-07
Norm of the params: 9.153151
     Influence (LOO): fixed 129 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008787829
Train loss (w/o reg) on all data: 0.00351522
Test loss (w/o reg) on all data: 0.011547313
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2637576e-07
Norm of the params: 10.268991
                Loss: fixed 127 labels. Loss 0.01155. Accuracy 0.996.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19860196
Train loss (w/o reg) on all data: 0.19071434
Test loss (w/o reg) on all data: 0.11228882
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3936748e-05
Norm of the params: 12.559956
              Random: fixed  37 labels. Loss 0.11229. Accuracy 0.977.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23822328
Train loss (w/o reg) on all data: 0.22988516
Test loss (w/o reg) on all data: 0.16592945
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.5842974e-05
Norm of the params: 12.91366
Flipped loss: 0.16593. Accuracy: 0.962
### Flips: 260, rs: 3, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153198
Train loss (w/o reg) on all data: 0.17199396
Test loss (w/o reg) on all data: 0.110242605
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9867664e-05
Norm of the params: 13.811609
     Influence (LOO): fixed  35 labels. Loss 0.11024. Accuracy 0.973.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119319715
Train loss (w/o reg) on all data: 0.103053235
Test loss (w/o reg) on all data: 0.14970337
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.480089e-05
Norm of the params: 18.036896
                Loss: fixed  50 labels. Loss 0.14970. Accuracy 0.943.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23039241
Train loss (w/o reg) on all data: 0.22201401
Test loss (w/o reg) on all data: 0.1666457
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.073241e-05
Norm of the params: 12.944806
              Random: fixed   4 labels. Loss 0.16665. Accuracy 0.958.
### Flips: 260, rs: 3, checks: 104
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11243662
Train loss (w/o reg) on all data: 0.10139883
Test loss (w/o reg) on all data: 0.07029956
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.745243e-06
Norm of the params: 14.857851
     Influence (LOO): fixed  75 labels. Loss 0.07030. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047992513
Train loss (w/o reg) on all data: 0.031875122
Test loss (w/o reg) on all data: 0.08491852
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.451203e-06
Norm of the params: 17.954046
                Loss: fixed  92 labels. Loss 0.08492. Accuracy 0.973.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22356912
Train loss (w/o reg) on all data: 0.21549337
Test loss (w/o reg) on all data: 0.14654273
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.325528e-05
Norm of the params: 12.708868
              Random: fixed  13 labels. Loss 0.14654. Accuracy 0.973.
### Flips: 260, rs: 3, checks: 156
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068122774
Train loss (w/o reg) on all data: 0.058983833
Test loss (w/o reg) on all data: 0.039570328
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3991396e-06
Norm of the params: 13.51957
     Influence (LOO): fixed 101 labels. Loss 0.03957. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022101525
Train loss (w/o reg) on all data: 0.011607933
Test loss (w/o reg) on all data: 0.02897941
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.441802e-06
Norm of the params: 14.486956
                Loss: fixed 113 labels. Loss 0.02898. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2196611
Train loss (w/o reg) on all data: 0.21154007
Test loss (w/o reg) on all data: 0.13873395
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4714506e-05
Norm of the params: 12.744429
              Random: fixed  19 labels. Loss 0.13873. Accuracy 0.969.
### Flips: 260, rs: 3, checks: 208
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033210464
Train loss (w/o reg) on all data: 0.025936212
Test loss (w/o reg) on all data: 0.01966998
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.610305e-06
Norm of the params: 12.061718
     Influence (LOO): fixed 118 labels. Loss 0.01967. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017750932
Train loss (w/o reg) on all data: 0.00856664
Test loss (w/o reg) on all data: 0.025402905
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.905152e-07
Norm of the params: 13.553076
                Loss: fixed 116 labels. Loss 0.02540. Accuracy 0.992.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21588716
Train loss (w/o reg) on all data: 0.20794025
Test loss (w/o reg) on all data: 0.1338801
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.1105683e-06
Norm of the params: 12.60707
              Random: fixed  24 labels. Loss 0.13388. Accuracy 0.981.
### Flips: 260, rs: 3, checks: 260
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017016698
Train loss (w/o reg) on all data: 0.0112848235
Test loss (w/o reg) on all data: 0.013738184
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4615085e-06
Norm of the params: 10.70689
     Influence (LOO): fixed 124 labels. Loss 0.01374. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013069526
Train loss (w/o reg) on all data: 0.005696051
Test loss (w/o reg) on all data: 0.024079531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.105551e-07
Norm of the params: 12.143702
                Loss: fixed 120 labels. Loss 0.02408. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20849712
Train loss (w/o reg) on all data: 0.20058252
Test loss (w/o reg) on all data: 0.12540194
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9526791e-05
Norm of the params: 12.581421
              Random: fixed  31 labels. Loss 0.12540. Accuracy 0.985.
### Flips: 260, rs: 3, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730321
Test loss (w/o reg) on all data: 0.012055119
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2004558e-07
Norm of the params: 9.153124
     Influence (LOO): fixed 127 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009105502
Train loss (w/o reg) on all data: 0.00358729
Test loss (w/o reg) on all data: 0.014599011
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3867152e-07
Norm of the params: 10.505439
                Loss: fixed 123 labels. Loss 0.01460. Accuracy 0.992.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19073933
Train loss (w/o reg) on all data: 0.18338573
Test loss (w/o reg) on all data: 0.10531192
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.638319e-06
Norm of the params: 12.127321
              Random: fixed  43 labels. Loss 0.10531. Accuracy 0.985.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25298938
Train loss (w/o reg) on all data: 0.24796115
Test loss (w/o reg) on all data: 0.19245122
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.6520022e-05
Norm of the params: 10.028185
Flipped loss: 0.19245. Accuracy: 0.943
### Flips: 260, rs: 4, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19688788
Train loss (w/o reg) on all data: 0.18905601
Test loss (w/o reg) on all data: 0.13341543
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.4529738e-05
Norm of the params: 12.515486
     Influence (LOO): fixed  37 labels. Loss 0.13342. Accuracy 0.958.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13398844
Train loss (w/o reg) on all data: 0.122533
Test loss (w/o reg) on all data: 0.14380498
Train acc on all data:  0.936007640878701
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.0488314e-05
Norm of the params: 15.136338
                Loss: fixed  52 labels. Loss 0.14380. Accuracy 0.950.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2420551
Train loss (w/o reg) on all data: 0.23696797
Test loss (w/o reg) on all data: 0.18390991
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.826772e-05
Norm of the params: 10.086766
              Random: fixed  10 labels. Loss 0.18391. Accuracy 0.935.
### Flips: 260, rs: 4, checks: 104
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1400176
Train loss (w/o reg) on all data: 0.13070177
Test loss (w/o reg) on all data: 0.091416396
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.226737e-06
Norm of the params: 13.649791
     Influence (LOO): fixed  69 labels. Loss 0.09142. Accuracy 0.985.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05944592
Train loss (w/o reg) on all data: 0.044889025
Test loss (w/o reg) on all data: 0.10020992
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.6208315e-06
Norm of the params: 17.062763
                Loss: fixed  96 labels. Loss 0.10021. Accuracy 0.950.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23538108
Train loss (w/o reg) on all data: 0.23039207
Test loss (w/o reg) on all data: 0.1723338
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.7328255e-05
Norm of the params: 9.988999
              Random: fixed  18 labels. Loss 0.17233. Accuracy 0.950.
### Flips: 260, rs: 4, checks: 156
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09211905
Train loss (w/o reg) on all data: 0.08335291
Test loss (w/o reg) on all data: 0.064110756
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.701994e-06
Norm of the params: 13.240954
     Influence (LOO): fixed  92 labels. Loss 0.06411. Accuracy 0.981.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018441502
Train loss (w/o reg) on all data: 0.009854032
Test loss (w/o reg) on all data: 0.025042651
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2486169e-06
Norm of the params: 13.105321
                Loss: fixed 123 labels. Loss 0.02504. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22813167
Train loss (w/o reg) on all data: 0.222758
Test loss (w/o reg) on all data: 0.16369659
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.8080363e-05
Norm of the params: 10.366944
              Random: fixed  24 labels. Loss 0.16370. Accuracy 0.966.
### Flips: 260, rs: 4, checks: 208
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045697484
Train loss (w/o reg) on all data: 0.036667332
Test loss (w/o reg) on all data: 0.027543437
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6629085e-06
Norm of the params: 13.438863
     Influence (LOO): fixed 114 labels. Loss 0.02754. Accuracy 0.989.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012132707
Train loss (w/o reg) on all data: 0.0055207023
Test loss (w/o reg) on all data: 0.01862254
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.6844175e-07
Norm of the params: 11.49957
                Loss: fixed 127 labels. Loss 0.01862. Accuracy 0.996.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21576507
Train loss (w/o reg) on all data: 0.2102397
Test loss (w/o reg) on all data: 0.14411338
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8575107e-05
Norm of the params: 10.512257
              Random: fixed  33 labels. Loss 0.14411. Accuracy 0.966.
### Flips: 260, rs: 4, checks: 260
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03095274
Train loss (w/o reg) on all data: 0.023455432
Test loss (w/o reg) on all data: 0.018765042
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.0634032e-06
Norm of the params: 12.245251
     Influence (LOO): fixed 123 labels. Loss 0.01877. Accuracy 0.989.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010996519
Train loss (w/o reg) on all data: 0.004720523
Test loss (w/o reg) on all data: 0.015894625
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2709616e-07
Norm of the params: 11.2035675
                Loss: fixed 128 labels. Loss 0.01589. Accuracy 0.996.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20608436
Train loss (w/o reg) on all data: 0.20045564
Test loss (w/o reg) on all data: 0.13956411
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2421324e-05
Norm of the params: 10.610105
              Random: fixed  42 labels. Loss 0.13956. Accuracy 0.973.
### Flips: 260, rs: 4, checks: 312
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026015127
Train loss (w/o reg) on all data: 0.019192642
Test loss (w/o reg) on all data: 0.014789196
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8082542e-06
Norm of the params: 11.681169
     Influence (LOO): fixed 125 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008888099
Train loss (w/o reg) on all data: 0.0035020374
Test loss (w/o reg) on all data: 0.012814676
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0466174e-07
Norm of the params: 10.378884
                Loss: fixed 130 labels. Loss 0.01281. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20176026
Train loss (w/o reg) on all data: 0.1960499
Test loss (w/o reg) on all data: 0.13320346
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.619763e-05
Norm of the params: 10.686776
              Random: fixed  45 labels. Loss 0.13320. Accuracy 0.977.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2524982
Train loss (w/o reg) on all data: 0.2446371
Test loss (w/o reg) on all data: 0.17243989
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5031378e-05
Norm of the params: 12.5388365
Flipped loss: 0.17244. Accuracy: 0.966
### Flips: 260, rs: 5, checks: 52
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19428828
Train loss (w/o reg) on all data: 0.18346667
Test loss (w/o reg) on all data: 0.13410096
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0203661e-05
Norm of the params: 14.711641
     Influence (LOO): fixed  37 labels. Loss 0.13410. Accuracy 0.950.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1480391
Train loss (w/o reg) on all data: 0.13311633
Test loss (w/o reg) on all data: 0.12405649
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3424403e-05
Norm of the params: 17.275866
                Loss: fixed  51 labels. Loss 0.12406. Accuracy 0.954.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24596915
Train loss (w/o reg) on all data: 0.23788422
Test loss (w/o reg) on all data: 0.16550377
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.2748e-05
Norm of the params: 12.716076
              Random: fixed   6 labels. Loss 0.16550. Accuracy 0.966.
### Flips: 260, rs: 5, checks: 104
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14763351
Train loss (w/o reg) on all data: 0.13627242
Test loss (w/o reg) on all data: 0.1051664
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1989798e-05
Norm of the params: 15.073879
     Influence (LOO): fixed  63 labels. Loss 0.10517. Accuracy 0.981.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06689895
Train loss (w/o reg) on all data: 0.050548203
Test loss (w/o reg) on all data: 0.04735549
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9444913e-06
Norm of the params: 18.083555
                Loss: fixed  98 labels. Loss 0.04736. Accuracy 0.985.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23789412
Train loss (w/o reg) on all data: 0.23056436
Test loss (w/o reg) on all data: 0.15195915
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.9588546e-05
Norm of the params: 12.1076565
              Random: fixed  16 labels. Loss 0.15196. Accuracy 0.969.
### Flips: 260, rs: 5, checks: 156
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09976202
Train loss (w/o reg) on all data: 0.090109415
Test loss (w/o reg) on all data: 0.07052168
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.201418e-06
Norm of the params: 13.89432
     Influence (LOO): fixed  88 labels. Loss 0.07052. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027878154
Train loss (w/o reg) on all data: 0.015644569
Test loss (w/o reg) on all data: 0.014716718
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.037884e-06
Norm of the params: 15.641985
                Loss: fixed 121 labels. Loss 0.01472. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22788362
Train loss (w/o reg) on all data: 0.22017448
Test loss (w/o reg) on all data: 0.14213096
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1842999e-05
Norm of the params: 12.417038
              Random: fixed  25 labels. Loss 0.14213. Accuracy 0.973.
### Flips: 260, rs: 5, checks: 208
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07515214
Train loss (w/o reg) on all data: 0.06528711
Test loss (w/o reg) on all data: 0.0542198
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2566301e-05
Norm of the params: 14.046366
     Influence (LOO): fixed 102 labels. Loss 0.05422. Accuracy 0.989.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017905008
Train loss (w/o reg) on all data: 0.0082513215
Test loss (w/o reg) on all data: 0.01362286
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.963449e-07
Norm of the params: 13.895099
                Loss: fixed 127 labels. Loss 0.01362. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22530581
Train loss (w/o reg) on all data: 0.21763533
Test loss (w/o reg) on all data: 0.14109014
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2022544e-05
Norm of the params: 12.385864
              Random: fixed  28 labels. Loss 0.14109. Accuracy 0.981.
### Flips: 260, rs: 5, checks: 260
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03551959
Train loss (w/o reg) on all data: 0.027714495
Test loss (w/o reg) on all data: 0.023839815
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4995683e-06
Norm of the params: 12.494073
     Influence (LOO): fixed 122 labels. Loss 0.02384. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016147695
Train loss (w/o reg) on all data: 0.0070888945
Test loss (w/o reg) on all data: 0.013999611
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.94718e-07
Norm of the params: 13.460166
                Loss: fixed 128 labels. Loss 0.01400. Accuracy 0.992.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2116629
Train loss (w/o reg) on all data: 0.20381324
Test loss (w/o reg) on all data: 0.12660459
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5084638e-05
Norm of the params: 12.5297
              Random: fixed  39 labels. Loss 0.12660. Accuracy 0.985.
### Flips: 260, rs: 5, checks: 312
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023152228
Train loss (w/o reg) on all data: 0.01687057
Test loss (w/o reg) on all data: 0.015022239
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7520722e-06
Norm of the params: 11.20862
     Influence (LOO): fixed 127 labels. Loss 0.01502. Accuracy 0.996.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012294389
Train loss (w/o reg) on all data: 0.005322118
Test loss (w/o reg) on all data: 0.013575088
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.984055e-07
Norm of the params: 11.808702
                Loss: fixed 130 labels. Loss 0.01358. Accuracy 0.996.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20297416
Train loss (w/o reg) on all data: 0.19531685
Test loss (w/o reg) on all data: 0.12043928
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2212045e-05
Norm of the params: 12.375216
              Random: fixed  45 labels. Loss 0.12044. Accuracy 0.981.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24962609
Train loss (w/o reg) on all data: 0.24312194
Test loss (w/o reg) on all data: 0.16291395
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7188193e-05
Norm of the params: 11.405386
Flipped loss: 0.16291. Accuracy: 0.966
### Flips: 260, rs: 6, checks: 52
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18988821
Train loss (w/o reg) on all data: 0.1805412
Test loss (w/o reg) on all data: 0.12608941
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.8759925e-06
Norm of the params: 13.672605
     Influence (LOO): fixed  34 labels. Loss 0.12609. Accuracy 0.977.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12563232
Train loss (w/o reg) on all data: 0.112958655
Test loss (w/o reg) on all data: 0.11429419
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.5468479e-05
Norm of the params: 15.920839
                Loss: fixed  52 labels. Loss 0.11429. Accuracy 0.939.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23906817
Train loss (w/o reg) on all data: 0.2323094
Test loss (w/o reg) on all data: 0.15666714
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.6775215e-05
Norm of the params: 11.6264925
              Random: fixed   8 labels. Loss 0.15667. Accuracy 0.958.
### Flips: 260, rs: 6, checks: 104
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13503267
Train loss (w/o reg) on all data: 0.1238619
Test loss (w/o reg) on all data: 0.07997753
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.976943e-05
Norm of the params: 14.947082
     Influence (LOO): fixed  65 labels. Loss 0.07998. Accuracy 0.989.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04079166
Train loss (w/o reg) on all data: 0.028698955
Test loss (w/o reg) on all data: 0.03526679
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1917504e-06
Norm of the params: 15.551661
                Loss: fixed 100 labels. Loss 0.03527. Accuracy 0.985.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2346354
Train loss (w/o reg) on all data: 0.22791998
Test loss (w/o reg) on all data: 0.15056239
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.3431355e-05
Norm of the params: 11.589152
              Random: fixed  13 labels. Loss 0.15056. Accuracy 0.969.
### Flips: 260, rs: 6, checks: 156
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07861169
Train loss (w/o reg) on all data: 0.0671931
Test loss (w/o reg) on all data: 0.04896559
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5930382e-06
Norm of the params: 15.111976
     Influence (LOO): fixed  90 labels. Loss 0.04897. Accuracy 0.992.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014456781
Train loss (w/o reg) on all data: 0.0069209407
Test loss (w/o reg) on all data: 0.011836598
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.353043e-07
Norm of the params: 12.276678
                Loss: fixed 118 labels. Loss 0.01184. Accuracy 0.996.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2232656
Train loss (w/o reg) on all data: 0.21655141
Test loss (w/o reg) on all data: 0.14911133
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3023767e-05
Norm of the params: 11.588087
              Random: fixed  22 labels. Loss 0.14911. Accuracy 0.973.
### Flips: 260, rs: 6, checks: 208
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046614796
Train loss (w/o reg) on all data: 0.035740297
Test loss (w/o reg) on all data: 0.03532358
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.119901e-06
Norm of the params: 14.747542
     Influence (LOO): fixed 105 labels. Loss 0.03532. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008380534
Train loss (w/o reg) on all data: 0.0030428143
Test loss (w/o reg) on all data: 0.010244903
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5123801e-07
Norm of the params: 10.332203
                Loss: fixed 122 labels. Loss 0.01024. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20860843
Train loss (w/o reg) on all data: 0.20118599
Test loss (w/o reg) on all data: 0.1385513
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5947822e-05
Norm of the params: 12.183967
              Random: fixed  32 labels. Loss 0.13855. Accuracy 0.973.
### Flips: 260, rs: 6, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037511226
Train loss (w/o reg) on all data: 0.028661216
Test loss (w/o reg) on all data: 0.027732864
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.179638e-06
Norm of the params: 13.304142
     Influence (LOO): fixed 112 labels. Loss 0.02773. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008380536
Train loss (w/o reg) on all data: 0.0030429712
Test loss (w/o reg) on all data: 0.010245311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8611173e-07
Norm of the params: 10.332051
                Loss: fixed 122 labels. Loss 0.01025. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20402552
Train loss (w/o reg) on all data: 0.19652769
Test loss (w/o reg) on all data: 0.13315521
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.5594577e-06
Norm of the params: 12.245675
              Random: fixed  36 labels. Loss 0.13316. Accuracy 0.969.
### Flips: 260, rs: 6, checks: 312
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018434927
Train loss (w/o reg) on all data: 0.013030028
Test loss (w/o reg) on all data: 0.0143997995
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.0645174e-07
Norm of the params: 10.397018
     Influence (LOO): fixed 121 labels. Loss 0.01440. Accuracy 0.996.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007623193
Train loss (w/o reg) on all data: 0.0027306154
Test loss (w/o reg) on all data: 0.0095780045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3087081e-07
Norm of the params: 9.891994
                Loss: fixed 123 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20239179
Train loss (w/o reg) on all data: 0.19483237
Test loss (w/o reg) on all data: 0.130053
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.36069275e-05
Norm of the params: 12.295869
              Random: fixed  38 labels. Loss 0.13005. Accuracy 0.977.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24346156
Train loss (w/o reg) on all data: 0.23604725
Test loss (w/o reg) on all data: 0.14060782
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.34052e-05
Norm of the params: 12.177289
Flipped loss: 0.14061. Accuracy: 0.992
### Flips: 260, rs: 7, checks: 52
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18704799
Train loss (w/o reg) on all data: 0.17659841
Test loss (w/o reg) on all data: 0.1057066
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.8275546e-05
Norm of the params: 14.456538
     Influence (LOO): fixed  31 labels. Loss 0.10571. Accuracy 0.985.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12423005
Train loss (w/o reg) on all data: 0.11227484
Test loss (w/o reg) on all data: 0.084108576
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.232271e-06
Norm of the params: 15.462996
                Loss: fixed  52 labels. Loss 0.08411. Accuracy 0.973.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2370628
Train loss (w/o reg) on all data: 0.23000099
Test loss (w/o reg) on all data: 0.13350688
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 0.00013699786
Norm of the params: 11.884286
              Random: fixed   6 labels. Loss 0.13351. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 104
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1279378
Train loss (w/o reg) on all data: 0.11680722
Test loss (w/o reg) on all data: 0.08051888
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.288652e-06
Norm of the params: 14.920163
     Influence (LOO): fixed  66 labels. Loss 0.08052. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035333194
Train loss (w/o reg) on all data: 0.02333298
Test loss (w/o reg) on all data: 0.03783853
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0186168e-06
Norm of the params: 15.492072
                Loss: fixed  98 labels. Loss 0.03784. Accuracy 0.985.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23096126
Train loss (w/o reg) on all data: 0.22384024
Test loss (w/o reg) on all data: 0.12642625
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3661241e-05
Norm of the params: 11.934008
              Random: fixed  13 labels. Loss 0.12643. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07908441
Train loss (w/o reg) on all data: 0.06845628
Test loss (w/o reg) on all data: 0.05186054
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.899475e-06
Norm of the params: 14.579529
     Influence (LOO): fixed  92 labels. Loss 0.05186. Accuracy 0.989.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01753061
Train loss (w/o reg) on all data: 0.00929878
Test loss (w/o reg) on all data: 0.0193218
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.080296e-07
Norm of the params: 12.83108
                Loss: fixed 112 labels. Loss 0.01932. Accuracy 0.989.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21822931
Train loss (w/o reg) on all data: 0.21092053
Test loss (w/o reg) on all data: 0.1186394
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1260195e-05
Norm of the params: 12.090317
              Random: fixed  21 labels. Loss 0.11864. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 208
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043823145
Train loss (w/o reg) on all data: 0.034902394
Test loss (w/o reg) on all data: 0.020695217
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4473989e-06
Norm of the params: 13.35721
     Influence (LOO): fixed 109 labels. Loss 0.02070. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011826772
Train loss (w/o reg) on all data: 0.0048248363
Test loss (w/o reg) on all data: 0.01528503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3595348e-06
Norm of the params: 11.833796
                Loss: fixed 117 labels. Loss 0.01529. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21220367
Train loss (w/o reg) on all data: 0.20475103
Test loss (w/o reg) on all data: 0.11589607
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.631113e-05
Norm of the params: 12.208717
              Random: fixed  27 labels. Loss 0.11590. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 260
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03168164
Train loss (w/o reg) on all data: 0.023702215
Test loss (w/o reg) on all data: 0.014496437
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.942486e-06
Norm of the params: 12.6328335
     Influence (LOO): fixed 115 labels. Loss 0.01450. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009019532
Train loss (w/o reg) on all data: 0.0033456723
Test loss (w/o reg) on all data: 0.012375046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.091509e-07
Norm of the params: 10.652568
                Loss: fixed 119 labels. Loss 0.01238. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20770405
Train loss (w/o reg) on all data: 0.20025374
Test loss (w/o reg) on all data: 0.11442186
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4890794e-05
Norm of the params: 12.206814
              Random: fixed  31 labels. Loss 0.11442. Accuracy 0.989.
### Flips: 260, rs: 7, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728305
Test loss (w/o reg) on all data: 0.012055155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4403876e-07
Norm of the params: 9.153347
     Influence (LOO): fixed 123 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009019533
Train loss (w/o reg) on all data: 0.0033456443
Test loss (w/o reg) on all data: 0.012373762
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6806265e-07
Norm of the params: 10.652595
                Loss: fixed 119 labels. Loss 0.01237. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922417
Train loss (w/o reg) on all data: 0.19157946
Test loss (w/o reg) on all data: 0.1074673
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9047874e-05
Norm of the params: 12.365047
              Random: fixed  35 labels. Loss 0.10747. Accuracy 0.989.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24783608
Train loss (w/o reg) on all data: 0.24003834
Test loss (w/o reg) on all data: 0.19042544
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4026739e-05
Norm of the params: 12.488187
Flipped loss: 0.19043. Accuracy: 0.954
### Flips: 260, rs: 8, checks: 52
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1836212
Train loss (w/o reg) on all data: 0.1733562
Test loss (w/o reg) on all data: 0.14507304
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.2899403e-05
Norm of the params: 14.328287
     Influence (LOO): fixed  36 labels. Loss 0.14507. Accuracy 0.954.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13984543
Train loss (w/o reg) on all data: 0.12626933
Test loss (w/o reg) on all data: 0.15347458
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.843053e-06
Norm of the params: 16.477928
                Loss: fixed  51 labels. Loss 0.15347. Accuracy 0.950.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24194846
Train loss (w/o reg) on all data: 0.23428218
Test loss (w/o reg) on all data: 0.17566022
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8621296e-05
Norm of the params: 12.382468
              Random: fixed   7 labels. Loss 0.17566. Accuracy 0.962.
### Flips: 260, rs: 8, checks: 104
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13085437
Train loss (w/o reg) on all data: 0.120187394
Test loss (w/o reg) on all data: 0.11851656
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2544623e-05
Norm of the params: 14.60614
     Influence (LOO): fixed  65 labels. Loss 0.11852. Accuracy 0.950.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061058655
Train loss (w/o reg) on all data: 0.04427083
Test loss (w/o reg) on all data: 0.059028517
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1426164e-06
Norm of the params: 18.323662
                Loss: fixed  95 labels. Loss 0.05903. Accuracy 0.989.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2338237
Train loss (w/o reg) on all data: 0.22595908
Test loss (w/o reg) on all data: 0.16615234
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.2137144e-05
Norm of the params: 12.541629
              Random: fixed  13 labels. Loss 0.16615. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 156
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08055989
Train loss (w/o reg) on all data: 0.06856721
Test loss (w/o reg) on all data: 0.087095395
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.850728e-06
Norm of the params: 15.487205
     Influence (LOO): fixed  91 labels. Loss 0.08710. Accuracy 0.966.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025876667
Train loss (w/o reg) on all data: 0.01418105
Test loss (w/o reg) on all data: 0.027450992
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.894132e-06
Norm of the params: 15.294194
                Loss: fixed 116 labels. Loss 0.02745. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22690275
Train loss (w/o reg) on all data: 0.21918017
Test loss (w/o reg) on all data: 0.15601765
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.559204e-05
Norm of the params: 12.42786
              Random: fixed  19 labels. Loss 0.15602. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 208
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04996347
Train loss (w/o reg) on all data: 0.039851338
Test loss (w/o reg) on all data: 0.054597862
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.276107e-06
Norm of the params: 14.221203
     Influence (LOO): fixed 110 labels. Loss 0.05460. Accuracy 0.977.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019525204
Train loss (w/o reg) on all data: 0.010042177
Test loss (w/o reg) on all data: 0.018837932
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.200512e-07
Norm of the params: 13.771729
                Loss: fixed 121 labels. Loss 0.01884. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21768399
Train loss (w/o reg) on all data: 0.21001951
Test loss (w/o reg) on all data: 0.14641485
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.50033e-05
Norm of the params: 12.381018
              Random: fixed  26 labels. Loss 0.14641. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 260
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028950095
Train loss (w/o reg) on all data: 0.021789383
Test loss (w/o reg) on all data: 0.032594696
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.25707e-06
Norm of the params: 11.967216
     Influence (LOO): fixed 121 labels. Loss 0.03259. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015453152
Train loss (w/o reg) on all data: 0.007082731
Test loss (w/o reg) on all data: 0.013106277
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.238828e-07
Norm of the params: 12.938642
                Loss: fixed 124 labels. Loss 0.01311. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20855056
Train loss (w/o reg) on all data: 0.20067593
Test loss (w/o reg) on all data: 0.13845263
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.3842544e-05
Norm of the params: 12.549602
              Random: fixed  32 labels. Loss 0.13845. Accuracy 0.966.
### Flips: 260, rs: 8, checks: 312
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022954892
Train loss (w/o reg) on all data: 0.016280556
Test loss (w/o reg) on all data: 0.02176772
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.261865e-06
Norm of the params: 11.553644
     Influence (LOO): fixed 124 labels. Loss 0.02177. Accuracy 0.992.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015383179
Train loss (w/o reg) on all data: 0.007173714
Test loss (w/o reg) on all data: 0.013800083
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5928898e-06
Norm of the params: 12.813638
                Loss: fixed 125 labels. Loss 0.01380. Accuracy 0.992.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19978534
Train loss (w/o reg) on all data: 0.19195975
Test loss (w/o reg) on all data: 0.12447481
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2888871e-05
Norm of the params: 12.510464
              Random: fixed  38 labels. Loss 0.12447. Accuracy 0.973.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25298735
Train loss (w/o reg) on all data: 0.2459694
Test loss (w/o reg) on all data: 0.20738716
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.5818301e-05
Norm of the params: 11.847334
Flipped loss: 0.20739. Accuracy: 0.916
### Flips: 260, rs: 9, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20395312
Train loss (w/o reg) on all data: 0.19441332
Test loss (w/o reg) on all data: 0.18330383
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.7051803e-05
Norm of the params: 13.812886
     Influence (LOO): fixed  29 labels. Loss 0.18330. Accuracy 0.916.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14749078
Train loss (w/o reg) on all data: 0.13476881
Test loss (w/o reg) on all data: 0.17716664
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.2452215e-05
Norm of the params: 15.951158
                Loss: fixed  50 labels. Loss 0.17717. Accuracy 0.931.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24600826
Train loss (w/o reg) on all data: 0.23887044
Test loss (w/o reg) on all data: 0.19946271
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.236979e-05
Norm of the params: 11.948069
              Random: fixed   7 labels. Loss 0.19946. Accuracy 0.931.
### Flips: 260, rs: 9, checks: 104
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16318187
Train loss (w/o reg) on all data: 0.15301068
Test loss (w/o reg) on all data: 0.14719792
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1063355e-05
Norm of the params: 14.262669
     Influence (LOO): fixed  57 labels. Loss 0.14720. Accuracy 0.954.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07538203
Train loss (w/o reg) on all data: 0.059204176
Test loss (w/o reg) on all data: 0.11349852
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.044216e-06
Norm of the params: 17.987694
                Loss: fixed  94 labels. Loss 0.11350. Accuracy 0.962.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24283835
Train loss (w/o reg) on all data: 0.2360156
Test loss (w/o reg) on all data: 0.18467972
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1022351e-05
Norm of the params: 11.681398
              Random: fixed  12 labels. Loss 0.18468. Accuracy 0.947.
### Flips: 260, rs: 9, checks: 156
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11465763
Train loss (w/o reg) on all data: 0.10376152
Test loss (w/o reg) on all data: 0.096374
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.4364438e-06
Norm of the params: 14.762192
     Influence (LOO): fixed  88 labels. Loss 0.09637. Accuracy 0.969.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027906433
Train loss (w/o reg) on all data: 0.015540725
Test loss (w/o reg) on all data: 0.05445366
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6048634e-05
Norm of the params: 15.726226
                Loss: fixed 120 labels. Loss 0.05445. Accuracy 0.973.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23784578
Train loss (w/o reg) on all data: 0.23086193
Test loss (w/o reg) on all data: 0.1764555
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.0547668e-05
Norm of the params: 11.818496
              Random: fixed  18 labels. Loss 0.17646. Accuracy 0.958.
### Flips: 260, rs: 9, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06313374
Train loss (w/o reg) on all data: 0.05379534
Test loss (w/o reg) on all data: 0.04870839
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.378853e-06
Norm of the params: 13.666308
     Influence (LOO): fixed 117 labels. Loss 0.04871. Accuracy 0.996.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020962693
Train loss (w/o reg) on all data: 0.011317199
Test loss (w/o reg) on all data: 0.03561645
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0505867e-06
Norm of the params: 13.8892
                Loss: fixed 126 labels. Loss 0.03562. Accuracy 0.985.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22215362
Train loss (w/o reg) on all data: 0.21466053
Test loss (w/o reg) on all data: 0.1569769
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.451492e-05
Norm of the params: 12.241809
              Random: fixed  32 labels. Loss 0.15698. Accuracy 0.958.
### Flips: 260, rs: 9, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035146415
Train loss (w/o reg) on all data: 0.026640363
Test loss (w/o reg) on all data: 0.02501009
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8422403e-06
Norm of the params: 13.043048
     Influence (LOO): fixed 129 labels. Loss 0.02501. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016446061
Train loss (w/o reg) on all data: 0.008108913
Test loss (w/o reg) on all data: 0.019332655
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.5565665e-07
Norm of the params: 12.9129
                Loss: fixed 131 labels. Loss 0.01933. Accuracy 0.996.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21075952
Train loss (w/o reg) on all data: 0.20310767
Test loss (w/o reg) on all data: 0.13831572
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.14319e-05
Norm of the params: 12.370811
              Random: fixed  41 labels. Loss 0.13832. Accuracy 0.973.
### Flips: 260, rs: 9, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224472
Train loss (w/o reg) on all data: 0.006245294
Test loss (w/o reg) on all data: 0.012817476
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2804478e-06
Norm of the params: 9.979156
     Influence (LOO): fixed 137 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008026175
Train loss (w/o reg) on all data: 0.002998488
Test loss (w/o reg) on all data: 0.015571272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2480236e-07
Norm of the params: 10.027651
                Loss: fixed 136 labels. Loss 0.01557. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20228401
Train loss (w/o reg) on all data: 0.19479966
Test loss (w/o reg) on all data: 0.12814218
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.6564844e-05
Norm of the params: 12.234659
              Random: fixed  48 labels. Loss 0.12814. Accuracy 0.966.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24003854
Train loss (w/o reg) on all data: 0.23344786
Test loss (w/o reg) on all data: 0.15466991
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1594869e-05
Norm of the params: 11.481012
Flipped loss: 0.15467. Accuracy: 0.973
### Flips: 260, rs: 10, checks: 52
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16810696
Train loss (w/o reg) on all data: 0.15879856
Test loss (w/o reg) on all data: 0.11824002
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.0473839e-05
Norm of the params: 13.644338
     Influence (LOO): fixed  40 labels. Loss 0.11824. Accuracy 0.969.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1257907
Train loss (w/o reg) on all data: 0.11357557
Test loss (w/o reg) on all data: 0.11712531
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.4765692e-05
Norm of the params: 15.630184
                Loss: fixed  51 labels. Loss 0.11713. Accuracy 0.950.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23137115
Train loss (w/o reg) on all data: 0.22457203
Test loss (w/o reg) on all data: 0.1449089
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.7361337e-05
Norm of the params: 11.661145
              Random: fixed   6 labels. Loss 0.14491. Accuracy 0.969.
### Flips: 260, rs: 10, checks: 104
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11677319
Train loss (w/o reg) on all data: 0.10604802
Test loss (w/o reg) on all data: 0.09721194
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0183166e-05
Norm of the params: 14.645937
     Influence (LOO): fixed  68 labels. Loss 0.09721. Accuracy 0.969.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03444886
Train loss (w/o reg) on all data: 0.022049041
Test loss (w/o reg) on all data: 0.060433373
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.1831872e-06
Norm of the params: 15.7479
                Loss: fixed  98 labels. Loss 0.06043. Accuracy 0.969.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22550559
Train loss (w/o reg) on all data: 0.21830274
Test loss (w/o reg) on all data: 0.13730623
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.1087e-05
Norm of the params: 12.002373
              Random: fixed  11 labels. Loss 0.13731. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 156
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06965953
Train loss (w/o reg) on all data: 0.05962507
Test loss (w/o reg) on all data: 0.061496947
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.449323e-06
Norm of the params: 14.166481
     Influence (LOO): fixed  93 labels. Loss 0.06150. Accuracy 0.985.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017898545
Train loss (w/o reg) on all data: 0.0090424195
Test loss (w/o reg) on all data: 0.025870865
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1283915e-06
Norm of the params: 13.308738
                Loss: fixed 111 labels. Loss 0.02587. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21599011
Train loss (w/o reg) on all data: 0.20881093
Test loss (w/o reg) on all data: 0.13050911
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6148027e-05
Norm of the params: 11.982649
              Random: fixed  19 labels. Loss 0.13051. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 208
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038351752
Train loss (w/o reg) on all data: 0.02942963
Test loss (w/o reg) on all data: 0.030787326
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.0233234e-06
Norm of the params: 13.358233
     Influence (LOO): fixed 109 labels. Loss 0.03079. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011400556
Train loss (w/o reg) on all data: 0.004863896
Test loss (w/o reg) on all data: 0.017277103
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.946958e-07
Norm of the params: 11.433863
                Loss: fixed 116 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20698284
Train loss (w/o reg) on all data: 0.19926451
Test loss (w/o reg) on all data: 0.12698862
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7675918e-05
Norm of the params: 12.424429
              Random: fixed  24 labels. Loss 0.12699. Accuracy 0.977.
### Flips: 260, rs: 10, checks: 260
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017666757
Train loss (w/o reg) on all data: 0.011349441
Test loss (w/o reg) on all data: 0.018976927
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6476962e-06
Norm of the params: 11.240389
     Influence (LOO): fixed 118 labels. Loss 0.01898. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010281924
Train loss (w/o reg) on all data: 0.00415328
Test loss (w/o reg) on all data: 0.01527837
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.37358e-06
Norm of the params: 11.071264
                Loss: fixed 117 labels. Loss 0.01528. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19962533
Train loss (w/o reg) on all data: 0.19173986
Test loss (w/o reg) on all data: 0.10951028
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0614108e-05
Norm of the params: 12.558242
              Random: fixed  32 labels. Loss 0.10951. Accuracy 0.985.
### Flips: 260, rs: 10, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011366566
Train loss (w/o reg) on all data: 0.0062762457
Test loss (w/o reg) on all data: 0.010718599
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.271099e-07
Norm of the params: 10.089916
     Influence (LOO): fixed 121 labels. Loss 0.01072. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008177739
Train loss (w/o reg) on all data: 0.0029811985
Test loss (w/o reg) on all data: 0.013066471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3654644e-06
Norm of the params: 10.194647
                Loss: fixed 119 labels. Loss 0.01307. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18981755
Train loss (w/o reg) on all data: 0.1819452
Test loss (w/o reg) on all data: 0.09744463
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.54513e-05
Norm of the params: 12.547778
              Random: fixed  39 labels. Loss 0.09744. Accuracy 0.992.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24632956
Train loss (w/o reg) on all data: 0.23878908
Test loss (w/o reg) on all data: 0.17552876
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.1176816e-05
Norm of the params: 12.280456
Flipped loss: 0.17553. Accuracy: 0.954
### Flips: 260, rs: 11, checks: 52
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19001316
Train loss (w/o reg) on all data: 0.17867284
Test loss (w/o reg) on all data: 0.14928003
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8982775e-05
Norm of the params: 15.060089
     Influence (LOO): fixed  32 labels. Loss 0.14928. Accuracy 0.969.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13536146
Train loss (w/o reg) on all data: 0.120579846
Test loss (w/o reg) on all data: 0.115445726
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.5843301e-05
Norm of the params: 17.193966
                Loss: fixed  52 labels. Loss 0.11545. Accuracy 0.954.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24214922
Train loss (w/o reg) on all data: 0.23487532
Test loss (w/o reg) on all data: 0.16970767
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.206529e-05
Norm of the params: 12.061428
              Random: fixed   6 labels. Loss 0.16971. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 104
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1373802
Train loss (w/o reg) on all data: 0.12592581
Test loss (w/o reg) on all data: 0.105671085
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7761404e-05
Norm of the params: 15.135645
     Influence (LOO): fixed  62 labels. Loss 0.10567. Accuracy 0.966.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052487034
Train loss (w/o reg) on all data: 0.03728479
Test loss (w/o reg) on all data: 0.05863557
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2981846e-05
Norm of the params: 17.436882
                Loss: fixed 100 labels. Loss 0.05864. Accuracy 0.977.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23401232
Train loss (w/o reg) on all data: 0.22624467
Test loss (w/o reg) on all data: 0.16835788
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.767755e-05
Norm of the params: 12.464065
              Random: fixed  13 labels. Loss 0.16836. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08333585
Train loss (w/o reg) on all data: 0.072704844
Test loss (w/o reg) on all data: 0.0595851
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3279357e-06
Norm of the params: 14.581494
     Influence (LOO): fixed  94 labels. Loss 0.05959. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028662201
Train loss (w/o reg) on all data: 0.016649224
Test loss (w/o reg) on all data: 0.039797164
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.035691e-06
Norm of the params: 15.500307
                Loss: fixed 117 labels. Loss 0.03980. Accuracy 0.985.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22184598
Train loss (w/o reg) on all data: 0.21399102
Test loss (w/o reg) on all data: 0.15487067
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.982903e-05
Norm of the params: 12.533934
              Random: fixed  22 labels. Loss 0.15487. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 208
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054741994
Train loss (w/o reg) on all data: 0.045679357
Test loss (w/o reg) on all data: 0.044940107
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.4021936e-06
Norm of the params: 13.463013
     Influence (LOO): fixed 108 labels. Loss 0.04494. Accuracy 0.985.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0197389
Train loss (w/o reg) on all data: 0.010056124
Test loss (w/o reg) on all data: 0.022717241
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1873548e-06
Norm of the params: 13.916017
                Loss: fixed 123 labels. Loss 0.02272. Accuracy 0.996.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20543978
Train loss (w/o reg) on all data: 0.1966315
Test loss (w/o reg) on all data: 0.1334861
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.3248216e-05
Norm of the params: 13.27273
              Random: fixed  35 labels. Loss 0.13349. Accuracy 0.966.
### Flips: 260, rs: 11, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034725875
Train loss (w/o reg) on all data: 0.026761949
Test loss (w/o reg) on all data: 0.027221149
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6054196e-06
Norm of the params: 12.620558
     Influence (LOO): fixed 122 labels. Loss 0.02722. Accuracy 0.989.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017834857
Train loss (w/o reg) on all data: 0.008656944
Test loss (w/o reg) on all data: 0.024078822
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2786588e-06
Norm of the params: 13.548369
                Loss: fixed 124 labels. Loss 0.02408. Accuracy 0.996.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20276164
Train loss (w/o reg) on all data: 0.19410414
Test loss (w/o reg) on all data: 0.12854058
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0342711e-05
Norm of the params: 13.1586485
              Random: fixed  38 labels. Loss 0.12854. Accuracy 0.969.
### Flips: 260, rs: 11, checks: 312
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022689201
Train loss (w/o reg) on all data: 0.016161997
Test loss (w/o reg) on all data: 0.019445427
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7312474e-06
Norm of the params: 11.425589
     Influence (LOO): fixed 127 labels. Loss 0.01945. Accuracy 0.996.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013559151
Train loss (w/o reg) on all data: 0.006217592
Test loss (w/o reg) on all data: 0.01510855
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.1354056e-07
Norm of the params: 12.117393
                Loss: fixed 127 labels. Loss 0.01511. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19413298
Train loss (w/o reg) on all data: 0.18547776
Test loss (w/o reg) on all data: 0.12159029
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.5890844e-06
Norm of the params: 13.1569195
              Random: fixed  45 labels. Loss 0.12159. Accuracy 0.966.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23474856
Train loss (w/o reg) on all data: 0.22610492
Test loss (w/o reg) on all data: 0.20624195
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.3497334e-05
Norm of the params: 13.148115
Flipped loss: 0.20624. Accuracy: 0.927
### Flips: 260, rs: 12, checks: 52
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17079476
Train loss (w/o reg) on all data: 0.16127312
Test loss (w/o reg) on all data: 0.15092853
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0685451e-05
Norm of the params: 13.799736
     Influence (LOO): fixed  37 labels. Loss 0.15093. Accuracy 0.943.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11804344
Train loss (w/o reg) on all data: 0.10254619
Test loss (w/o reg) on all data: 0.16817753
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.07113065e-05
Norm of the params: 17.605251
                Loss: fixed  49 labels. Loss 0.16818. Accuracy 0.927.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22989036
Train loss (w/o reg) on all data: 0.2212967
Test loss (w/o reg) on all data: 0.20135756
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.466866e-05
Norm of the params: 13.110039
              Random: fixed   4 labels. Loss 0.20136. Accuracy 0.931.
### Flips: 260, rs: 12, checks: 104
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11572083
Train loss (w/o reg) on all data: 0.10594267
Test loss (w/o reg) on all data: 0.09300656
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.972986e-06
Norm of the params: 13.984395
     Influence (LOO): fixed  70 labels. Loss 0.09301. Accuracy 0.969.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043713715
Train loss (w/o reg) on all data: 0.028908975
Test loss (w/o reg) on all data: 0.06706027
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8293222e-06
Norm of the params: 17.207405
                Loss: fixed  91 labels. Loss 0.06706. Accuracy 0.966.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21913533
Train loss (w/o reg) on all data: 0.21023181
Test loss (w/o reg) on all data: 0.19518876
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.1491145e-05
Norm of the params: 13.344298
              Random: fixed  12 labels. Loss 0.19519. Accuracy 0.939.
### Flips: 260, rs: 12, checks: 156
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0654465
Train loss (w/o reg) on all data: 0.05590138
Test loss (w/o reg) on all data: 0.054387372
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7182913e-06
Norm of the params: 13.816749
     Influence (LOO): fixed  95 labels. Loss 0.05439. Accuracy 0.985.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026143845
Train loss (w/o reg) on all data: 0.014875026
Test loss (w/o reg) on all data: 0.027485952
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.84486e-07
Norm of the params: 15.012542
                Loss: fixed 107 labels. Loss 0.02749. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21253487
Train loss (w/o reg) on all data: 0.20356484
Test loss (w/o reg) on all data: 0.19483574
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.0559633e-05
Norm of the params: 13.3940525
              Random: fixed  17 labels. Loss 0.19484. Accuracy 0.939.
### Flips: 260, rs: 12, checks: 208
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02885611
Train loss (w/o reg) on all data: 0.020508789
Test loss (w/o reg) on all data: 0.028039623
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5764929e-06
Norm of the params: 12.920776
     Influence (LOO): fixed 113 labels. Loss 0.02804. Accuracy 0.992.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022530135
Train loss (w/o reg) on all data: 0.012479742
Test loss (w/o reg) on all data: 0.017260747
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.096103e-06
Norm of the params: 14.177725
                Loss: fixed 113 labels. Loss 0.01726. Accuracy 0.996.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20775217
Train loss (w/o reg) on all data: 0.19889483
Test loss (w/o reg) on all data: 0.18044792
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.606644e-05
Norm of the params: 13.309652
              Random: fixed  23 labels. Loss 0.18045. Accuracy 0.943.
### Flips: 260, rs: 12, checks: 260
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012206843
Train loss (w/o reg) on all data: 0.0065272874
Test loss (w/o reg) on all data: 0.013513548
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1576805e-06
Norm of the params: 10.657913
     Influence (LOO): fixed 122 labels. Loss 0.01351. Accuracy 0.996.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013092186
Train loss (w/o reg) on all data: 0.0060736383
Test loss (w/o reg) on all data: 0.020320307
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.029525e-07
Norm of the params: 11.847826
                Loss: fixed 118 labels. Loss 0.02032. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20241788
Train loss (w/o reg) on all data: 0.19413476
Test loss (w/o reg) on all data: 0.15651159
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2897017e-05
Norm of the params: 12.870997
              Random: fixed  29 labels. Loss 0.15651. Accuracy 0.954.
### Flips: 260, rs: 12, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008062667
Train loss (w/o reg) on all data: 0.003014576
Test loss (w/o reg) on all data: 0.01026558
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7906646e-07
Norm of the params: 10.0479765
     Influence (LOO): fixed 123 labels. Loss 0.01027. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172917
Test loss (w/o reg) on all data: 0.012055615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2581313e-07
Norm of the params: 9.153254
                Loss: fixed 124 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18921694
Train loss (w/o reg) on all data: 0.18044382
Test loss (w/o reg) on all data: 0.13817888
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5036311e-05
Norm of the params: 13.246221
              Random: fixed  40 labels. Loss 0.13818. Accuracy 0.958.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2369321
Train loss (w/o reg) on all data: 0.22840108
Test loss (w/o reg) on all data: 0.17871706
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.785657e-06
Norm of the params: 13.062175
Flipped loss: 0.17872. Accuracy: 0.947
### Flips: 260, rs: 13, checks: 52
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17376447
Train loss (w/o reg) on all data: 0.16342045
Test loss (w/o reg) on all data: 0.12259166
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.168295e-06
Norm of the params: 14.383338
     Influence (LOO): fixed  37 labels. Loss 0.12259. Accuracy 0.958.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12381692
Train loss (w/o reg) on all data: 0.11017701
Test loss (w/o reg) on all data: 0.15620886
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.954172e-06
Norm of the params: 16.516605
                Loss: fixed  51 labels. Loss 0.15621. Accuracy 0.947.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22964782
Train loss (w/o reg) on all data: 0.22106187
Test loss (w/o reg) on all data: 0.17120478
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.415802e-05
Norm of the params: 13.1041565
              Random: fixed   5 labels. Loss 0.17120. Accuracy 0.950.
### Flips: 260, rs: 13, checks: 104
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117324576
Train loss (w/o reg) on all data: 0.10678661
Test loss (w/o reg) on all data: 0.07872902
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.7418437e-06
Norm of the params: 14.517555
     Influence (LOO): fixed  66 labels. Loss 0.07873. Accuracy 0.981.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048349135
Train loss (w/o reg) on all data: 0.03471605
Test loss (w/o reg) on all data: 0.05940231
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1029979e-06
Norm of the params: 16.51247
                Loss: fixed  96 labels. Loss 0.05940. Accuracy 0.973.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22632374
Train loss (w/o reg) on all data: 0.21735732
Test loss (w/o reg) on all data: 0.17245768
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.692292e-05
Norm of the params: 13.391355
              Random: fixed   9 labels. Loss 0.17246. Accuracy 0.958.
### Flips: 260, rs: 13, checks: 156
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064844176
Train loss (w/o reg) on all data: 0.054507434
Test loss (w/o reg) on all data: 0.0449086
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.860693e-06
Norm of the params: 14.37828
     Influence (LOO): fixed  93 labels. Loss 0.04491. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017743547
Train loss (w/o reg) on all data: 0.008808635
Test loss (w/o reg) on all data: 0.014451245
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.2025927e-07
Norm of the params: 13.367806
                Loss: fixed 112 labels. Loss 0.01445. Accuracy 0.996.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22258444
Train loss (w/o reg) on all data: 0.21351491
Test loss (w/o reg) on all data: 0.16356312
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.7510963e-05
Norm of the params: 13.468134
              Random: fixed  12 labels. Loss 0.16356. Accuracy 0.966.
### Flips: 260, rs: 13, checks: 208
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037340302
Train loss (w/o reg) on all data: 0.028702855
Test loss (w/o reg) on all data: 0.03373049
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3248293e-06
Norm of the params: 13.1434
     Influence (LOO): fixed 107 labels. Loss 0.03373. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00939632
Train loss (w/o reg) on all data: 0.0036863112
Test loss (w/o reg) on all data: 0.010868342
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7321788e-06
Norm of the params: 10.686449
                Loss: fixed 117 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21762685
Train loss (w/o reg) on all data: 0.208822
Test loss (w/o reg) on all data: 0.14747171
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.1039664e-05
Norm of the params: 13.2701645
              Random: fixed  17 labels. Loss 0.14747. Accuracy 0.969.
### Flips: 260, rs: 13, checks: 260
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021283872
Train loss (w/o reg) on all data: 0.014824717
Test loss (w/o reg) on all data: 0.020236393
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.154568e-07
Norm of the params: 11.365874
     Influence (LOO): fixed 115 labels. Loss 0.02024. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345538
Train loss (w/o reg) on all data: 0.0031104393
Test loss (w/o reg) on all data: 0.011751124
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5301389e-07
Norm of the params: 10.232399
                Loss: fixed 118 labels. Loss 0.01175. Accuracy 0.996.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21081913
Train loss (w/o reg) on all data: 0.2018946
Test loss (w/o reg) on all data: 0.14511152
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7892286e-05
Norm of the params: 13.360032
              Random: fixed  23 labels. Loss 0.14511. Accuracy 0.966.
### Flips: 260, rs: 13, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012875004
Train loss (w/o reg) on all data: 0.007557876
Test loss (w/o reg) on all data: 0.014518705
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.807163e-06
Norm of the params: 10.312254
     Influence (LOO): fixed 118 labels. Loss 0.01452. Accuracy 0.989.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971535
Train loss (w/o reg) on all data: 0.0025172182
Test loss (w/o reg) on all data: 0.011187391
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1309865e-07
Norm of the params: 9.438556
                Loss: fixed 119 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20603468
Train loss (w/o reg) on all data: 0.19776565
Test loss (w/o reg) on all data: 0.12996721
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.2836015e-05
Norm of the params: 12.860035
              Random: fixed  29 labels. Loss 0.12997. Accuracy 0.977.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2457018
Train loss (w/o reg) on all data: 0.23854466
Test loss (w/o reg) on all data: 0.18192238
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3262333e-05
Norm of the params: 11.964234
Flipped loss: 0.18192. Accuracy: 0.954
### Flips: 260, rs: 14, checks: 52
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920977
Train loss (w/o reg) on all data: 0.18060519
Test loss (w/o reg) on all data: 0.13736372
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8669889e-05
Norm of the params: 13.118372
     Influence (LOO): fixed  35 labels. Loss 0.13736. Accuracy 0.977.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13356614
Train loss (w/o reg) on all data: 0.122110344
Test loss (w/o reg) on all data: 0.12277971
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.4755783e-05
Norm of the params: 15.136575
                Loss: fixed  51 labels. Loss 0.12278. Accuracy 0.966.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23053703
Train loss (w/o reg) on all data: 0.22300693
Test loss (w/o reg) on all data: 0.1644639
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3164797e-05
Norm of the params: 12.271998
              Random: fixed  10 labels. Loss 0.16446. Accuracy 0.966.
### Flips: 260, rs: 14, checks: 104
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13752986
Train loss (w/o reg) on all data: 0.12791692
Test loss (w/o reg) on all data: 0.09336325
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2291477e-05
Norm of the params: 13.865744
     Influence (LOO): fixed  66 labels. Loss 0.09336. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046818905
Train loss (w/o reg) on all data: 0.033656403
Test loss (w/o reg) on all data: 0.051680177
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.821028e-06
Norm of the params: 16.224981
                Loss: fixed  98 labels. Loss 0.05168. Accuracy 0.981.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173049
Train loss (w/o reg) on all data: 0.21454626
Test loss (w/o reg) on all data: 0.15084107
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.09258e-05
Norm of the params: 11.986845
              Random: fixed  18 labels. Loss 0.15084. Accuracy 0.969.
### Flips: 260, rs: 14, checks: 156
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08511189
Train loss (w/o reg) on all data: 0.0750699
Test loss (w/o reg) on all data: 0.053601716
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.410825e-06
Norm of the params: 14.171803
     Influence (LOO): fixed  93 labels. Loss 0.05360. Accuracy 0.989.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019431282
Train loss (w/o reg) on all data: 0.010418869
Test loss (w/o reg) on all data: 0.019725367
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.097027e-07
Norm of the params: 13.425656
                Loss: fixed 116 labels. Loss 0.01973. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21413088
Train loss (w/o reg) on all data: 0.20724653
Test loss (w/o reg) on all data: 0.14281754
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6437863e-05
Norm of the params: 11.734008
              Random: fixed  24 labels. Loss 0.14282. Accuracy 0.966.
### Flips: 260, rs: 14, checks: 208
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047527015
Train loss (w/o reg) on all data: 0.039878156
Test loss (w/o reg) on all data: 0.03578636
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1166771e-06
Norm of the params: 12.368394
     Influence (LOO): fixed 113 labels. Loss 0.03579. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01277332
Train loss (w/o reg) on all data: 0.005637519
Test loss (w/o reg) on all data: 0.013552967
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.4143996e-07
Norm of the params: 11.946382
                Loss: fixed 121 labels. Loss 0.01355. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20790946
Train loss (w/o reg) on all data: 0.20133059
Test loss (w/o reg) on all data: 0.12815931
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9735498e-05
Norm of the params: 11.470733
              Random: fixed  31 labels. Loss 0.12816. Accuracy 0.977.
### Flips: 260, rs: 14, checks: 260
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015799087
Train loss (w/o reg) on all data: 0.009905371
Test loss (w/o reg) on all data: 0.024313906
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1450364e-06
Norm of the params: 10.856993
     Influence (LOO): fixed 122 labels. Loss 0.02431. Accuracy 0.989.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011847426
Train loss (w/o reg) on all data: 0.0050396663
Test loss (w/o reg) on all data: 0.012508458
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.927397e-07
Norm of the params: 11.668556
                Loss: fixed 122 labels. Loss 0.01251. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20434988
Train loss (w/o reg) on all data: 0.19781369
Test loss (w/o reg) on all data: 0.1265336
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4869109e-05
Norm of the params: 11.433442
              Random: fixed  35 labels. Loss 0.12653. Accuracy 0.973.
### Flips: 260, rs: 14, checks: 312
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011851839
Train loss (w/o reg) on all data: 0.006667019
Test loss (w/o reg) on all data: 0.0123471
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.665199e-07
Norm of the params: 10.183143
     Influence (LOO): fixed 124 labels. Loss 0.01235. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009739217
Train loss (w/o reg) on all data: 0.0038063487
Test loss (w/o reg) on all data: 0.016786441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1200235e-07
Norm of the params: 10.892997
                Loss: fixed 123 labels. Loss 0.01679. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19844607
Train loss (w/o reg) on all data: 0.19192904
Test loss (w/o reg) on all data: 0.11673138
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.220207e-06
Norm of the params: 11.416671
              Random: fixed  42 labels. Loss 0.11673. Accuracy 0.989.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24088313
Train loss (w/o reg) on all data: 0.23403724
Test loss (w/o reg) on all data: 0.15550299
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4208042e-05
Norm of the params: 11.701187
Flipped loss: 0.15550. Accuracy: 0.977
### Flips: 260, rs: 15, checks: 52
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18022148
Train loss (w/o reg) on all data: 0.17104618
Test loss (w/o reg) on all data: 0.11559143
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8237484e-05
Norm of the params: 13.54644
     Influence (LOO): fixed  37 labels. Loss 0.11559. Accuracy 0.973.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12413616
Train loss (w/o reg) on all data: 0.11326244
Test loss (w/o reg) on all data: 0.103980966
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.4031098e-06
Norm of the params: 14.747014
                Loss: fixed  50 labels. Loss 0.10398. Accuracy 0.969.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23571737
Train loss (w/o reg) on all data: 0.22873224
Test loss (w/o reg) on all data: 0.14506903
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1400691e-05
Norm of the params: 11.819584
              Random: fixed   6 labels. Loss 0.14507. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13235548
Train loss (w/o reg) on all data: 0.123394676
Test loss (w/o reg) on all data: 0.0972135
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.8116652e-06
Norm of the params: 13.387159
     Influence (LOO): fixed  62 labels. Loss 0.09721. Accuracy 0.969.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046620816
Train loss (w/o reg) on all data: 0.032770272
Test loss (w/o reg) on all data: 0.04191989
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.197976e-06
Norm of the params: 16.643644
                Loss: fixed  94 labels. Loss 0.04192. Accuracy 0.981.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23259656
Train loss (w/o reg) on all data: 0.2257129
Test loss (w/o reg) on all data: 0.1426199
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5650737e-05
Norm of the params: 11.733424
              Random: fixed  10 labels. Loss 0.14262. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 156
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08128933
Train loss (w/o reg) on all data: 0.07267933
Test loss (w/o reg) on all data: 0.07263048
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.917188e-06
Norm of the params: 13.1224985
     Influence (LOO): fixed  86 labels. Loss 0.07263. Accuracy 0.969.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018214192
Train loss (w/o reg) on all data: 0.009943902
Test loss (w/o reg) on all data: 0.016336022
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4521473e-06
Norm of the params: 12.86102
                Loss: fixed 114 labels. Loss 0.01634. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22333112
Train loss (w/o reg) on all data: 0.21651877
Test loss (w/o reg) on all data: 0.12968227
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.312189e-05
Norm of the params: 11.672489
              Random: fixed  17 labels. Loss 0.12968. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 208
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03085778
Train loss (w/o reg) on all data: 0.023672808
Test loss (w/o reg) on all data: 0.024299411
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3495348e-06
Norm of the params: 11.987469
     Influence (LOO): fixed 108 labels. Loss 0.02430. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011271954
Train loss (w/o reg) on all data: 0.0050860844
Test loss (w/o reg) on all data: 0.012287448
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.238325e-06
Norm of the params: 11.122832
                Loss: fixed 119 labels. Loss 0.01229. Accuracy 0.996.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20862237
Train loss (w/o reg) on all data: 0.20170319
Test loss (w/o reg) on all data: 0.11697437
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9797171e-05
Norm of the params: 11.763647
              Random: fixed  28 labels. Loss 0.11697. Accuracy 0.985.
### Flips: 260, rs: 15, checks: 260
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016013231
Train loss (w/o reg) on all data: 0.008954374
Test loss (w/o reg) on all data: 0.010582931
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.658002e-07
Norm of the params: 11.8818
     Influence (LOO): fixed 115 labels. Loss 0.01058. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012054852
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0887794e-07
Norm of the params: 9.153226
                Loss: fixed 122 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1918229
Train loss (w/o reg) on all data: 0.1841041
Test loss (w/o reg) on all data: 0.10465372
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.26967225e-05
Norm of the params: 12.424815
              Random: fixed  36 labels. Loss 0.10465. Accuracy 0.981.
### Flips: 260, rs: 15, checks: 312
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011884762
Train loss (w/o reg) on all data: 0.0056841355
Test loss (w/o reg) on all data: 0.012078928
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7670537e-06
Norm of the params: 11.136092
     Influence (LOO): fixed 119 labels. Loss 0.01208. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730363
Test loss (w/o reg) on all data: 0.012055683
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2148102e-07
Norm of the params: 9.153123
                Loss: fixed 122 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1793283
Train loss (w/o reg) on all data: 0.171444
Test loss (w/o reg) on all data: 0.096239485
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1421397e-05
Norm of the params: 12.557305
              Random: fixed  43 labels. Loss 0.09624. Accuracy 0.985.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24631318
Train loss (w/o reg) on all data: 0.23868255
Test loss (w/o reg) on all data: 0.20465574
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.45212225e-05
Norm of the params: 12.353651
Flipped loss: 0.20466. Accuracy: 0.924
### Flips: 260, rs: 16, checks: 52
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18969026
Train loss (w/o reg) on all data: 0.17927365
Test loss (w/o reg) on all data: 0.17484014
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.20037175e-05
Norm of the params: 14.433714
     Influence (LOO): fixed  37 labels. Loss 0.17484. Accuracy 0.935.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13871033
Train loss (w/o reg) on all data: 0.12275124
Test loss (w/o reg) on all data: 0.17652483
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.5707135e-05
Norm of the params: 17.865664
                Loss: fixed  52 labels. Loss 0.17652. Accuracy 0.920.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24293652
Train loss (w/o reg) on all data: 0.2351377
Test loss (w/o reg) on all data: 0.20165566
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 6.791803e-05
Norm of the params: 12.489053
              Random: fixed   4 labels. Loss 0.20166. Accuracy 0.924.
### Flips: 260, rs: 16, checks: 104
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13470158
Train loss (w/o reg) on all data: 0.12298869
Test loss (w/o reg) on all data: 0.12662093
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2432631e-05
Norm of the params: 15.305484
     Influence (LOO): fixed  67 labels. Loss 0.12662. Accuracy 0.962.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06393824
Train loss (w/o reg) on all data: 0.048366915
Test loss (w/o reg) on all data: 0.11181443
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.091278e-06
Norm of the params: 17.647278
                Loss: fixed  92 labels. Loss 0.11181. Accuracy 0.954.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23505923
Train loss (w/o reg) on all data: 0.22659838
Test loss (w/o reg) on all data: 0.20104983
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 6.1601786e-05
Norm of the params: 13.0083475
              Random: fixed  10 labels. Loss 0.20105. Accuracy 0.916.
### Flips: 260, rs: 16, checks: 156
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090770334
Train loss (w/o reg) on all data: 0.07903311
Test loss (w/o reg) on all data: 0.07901652
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.417454e-05
Norm of the params: 15.321376
     Influence (LOO): fixed  92 labels. Loss 0.07902. Accuracy 0.985.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031717636
Train loss (w/o reg) on all data: 0.018725732
Test loss (w/o reg) on all data: 0.05976919
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.142019e-06
Norm of the params: 16.119493
                Loss: fixed 113 labels. Loss 0.05977. Accuracy 0.973.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22872353
Train loss (w/o reg) on all data: 0.21977825
Test loss (w/o reg) on all data: 0.20030938
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 6.389172e-05
Norm of the params: 13.375559
              Random: fixed  15 labels. Loss 0.20031. Accuracy 0.924.
### Flips: 260, rs: 16, checks: 208
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06462543
Train loss (w/o reg) on all data: 0.055401865
Test loss (w/o reg) on all data: 0.049461
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.537296e-06
Norm of the params: 13.582019
     Influence (LOO): fixed 110 labels. Loss 0.04946. Accuracy 0.989.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020123018
Train loss (w/o reg) on all data: 0.010957565
Test loss (w/o reg) on all data: 0.02388298
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.739578e-06
Norm of the params: 13.539167
                Loss: fixed 124 labels. Loss 0.02388. Accuracy 0.996.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21955831
Train loss (w/o reg) on all data: 0.21038932
Test loss (w/o reg) on all data: 0.1858798
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.2453194e-05
Norm of the params: 13.541782
              Random: fixed  21 labels. Loss 0.18588. Accuracy 0.935.
### Flips: 260, rs: 16, checks: 260
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025208155
Train loss (w/o reg) on all data: 0.018216822
Test loss (w/o reg) on all data: 0.022768458
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6267824e-06
Norm of the params: 11.824832
     Influence (LOO): fixed 126 labels. Loss 0.02277. Accuracy 0.989.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016105955
Train loss (w/o reg) on all data: 0.007855411
Test loss (w/o reg) on all data: 0.01758911
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1538576e-06
Norm of the params: 12.845656
                Loss: fixed 127 labels. Loss 0.01759. Accuracy 0.996.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2088411
Train loss (w/o reg) on all data: 0.19834591
Test loss (w/o reg) on all data: 0.19070137
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9373541e-05
Norm of the params: 14.488053
              Random: fixed  29 labels. Loss 0.19070. Accuracy 0.935.
### Flips: 260, rs: 16, checks: 312
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01212627
Train loss (w/o reg) on all data: 0.0065874383
Test loss (w/o reg) on all data: 0.017487254
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.82598e-07
Norm of the params: 10.525048
     Influence (LOO): fixed 131 labels. Loss 0.01749. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008001955
Train loss (w/o reg) on all data: 0.0031207525
Test loss (w/o reg) on all data: 0.0134960115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1815984e-08
Norm of the params: 9.880489
                Loss: fixed 132 labels. Loss 0.01350. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2017661
Train loss (w/o reg) on all data: 0.19127384
Test loss (w/o reg) on all data: 0.16563511
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.3489889e-05
Norm of the params: 14.486042
              Random: fixed  37 labels. Loss 0.16564. Accuracy 0.947.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2475843
Train loss (w/o reg) on all data: 0.23996887
Test loss (w/o reg) on all data: 0.15559296
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.0100283e-05
Norm of the params: 12.341344
Flipped loss: 0.15559. Accuracy: 0.977
### Flips: 260, rs: 17, checks: 52
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18595284
Train loss (w/o reg) on all data: 0.17508188
Test loss (w/o reg) on all data: 0.13241537
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2842429e-05
Norm of the params: 14.745148
     Influence (LOO): fixed  37 labels. Loss 0.13242. Accuracy 0.954.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13175404
Train loss (w/o reg) on all data: 0.1144295
Test loss (w/o reg) on all data: 0.09968024
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9732575e-05
Norm of the params: 18.614258
                Loss: fixed  51 labels. Loss 0.09968. Accuracy 0.966.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23446372
Train loss (w/o reg) on all data: 0.22670496
Test loss (w/o reg) on all data: 0.14570948
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.489039e-06
Norm of the params: 12.4569435
              Random: fixed  13 labels. Loss 0.14571. Accuracy 0.969.
### Flips: 260, rs: 17, checks: 104
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13114229
Train loss (w/o reg) on all data: 0.11829807
Test loss (w/o reg) on all data: 0.11411468
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.496705e-06
Norm of the params: 16.027617
     Influence (LOO): fixed  65 labels. Loss 0.11411. Accuracy 0.973.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0565404
Train loss (w/o reg) on all data: 0.037966087
Test loss (w/o reg) on all data: 0.05055948
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8156119e-06
Norm of the params: 19.273977
                Loss: fixed  98 labels. Loss 0.05056. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22654472
Train loss (w/o reg) on all data: 0.2187579
Test loss (w/o reg) on all data: 0.13136898
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1100197e-05
Norm of the params: 12.479449
              Random: fixed  20 labels. Loss 0.13137. Accuracy 0.981.
### Flips: 260, rs: 17, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086715735
Train loss (w/o reg) on all data: 0.07633076
Test loss (w/o reg) on all data: 0.087446526
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.924015e-06
Norm of the params: 14.411782
     Influence (LOO): fixed  93 labels. Loss 0.08745. Accuracy 0.973.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030906063
Train loss (w/o reg) on all data: 0.01710938
Test loss (w/o reg) on all data: 0.031580426
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.480669e-06
Norm of the params: 16.611252
                Loss: fixed 116 labels. Loss 0.03158. Accuracy 0.989.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22041133
Train loss (w/o reg) on all data: 0.21295737
Test loss (w/o reg) on all data: 0.12220857
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8676583e-05
Norm of the params: 12.2098055
              Random: fixed  24 labels. Loss 0.12221. Accuracy 0.981.
### Flips: 260, rs: 17, checks: 208
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05247642
Train loss (w/o reg) on all data: 0.043799605
Test loss (w/o reg) on all data: 0.034837533
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2721969e-06
Norm of the params: 13.17332
     Influence (LOO): fixed 113 labels. Loss 0.03484. Accuracy 0.989.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0191413
Train loss (w/o reg) on all data: 0.009504677
Test loss (w/o reg) on all data: 0.011840351
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3221787e-06
Norm of the params: 13.882812
                Loss: fixed 123 labels. Loss 0.01184. Accuracy 1.000.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2119109
Train loss (w/o reg) on all data: 0.20354536
Test loss (w/o reg) on all data: 0.11711141
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.2173495e-06
Norm of the params: 12.934864
              Random: fixed  30 labels. Loss 0.11711. Accuracy 0.985.
### Flips: 260, rs: 17, checks: 260
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030076161
Train loss (w/o reg) on all data: 0.02366346
Test loss (w/o reg) on all data: 0.024080133
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3374438e-06
Norm of the params: 11.324931
     Influence (LOO): fixed 124 labels. Loss 0.02408. Accuracy 0.992.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016610056
Train loss (w/o reg) on all data: 0.0077842725
Test loss (w/o reg) on all data: 0.015145961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8052857e-06
Norm of the params: 13.28592
                Loss: fixed 125 labels. Loss 0.01515. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20544943
Train loss (w/o reg) on all data: 0.19701387
Test loss (w/o reg) on all data: 0.10331106
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.33531e-05
Norm of the params: 12.988886
              Random: fixed  36 labels. Loss 0.10331. Accuracy 0.996.
### Flips: 260, rs: 17, checks: 312
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023814412
Train loss (w/o reg) on all data: 0.01779459
Test loss (w/o reg) on all data: 0.022930747
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1682197e-06
Norm of the params: 10.97253
     Influence (LOO): fixed 127 labels. Loss 0.02293. Accuracy 0.992.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014214225
Train loss (w/o reg) on all data: 0.006422092
Test loss (w/o reg) on all data: 0.010461543
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.1139025e-07
Norm of the params: 12.483696
                Loss: fixed 128 labels. Loss 0.01046. Accuracy 0.996.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20145747
Train loss (w/o reg) on all data: 0.19378498
Test loss (w/o reg) on all data: 0.10177238
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8788627e-05
Norm of the params: 12.387479
              Random: fixed  41 labels. Loss 0.10177. Accuracy 0.996.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24249405
Train loss (w/o reg) on all data: 0.23582067
Test loss (w/o reg) on all data: 0.14373848
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1803313e-05
Norm of the params: 11.552818
Flipped loss: 0.14374. Accuracy: 0.985
### Flips: 260, rs: 18, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17886978
Train loss (w/o reg) on all data: 0.16817148
Test loss (w/o reg) on all data: 0.11515273
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.670666e-06
Norm of the params: 14.627575
     Influence (LOO): fixed  35 labels. Loss 0.11515. Accuracy 0.977.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123970546
Train loss (w/o reg) on all data: 0.111247666
Test loss (w/o reg) on all data: 0.09677535
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.765632e-05
Norm of the params: 15.951728
                Loss: fixed  52 labels. Loss 0.09678. Accuracy 0.977.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23365173
Train loss (w/o reg) on all data: 0.22705515
Test loss (w/o reg) on all data: 0.13196866
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5636002e-05
Norm of the params: 11.486153
              Random: fixed   8 labels. Loss 0.13197. Accuracy 0.985.
### Flips: 260, rs: 18, checks: 104
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115330756
Train loss (w/o reg) on all data: 0.1032043
Test loss (w/o reg) on all data: 0.07886525
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.14519e-05
Norm of the params: 15.573347
     Influence (LOO): fixed  67 labels. Loss 0.07887. Accuracy 0.977.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039344713
Train loss (w/o reg) on all data: 0.024618916
Test loss (w/o reg) on all data: 0.03866099
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9849326e-06
Norm of the params: 17.161467
                Loss: fixed  95 labels. Loss 0.03866. Accuracy 0.985.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22141428
Train loss (w/o reg) on all data: 0.21480133
Test loss (w/o reg) on all data: 0.12184795
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8193277e-05
Norm of the params: 11.500391
              Random: fixed  16 labels. Loss 0.12185. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 156
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06000671
Train loss (w/o reg) on all data: 0.049329855
Test loss (w/o reg) on all data: 0.038517397
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8108034e-06
Norm of the params: 14.612909
     Influence (LOO): fixed  98 labels. Loss 0.03852. Accuracy 0.989.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014693778
Train loss (w/o reg) on all data: 0.006667848
Test loss (w/o reg) on all data: 0.01578402
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0103196e-06
Norm of the params: 12.669592
                Loss: fixed 115 labels. Loss 0.01578. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21199554
Train loss (w/o reg) on all data: 0.20512585
Test loss (w/o reg) on all data: 0.11414352
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6857124e-05
Norm of the params: 11.721511
              Random: fixed  23 labels. Loss 0.11414. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 208
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022103552
Train loss (w/o reg) on all data: 0.013506802
Test loss (w/o reg) on all data: 0.016483702
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5349043e-06
Norm of the params: 13.112398
     Influence (LOO): fixed 112 labels. Loss 0.01648. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011027014
Train loss (w/o reg) on all data: 0.0045428746
Test loss (w/o reg) on all data: 0.012272717
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4740023e-07
Norm of the params: 11.387835
                Loss: fixed 117 labels. Loss 0.01227. Accuracy 0.996.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20693849
Train loss (w/o reg) on all data: 0.20027867
Test loss (w/o reg) on all data: 0.11417378
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8817638e-05
Norm of the params: 11.541078
              Random: fixed  28 labels. Loss 0.11417. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 260
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012861236
Train loss (w/o reg) on all data: 0.007255068
Test loss (w/o reg) on all data: 0.012154001
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.22434e-07
Norm of the params: 10.588832
     Influence (LOO): fixed 118 labels. Loss 0.01215. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011027012
Train loss (w/o reg) on all data: 0.004543106
Test loss (w/o reg) on all data: 0.0122725265
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3441701e-06
Norm of the params: 11.387631
                Loss: fixed 117 labels. Loss 0.01227. Accuracy 0.996.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19993478
Train loss (w/o reg) on all data: 0.19362976
Test loss (w/o reg) on all data: 0.11177011
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.755194e-06
Norm of the params: 11.229444
              Random: fixed  33 labels. Loss 0.11177. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729723
Test loss (w/o reg) on all data: 0.012054762
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7294632e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed 120 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008934774
Train loss (w/o reg) on all data: 0.0035451413
Test loss (w/o reg) on all data: 0.012451435
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7663438e-07
Norm of the params: 10.382324
                Loss: fixed 118 labels. Loss 0.01245. Accuracy 0.996.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19467995
Train loss (w/o reg) on all data: 0.1881123
Test loss (w/o reg) on all data: 0.108542114
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1093427e-05
Norm of the params: 11.460925
              Random: fixed  37 labels. Loss 0.10854. Accuracy 0.985.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23707531
Train loss (w/o reg) on all data: 0.2310439
Test loss (w/o reg) on all data: 0.20230384
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.9580304e-05
Norm of the params: 10.983089
Flipped loss: 0.20230. Accuracy: 0.916
### Flips: 260, rs: 19, checks: 52
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17679974
Train loss (w/o reg) on all data: 0.16829482
Test loss (w/o reg) on all data: 0.16311163
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.5053643e-05
Norm of the params: 13.042189
     Influence (LOO): fixed  37 labels. Loss 0.16311. Accuracy 0.947.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123481594
Train loss (w/o reg) on all data: 0.11008647
Test loss (w/o reg) on all data: 0.14963977
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.9514604e-05
Norm of the params: 16.367725
                Loss: fixed  50 labels. Loss 0.14964. Accuracy 0.931.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23633377
Train loss (w/o reg) on all data: 0.23028795
Test loss (w/o reg) on all data: 0.20087567
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.5831612e-05
Norm of the params: 10.996191
              Random: fixed   2 labels. Loss 0.20088. Accuracy 0.916.
### Flips: 260, rs: 19, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124866225
Train loss (w/o reg) on all data: 0.11508824
Test loss (w/o reg) on all data: 0.11365333
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.303321e-05
Norm of the params: 13.984268
     Influence (LOO): fixed  68 labels. Loss 0.11365. Accuracy 0.962.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05359106
Train loss (w/o reg) on all data: 0.03727915
Test loss (w/o reg) on all data: 0.11005296
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1623455e-06
Norm of the params: 18.062065
                Loss: fixed  90 labels. Loss 0.11005. Accuracy 0.954.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23167023
Train loss (w/o reg) on all data: 0.22529085
Test loss (w/o reg) on all data: 0.19212805
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.3969795e-05
Norm of the params: 11.295468
              Random: fixed   7 labels. Loss 0.19213. Accuracy 0.920.
### Flips: 260, rs: 19, checks: 156
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07104657
Train loss (w/o reg) on all data: 0.060403887
Test loss (w/o reg) on all data: 0.09014158
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.399745e-06
Norm of the params: 14.589505
     Influence (LOO): fixed  94 labels. Loss 0.09014. Accuracy 0.954.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030091684
Train loss (w/o reg) on all data: 0.017431024
Test loss (w/o reg) on all data: 0.0457887
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.5382433e-06
Norm of the params: 15.912674
                Loss: fixed 109 labels. Loss 0.04579. Accuracy 0.977.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22842565
Train loss (w/o reg) on all data: 0.22195786
Test loss (w/o reg) on all data: 0.17663087
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.6759253e-05
Norm of the params: 11.373467
              Random: fixed  13 labels. Loss 0.17663. Accuracy 0.935.
### Flips: 260, rs: 19, checks: 208
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041932225
Train loss (w/o reg) on all data: 0.032126438
Test loss (w/o reg) on all data: 0.07962361
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6328767e-06
Norm of the params: 14.004135
     Influence (LOO): fixed 108 labels. Loss 0.07962. Accuracy 0.966.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023181574
Train loss (w/o reg) on all data: 0.012260721
Test loss (w/o reg) on all data: 0.037827704
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7818849e-06
Norm of the params: 14.77894
                Loss: fixed 117 labels. Loss 0.03783. Accuracy 0.985.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21662028
Train loss (w/o reg) on all data: 0.20985985
Test loss (w/o reg) on all data: 0.1690148
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 0.00010935787
Norm of the params: 11.627927
              Random: fixed  21 labels. Loss 0.16901. Accuracy 0.943.
### Flips: 260, rs: 19, checks: 260
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02497743
Train loss (w/o reg) on all data: 0.015852325
Test loss (w/o reg) on all data: 0.065733664
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0776011e-06
Norm of the params: 13.5093355
     Influence (LOO): fixed 115 labels. Loss 0.06573. Accuracy 0.969.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01419461
Train loss (w/o reg) on all data: 0.0064139515
Test loss (w/o reg) on all data: 0.024016311
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1843565e-06
Norm of the params: 12.474501
                Loss: fixed 123 labels. Loss 0.02402. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21071835
Train loss (w/o reg) on all data: 0.20369117
Test loss (w/o reg) on all data: 0.15688428
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.4820401e-05
Norm of the params: 11.855111
              Random: fixed  27 labels. Loss 0.15688. Accuracy 0.943.
### Flips: 260, rs: 19, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01137143
Train loss (w/o reg) on all data: 0.0058916872
Test loss (w/o reg) on all data: 0.024280591
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8260542e-06
Norm of the params: 10.468757
     Influence (LOO): fixed 126 labels. Loss 0.02428. Accuracy 0.989.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011434048
Train loss (w/o reg) on all data: 0.004718617
Test loss (w/o reg) on all data: 0.017107505
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0983204e-06
Norm of the params: 11.58916
                Loss: fixed 125 labels. Loss 0.01711. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20439965
Train loss (w/o reg) on all data: 0.19778208
Test loss (w/o reg) on all data: 0.14712359
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.6627982e-05
Norm of the params: 11.504397
              Random: fixed  34 labels. Loss 0.14712. Accuracy 0.954.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24502787
Train loss (w/o reg) on all data: 0.23730686
Test loss (w/o reg) on all data: 0.17496814
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.297394e-05
Norm of the params: 12.426595
Flipped loss: 0.17497. Accuracy: 0.958
### Flips: 260, rs: 20, checks: 52
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18398546
Train loss (w/o reg) on all data: 0.17358515
Test loss (w/o reg) on all data: 0.13757552
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5444004e-05
Norm of the params: 14.422422
     Influence (LOO): fixed  37 labels. Loss 0.13758. Accuracy 0.977.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13701676
Train loss (w/o reg) on all data: 0.123866126
Test loss (w/o reg) on all data: 0.12484259
Train acc on all data:  0.941738299904489
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.353534e-06
Norm of the params: 16.217669
                Loss: fixed  51 labels. Loss 0.12484. Accuracy 0.950.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23826727
Train loss (w/o reg) on all data: 0.23090385
Test loss (w/o reg) on all data: 0.16396789
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5301783e-05
Norm of the params: 12.135423
              Random: fixed   8 labels. Loss 0.16397. Accuracy 0.973.
### Flips: 260, rs: 20, checks: 104
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13263333
Train loss (w/o reg) on all data: 0.12193489
Test loss (w/o reg) on all data: 0.10819808
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.7582278e-05
Norm of the params: 14.627672
     Influence (LOO): fixed  65 labels. Loss 0.10820. Accuracy 0.962.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052793246
Train loss (w/o reg) on all data: 0.038168777
Test loss (w/o reg) on all data: 0.07279281
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.3671306e-06
Norm of the params: 17.102322
                Loss: fixed 100 labels. Loss 0.07279. Accuracy 0.985.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23223275
Train loss (w/o reg) on all data: 0.2249562
Test loss (w/o reg) on all data: 0.15851124
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2440422e-05
Norm of the params: 12.063618
              Random: fixed  14 labels. Loss 0.15851. Accuracy 0.966.
### Flips: 260, rs: 20, checks: 156
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073840186
Train loss (w/o reg) on all data: 0.0637466
Test loss (w/o reg) on all data: 0.056606904
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.549645e-06
Norm of the params: 14.2081585
     Influence (LOO): fixed  94 labels. Loss 0.05661. Accuracy 0.977.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02350492
Train loss (w/o reg) on all data: 0.012280912
Test loss (w/o reg) on all data: 0.04090504
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.894358e-06
Norm of the params: 14.982662
                Loss: fixed 115 labels. Loss 0.04091. Accuracy 0.989.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22328442
Train loss (w/o reg) on all data: 0.2159523
Test loss (w/o reg) on all data: 0.15455501
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4409312e-05
Norm of the params: 12.1096
              Random: fixed  20 labels. Loss 0.15456. Accuracy 0.966.
### Flips: 260, rs: 20, checks: 208
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053810004
Train loss (w/o reg) on all data: 0.04349078
Test loss (w/o reg) on all data: 0.049633358
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.0196204e-06
Norm of the params: 14.366088
     Influence (LOO): fixed 104 labels. Loss 0.04963. Accuracy 0.977.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01727261
Train loss (w/o reg) on all data: 0.007658699
Test loss (w/o reg) on all data: 0.043795146
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1213979e-06
Norm of the params: 13.866444
                Loss: fixed 118 labels. Loss 0.04380. Accuracy 0.989.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21413212
Train loss (w/o reg) on all data: 0.20615268
Test loss (w/o reg) on all data: 0.14399604
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0417185e-05
Norm of the params: 12.632844
              Random: fixed  26 labels. Loss 0.14400. Accuracy 0.973.
### Flips: 260, rs: 20, checks: 260
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027270153
Train loss (w/o reg) on all data: 0.018454207
Test loss (w/o reg) on all data: 0.028852116
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.1575056e-06
Norm of the params: 13.278514
     Influence (LOO): fixed 117 labels. Loss 0.02885. Accuracy 0.985.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014752887
Train loss (w/o reg) on all data: 0.006305139
Test loss (w/o reg) on all data: 0.041109517
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6722939e-06
Norm of the params: 12.998267
                Loss: fixed 120 labels. Loss 0.04111. Accuracy 0.985.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20823029
Train loss (w/o reg) on all data: 0.20045075
Test loss (w/o reg) on all data: 0.13627054
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2270195e-05
Norm of the params: 12.4736
              Random: fixed  32 labels. Loss 0.13627. Accuracy 0.981.
### Flips: 260, rs: 20, checks: 312
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017818792
Train loss (w/o reg) on all data: 0.010719122
Test loss (w/o reg) on all data: 0.01677668
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.6763627e-07
Norm of the params: 11.916098
     Influence (LOO): fixed 122 labels. Loss 0.01678. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01122853
Train loss (w/o reg) on all data: 0.004279509
Test loss (w/o reg) on all data: 0.030524377
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.1593435e-07
Norm of the params: 11.788996
                Loss: fixed 121 labels. Loss 0.03052. Accuracy 0.985.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19967812
Train loss (w/o reg) on all data: 0.19190173
Test loss (w/o reg) on all data: 0.12952225
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.6783815e-05
Norm of the params: 12.471087
              Random: fixed  38 labels. Loss 0.12952. Accuracy 0.977.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2363765
Train loss (w/o reg) on all data: 0.227256
Test loss (w/o reg) on all data: 0.16081855
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.2534872e-05
Norm of the params: 13.505921
Flipped loss: 0.16082. Accuracy: 0.969
### Flips: 260, rs: 21, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18015878
Train loss (w/o reg) on all data: 0.16799082
Test loss (w/o reg) on all data: 0.10990919
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0000654e-05
Norm of the params: 15.5999775
     Influence (LOO): fixed  34 labels. Loss 0.10991. Accuracy 0.985.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12495912
Train loss (w/o reg) on all data: 0.10953059
Test loss (w/o reg) on all data: 0.122424975
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.853677e-05
Norm of the params: 17.566175
                Loss: fixed  51 labels. Loss 0.12242. Accuracy 0.943.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23204426
Train loss (w/o reg) on all data: 0.22289343
Test loss (w/o reg) on all data: 0.14741392
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.5243235e-05
Norm of the params: 13.528361
              Random: fixed   7 labels. Loss 0.14741. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11860545
Train loss (w/o reg) on all data: 0.10720626
Test loss (w/o reg) on all data: 0.07253902
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2266888e-05
Norm of the params: 15.099131
     Influence (LOO): fixed  68 labels. Loss 0.07254. Accuracy 0.981.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048489027
Train loss (w/o reg) on all data: 0.032777984
Test loss (w/o reg) on all data: 0.042168815
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0746096e-05
Norm of the params: 17.726274
                Loss: fixed  95 labels. Loss 0.04217. Accuracy 0.985.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22666779
Train loss (w/o reg) on all data: 0.21802935
Test loss (w/o reg) on all data: 0.14350727
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.526891e-05
Norm of the params: 13.14416
              Random: fixed  14 labels. Loss 0.14351. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06840945
Train loss (w/o reg) on all data: 0.05975776
Test loss (w/o reg) on all data: 0.042500474
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1935093e-05
Norm of the params: 13.154231
     Influence (LOO): fixed  96 labels. Loss 0.04250. Accuracy 0.985.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025908219
Train loss (w/o reg) on all data: 0.014218138
Test loss (w/o reg) on all data: 0.0299364
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0178021e-06
Norm of the params: 15.290573
                Loss: fixed 108 labels. Loss 0.02994. Accuracy 0.992.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21707173
Train loss (w/o reg) on all data: 0.2078618
Test loss (w/o reg) on all data: 0.13859032
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2442606e-05
Norm of the params: 13.5719795
              Random: fixed  20 labels. Loss 0.13859. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029771715
Train loss (w/o reg) on all data: 0.022237744
Test loss (w/o reg) on all data: 0.015438047
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5257793e-06
Norm of the params: 12.275155
     Influence (LOO): fixed 112 labels. Loss 0.01544. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018582799
Train loss (w/o reg) on all data: 0.008981089
Test loss (w/o reg) on all data: 0.023184994
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0076157e-06
Norm of the params: 13.85764
                Loss: fixed 112 labels. Loss 0.02318. Accuracy 0.989.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20149486
Train loss (w/o reg) on all data: 0.19133049
Test loss (w/o reg) on all data: 0.12800948
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.374028e-05
Norm of the params: 14.25789
              Random: fixed  28 labels. Loss 0.12801. Accuracy 0.973.
### Flips: 260, rs: 21, checks: 260
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016455805
Train loss (w/o reg) on all data: 0.010376033
Test loss (w/o reg) on all data: 0.016363787
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.141028e-07
Norm of the params: 11.027033
     Influence (LOO): fixed 119 labels. Loss 0.01636. Accuracy 0.992.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016684912
Train loss (w/o reg) on all data: 0.007788874
Test loss (w/o reg) on all data: 0.015620297
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.736425e-07
Norm of the params: 13.338695
                Loss: fixed 114 labels. Loss 0.01562. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18899038
Train loss (w/o reg) on all data: 0.1787503
Test loss (w/o reg) on all data: 0.12155881
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5130682e-05
Norm of the params: 14.310886
              Random: fixed  38 labels. Loss 0.12156. Accuracy 0.973.
### Flips: 260, rs: 21, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729358
Test loss (w/o reg) on all data: 0.01205441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7284203e-07
Norm of the params: 9.153233
     Influence (LOO): fixed 122 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016684912
Train loss (w/o reg) on all data: 0.0077882134
Test loss (w/o reg) on all data: 0.015619955
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6451158e-06
Norm of the params: 13.33919
                Loss: fixed 114 labels. Loss 0.01562. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1811102
Train loss (w/o reg) on all data: 0.17025691
Test loss (w/o reg) on all data: 0.11944789
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.0325613e-05
Norm of the params: 14.73315
              Random: fixed  42 labels. Loss 0.11945. Accuracy 0.977.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25336912
Train loss (w/o reg) on all data: 0.24652572
Test loss (w/o reg) on all data: 0.17094558
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.979857e-05
Norm of the params: 11.699065
Flipped loss: 0.17095. Accuracy: 0.973
### Flips: 260, rs: 22, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18887053
Train loss (w/o reg) on all data: 0.17816965
Test loss (w/o reg) on all data: 0.12914994
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.016909e-05
Norm of the params: 14.629345
     Influence (LOO): fixed  39 labels. Loss 0.12915. Accuracy 0.977.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13975506
Train loss (w/o reg) on all data: 0.1279259
Test loss (w/o reg) on all data: 0.10128112
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1410537e-05
Norm of the params: 15.381258
                Loss: fixed  52 labels. Loss 0.10128. Accuracy 0.958.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2475923
Train loss (w/o reg) on all data: 0.24068694
Test loss (w/o reg) on all data: 0.16652024
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.889185e-05
Norm of the params: 11.751903
              Random: fixed   4 labels. Loss 0.16652. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 104
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14276949
Train loss (w/o reg) on all data: 0.13027154
Test loss (w/o reg) on all data: 0.1025122
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.231796e-06
Norm of the params: 15.81009
     Influence (LOO): fixed  65 labels. Loss 0.10251. Accuracy 0.981.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050956108
Train loss (w/o reg) on all data: 0.037446547
Test loss (w/o reg) on all data: 0.035908375
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7967262e-06
Norm of the params: 16.437494
                Loss: fixed 100 labels. Loss 0.03591. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24578105
Train loss (w/o reg) on all data: 0.23864333
Test loss (w/o reg) on all data: 0.16478041
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5259788e-05
Norm of the params: 11.947981
              Random: fixed   7 labels. Loss 0.16478. Accuracy 0.977.
### Flips: 260, rs: 22, checks: 156
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09968336
Train loss (w/o reg) on all data: 0.0884645
Test loss (w/o reg) on all data: 0.065297924
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.952255e-06
Norm of the params: 14.979226
     Influence (LOO): fixed  91 labels. Loss 0.06530. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01830814
Train loss (w/o reg) on all data: 0.009276678
Test loss (w/o reg) on all data: 0.018081423
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.344437e-07
Norm of the params: 13.439837
                Loss: fixed 122 labels. Loss 0.01808. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23796435
Train loss (w/o reg) on all data: 0.23076506
Test loss (w/o reg) on all data: 0.16271196
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1248952e-05
Norm of the params: 11.999405
              Random: fixed  13 labels. Loss 0.16271. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 208
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059921008
Train loss (w/o reg) on all data: 0.05064755
Test loss (w/o reg) on all data: 0.032482963
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5453327e-06
Norm of the params: 13.618706
     Influence (LOO): fixed 109 labels. Loss 0.03248. Accuracy 0.996.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015574969
Train loss (w/o reg) on all data: 0.007276312
Test loss (w/o reg) on all data: 0.01862653
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8465165e-06
Norm of the params: 12.883056
                Loss: fixed 124 labels. Loss 0.01863. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2273601
Train loss (w/o reg) on all data: 0.2193149
Test loss (w/o reg) on all data: 0.1501914
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.4294713e-05
Norm of the params: 12.684797
              Random: fixed  20 labels. Loss 0.15019. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 260
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02173854
Train loss (w/o reg) on all data: 0.015049242
Test loss (w/o reg) on all data: 0.013742464
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.3705556e-07
Norm of the params: 11.566587
     Influence (LOO): fixed 125 labels. Loss 0.01374. Accuracy 0.996.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010662421
Train loss (w/o reg) on all data: 0.00450973
Test loss (w/o reg) on all data: 0.01363715
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2738767e-07
Norm of the params: 11.092962
                Loss: fixed 126 labels. Loss 0.01364. Accuracy 0.992.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22108234
Train loss (w/o reg) on all data: 0.21389915
Test loss (w/o reg) on all data: 0.1346103
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8552593e-05
Norm of the params: 11.98599
              Random: fixed  29 labels. Loss 0.13461. Accuracy 0.985.
### Flips: 260, rs: 22, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013299493
Train loss (w/o reg) on all data: 0.0083822915
Test loss (w/o reg) on all data: 0.0140360715
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.612657e-06
Norm of the params: 9.916856
     Influence (LOO): fixed 128 labels. Loss 0.01404. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009419196
Train loss (w/o reg) on all data: 0.0037441757
Test loss (w/o reg) on all data: 0.012035386
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9723444e-07
Norm of the params: 10.653657
                Loss: fixed 127 labels. Loss 0.01204. Accuracy 0.996.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21315289
Train loss (w/o reg) on all data: 0.20597005
Test loss (w/o reg) on all data: 0.120450094
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8215793e-05
Norm of the params: 11.985695
              Random: fixed  35 labels. Loss 0.12045. Accuracy 0.989.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23858164
Train loss (w/o reg) on all data: 0.2301467
Test loss (w/o reg) on all data: 0.16859373
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 0.00013779658
Norm of the params: 12.988407
Flipped loss: 0.16859. Accuracy: 0.966
### Flips: 260, rs: 23, checks: 52
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16516407
Train loss (w/o reg) on all data: 0.15525337
Test loss (w/o reg) on all data: 0.14724725
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.0575997e-05
Norm of the params: 14.078852
     Influence (LOO): fixed  39 labels. Loss 0.14725. Accuracy 0.947.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12291562
Train loss (w/o reg) on all data: 0.10851819
Test loss (w/o reg) on all data: 0.121524625
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.381357e-06
Norm of the params: 16.969048
                Loss: fixed  51 labels. Loss 0.12152. Accuracy 0.950.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720711
Train loss (w/o reg) on all data: 0.21857066
Test loss (w/o reg) on all data: 0.15253209
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1147643e-05
Norm of the params: 13.142638
              Random: fixed   9 labels. Loss 0.15253. Accuracy 0.962.
### Flips: 260, rs: 23, checks: 104
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11227177
Train loss (w/o reg) on all data: 0.09979445
Test loss (w/o reg) on all data: 0.1313721
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.6379629e-05
Norm of the params: 15.79704
     Influence (LOO): fixed  65 labels. Loss 0.13137. Accuracy 0.943.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041702427
Train loss (w/o reg) on all data: 0.026187671
Test loss (w/o reg) on all data: 0.05981865
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.892299e-06
Norm of the params: 17.615196
                Loss: fixed  97 labels. Loss 0.05982. Accuracy 0.981.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22286293
Train loss (w/o reg) on all data: 0.21443984
Test loss (w/o reg) on all data: 0.14674567
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5378671e-05
Norm of the params: 12.979285
              Random: fixed  13 labels. Loss 0.14675. Accuracy 0.973.
### Flips: 260, rs: 23, checks: 156
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07343808
Train loss (w/o reg) on all data: 0.062311705
Test loss (w/o reg) on all data: 0.10691176
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7743652e-06
Norm of the params: 14.917356
     Influence (LOO): fixed  92 labels. Loss 0.10691. Accuracy 0.962.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02076965
Train loss (w/o reg) on all data: 0.01011011
Test loss (w/o reg) on all data: 0.018809127
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.83078e-07
Norm of the params: 14.601055
                Loss: fixed 112 labels. Loss 0.01881. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2130148
Train loss (w/o reg) on all data: 0.20411934
Test loss (w/o reg) on all data: 0.14186119
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.942259e-05
Norm of the params: 13.33826
              Random: fixed  21 labels. Loss 0.14186. Accuracy 0.973.
### Flips: 260, rs: 23, checks: 208
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036232922
Train loss (w/o reg) on all data: 0.028360125
Test loss (w/o reg) on all data: 0.047046013
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9679997e-06
Norm of the params: 12.548145
     Influence (LOO): fixed 111 labels. Loss 0.04705. Accuracy 0.985.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01378904
Train loss (w/o reg) on all data: 0.006228533
Test loss (w/o reg) on all data: 0.013393263
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6303478e-06
Norm of the params: 12.296753
                Loss: fixed 117 labels. Loss 0.01339. Accuracy 0.996.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20261635
Train loss (w/o reg) on all data: 0.19401446
Test loss (w/o reg) on all data: 0.12789646
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.48257e-05
Norm of the params: 13.116316
              Random: fixed  29 labels. Loss 0.12790. Accuracy 0.985.
### Flips: 260, rs: 23, checks: 260
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023872307
Train loss (w/o reg) on all data: 0.017415117
Test loss (w/o reg) on all data: 0.020209135
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6186053e-06
Norm of the params: 11.364145
     Influence (LOO): fixed 118 labels. Loss 0.02021. Accuracy 0.996.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009296384
Train loss (w/o reg) on all data: 0.0037493387
Test loss (w/o reg) on all data: 0.012860357
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.254022e-07
Norm of the params: 10.532849
                Loss: fixed 119 labels. Loss 0.01286. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19476034
Train loss (w/o reg) on all data: 0.1864082
Test loss (w/o reg) on all data: 0.12426215
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.403516e-05
Norm of the params: 12.924493
              Random: fixed  33 labels. Loss 0.12426. Accuracy 0.985.
### Flips: 260, rs: 23, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172763
Test loss (w/o reg) on all data: 0.012055249
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.982526e-07
Norm of the params: 9.15342
     Influence (LOO): fixed 124 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076053618
Train loss (w/o reg) on all data: 0.0027933915
Test loss (w/o reg) on all data: 0.011897639
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.390133e-07
Norm of the params: 9.810168
                Loss: fixed 121 labels. Loss 0.01190. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18657061
Train loss (w/o reg) on all data: 0.17843087
Test loss (w/o reg) on all data: 0.11599773
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2084527e-05
Norm of the params: 12.759109
              Random: fixed  40 labels. Loss 0.11600. Accuracy 0.981.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663404
Train loss (w/o reg) on all data: 0.23975947
Test loss (w/o reg) on all data: 0.15188852
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7730307e-05
Norm of the params: 11.725671
Flipped loss: 0.15189. Accuracy: 0.985
### Flips: 260, rs: 24, checks: 52
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18661422
Train loss (w/o reg) on all data: 0.17697057
Test loss (w/o reg) on all data: 0.11811953
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.6076944e-05
Norm of the params: 13.887869
     Influence (LOO): fixed  34 labels. Loss 0.11812. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13207585
Train loss (w/o reg) on all data: 0.119867966
Test loss (w/o reg) on all data: 0.07927338
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.778147e-06
Norm of the params: 15.62554
                Loss: fixed  52 labels. Loss 0.07927. Accuracy 0.981.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2383318
Train loss (w/o reg) on all data: 0.23157275
Test loss (w/o reg) on all data: 0.14740993
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0623523e-05
Norm of the params: 11.626741
              Random: fixed   6 labels. Loss 0.14741. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 104
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13440362
Train loss (w/o reg) on all data: 0.12420137
Test loss (w/o reg) on all data: 0.08829977
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3277881e-05
Norm of the params: 14.284428
     Influence (LOO): fixed  63 labels. Loss 0.08830. Accuracy 0.989.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04153587
Train loss (w/o reg) on all data: 0.02725736
Test loss (w/o reg) on all data: 0.03365371
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.701769e-06
Norm of the params: 16.89882
                Loss: fixed  96 labels. Loss 0.03365. Accuracy 0.989.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2276174
Train loss (w/o reg) on all data: 0.22089592
Test loss (w/o reg) on all data: 0.13542993
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1801078e-05
Norm of the params: 11.594383
              Random: fixed  14 labels. Loss 0.13543. Accuracy 0.989.
### Flips: 260, rs: 24, checks: 156
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084101886
Train loss (w/o reg) on all data: 0.07259263
Test loss (w/o reg) on all data: 0.055209175
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1957505e-05
Norm of the params: 15.171854
     Influence (LOO): fixed  89 labels. Loss 0.05521. Accuracy 0.989.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014426155
Train loss (w/o reg) on all data: 0.0066045257
Test loss (w/o reg) on all data: 0.015921045
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.870829e-07
Norm of the params: 12.507302
                Loss: fixed 115 labels. Loss 0.01592. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22481653
Train loss (w/o reg) on all data: 0.21810119
Test loss (w/o reg) on all data: 0.13368292
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.9805672e-05
Norm of the params: 11.589087
              Random: fixed  18 labels. Loss 0.13368. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 208
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049858138
Train loss (w/o reg) on all data: 0.04095152
Test loss (w/o reg) on all data: 0.034291655
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6656585e-06
Norm of the params: 13.346622
     Influence (LOO): fixed 106 labels. Loss 0.03429. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073450767
Train loss (w/o reg) on all data: 0.0025869503
Test loss (w/o reg) on all data: 0.013704242
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.913529e-07
Norm of the params: 9.755128
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21024966
Train loss (w/o reg) on all data: 0.20295021
Test loss (w/o reg) on all data: 0.115347974
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.408512e-05
Norm of the params: 12.082589
              Random: fixed  27 labels. Loss 0.11535. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 260
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.005539569
Test loss (w/o reg) on all data: 0.013845892
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.526182e-07
Norm of the params: 9.949938
     Influence (LOO): fixed 120 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007345078
Train loss (w/o reg) on all data: 0.0025870255
Test loss (w/o reg) on all data: 0.013702101
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.933867e-07
Norm of the params: 9.755053
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20207028
Train loss (w/o reg) on all data: 0.19468942
Test loss (w/o reg) on all data: 0.11122631
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2718923e-05
Norm of the params: 12.149779
              Random: fixed  33 labels. Loss 0.11123. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 312
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489635
Train loss (w/o reg) on all data: 0.0055397595
Test loss (w/o reg) on all data: 0.013844422
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.346151e-07
Norm of the params: 9.949749
     Influence (LOO): fixed 120 labels. Loss 0.01384. Accuracy 0.989.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073450767
Train loss (w/o reg) on all data: 0.0025869787
Test loss (w/o reg) on all data: 0.013702669
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1889886e-07
Norm of the params: 9.755099
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187806
Train loss (w/o reg) on all data: 0.18095294
Test loss (w/o reg) on all data: 0.10395534
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.897536e-06
Norm of the params: 11.707313
              Random: fixed  44 labels. Loss 0.10396. Accuracy 0.989.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25013193
Train loss (w/o reg) on all data: 0.24376217
Test loss (w/o reg) on all data: 0.16734417
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6205286e-05
Norm of the params: 11.286969
Flipped loss: 0.16734. Accuracy: 0.969
### Flips: 260, rs: 25, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17930962
Train loss (w/o reg) on all data: 0.17090462
Test loss (w/o reg) on all data: 0.10443968
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5912974e-05
Norm of the params: 12.965341
     Influence (LOO): fixed  42 labels. Loss 0.10444. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13677739
Train loss (w/o reg) on all data: 0.12340025
Test loss (w/o reg) on all data: 0.108721614
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0103136e-05
Norm of the params: 16.356733
                Loss: fixed  52 labels. Loss 0.10872. Accuracy 0.962.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24077679
Train loss (w/o reg) on all data: 0.23403913
Test loss (w/o reg) on all data: 0.15608148
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.0810945e-05
Norm of the params: 11.608334
              Random: fixed   6 labels. Loss 0.15608. Accuracy 0.966.
### Flips: 260, rs: 25, checks: 104
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12312625
Train loss (w/o reg) on all data: 0.113624364
Test loss (w/o reg) on all data: 0.06923161
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1036702e-05
Norm of the params: 13.7854185
     Influence (LOO): fixed  73 labels. Loss 0.06923. Accuracy 0.977.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050727457
Train loss (w/o reg) on all data: 0.03716844
Test loss (w/o reg) on all data: 0.036594246
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0778825e-05
Norm of the params: 16.467556
                Loss: fixed  99 labels. Loss 0.03659. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2304001
Train loss (w/o reg) on all data: 0.22378942
Test loss (w/o reg) on all data: 0.13607237
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.24075095e-05
Norm of the params: 11.498419
              Random: fixed  15 labels. Loss 0.13607. Accuracy 0.981.
### Flips: 260, rs: 25, checks: 156
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08381819
Train loss (w/o reg) on all data: 0.07460316
Test loss (w/o reg) on all data: 0.04864087
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2064307e-05
Norm of the params: 13.575735
     Influence (LOO): fixed  93 labels. Loss 0.04864. Accuracy 0.981.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016812574
Train loss (w/o reg) on all data: 0.007971212
Test loss (w/o reg) on all data: 0.020547755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3921593e-06
Norm of the params: 13.297641
                Loss: fixed 116 labels. Loss 0.02055. Accuracy 0.992.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21875384
Train loss (w/o reg) on all data: 0.21190841
Test loss (w/o reg) on all data: 0.13008016
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 0.00012574511
Norm of the params: 11.700793
              Random: fixed  24 labels. Loss 0.13008. Accuracy 0.973.
### Flips: 260, rs: 25, checks: 208
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042636883
Train loss (w/o reg) on all data: 0.03543093
Test loss (w/o reg) on all data: 0.018430611
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9155592e-06
Norm of the params: 12.004958
     Influence (LOO): fixed 115 labels. Loss 0.01843. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016415086
Train loss (w/o reg) on all data: 0.0077385805
Test loss (w/o reg) on all data: 0.02313635
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.730486e-07
Norm of the params: 13.173082
                Loss: fixed 118 labels. Loss 0.02314. Accuracy 0.989.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21084118
Train loss (w/o reg) on all data: 0.20405377
Test loss (w/o reg) on all data: 0.122355714
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3860526e-05
Norm of the params: 11.651095
              Random: fixed  31 labels. Loss 0.12236. Accuracy 0.973.
### Flips: 260, rs: 25, checks: 260
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0152252205
Train loss (w/o reg) on all data: 0.008902432
Test loss (w/o reg) on all data: 0.019896373
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.104957e-07
Norm of the params: 11.245255
     Influence (LOO): fixed 123 labels. Loss 0.01990. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015287606
Train loss (w/o reg) on all data: 0.006951111
Test loss (w/o reg) on all data: 0.0248965
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5339848e-06
Norm of the params: 12.912393
                Loss: fixed 119 labels. Loss 0.02490. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20552674
Train loss (w/o reg) on all data: 0.19856983
Test loss (w/o reg) on all data: 0.11343492
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 0.000118942946
Norm of the params: 11.795677
              Random: fixed  36 labels. Loss 0.11343. Accuracy 0.981.
### Flips: 260, rs: 25, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729544
Test loss (w/o reg) on all data: 0.012055357
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2540427e-07
Norm of the params: 9.153211
     Influence (LOO): fixed 126 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008785678
Train loss (w/o reg) on all data: 0.003304768
Test loss (w/o reg) on all data: 0.022252418
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.692678e-07
Norm of the params: 10.469871
                Loss: fixed 123 labels. Loss 0.02225. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1976664
Train loss (w/o reg) on all data: 0.19054304
Test loss (w/o reg) on all data: 0.10513236
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.354583e-05
Norm of the params: 11.9359665
              Random: fixed  43 labels. Loss 0.10513. Accuracy 0.985.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24594527
Train loss (w/o reg) on all data: 0.23963086
Test loss (w/o reg) on all data: 0.17909147
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1785823e-05
Norm of the params: 11.237804
Flipped loss: 0.17909. Accuracy: 0.958
### Flips: 260, rs: 26, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19423357
Train loss (w/o reg) on all data: 0.18512021
Test loss (w/o reg) on all data: 0.15050198
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.75437e-05
Norm of the params: 13.500631
     Influence (LOO): fixed  34 labels. Loss 0.15050. Accuracy 0.966.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13143475
Train loss (w/o reg) on all data: 0.11615046
Test loss (w/o reg) on all data: 0.13109818
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2365878e-05
Norm of the params: 17.483875
                Loss: fixed  51 labels. Loss 0.13110. Accuracy 0.954.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24117637
Train loss (w/o reg) on all data: 0.23481856
Test loss (w/o reg) on all data: 0.17224695
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.2489555e-05
Norm of the params: 11.27635
              Random: fixed   6 labels. Loss 0.17225. Accuracy 0.958.
### Flips: 260, rs: 26, checks: 104
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1315502
Train loss (w/o reg) on all data: 0.118964486
Test loss (w/o reg) on all data: 0.11517973
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.291708e-06
Norm of the params: 15.865501
     Influence (LOO): fixed  66 labels. Loss 0.11518. Accuracy 0.977.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058567293
Train loss (w/o reg) on all data: 0.042634748
Test loss (w/o reg) on all data: 0.07864693
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.033953e-06
Norm of the params: 17.850796
                Loss: fixed  91 labels. Loss 0.07865. Accuracy 0.958.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23595925
Train loss (w/o reg) on all data: 0.22953674
Test loss (w/o reg) on all data: 0.15764469
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0436064e-05
Norm of the params: 11.333585
              Random: fixed  14 labels. Loss 0.15764. Accuracy 0.969.
### Flips: 260, rs: 26, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09527127
Train loss (w/o reg) on all data: 0.08354422
Test loss (w/o reg) on all data: 0.07515701
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.891241e-06
Norm of the params: 15.314732
     Influence (LOO): fixed  88 labels. Loss 0.07516. Accuracy 0.981.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029305361
Train loss (w/o reg) on all data: 0.01725182
Test loss (w/o reg) on all data: 0.023700366
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5538365e-06
Norm of the params: 15.526456
                Loss: fixed 115 labels. Loss 0.02370. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23276706
Train loss (w/o reg) on all data: 0.22625017
Test loss (w/o reg) on all data: 0.15452997
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.7031422e-05
Norm of the params: 11.416556
              Random: fixed  17 labels. Loss 0.15453. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 208
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07184114
Train loss (w/o reg) on all data: 0.061886035
Test loss (w/o reg) on all data: 0.052088622
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.477053e-06
Norm of the params: 14.110354
     Influence (LOO): fixed 104 labels. Loss 0.05209. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017817594
Train loss (w/o reg) on all data: 0.009109682
Test loss (w/o reg) on all data: 0.01594447
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8759873e-06
Norm of the params: 13.196902
                Loss: fixed 123 labels. Loss 0.01594. Accuracy 0.989.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2292767
Train loss (w/o reg) on all data: 0.22298422
Test loss (w/o reg) on all data: 0.1469416
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5810075e-05
Norm of the params: 11.218264
              Random: fixed  22 labels. Loss 0.14694. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 260
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040041618
Train loss (w/o reg) on all data: 0.03156444
Test loss (w/o reg) on all data: 0.027588593
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5436702e-06
Norm of the params: 13.020889
     Influence (LOO): fixed 120 labels. Loss 0.02759. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0108633265
Train loss (w/o reg) on all data: 0.0046854992
Test loss (w/o reg) on all data: 0.010360816
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.118486e-07
Norm of the params: 11.115599
                Loss: fixed 128 labels. Loss 0.01036. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22221732
Train loss (w/o reg) on all data: 0.21535753
Test loss (w/o reg) on all data: 0.14417289
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7570444e-05
Norm of the params: 11.713069
              Random: fixed  29 labels. Loss 0.14417. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 312
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018421968
Train loss (w/o reg) on all data: 0.012005994
Test loss (w/o reg) on all data: 0.012392355
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.5208665e-07
Norm of the params: 11.327819
     Influence (LOO): fixed 128 labels. Loss 0.01239. Accuracy 0.996.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010043117
Train loss (w/o reg) on all data: 0.0045275628
Test loss (w/o reg) on all data: 0.008518677
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.2272964e-07
Norm of the params: 10.502909
                Loss: fixed 130 labels. Loss 0.00852. Accuracy 0.996.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21613489
Train loss (w/o reg) on all data: 0.20926465
Test loss (w/o reg) on all data: 0.12641092
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2774002e-05
Norm of the params: 11.721978
              Random: fixed  36 labels. Loss 0.12641. Accuracy 0.981.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25227785
Train loss (w/o reg) on all data: 0.24467823
Test loss (w/o reg) on all data: 0.1988847
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.306935e-05
Norm of the params: 12.32852
Flipped loss: 0.19888. Accuracy: 0.943
### Flips: 260, rs: 27, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19569764
Train loss (w/o reg) on all data: 0.18577087
Test loss (w/o reg) on all data: 0.15334144
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.7728378e-05
Norm of the params: 14.090256
     Influence (LOO): fixed  35 labels. Loss 0.15334. Accuracy 0.954.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14166619
Train loss (w/o reg) on all data: 0.12825203
Test loss (w/o reg) on all data: 0.14340183
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 9.747678e-06
Norm of the params: 16.379353
                Loss: fixed  51 labels. Loss 0.14340. Accuracy 0.931.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24317786
Train loss (w/o reg) on all data: 0.23564179
Test loss (w/o reg) on all data: 0.18618177
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4213678e-05
Norm of the params: 12.276861
              Random: fixed   8 labels. Loss 0.18618. Accuracy 0.943.
### Flips: 260, rs: 27, checks: 104
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15163301
Train loss (w/o reg) on all data: 0.141113
Test loss (w/o reg) on all data: 0.11535978
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7646898e-05
Norm of the params: 14.505183
     Influence (LOO): fixed  64 labels. Loss 0.11536. Accuracy 0.966.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058852635
Train loss (w/o reg) on all data: 0.043326747
Test loss (w/o reg) on all data: 0.068506315
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.4450137e-06
Norm of the params: 17.621513
                Loss: fixed  98 labels. Loss 0.06851. Accuracy 0.981.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24074642
Train loss (w/o reg) on all data: 0.23333205
Test loss (w/o reg) on all data: 0.17184272
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0568097e-05
Norm of the params: 12.177332
              Random: fixed  14 labels. Loss 0.17184. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10603021
Train loss (w/o reg) on all data: 0.09587028
Test loss (w/o reg) on all data: 0.07544129
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.402863e-06
Norm of the params: 14.254775
     Influence (LOO): fixed  92 labels. Loss 0.07544. Accuracy 0.985.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025208808
Train loss (w/o reg) on all data: 0.014340553
Test loss (w/o reg) on all data: 0.031118957
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5036962e-06
Norm of the params: 14.743307
                Loss: fixed 122 labels. Loss 0.03112. Accuracy 0.981.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23254068
Train loss (w/o reg) on all data: 0.22512875
Test loss (w/o reg) on all data: 0.15665042
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.3351483e-05
Norm of the params: 12.175326
              Random: fixed  22 labels. Loss 0.15665. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 208
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06469352
Train loss (w/o reg) on all data: 0.05565743
Test loss (w/o reg) on all data: 0.04398252
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3995951e-06
Norm of the params: 13.443277
     Influence (LOO): fixed 113 labels. Loss 0.04398. Accuracy 0.996.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016087659
Train loss (w/o reg) on all data: 0.007253102
Test loss (w/o reg) on all data: 0.019563885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0602006e-07
Norm of the params: 13.292522
                Loss: fixed 128 labels. Loss 0.01956. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22568607
Train loss (w/o reg) on all data: 0.21859002
Test loss (w/o reg) on all data: 0.14522283
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1785442e-05
Norm of the params: 11.913064
              Random: fixed  29 labels. Loss 0.14522. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 260
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024233358
Train loss (w/o reg) on all data: 0.017488703
Test loss (w/o reg) on all data: 0.017765997
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4837037e-06
Norm of the params: 11.614348
     Influence (LOO): fixed 130 labels. Loss 0.01777. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011459665
Train loss (w/o reg) on all data: 0.0047177635
Test loss (w/o reg) on all data: 0.019060452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1114673e-07
Norm of the params: 11.611979
                Loss: fixed 131 labels. Loss 0.01906. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21817686
Train loss (w/o reg) on all data: 0.21043833
Test loss (w/o reg) on all data: 0.13687876
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.606259e-05
Norm of the params: 12.440681
              Random: fixed  34 labels. Loss 0.13688. Accuracy 0.966.
### Flips: 260, rs: 27, checks: 312
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01300434
Train loss (w/o reg) on all data: 0.0077468106
Test loss (w/o reg) on all data: 0.013980189
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2088212e-07
Norm of the params: 10.254297
     Influence (LOO): fixed 134 labels. Loss 0.01398. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010201583
Train loss (w/o reg) on all data: 0.004012257
Test loss (w/o reg) on all data: 0.017736454
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.348066e-07
Norm of the params: 11.125938
                Loss: fixed 132 labels. Loss 0.01774. Accuracy 0.992.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21310332
Train loss (w/o reg) on all data: 0.20501882
Test loss (w/o reg) on all data: 0.13639179
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.860225e-05
Norm of the params: 12.715737
              Random: fixed  38 labels. Loss 0.13639. Accuracy 0.973.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26557222
Train loss (w/o reg) on all data: 0.26069033
Test loss (w/o reg) on all data: 0.17202018
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7215054e-05
Norm of the params: 9.881179
Flipped loss: 0.17202. Accuracy: 0.977
### Flips: 260, rs: 28, checks: 52
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20873806
Train loss (w/o reg) on all data: 0.20037887
Test loss (w/o reg) on all data: 0.13623324
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1858145e-05
Norm of the params: 12.929962
     Influence (LOO): fixed  35 labels. Loss 0.13623. Accuracy 0.954.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15955299
Train loss (w/o reg) on all data: 0.15019463
Test loss (w/o reg) on all data: 0.109984554
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5089004e-05
Norm of the params: 13.680912
                Loss: fixed  51 labels. Loss 0.10998. Accuracy 0.966.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25798115
Train loss (w/o reg) on all data: 0.25265652
Test loss (w/o reg) on all data: 0.16481645
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.4872845e-05
Norm of the params: 10.319517
              Random: fixed   5 labels. Loss 0.16482. Accuracy 0.977.
### Flips: 260, rs: 28, checks: 104
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674547
Train loss (w/o reg) on all data: 0.13538912
Test loss (w/o reg) on all data: 0.09213539
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.0902607e-05
Norm of the params: 15.070741
     Influence (LOO): fixed  69 labels. Loss 0.09214. Accuracy 0.973.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602101
Train loss (w/o reg) on all data: 0.047003455
Test loss (w/o reg) on all data: 0.055639174
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6198004e-06
Norm of the params: 16.252167
                Loss: fixed 102 labels. Loss 0.05564. Accuracy 0.973.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24951814
Train loss (w/o reg) on all data: 0.24400146
Test loss (w/o reg) on all data: 0.15220276
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.5074485e-05
Norm of the params: 10.503983
              Random: fixed  14 labels. Loss 0.15220. Accuracy 0.981.
### Flips: 260, rs: 28, checks: 156
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09243521
Train loss (w/o reg) on all data: 0.08064256
Test loss (w/o reg) on all data: 0.071907826
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.128641e-06
Norm of the params: 15.357511
     Influence (LOO): fixed  99 labels. Loss 0.07191. Accuracy 0.977.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01672143
Train loss (w/o reg) on all data: 0.008381943
Test loss (w/o reg) on all data: 0.028920619
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.172856e-07
Norm of the params: 12.914709
                Loss: fixed 129 labels. Loss 0.02892. Accuracy 0.992.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24205638
Train loss (w/o reg) on all data: 0.23673046
Test loss (w/o reg) on all data: 0.14570637
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6269003e-05
Norm of the params: 10.320776
              Random: fixed  22 labels. Loss 0.14571. Accuracy 0.977.
### Flips: 260, rs: 28, checks: 208
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057918485
Train loss (w/o reg) on all data: 0.048477467
Test loss (w/o reg) on all data: 0.040213235
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9162265e-06
Norm of the params: 13.741193
     Influence (LOO): fixed 118 labels. Loss 0.04021. Accuracy 0.996.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012761949
Train loss (w/o reg) on all data: 0.00599262
Test loss (w/o reg) on all data: 0.024280408
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0987155e-07
Norm of the params: 11.635573
                Loss: fixed 132 labels. Loss 0.02428. Accuracy 0.992.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720204
Train loss (w/o reg) on all data: 0.22112282
Test loss (w/o reg) on all data: 0.13196209
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0215209e-05
Norm of the params: 11.026539
              Random: fixed  30 labels. Loss 0.13196. Accuracy 0.985.
### Flips: 260, rs: 28, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029932506
Train loss (w/o reg) on all data: 0.022162339
Test loss (w/o reg) on all data: 0.023226496
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4475965e-06
Norm of the params: 12.466088
     Influence (LOO): fixed 129 labels. Loss 0.02323. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0089196125
Train loss (w/o reg) on all data: 0.0034987747
Test loss (w/o reg) on all data: 0.021754844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6708734e-07
Norm of the params: 10.412337
                Loss: fixed 135 labels. Loss 0.02175. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22251186
Train loss (w/o reg) on all data: 0.2163467
Test loss (w/o reg) on all data: 0.1281784
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3133626e-05
Norm of the params: 11.104199
              Random: fixed  34 labels. Loss 0.12818. Accuracy 0.989.
### Flips: 260, rs: 28, checks: 312
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015367977
Train loss (w/o reg) on all data: 0.008857663
Test loss (w/o reg) on all data: 0.013294397
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1208828e-06
Norm of the params: 11.410797
     Influence (LOO): fixed 134 labels. Loss 0.01329. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008919613
Train loss (w/o reg) on all data: 0.003498803
Test loss (w/o reg) on all data: 0.021754317
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1758139e-07
Norm of the params: 10.412311
                Loss: fixed 135 labels. Loss 0.02175. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21384689
Train loss (w/o reg) on all data: 0.20771237
Test loss (w/o reg) on all data: 0.122593015
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.450858e-05
Norm of the params: 11.076568
              Random: fixed  40 labels. Loss 0.12259. Accuracy 0.981.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25015706
Train loss (w/o reg) on all data: 0.24168038
Test loss (w/o reg) on all data: 0.17777388
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.286039e-05
Norm of the params: 13.020494
Flipped loss: 0.17777. Accuracy: 0.939
### Flips: 260, rs: 29, checks: 52
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1881894
Train loss (w/o reg) on all data: 0.17566003
Test loss (w/o reg) on all data: 0.12445667
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3406618e-05
Norm of the params: 15.829953
     Influence (LOO): fixed  35 labels. Loss 0.12446. Accuracy 0.962.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13391185
Train loss (w/o reg) on all data: 0.11729979
Test loss (w/o reg) on all data: 0.10891917
Train acc on all data:  0.944603629417383
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0303015e-05
Norm of the params: 18.22749
                Loss: fixed  52 labels. Loss 0.10892. Accuracy 0.950.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24599664
Train loss (w/o reg) on all data: 0.23683451
Test loss (w/o reg) on all data: 0.16572246
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3626299e-05
Norm of the params: 13.536712
              Random: fixed   7 labels. Loss 0.16572. Accuracy 0.969.
### Flips: 260, rs: 29, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13869125
Train loss (w/o reg) on all data: 0.12470723
Test loss (w/o reg) on all data: 0.09720094
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.5588803e-06
Norm of the params: 16.72365
     Influence (LOO): fixed  58 labels. Loss 0.09720. Accuracy 0.969.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056144632
Train loss (w/o reg) on all data: 0.04062522
Test loss (w/o reg) on all data: 0.07347073
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.3761487e-06
Norm of the params: 17.61784
                Loss: fixed 100 labels. Loss 0.07347. Accuracy 0.966.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24137014
Train loss (w/o reg) on all data: 0.23244292
Test loss (w/o reg) on all data: 0.16214219
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.2645595e-05
Norm of the params: 13.362056
              Random: fixed  11 labels. Loss 0.16214. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 156
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09772747
Train loss (w/o reg) on all data: 0.085319355
Test loss (w/o reg) on all data: 0.059965566
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.131884e-05
Norm of the params: 15.753167
     Influence (LOO): fixed  87 labels. Loss 0.05997. Accuracy 0.981.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021682162
Train loss (w/o reg) on all data: 0.011651872
Test loss (w/o reg) on all data: 0.03275237
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.185561e-06
Norm of the params: 14.163537
                Loss: fixed 121 labels. Loss 0.03275. Accuracy 0.977.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23706523
Train loss (w/o reg) on all data: 0.2284258
Test loss (w/o reg) on all data: 0.15809023
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.9369625e-05
Norm of the params: 13.144899
              Random: fixed  16 labels. Loss 0.15809. Accuracy 0.962.
### Flips: 260, rs: 29, checks: 208
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06082175
Train loss (w/o reg) on all data: 0.049041197
Test loss (w/o reg) on all data: 0.036512773
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.172051e-06
Norm of the params: 15.349628
     Influence (LOO): fixed 107 labels. Loss 0.03651. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019958649
Train loss (w/o reg) on all data: 0.010563345
Test loss (w/o reg) on all data: 0.03507888
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5574889e-06
Norm of the params: 13.707884
                Loss: fixed 123 labels. Loss 0.03508. Accuracy 0.985.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23661542
Train loss (w/o reg) on all data: 0.22838771
Test loss (w/o reg) on all data: 0.15413132
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.4830562e-05
Norm of the params: 12.827865
              Random: fixed  18 labels. Loss 0.15413. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 260
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047812413
Train loss (w/o reg) on all data: 0.037647463
Test loss (w/o reg) on all data: 0.025259992
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.6544127e-06
Norm of the params: 14.258296
     Influence (LOO): fixed 116 labels. Loss 0.02526. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014376717
Train loss (w/o reg) on all data: 0.0069112107
Test loss (w/o reg) on all data: 0.026016966
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.231042e-07
Norm of the params: 12.219253
                Loss: fixed 128 labels. Loss 0.02602. Accuracy 0.992.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22602698
Train loss (w/o reg) on all data: 0.21773988
Test loss (w/o reg) on all data: 0.14604644
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.196003e-05
Norm of the params: 12.874079
              Random: fixed  26 labels. Loss 0.14605. Accuracy 0.977.
### Flips: 260, rs: 29, checks: 312
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030113535
Train loss (w/o reg) on all data: 0.021270994
Test loss (w/o reg) on all data: 0.023266796
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8939515e-06
Norm of the params: 13.298528
     Influence (LOO): fixed 124 labels. Loss 0.02327. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009614047
Train loss (w/o reg) on all data: 0.0037693877
Test loss (w/o reg) on all data: 0.0217413
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5744523e-06
Norm of the params: 10.811715
                Loss: fixed 130 labels. Loss 0.02174. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22392055
Train loss (w/o reg) on all data: 0.21563835
Test loss (w/o reg) on all data: 0.14360045
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4768463e-05
Norm of the params: 12.8702755
              Random: fixed  27 labels. Loss 0.14360. Accuracy 0.977.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24560373
Train loss (w/o reg) on all data: 0.23900868
Test loss (w/o reg) on all data: 0.20533784
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.747158e-05
Norm of the params: 11.484807
Flipped loss: 0.20534. Accuracy: 0.935
### Flips: 260, rs: 30, checks: 52
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18170266
Train loss (w/o reg) on all data: 0.17280816
Test loss (w/o reg) on all data: 0.16967478
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0960882e-05
Norm of the params: 13.337548
     Influence (LOO): fixed  36 labels. Loss 0.16967. Accuracy 0.935.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1220791
Train loss (w/o reg) on all data: 0.10865777
Test loss (w/o reg) on all data: 0.17622323
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.299868e-05
Norm of the params: 16.383728
                Loss: fixed  51 labels. Loss 0.17622. Accuracy 0.939.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23943737
Train loss (w/o reg) on all data: 0.23266505
Test loss (w/o reg) on all data: 0.19947666
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.844696e-05
Norm of the params: 11.638143
              Random: fixed   4 labels. Loss 0.19948. Accuracy 0.935.
### Flips: 260, rs: 30, checks: 104
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13799275
Train loss (w/o reg) on all data: 0.12729266
Test loss (w/o reg) on all data: 0.1291821
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.027077e-06
Norm of the params: 14.628802
     Influence (LOO): fixed  63 labels. Loss 0.12918. Accuracy 0.954.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056082457
Train loss (w/o reg) on all data: 0.040197816
Test loss (w/o reg) on all data: 0.09504888
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2373209e-05
Norm of the params: 17.823938
                Loss: fixed  93 labels. Loss 0.09505. Accuracy 0.962.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23077068
Train loss (w/o reg) on all data: 0.22368045
Test loss (w/o reg) on all data: 0.19320343
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.811966e-05
Norm of the params: 11.908176
              Random: fixed  10 labels. Loss 0.19320. Accuracy 0.939.
### Flips: 260, rs: 30, checks: 156
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087739415
Train loss (w/o reg) on all data: 0.07718005
Test loss (w/o reg) on all data: 0.0726926
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.495588e-06
Norm of the params: 14.532284
     Influence (LOO): fixed  92 labels. Loss 0.07269. Accuracy 0.973.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024666334
Train loss (w/o reg) on all data: 0.013241487
Test loss (w/o reg) on all data: 0.052316677
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2665404e-06
Norm of the params: 15.116116
                Loss: fixed 114 labels. Loss 0.05232. Accuracy 0.985.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22376934
Train loss (w/o reg) on all data: 0.21647759
Test loss (w/o reg) on all data: 0.18451653
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.4149973e-05
Norm of the params: 12.076218
              Random: fixed  16 labels. Loss 0.18452. Accuracy 0.935.
### Flips: 260, rs: 30, checks: 208
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043491416
Train loss (w/o reg) on all data: 0.032926857
Test loss (w/o reg) on all data: 0.06446188
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.562131e-06
Norm of the params: 14.535857
     Influence (LOO): fixed 112 labels. Loss 0.06446. Accuracy 0.973.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01604971
Train loss (w/o reg) on all data: 0.0075188437
Test loss (w/o reg) on all data: 0.038099196
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1547158e-06
Norm of the params: 13.0620575
                Loss: fixed 120 labels. Loss 0.03810. Accuracy 0.985.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21848063
Train loss (w/o reg) on all data: 0.21107173
Test loss (w/o reg) on all data: 0.17185949
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1982504e-05
Norm of the params: 12.172846
              Random: fixed  23 labels. Loss 0.17186. Accuracy 0.947.
### Flips: 260, rs: 30, checks: 260
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016343156
Train loss (w/o reg) on all data: 0.009487815
Test loss (w/o reg) on all data: 0.03015141
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.3971706e-07
Norm of the params: 11.709263
     Influence (LOO): fixed 126 labels. Loss 0.03015. Accuracy 0.985.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010782183
Train loss (w/o reg) on all data: 0.0047572386
Test loss (w/o reg) on all data: 0.016658021
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.229138e-07
Norm of the params: 10.977199
                Loss: fixed 126 labels. Loss 0.01666. Accuracy 0.996.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20854697
Train loss (w/o reg) on all data: 0.20115563
Test loss (w/o reg) on all data: 0.14628763
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.132611e-05
Norm of the params: 12.1584
              Random: fixed  31 labels. Loss 0.14629. Accuracy 0.954.
### Flips: 260, rs: 30, checks: 312
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008166052
Train loss (w/o reg) on all data: 0.0033439025
Test loss (w/o reg) on all data: 0.018008474
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9863463e-07
Norm of the params: 9.820539
     Influence (LOO): fixed 129 labels. Loss 0.01801. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0094912555
Train loss (w/o reg) on all data: 0.004044884
Test loss (w/o reg) on all data: 0.018072685
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3635694e-07
Norm of the params: 10.43683
                Loss: fixed 127 labels. Loss 0.01807. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20527333
Train loss (w/o reg) on all data: 0.19813886
Test loss (w/o reg) on all data: 0.14204453
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8674991e-05
Norm of the params: 11.945267
              Random: fixed  36 labels. Loss 0.14204. Accuracy 0.958.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2569096
Train loss (w/o reg) on all data: 0.2508044
Test loss (w/o reg) on all data: 0.16324641
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.7759604e-05
Norm of the params: 11.05007
Flipped loss: 0.16325. Accuracy: 0.973
### Flips: 260, rs: 31, checks: 52
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19918385
Train loss (w/o reg) on all data: 0.18958487
Test loss (w/o reg) on all data: 0.14074244
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.31289125e-05
Norm of the params: 13.855673
     Influence (LOO): fixed  31 labels. Loss 0.14074. Accuracy 0.966.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13837907
Train loss (w/o reg) on all data: 0.12528509
Test loss (w/o reg) on all data: 0.108357094
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1506615e-05
Norm of the params: 16.182697
                Loss: fixed  52 labels. Loss 0.10836. Accuracy 0.962.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25570726
Train loss (w/o reg) on all data: 0.24995051
Test loss (w/o reg) on all data: 0.15685026
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.0925445e-05
Norm of the params: 10.730105
              Random: fixed   4 labels. Loss 0.15685. Accuracy 0.977.
### Flips: 260, rs: 31, checks: 104
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13975525
Train loss (w/o reg) on all data: 0.12865874
Test loss (w/o reg) on all data: 0.12322029
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.5873187e-05
Norm of the params: 14.897318
     Influence (LOO): fixed  62 labels. Loss 0.12322. Accuracy 0.958.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06462693
Train loss (w/o reg) on all data: 0.04912978
Test loss (w/o reg) on all data: 0.056583587
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8086817e-05
Norm of the params: 17.6052
                Loss: fixed  97 labels. Loss 0.05658. Accuracy 0.977.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25005257
Train loss (w/o reg) on all data: 0.24431942
Test loss (w/o reg) on all data: 0.1509308
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.249469e-05
Norm of the params: 10.708089
              Random: fixed  10 labels. Loss 0.15093. Accuracy 0.981.
### Flips: 260, rs: 31, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09138612
Train loss (w/o reg) on all data: 0.080103785
Test loss (w/o reg) on all data: 0.082347736
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0793232e-05
Norm of the params: 15.021539
     Influence (LOO): fixed  90 labels. Loss 0.08235. Accuracy 0.966.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016625438
Train loss (w/o reg) on all data: 0.008237325
Test loss (w/o reg) on all data: 0.022054065
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.739801e-07
Norm of the params: 12.952308
                Loss: fixed 126 labels. Loss 0.02205. Accuracy 0.992.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2424471
Train loss (w/o reg) on all data: 0.23676443
Test loss (w/o reg) on all data: 0.14541282
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1539912e-05
Norm of the params: 10.660823
              Random: fixed  16 labels. Loss 0.14541. Accuracy 0.989.
### Flips: 260, rs: 31, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05117435
Train loss (w/o reg) on all data: 0.041031547
Test loss (w/o reg) on all data: 0.04357987
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.624964e-06
Norm of the params: 14.242754
     Influence (LOO): fixed 112 labels. Loss 0.04358. Accuracy 0.981.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730498
Test loss (w/o reg) on all data: 0.012054901
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.880195e-07
Norm of the params: 9.153108
                Loss: fixed 131 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23661388
Train loss (w/o reg) on all data: 0.23112842
Test loss (w/o reg) on all data: 0.13632476
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8343482e-05
Norm of the params: 10.474216
              Random: fixed  22 labels. Loss 0.13632. Accuracy 0.985.
### Flips: 260, rs: 31, checks: 260
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02462891
Train loss (w/o reg) on all data: 0.015737955
Test loss (w/o reg) on all data: 0.017707871
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.4187706e-06
Norm of the params: 13.334883
     Influence (LOO): fixed 125 labels. Loss 0.01771. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729032
Test loss (w/o reg) on all data: 0.012056429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.197109e-07
Norm of the params: 9.153269
                Loss: fixed 131 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22563612
Train loss (w/o reg) on all data: 0.21996877
Test loss (w/o reg) on all data: 0.120969236
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.739557e-05
Norm of the params: 10.646465
              Random: fixed  30 labels. Loss 0.12097. Accuracy 0.992.
### Flips: 260, rs: 31, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0151601285
Train loss (w/o reg) on all data: 0.008436807
Test loss (w/o reg) on all data: 0.015782708
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.264551e-07
Norm of the params: 11.595966
     Influence (LOO): fixed 129 labels. Loss 0.01578. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012055717
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3211427e-07
Norm of the params: 9.153228
                Loss: fixed 131 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21505222
Train loss (w/o reg) on all data: 0.20927702
Test loss (w/o reg) on all data: 0.11141649
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 0.0001257306
Norm of the params: 10.747275
              Random: fixed  37 labels. Loss 0.11142. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23497781
Train loss (w/o reg) on all data: 0.22906703
Test loss (w/o reg) on all data: 0.17107236
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2161289e-05
Norm of the params: 10.872701
Flipped loss: 0.17107. Accuracy: 0.954
### Flips: 260, rs: 32, checks: 52
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17156029
Train loss (w/o reg) on all data: 0.1622669
Test loss (w/o reg) on all data: 0.13726737
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.372968e-06
Norm of the params: 13.63333
     Influence (LOO): fixed  35 labels. Loss 0.13727. Accuracy 0.954.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113250494
Train loss (w/o reg) on all data: 0.10004185
Test loss (w/o reg) on all data: 0.13170539
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.1048967e-05
Norm of the params: 16.253395
                Loss: fixed  52 labels. Loss 0.13171. Accuracy 0.939.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22387828
Train loss (w/o reg) on all data: 0.21780112
Test loss (w/o reg) on all data: 0.16518065
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.780813e-05
Norm of the params: 11.024657
              Random: fixed   8 labels. Loss 0.16518. Accuracy 0.954.
### Flips: 260, rs: 32, checks: 104
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12249971
Train loss (w/o reg) on all data: 0.11324981
Test loss (w/o reg) on all data: 0.10964409
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4419313e-05
Norm of the params: 13.601399
     Influence (LOO): fixed  64 labels. Loss 0.10964. Accuracy 0.962.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035043813
Train loss (w/o reg) on all data: 0.021121964
Test loss (w/o reg) on all data: 0.057920635
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.111636e-06
Norm of the params: 16.686432
                Loss: fixed  98 labels. Loss 0.05792. Accuracy 0.981.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21493846
Train loss (w/o reg) on all data: 0.20887212
Test loss (w/o reg) on all data: 0.15119907
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.485441e-05
Norm of the params: 11.014847
              Random: fixed  16 labels. Loss 0.15120. Accuracy 0.962.
### Flips: 260, rs: 32, checks: 156
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06654094
Train loss (w/o reg) on all data: 0.05688957
Test loss (w/o reg) on all data: 0.06760576
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.312816e-06
Norm of the params: 13.893431
     Influence (LOO): fixed  93 labels. Loss 0.06761. Accuracy 0.977.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017709764
Train loss (w/o reg) on all data: 0.008711736
Test loss (w/o reg) on all data: 0.020395473
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0412987e-06
Norm of the params: 13.414937
                Loss: fixed 112 labels. Loss 0.02040. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20492221
Train loss (w/o reg) on all data: 0.19867297
Test loss (w/o reg) on all data: 0.14341114
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.793268e-05
Norm of the params: 11.17967
              Random: fixed  24 labels. Loss 0.14341. Accuracy 0.962.
### Flips: 260, rs: 32, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028317902
Train loss (w/o reg) on all data: 0.01995087
Test loss (w/o reg) on all data: 0.041116565
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0594623e-06
Norm of the params: 12.936023
     Influence (LOO): fixed 109 labels. Loss 0.04112. Accuracy 0.985.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011212505
Train loss (w/o reg) on all data: 0.0046753692
Test loss (w/o reg) on all data: 0.015690906
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7410407e-06
Norm of the params: 11.4342785
                Loss: fixed 116 labels. Loss 0.01569. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19211927
Train loss (w/o reg) on all data: 0.18554041
Test loss (w/o reg) on all data: 0.1306396
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.74969e-05
Norm of the params: 11.470709
              Random: fixed  34 labels. Loss 0.13064. Accuracy 0.966.
### Flips: 260, rs: 32, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.01205533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8870718e-07
Norm of the params: 9.153216
     Influence (LOO): fixed 120 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0086955875
Train loss (w/o reg) on all data: 0.0032690922
Test loss (w/o reg) on all data: 0.01668329
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.882374e-07
Norm of the params: 10.417769
                Loss: fixed 117 labels. Loss 0.01668. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920329
Train loss (w/o reg) on all data: 0.18271108
Test loss (w/o reg) on all data: 0.1282802
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.4368745e-06
Norm of the params: 11.394919
              Random: fixed  36 labels. Loss 0.12828. Accuracy 0.969.
### Flips: 260, rs: 32, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055623
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1317722e-07
Norm of the params: 9.153198
     Influence (LOO): fixed 120 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008695588
Train loss (w/o reg) on all data: 0.003269118
Test loss (w/o reg) on all data: 0.016682714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.118704e-07
Norm of the params: 10.417746
                Loss: fixed 117 labels. Loss 0.01668. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18338262
Train loss (w/o reg) on all data: 0.17725362
Test loss (w/o reg) on all data: 0.12291987
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.406504e-05
Norm of the params: 11.071584
              Random: fixed  41 labels. Loss 0.12292. Accuracy 0.969.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2432673
Train loss (w/o reg) on all data: 0.23446679
Test loss (w/o reg) on all data: 0.16739419
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.4398843e-05
Norm of the params: 13.266881
Flipped loss: 0.16739. Accuracy: 0.958
### Flips: 260, rs: 33, checks: 52
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18529911
Train loss (w/o reg) on all data: 0.17537813
Test loss (w/o reg) on all data: 0.107248604
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0857173e-05
Norm of the params: 14.086151
     Influence (LOO): fixed  34 labels. Loss 0.10725. Accuracy 0.973.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1290431
Train loss (w/o reg) on all data: 0.11333505
Test loss (w/o reg) on all data: 0.111830205
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3273294e-05
Norm of the params: 17.724592
                Loss: fixed  51 labels. Loss 0.11183. Accuracy 0.962.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24074921
Train loss (w/o reg) on all data: 0.2317543
Test loss (w/o reg) on all data: 0.16218716
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5574831e-05
Norm of the params: 13.412612
              Random: fixed   4 labels. Loss 0.16219. Accuracy 0.962.
### Flips: 260, rs: 33, checks: 104
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12867905
Train loss (w/o reg) on all data: 0.11724558
Test loss (w/o reg) on all data: 0.08689716
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.4143115e-06
Norm of the params: 15.121822
     Influence (LOO): fixed  66 labels. Loss 0.08690. Accuracy 0.969.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05720242
Train loss (w/o reg) on all data: 0.039981376
Test loss (w/o reg) on all data: 0.053266056
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.849888e-06
Norm of the params: 18.558582
                Loss: fixed  93 labels. Loss 0.05327. Accuracy 0.985.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23468548
Train loss (w/o reg) on all data: 0.2259609
Test loss (w/o reg) on all data: 0.15167813
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.722788e-05
Norm of the params: 13.209531
              Random: fixed  12 labels. Loss 0.15168. Accuracy 0.966.
### Flips: 260, rs: 33, checks: 156
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08843988
Train loss (w/o reg) on all data: 0.07606884
Test loss (w/o reg) on all data: 0.0734057
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.850821e-06
Norm of the params: 15.729618
     Influence (LOO): fixed  88 labels. Loss 0.07341. Accuracy 0.973.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0386521
Train loss (w/o reg) on all data: 0.02397884
Test loss (w/o reg) on all data: 0.03676547
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1823579e-05
Norm of the params: 17.130827
                Loss: fixed 109 labels. Loss 0.03677. Accuracy 0.981.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22722414
Train loss (w/o reg) on all data: 0.21825062
Test loss (w/o reg) on all data: 0.1465173
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4669411e-05
Norm of the params: 13.396665
              Random: fixed  19 labels. Loss 0.14652. Accuracy 0.969.
### Flips: 260, rs: 33, checks: 208
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061920293
Train loss (w/o reg) on all data: 0.04965852
Test loss (w/o reg) on all data: 0.054153133
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.325239e-06
Norm of the params: 15.659998
     Influence (LOO): fixed 104 labels. Loss 0.05415. Accuracy 0.981.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02207293
Train loss (w/o reg) on all data: 0.011371813
Test loss (w/o reg) on all data: 0.01919567
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7810746e-06
Norm of the params: 14.629502
                Loss: fixed 122 labels. Loss 0.01920. Accuracy 0.992.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22480643
Train loss (w/o reg) on all data: 0.21580626
Test loss (w/o reg) on all data: 0.1465498
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4073888e-05
Norm of the params: 13.4165325
              Random: fixed  21 labels. Loss 0.14655. Accuracy 0.973.
### Flips: 260, rs: 33, checks: 260
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02227194
Train loss (w/o reg) on all data: 0.014755124
Test loss (w/o reg) on all data: 0.025079919
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.76586e-07
Norm of the params: 12.261171
     Influence (LOO): fixed 124 labels. Loss 0.02508. Accuracy 0.996.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014034327
Train loss (w/o reg) on all data: 0.0064480724
Test loss (w/o reg) on all data: 0.017633583
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.288153e-07
Norm of the params: 12.317674
                Loss: fixed 128 labels. Loss 0.01763. Accuracy 0.992.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21813636
Train loss (w/o reg) on all data: 0.20871079
Test loss (w/o reg) on all data: 0.14230433
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8750687e-05
Norm of the params: 13.729942
              Random: fixed  28 labels. Loss 0.14230. Accuracy 0.969.
### Flips: 260, rs: 33, checks: 312
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015800059
Train loss (w/o reg) on all data: 0.01051801
Test loss (w/o reg) on all data: 0.0144287655
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6541567e-06
Norm of the params: 10.27818
     Influence (LOO): fixed 129 labels. Loss 0.01443. Accuracy 0.996.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010151774
Train loss (w/o reg) on all data: 0.0039737606
Test loss (w/o reg) on all data: 0.016602185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4052647e-07
Norm of the params: 11.115767
                Loss: fixed 130 labels. Loss 0.01660. Accuracy 0.992.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209966
Train loss (w/o reg) on all data: 0.20104393
Test loss (w/o reg) on all data: 0.12645349
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0608894e-05
Norm of the params: 13.358199
              Random: fixed  35 labels. Loss 0.12645. Accuracy 0.977.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24228488
Train loss (w/o reg) on all data: 0.2346786
Test loss (w/o reg) on all data: 0.17044666
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.258935e-05
Norm of the params: 12.333928
Flipped loss: 0.17045. Accuracy: 0.958
### Flips: 260, rs: 34, checks: 52
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18499665
Train loss (w/o reg) on all data: 0.17508
Test loss (w/o reg) on all data: 0.12701012
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1500978e-05
Norm of the params: 14.083073
     Influence (LOO): fixed  37 labels. Loss 0.12701. Accuracy 0.981.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13161847
Train loss (w/o reg) on all data: 0.11790592
Test loss (w/o reg) on all data: 0.10306883
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.8213864e-05
Norm of the params: 16.560522
                Loss: fixed  52 labels. Loss 0.10307. Accuracy 0.962.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23736852
Train loss (w/o reg) on all data: 0.22967634
Test loss (w/o reg) on all data: 0.16513781
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4986878e-05
Norm of the params: 12.403382
              Random: fixed   3 labels. Loss 0.16514. Accuracy 0.954.
### Flips: 260, rs: 34, checks: 104
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12555157
Train loss (w/o reg) on all data: 0.11390459
Test loss (w/o reg) on all data: 0.085330635
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.260079e-05
Norm of the params: 15.262359
     Influence (LOO): fixed  69 labels. Loss 0.08533. Accuracy 0.977.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048342813
Train loss (w/o reg) on all data: 0.033481386
Test loss (w/o reg) on all data: 0.046668474
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3413544e-06
Norm of the params: 17.240318
                Loss: fixed  99 labels. Loss 0.04667. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23076057
Train loss (w/o reg) on all data: 0.22294076
Test loss (w/o reg) on all data: 0.15017477
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6997584e-05
Norm of the params: 12.505848
              Random: fixed  13 labels. Loss 0.15017. Accuracy 0.973.
### Flips: 260, rs: 34, checks: 156
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07528601
Train loss (w/o reg) on all data: 0.06392777
Test loss (w/o reg) on all data: 0.047230642
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1962595e-06
Norm of the params: 15.071985
     Influence (LOO): fixed  94 labels. Loss 0.04723. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019088976
Train loss (w/o reg) on all data: 0.009215371
Test loss (w/o reg) on all data: 0.020031475
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2942186e-06
Norm of the params: 14.052476
                Loss: fixed 116 labels. Loss 0.02003. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22469398
Train loss (w/o reg) on all data: 0.21659586
Test loss (w/o reg) on all data: 0.13589782
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.446869e-05
Norm of the params: 12.726447
              Random: fixed  18 labels. Loss 0.13590. Accuracy 0.981.
### Flips: 260, rs: 34, checks: 208
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034950763
Train loss (w/o reg) on all data: 0.026284067
Test loss (w/o reg) on all data: 0.024715137
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1305655e-06
Norm of the params: 13.165635
     Influence (LOO): fixed 114 labels. Loss 0.02472. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013360028
Train loss (w/o reg) on all data: 0.005799917
Test loss (w/o reg) on all data: 0.014159377
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6150565e-07
Norm of the params: 12.296431
                Loss: fixed 119 labels. Loss 0.01416. Accuracy 0.996.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21869616
Train loss (w/o reg) on all data: 0.21045648
Test loss (w/o reg) on all data: 0.13168669
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.967862e-06
Norm of the params: 12.837196
              Random: fixed  22 labels. Loss 0.13169. Accuracy 0.985.
### Flips: 260, rs: 34, checks: 260
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018020893
Train loss (w/o reg) on all data: 0.0115855085
Test loss (w/o reg) on all data: 0.013760181
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2868779e-06
Norm of the params: 11.344942
     Influence (LOO): fixed 120 labels. Loss 0.01376. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012127224
Train loss (w/o reg) on all data: 0.005182725
Test loss (w/o reg) on all data: 0.0123732155
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.759115e-07
Norm of the params: 11.785159
                Loss: fixed 120 labels. Loss 0.01237. Accuracy 0.996.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21388692
Train loss (w/o reg) on all data: 0.20585349
Test loss (w/o reg) on all data: 0.12558544
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9101313e-05
Norm of the params: 12.675509
              Random: fixed  27 labels. Loss 0.12559. Accuracy 0.985.
### Flips: 260, rs: 34, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.0054477933
Test loss (w/o reg) on all data: 0.012245029
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.1408115e-07
Norm of the params: 9.544172
     Influence (LOO): fixed 123 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009416
Train loss (w/o reg) on all data: 0.003692637
Test loss (w/o reg) on all data: 0.010481183
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1722572e-07
Norm of the params: 10.698937
                Loss: fixed 122 labels. Loss 0.01048. Accuracy 0.996.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20512423
Train loss (w/o reg) on all data: 0.19697584
Test loss (w/o reg) on all data: 0.12263538
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.216031e-05
Norm of the params: 12.765881
              Random: fixed  32 labels. Loss 0.12264. Accuracy 0.981.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24607635
Train loss (w/o reg) on all data: 0.23907062
Test loss (w/o reg) on all data: 0.150899
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.9159835e-05
Norm of the params: 11.836994
Flipped loss: 0.15090. Accuracy: 0.969
### Flips: 260, rs: 35, checks: 52
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17569976
Train loss (w/o reg) on all data: 0.16368073
Test loss (w/o reg) on all data: 0.099547416
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1049695e-05
Norm of the params: 15.504207
     Influence (LOO): fixed  39 labels. Loss 0.09955. Accuracy 0.973.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12843661
Train loss (w/o reg) on all data: 0.113192074
Test loss (w/o reg) on all data: 0.09372065
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.1672246e-06
Norm of the params: 17.461123
                Loss: fixed  52 labels. Loss 0.09372. Accuracy 0.966.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23902096
Train loss (w/o reg) on all data: 0.23272689
Test loss (w/o reg) on all data: 0.13040511
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4464985e-05
Norm of the params: 11.219692
              Random: fixed  11 labels. Loss 0.13041. Accuracy 0.981.
### Flips: 260, rs: 35, checks: 104
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12735607
Train loss (w/o reg) on all data: 0.11358655
Test loss (w/o reg) on all data: 0.06760665
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9885736e-05
Norm of the params: 16.59489
     Influence (LOO): fixed  66 labels. Loss 0.06761. Accuracy 0.989.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041981958
Train loss (w/o reg) on all data: 0.028404674
Test loss (w/o reg) on all data: 0.033558924
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.794911e-06
Norm of the params: 16.478643
                Loss: fixed  98 labels. Loss 0.03356. Accuracy 0.989.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23231708
Train loss (w/o reg) on all data: 0.22652708
Test loss (w/o reg) on all data: 0.12205331
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.558704e-05
Norm of the params: 10.761044
              Random: fixed  17 labels. Loss 0.12205. Accuracy 0.989.
### Flips: 260, rs: 35, checks: 156
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08658737
Train loss (w/o reg) on all data: 0.0748529
Test loss (w/o reg) on all data: 0.04538143
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9105428e-05
Norm of the params: 15.319578
     Influence (LOO): fixed  88 labels. Loss 0.04538. Accuracy 0.996.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017528951
Train loss (w/o reg) on all data: 0.009007242
Test loss (w/o reg) on all data: 0.012936976
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2264792e-06
Norm of the params: 13.055046
                Loss: fixed 115 labels. Loss 0.01294. Accuracy 0.996.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22313721
Train loss (w/o reg) on all data: 0.21755789
Test loss (w/o reg) on all data: 0.11416706
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7690287e-05
Norm of the params: 10.563449
              Random: fixed  23 labels. Loss 0.11417. Accuracy 0.992.
### Flips: 260, rs: 35, checks: 208
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04663935
Train loss (w/o reg) on all data: 0.037314415
Test loss (w/o reg) on all data: 0.025582163
Train acc on all data:  0.9866284622731614
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2244675e-06
Norm of the params: 13.656454
     Influence (LOO): fixed 106 labels. Loss 0.02558. Accuracy 1.000.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009135033
Train loss (w/o reg) on all data: 0.0035337529
Test loss (w/o reg) on all data: 0.005869237
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1668213e-06
Norm of the params: 10.584214
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2134243
Train loss (w/o reg) on all data: 0.20785367
Test loss (w/o reg) on all data: 0.11316453
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.203077e-05
Norm of the params: 10.555212
              Random: fixed  29 labels. Loss 0.11316. Accuracy 0.992.
### Flips: 260, rs: 35, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011604913
Train loss (w/o reg) on all data: 0.006573452
Test loss (w/o reg) on all data: 0.0129234865
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.783676e-07
Norm of the params: 10.031411
     Influence (LOO): fixed 120 labels. Loss 0.01292. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00913503
Train loss (w/o reg) on all data: 0.0035338197
Test loss (w/o reg) on all data: 0.0058696223
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.206955e-07
Norm of the params: 10.58415
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20046285
Train loss (w/o reg) on all data: 0.19440058
Test loss (w/o reg) on all data: 0.10201758
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.624605e-06
Norm of the params: 11.011146
              Random: fixed  37 labels. Loss 0.10202. Accuracy 0.989.
### Flips: 260, rs: 35, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009954039
Train loss (w/o reg) on all data: 0.0054101157
Test loss (w/o reg) on all data: 0.0060221828
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 8.4798097e-07
Norm of the params: 9.53302
     Influence (LOO): fixed 121 labels. Loss 0.00602. Accuracy 1.000.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009135035
Train loss (w/o reg) on all data: 0.0035337827
Test loss (w/o reg) on all data: 0.00586966
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0995392e-07
Norm of the params: 10.584188
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19635019
Train loss (w/o reg) on all data: 0.19011559
Test loss (w/o reg) on all data: 0.09886033
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7690576e-06
Norm of the params: 11.166556
              Random: fixed  40 labels. Loss 0.09886. Accuracy 0.989.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23790336
Train loss (w/o reg) on all data: 0.23002206
Test loss (w/o reg) on all data: 0.15983814
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.480731e-05
Norm of the params: 12.554921
Flipped loss: 0.15984. Accuracy: 0.966
### Flips: 260, rs: 36, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17053193
Train loss (w/o reg) on all data: 0.15801816
Test loss (w/o reg) on all data: 0.13047439
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.890099e-06
Norm of the params: 15.82009
     Influence (LOO): fixed  39 labels. Loss 0.13047. Accuracy 0.969.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12715633
Train loss (w/o reg) on all data: 0.11081726
Test loss (w/o reg) on all data: 0.1162755
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.944334e-05
Norm of the params: 18.077097
                Loss: fixed  51 labels. Loss 0.11628. Accuracy 0.962.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2346784
Train loss (w/o reg) on all data: 0.22693294
Test loss (w/o reg) on all data: 0.15786743
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4754425e-05
Norm of the params: 12.446256
              Random: fixed   3 labels. Loss 0.15787. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 104
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13042864
Train loss (w/o reg) on all data: 0.117908515
Test loss (w/o reg) on all data: 0.098533385
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0727859e-05
Norm of the params: 15.824115
     Influence (LOO): fixed  67 labels. Loss 0.09853. Accuracy 0.981.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043077543
Train loss (w/o reg) on all data: 0.027092617
Test loss (w/o reg) on all data: 0.06821105
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.4676883e-06
Norm of the params: 17.880114
                Loss: fixed  99 labels. Loss 0.06821. Accuracy 0.966.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23051548
Train loss (w/o reg) on all data: 0.22248909
Test loss (w/o reg) on all data: 0.15690465
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.513035e-05
Norm of the params: 12.669958
              Random: fixed   5 labels. Loss 0.15690. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 156
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08240365
Train loss (w/o reg) on all data: 0.07029579
Test loss (w/o reg) on all data: 0.06003208
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9414689e-05
Norm of the params: 15.561401
     Influence (LOO): fixed  94 labels. Loss 0.06003. Accuracy 0.977.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026398554
Train loss (w/o reg) on all data: 0.014054117
Test loss (w/o reg) on all data: 0.023621675
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.87063e-06
Norm of the params: 15.712693
                Loss: fixed 111 labels. Loss 0.02362. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22306548
Train loss (w/o reg) on all data: 0.21477309
Test loss (w/o reg) on all data: 0.15110645
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.125178e-05
Norm of the params: 12.878195
              Random: fixed  11 labels. Loss 0.15111. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 208
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039874583
Train loss (w/o reg) on all data: 0.030224169
Test loss (w/o reg) on all data: 0.028858671
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7536417e-06
Norm of the params: 13.892741
     Influence (LOO): fixed 112 labels. Loss 0.02886. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019540109
Train loss (w/o reg) on all data: 0.009169658
Test loss (w/o reg) on all data: 0.014529203
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4944125e-06
Norm of the params: 14.401702
                Loss: fixed 116 labels. Loss 0.01453. Accuracy 0.996.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21922937
Train loss (w/o reg) on all data: 0.21079475
Test loss (w/o reg) on all data: 0.1388995
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5631964e-05
Norm of the params: 12.988171
              Random: fixed  17 labels. Loss 0.13890. Accuracy 0.973.
### Flips: 260, rs: 36, checks: 260
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021098362
Train loss (w/o reg) on all data: 0.014752482
Test loss (w/o reg) on all data: 0.017993493
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3035545e-06
Norm of the params: 11.265772
     Influence (LOO): fixed 121 labels. Loss 0.01799. Accuracy 0.996.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013883283
Train loss (w/o reg) on all data: 0.005859994
Test loss (w/o reg) on all data: 0.015900094
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.6618563e-07
Norm of the params: 12.667509
                Loss: fixed 119 labels. Loss 0.01590. Accuracy 0.996.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21659449
Train loss (w/o reg) on all data: 0.208155
Test loss (w/o reg) on all data: 0.13292088
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5321311e-05
Norm of the params: 12.991905
              Random: fixed  20 labels. Loss 0.13292. Accuracy 0.981.
### Flips: 260, rs: 36, checks: 312
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489632
Train loss (w/o reg) on all data: 0.0055393577
Test loss (w/o reg) on all data: 0.013846117
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.134167e-07
Norm of the params: 9.9501505
     Influence (LOO): fixed 124 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010344867
Train loss (w/o reg) on all data: 0.0040953653
Test loss (w/o reg) on all data: 0.013821318
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3797504e-07
Norm of the params: 11.179894
                Loss: fixed 121 labels. Loss 0.01382. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20582342
Train loss (w/o reg) on all data: 0.1971339
Test loss (w/o reg) on all data: 0.12851943
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.3582164e-05
Norm of the params: 13.182957
              Random: fixed  26 labels. Loss 0.12852. Accuracy 0.977.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24842286
Train loss (w/o reg) on all data: 0.24126302
Test loss (w/o reg) on all data: 0.17699064
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.075757e-05
Norm of the params: 11.966491
Flipped loss: 0.17699. Accuracy: 0.966
### Flips: 260, rs: 37, checks: 52
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17848222
Train loss (w/o reg) on all data: 0.16762465
Test loss (w/o reg) on all data: 0.13165161
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0392214e-05
Norm of the params: 14.736054
     Influence (LOO): fixed  38 labels. Loss 0.13165. Accuracy 0.973.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13007265
Train loss (w/o reg) on all data: 0.11608228
Test loss (w/o reg) on all data: 0.13264121
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.374392e-06
Norm of the params: 16.727446
                Loss: fixed  52 labels. Loss 0.13264. Accuracy 0.947.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24727607
Train loss (w/o reg) on all data: 0.24005581
Test loss (w/o reg) on all data: 0.17495614
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.366748e-05
Norm of the params: 12.016868
              Random: fixed   2 labels. Loss 0.17496. Accuracy 0.966.
### Flips: 260, rs: 37, checks: 104
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13107619
Train loss (w/o reg) on all data: 0.11877813
Test loss (w/o reg) on all data: 0.10709827
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3534414e-05
Norm of the params: 15.683143
     Influence (LOO): fixed  66 labels. Loss 0.10710. Accuracy 0.985.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041347496
Train loss (w/o reg) on all data: 0.027330143
Test loss (w/o reg) on all data: 0.040240217
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.888715e-06
Norm of the params: 16.743567
                Loss: fixed 101 labels. Loss 0.04024. Accuracy 0.989.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24002838
Train loss (w/o reg) on all data: 0.23277782
Test loss (w/o reg) on all data: 0.16217354
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9202747e-05
Norm of the params: 12.042055
              Random: fixed   9 labels. Loss 0.16217. Accuracy 0.985.
### Flips: 260, rs: 37, checks: 156
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08677063
Train loss (w/o reg) on all data: 0.075133234
Test loss (w/o reg) on all data: 0.07967954
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.1754573e-06
Norm of the params: 15.256081
     Influence (LOO): fixed  93 labels. Loss 0.07968. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018634442
Train loss (w/o reg) on all data: 0.0084473705
Test loss (w/o reg) on all data: 0.027552243
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.099429e-07
Norm of the params: 14.273803
                Loss: fixed 117 labels. Loss 0.02755. Accuracy 0.989.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2337705
Train loss (w/o reg) on all data: 0.22646286
Test loss (w/o reg) on all data: 0.14462186
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00042225e-05
Norm of the params: 12.089375
              Random: fixed  16 labels. Loss 0.14462. Accuracy 0.992.
### Flips: 260, rs: 37, checks: 208
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04771938
Train loss (w/o reg) on all data: 0.03754934
Test loss (w/o reg) on all data: 0.05925686
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5033046e-06
Norm of the params: 14.261866
     Influence (LOO): fixed 110 labels. Loss 0.05926. Accuracy 0.989.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013132797
Train loss (w/o reg) on all data: 0.005688963
Test loss (w/o reg) on all data: 0.021239074
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.215947e-06
Norm of the params: 12.201503
                Loss: fixed 121 labels. Loss 0.02124. Accuracy 0.992.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22639874
Train loss (w/o reg) on all data: 0.21888448
Test loss (w/o reg) on all data: 0.13750318
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6694083e-05
Norm of the params: 12.259086
              Random: fixed  21 labels. Loss 0.13750. Accuracy 0.992.
### Flips: 260, rs: 37, checks: 260
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022514202
Train loss (w/o reg) on all data: 0.015119843
Test loss (w/o reg) on all data: 0.02661293
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4842814e-06
Norm of the params: 12.160888
     Influence (LOO): fixed 120 labels. Loss 0.02661. Accuracy 0.992.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010560146
Train loss (w/o reg) on all data: 0.004315263
Test loss (w/o reg) on all data: 0.019344684
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0095117e-06
Norm of the params: 11.175761
                Loss: fixed 122 labels. Loss 0.01934. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21440955
Train loss (w/o reg) on all data: 0.20643161
Test loss (w/o reg) on all data: 0.1252371
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.14851e-05
Norm of the params: 12.631656
              Random: fixed  29 labels. Loss 0.12524. Accuracy 0.989.
### Flips: 260, rs: 37, checks: 312
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013053302
Train loss (w/o reg) on all data: 0.007583063
Test loss (w/o reg) on all data: 0.017072685
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.979793e-07
Norm of the params: 10.459674
     Influence (LOO): fixed 123 labels. Loss 0.01707. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010560146
Train loss (w/o reg) on all data: 0.0043156524
Test loss (w/o reg) on all data: 0.01934638
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9930845e-07
Norm of the params: 11.175412
                Loss: fixed 122 labels. Loss 0.01935. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19781686
Train loss (w/o reg) on all data: 0.1899416
Test loss (w/o reg) on all data: 0.10731826
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5653782e-05
Norm of the params: 12.550106
              Random: fixed  41 labels. Loss 0.10732. Accuracy 0.996.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2511049
Train loss (w/o reg) on all data: 0.24406609
Test loss (w/o reg) on all data: 0.16524702
Train acc on all data:  0.889207258834766
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.0805943e-05
Norm of the params: 11.864909
Flipped loss: 0.16525. Accuracy: 0.950
### Flips: 260, rs: 38, checks: 52
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19415013
Train loss (w/o reg) on all data: 0.18386316
Test loss (w/o reg) on all data: 0.13172767
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6420885e-05
Norm of the params: 14.343623
     Influence (LOO): fixed  35 labels. Loss 0.13173. Accuracy 0.966.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14355534
Train loss (w/o reg) on all data: 0.13049053
Test loss (w/o reg) on all data: 0.117203884
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.167509e-06
Norm of the params: 16.164663
                Loss: fixed  51 labels. Loss 0.11720. Accuracy 0.947.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24010944
Train loss (w/o reg) on all data: 0.23310131
Test loss (w/o reg) on all data: 0.15391655
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.415653e-05
Norm of the params: 11.839038
              Random: fixed  10 labels. Loss 0.15392. Accuracy 0.962.
### Flips: 260, rs: 38, checks: 104
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14206469
Train loss (w/o reg) on all data: 0.13038966
Test loss (w/o reg) on all data: 0.0978361
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6112124e-05
Norm of the params: 15.280723
     Influence (LOO): fixed  65 labels. Loss 0.09784. Accuracy 0.973.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06602065
Train loss (w/o reg) on all data: 0.051285453
Test loss (w/o reg) on all data: 0.06991266
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.900919e-06
Norm of the params: 17.166943
                Loss: fixed  95 labels. Loss 0.06991. Accuracy 0.977.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23003906
Train loss (w/o reg) on all data: 0.22294027
Test loss (w/o reg) on all data: 0.15167399
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.9855844e-05
Norm of the params: 11.915358
              Random: fixed  18 labels. Loss 0.15167. Accuracy 0.950.
### Flips: 260, rs: 38, checks: 156
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09734319
Train loss (w/o reg) on all data: 0.08661325
Test loss (w/o reg) on all data: 0.047763
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.349983e-06
Norm of the params: 14.649191
     Influence (LOO): fixed  93 labels. Loss 0.04776. Accuracy 0.989.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025832484
Train loss (w/o reg) on all data: 0.014877879
Test loss (w/o reg) on all data: 0.017677132
Train acc on all data:  0.9980897803247374
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8911394e-06
Norm of the params: 14.801759
                Loss: fixed 119 labels. Loss 0.01768. Accuracy 1.000.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21640122
Train loss (w/o reg) on all data: 0.20879246
Test loss (w/o reg) on all data: 0.13060269
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8010633e-05
Norm of the params: 12.335934
              Random: fixed  29 labels. Loss 0.13060. Accuracy 0.973.
### Flips: 260, rs: 38, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0488242
Train loss (w/o reg) on all data: 0.039660577
Test loss (w/o reg) on all data: 0.01936302
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9804725e-06
Norm of the params: 13.537815
     Influence (LOO): fixed 114 labels. Loss 0.01936. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01754256
Train loss (w/o reg) on all data: 0.00882897
Test loss (w/o reg) on all data: 0.007449364
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.303021e-06
Norm of the params: 13.201205
                Loss: fixed 125 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20792809
Train loss (w/o reg) on all data: 0.20032729
Test loss (w/o reg) on all data: 0.11901566
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.200212e-05
Norm of the params: 12.329476
              Random: fixed  36 labels. Loss 0.11902. Accuracy 0.989.
### Flips: 260, rs: 38, checks: 260
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02493031
Train loss (w/o reg) on all data: 0.018072933
Test loss (w/o reg) on all data: 0.01504479
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3194391e-06
Norm of the params: 11.711
     Influence (LOO): fixed 127 labels. Loss 0.01504. Accuracy 0.996.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013716024
Train loss (w/o reg) on all data: 0.0063428404
Test loss (w/o reg) on all data: 0.0067364834
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 2.08398e-06
Norm of the params: 12.143462
                Loss: fixed 128 labels. Loss 0.00674. Accuracy 1.000.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20387602
Train loss (w/o reg) on all data: 0.19627339
Test loss (w/o reg) on all data: 0.11314659
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8139086e-05
Norm of the params: 12.330961
              Random: fixed  40 labels. Loss 0.11315. Accuracy 0.989.
### Flips: 260, rs: 38, checks: 312
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016603706
Train loss (w/o reg) on all data: 0.010851948
Test loss (w/o reg) on all data: 0.01358988
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.5253174e-07
Norm of the params: 10.725445
     Influence (LOO): fixed 130 labels. Loss 0.01359. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009581836
Train loss (w/o reg) on all data: 0.0040776595
Test loss (w/o reg) on all data: 0.0063123563
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1082686e-07
Norm of the params: 10.492069
                Loss: fixed 130 labels. Loss 0.00631. Accuracy 1.000.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19843724
Train loss (w/o reg) on all data: 0.19061005
Test loss (w/o reg) on all data: 0.10658834
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2578124e-05
Norm of the params: 12.511755
              Random: fixed  45 labels. Loss 0.10659. Accuracy 0.989.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25208217
Train loss (w/o reg) on all data: 0.24381304
Test loss (w/o reg) on all data: 0.1768431
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5027415e-05
Norm of the params: 12.860129
Flipped loss: 0.17684. Accuracy: 0.966
### Flips: 260, rs: 39, checks: 52
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18981676
Train loss (w/o reg) on all data: 0.1792476
Test loss (w/o reg) on all data: 0.12720862
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1512626e-05
Norm of the params: 14.539017
     Influence (LOO): fixed  34 labels. Loss 0.12721. Accuracy 0.973.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14046301
Train loss (w/o reg) on all data: 0.12568843
Test loss (w/o reg) on all data: 0.13297586
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.4940855e-05
Norm of the params: 17.189865
                Loss: fixed  52 labels. Loss 0.13298. Accuracy 0.947.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2492665
Train loss (w/o reg) on all data: 0.24117665
Test loss (w/o reg) on all data: 0.17330943
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.0369152e-05
Norm of the params: 12.719954
              Random: fixed   3 labels. Loss 0.17331. Accuracy 0.969.
### Flips: 260, rs: 39, checks: 104
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1426228
Train loss (w/o reg) on all data: 0.13276422
Test loss (w/o reg) on all data: 0.085730076
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.0763185e-06
Norm of the params: 14.041776
     Influence (LOO): fixed  65 labels. Loss 0.08573. Accuracy 0.985.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05369138
Train loss (w/o reg) on all data: 0.038393285
Test loss (w/o reg) on all data: 0.0643097
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.312839e-05
Norm of the params: 17.491766
                Loss: fixed  99 labels. Loss 0.06431. Accuracy 0.973.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23962532
Train loss (w/o reg) on all data: 0.23097436
Test loss (w/o reg) on all data: 0.16666253
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 0.000102192644
Norm of the params: 13.153671
              Random: fixed  10 labels. Loss 0.16666. Accuracy 0.962.
### Flips: 260, rs: 39, checks: 156
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09063938
Train loss (w/o reg) on all data: 0.080353126
Test loss (w/o reg) on all data: 0.051421143
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0557545e-05
Norm of the params: 14.3431225
     Influence (LOO): fixed  90 labels. Loss 0.05142. Accuracy 0.989.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020509396
Train loss (w/o reg) on all data: 0.011690681
Test loss (w/o reg) on all data: 0.013863249
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1274234e-06
Norm of the params: 13.280598
                Loss: fixed 121 labels. Loss 0.01386. Accuracy 0.996.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2267266
Train loss (w/o reg) on all data: 0.2182804
Test loss (w/o reg) on all data: 0.15126832
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.295532e-05
Norm of the params: 12.997076
              Random: fixed  22 labels. Loss 0.15127. Accuracy 0.966.
### Flips: 260, rs: 39, checks: 208
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056681376
Train loss (w/o reg) on all data: 0.047940563
Test loss (w/o reg) on all data: 0.030546071
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.901027e-06
Norm of the params: 13.221809
     Influence (LOO): fixed 108 labels. Loss 0.03055. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014661532
Train loss (w/o reg) on all data: 0.0067741866
Test loss (w/o reg) on all data: 0.01093934
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.732098e-07
Norm of the params: 12.559734
                Loss: fixed 123 labels. Loss 0.01094. Accuracy 0.996.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21779792
Train loss (w/o reg) on all data: 0.20933776
Test loss (w/o reg) on all data: 0.14205875
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.18521775e-05
Norm of the params: 13.007816
              Random: fixed  28 labels. Loss 0.14206. Accuracy 0.962.
### Flips: 260, rs: 39, checks: 260
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028152222
Train loss (w/o reg) on all data: 0.021305429
Test loss (w/o reg) on all data: 0.020639863
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.30044455e-05
Norm of the params: 11.70196
     Influence (LOO): fixed 119 labels. Loss 0.02064. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014661532
Train loss (w/o reg) on all data: 0.0067739603
Test loss (w/o reg) on all data: 0.010939816
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.6655733e-07
Norm of the params: 12.559914
                Loss: fixed 123 labels. Loss 0.01094. Accuracy 0.996.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20963511
Train loss (w/o reg) on all data: 0.20115148
Test loss (w/o reg) on all data: 0.14063172
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.738248e-06
Norm of the params: 13.025841
              Random: fixed  35 labels. Loss 0.14063. Accuracy 0.973.
### Flips: 260, rs: 39, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015056493
Train loss (w/o reg) on all data: 0.009467833
Test loss (w/o reg) on all data: 0.014698382
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.154993e-06
Norm of the params: 10.572284
     Influence (LOO): fixed 123 labels. Loss 0.01470. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009598745
Train loss (w/o reg) on all data: 0.0038187616
Test loss (w/o reg) on all data: 0.009982576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3483664e-07
Norm of the params: 10.751729
                Loss: fixed 125 labels. Loss 0.00998. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19999173
Train loss (w/o reg) on all data: 0.19106226
Test loss (w/o reg) on all data: 0.13245419
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.992381e-05
Norm of the params: 13.363741
              Random: fixed  43 labels. Loss 0.13245. Accuracy 0.985.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2544987
Train loss (w/o reg) on all data: 0.24698827
Test loss (w/o reg) on all data: 0.20819016
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.3238393e-05
Norm of the params: 12.255959
Flipped loss: 0.20819. Accuracy: 0.939
### Flips: 312, rs: 0, checks: 52
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19993503
Train loss (w/o reg) on all data: 0.19017817
Test loss (w/o reg) on all data: 0.16475093
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.6365076e-05
Norm of the params: 13.9691515
     Influence (LOO): fixed  35 labels. Loss 0.16475. Accuracy 0.939.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14254858
Train loss (w/o reg) on all data: 0.12977386
Test loss (w/o reg) on all data: 0.18841682
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 5.6518065e-06
Norm of the params: 15.984198
                Loss: fixed  51 labels. Loss 0.18842. Accuracy 0.912.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24885242
Train loss (w/o reg) on all data: 0.24159297
Test loss (w/o reg) on all data: 0.20115633
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.356601e-05
Norm of the params: 12.049439
              Random: fixed   6 labels. Loss 0.20116. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 104
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15088679
Train loss (w/o reg) on all data: 0.13917969
Test loss (w/o reg) on all data: 0.123749144
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4545032e-05
Norm of the params: 15.3017
     Influence (LOO): fixed  63 labels. Loss 0.12375. Accuracy 0.958.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05891238
Train loss (w/o reg) on all data: 0.041278265
Test loss (w/o reg) on all data: 0.12190152
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9478837e-06
Norm of the params: 18.779837
                Loss: fixed  96 labels. Loss 0.12190. Accuracy 0.966.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24898188
Train loss (w/o reg) on all data: 0.24178323
Test loss (w/o reg) on all data: 0.2005517
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.144606e-05
Norm of the params: 11.998874
              Random: fixed   7 labels. Loss 0.20055. Accuracy 0.939.
### Flips: 312, rs: 0, checks: 156
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112249665
Train loss (w/o reg) on all data: 0.10180942
Test loss (w/o reg) on all data: 0.086929105
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.37851075e-05
Norm of the params: 14.450084
     Influence (LOO): fixed  90 labels. Loss 0.08693. Accuracy 0.973.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03327683
Train loss (w/o reg) on all data: 0.019660896
Test loss (w/o reg) on all data: 0.061143287
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.534654e-06
Norm of the params: 16.50208
                Loss: fixed 118 labels. Loss 0.06114. Accuracy 0.977.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24076763
Train loss (w/o reg) on all data: 0.23337966
Test loss (w/o reg) on all data: 0.18835355
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 8.06793e-05
Norm of the params: 12.155623
              Random: fixed  14 labels. Loss 0.18835. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 208
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0748551
Train loss (w/o reg) on all data: 0.06511949
Test loss (w/o reg) on all data: 0.044028316
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0620539e-05
Norm of the params: 13.953929
     Influence (LOO): fixed 115 labels. Loss 0.04403. Accuracy 0.989.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02695021
Train loss (w/o reg) on all data: 0.015535385
Test loss (w/o reg) on all data: 0.046929803
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8056667e-06
Norm of the params: 15.109483
                Loss: fixed 127 labels. Loss 0.04693. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22572523
Train loss (w/o reg) on all data: 0.21773112
Test loss (w/o reg) on all data: 0.18446372
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.536389e-05
Norm of the params: 12.644456
              Random: fixed  23 labels. Loss 0.18446. Accuracy 0.931.
### Flips: 312, rs: 0, checks: 260
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03907577
Train loss (w/o reg) on all data: 0.030791657
Test loss (w/o reg) on all data: 0.016693631
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.616466e-06
Norm of the params: 12.871762
     Influence (LOO): fixed 130 labels. Loss 0.01669. Accuracy 0.996.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018530868
Train loss (w/o reg) on all data: 0.009797507
Test loss (w/o reg) on all data: 0.031890914
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8029612e-06
Norm of the params: 13.216173
                Loss: fixed 132 labels. Loss 0.03189. Accuracy 0.989.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21639015
Train loss (w/o reg) on all data: 0.20829387
Test loss (w/o reg) on all data: 0.16389197
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 6.954088e-05
Norm of the params: 12.724998
              Random: fixed  31 labels. Loss 0.16389. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 312
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012864316
Train loss (w/o reg) on all data: 0.0072642136
Test loss (w/o reg) on all data: 0.010811293
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3151755e-06
Norm of the params: 10.583102
     Influence (LOO): fixed 139 labels. Loss 0.01081. Accuracy 0.996.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016672123
Train loss (w/o reg) on all data: 0.008152772
Test loss (w/o reg) on all data: 0.03552286
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.0757115e-07
Norm of the params: 13.053237
                Loss: fixed 133 labels. Loss 0.03552. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20829014
Train loss (w/o reg) on all data: 0.1997678
Test loss (w/o reg) on all data: 0.13835941
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5109695e-05
Norm of the params: 13.055533
              Random: fixed  39 labels. Loss 0.13836. Accuracy 0.962.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2583847
Train loss (w/o reg) on all data: 0.2518861
Test loss (w/o reg) on all data: 0.1977659
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.266655e-05
Norm of the params: 11.400536
Flipped loss: 0.19777. Accuracy: 0.950
### Flips: 312, rs: 1, checks: 52
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20463747
Train loss (w/o reg) on all data: 0.19409077
Test loss (w/o reg) on all data: 0.16433692
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1564365e-05
Norm of the params: 14.523571
     Influence (LOO): fixed  32 labels. Loss 0.16434. Accuracy 0.966.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14258757
Train loss (w/o reg) on all data: 0.12960352
Test loss (w/o reg) on all data: 0.13095675
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.7040658e-05
Norm of the params: 16.114624
                Loss: fixed  52 labels. Loss 0.13096. Accuracy 0.939.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25638828
Train loss (w/o reg) on all data: 0.25001064
Test loss (w/o reg) on all data: 0.19183868
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.5593125e-05
Norm of the params: 11.293916
              Random: fixed   3 labels. Loss 0.19184. Accuracy 0.954.
### Flips: 312, rs: 1, checks: 104
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1592609
Train loss (w/o reg) on all data: 0.14707501
Test loss (w/o reg) on all data: 0.12850173
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.270005e-05
Norm of the params: 15.611464
     Influence (LOO): fixed  60 labels. Loss 0.12850. Accuracy 0.969.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062824845
Train loss (w/o reg) on all data: 0.047273565
Test loss (w/o reg) on all data: 0.08938385
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0728944e-05
Norm of the params: 17.635918
                Loss: fixed 100 labels. Loss 0.08938. Accuracy 0.962.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25523686
Train loss (w/o reg) on all data: 0.24898724
Test loss (w/o reg) on all data: 0.18685998
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.4871244e-05
Norm of the params: 11.179993
              Random: fixed   7 labels. Loss 0.18686. Accuracy 0.958.
### Flips: 312, rs: 1, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12089513
Train loss (w/o reg) on all data: 0.10929946
Test loss (w/o reg) on all data: 0.093093656
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.374974e-06
Norm of the params: 15.228704
     Influence (LOO): fixed  89 labels. Loss 0.09309. Accuracy 0.981.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029547136
Train loss (w/o reg) on all data: 0.01761293
Test loss (w/o reg) on all data: 0.04853904
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8693934e-06
Norm of the params: 15.449407
                Loss: fixed 122 labels. Loss 0.04854. Accuracy 0.981.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24159384
Train loss (w/o reg) on all data: 0.23538166
Test loss (w/o reg) on all data: 0.16181387
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.214706e-05
Norm of the params: 11.1464615
              Random: fixed  21 labels. Loss 0.16181. Accuracy 0.966.
### Flips: 312, rs: 1, checks: 208
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060567938
Train loss (w/o reg) on all data: 0.05039157
Test loss (w/o reg) on all data: 0.0330457
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.272391e-06
Norm of the params: 14.266302
     Influence (LOO): fixed 118 labels. Loss 0.03305. Accuracy 0.985.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016023101
Train loss (w/o reg) on all data: 0.0076487577
Test loss (w/o reg) on all data: 0.014318378
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1330861e-06
Norm of the params: 12.941672
                Loss: fixed 133 labels. Loss 0.01432. Accuracy 0.996.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23120442
Train loss (w/o reg) on all data: 0.22516581
Test loss (w/o reg) on all data: 0.14908826
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5418433e-05
Norm of the params: 10.989635
              Random: fixed  29 labels. Loss 0.14909. Accuracy 0.969.
### Flips: 312, rs: 1, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041226257
Train loss (w/o reg) on all data: 0.031568337
Test loss (w/o reg) on all data: 0.022357771
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.6686475e-06
Norm of the params: 13.898143
     Influence (LOO): fixed 127 labels. Loss 0.02236. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0160231
Train loss (w/o reg) on all data: 0.0076482897
Test loss (w/o reg) on all data: 0.014317656
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1147694e-06
Norm of the params: 12.942034
                Loss: fixed 133 labels. Loss 0.01432. Accuracy 0.996.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22397652
Train loss (w/o reg) on all data: 0.21732718
Test loss (w/o reg) on all data: 0.14419454
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.613691e-05
Norm of the params: 11.531989
              Random: fixed  37 labels. Loss 0.14419. Accuracy 0.989.
### Flips: 312, rs: 1, checks: 312
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024597365
Train loss (w/o reg) on all data: 0.017002452
Test loss (w/o reg) on all data: 0.013702635
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.0629015e-07
Norm of the params: 12.324701
     Influence (LOO): fixed 134 labels. Loss 0.01370. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012558039
Train loss (w/o reg) on all data: 0.0053638094
Test loss (w/o reg) on all data: 0.010840198
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6801992e-07
Norm of the params: 11.995192
                Loss: fixed 136 labels. Loss 0.01084. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21653369
Train loss (w/o reg) on all data: 0.20985557
Test loss (w/o reg) on all data: 0.13225222
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.36579065e-05
Norm of the params: 11.556924
              Random: fixed  43 labels. Loss 0.13225. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24918741
Train loss (w/o reg) on all data: 0.24195807
Test loss (w/o reg) on all data: 0.16883358
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.1790975e-05
Norm of the params: 12.024425
Flipped loss: 0.16883. Accuracy: 0.958
### Flips: 312, rs: 2, checks: 52
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19134685
Train loss (w/o reg) on all data: 0.1826231
Test loss (w/o reg) on all data: 0.119577006
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7050515e-05
Norm of the params: 13.2088995
     Influence (LOO): fixed  39 labels. Loss 0.11958. Accuracy 0.962.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13293321
Train loss (w/o reg) on all data: 0.118350685
Test loss (w/o reg) on all data: 0.117493905
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.59883e-05
Norm of the params: 17.07778
                Loss: fixed  51 labels. Loss 0.11749. Accuracy 0.947.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24506995
Train loss (w/o reg) on all data: 0.23775461
Test loss (w/o reg) on all data: 0.162742
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.312851e-05
Norm of the params: 12.095734
              Random: fixed   5 labels. Loss 0.16274. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 104
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13787019
Train loss (w/o reg) on all data: 0.12850729
Test loss (w/o reg) on all data: 0.08132682
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.203867e-05
Norm of the params: 13.684235
     Influence (LOO): fixed  72 labels. Loss 0.08133. Accuracy 0.985.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045343235
Train loss (w/o reg) on all data: 0.028238382
Test loss (w/o reg) on all data: 0.06933797
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5886782e-05
Norm of the params: 18.495865
                Loss: fixed  96 labels. Loss 0.06934. Accuracy 0.969.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23764509
Train loss (w/o reg) on all data: 0.22984648
Test loss (w/o reg) on all data: 0.15998866
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.813429e-05
Norm of the params: 12.488884
              Random: fixed  12 labels. Loss 0.15999. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 156
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086922884
Train loss (w/o reg) on all data: 0.07705053
Test loss (w/o reg) on all data: 0.055222362
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.052559e-06
Norm of the params: 14.051589
     Influence (LOO): fixed  97 labels. Loss 0.05522. Accuracy 0.985.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02892083
Train loss (w/o reg) on all data: 0.016392054
Test loss (w/o reg) on all data: 0.028794538
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9320735e-06
Norm of the params: 15.829577
                Loss: fixed 114 labels. Loss 0.02879. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2291297
Train loss (w/o reg) on all data: 0.22099264
Test loss (w/o reg) on all data: 0.15794697
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0155238e-05
Norm of the params: 12.757009
              Random: fixed  18 labels. Loss 0.15795. Accuracy 0.954.
### Flips: 312, rs: 2, checks: 208
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04174959
Train loss (w/o reg) on all data: 0.032956135
Test loss (w/o reg) on all data: 0.03247249
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0246135e-06
Norm of the params: 13.261564
     Influence (LOO): fixed 117 labels. Loss 0.03247. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017870136
Train loss (w/o reg) on all data: 0.008549347
Test loss (w/o reg) on all data: 0.014080457
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.489138e-06
Norm of the params: 13.653418
                Loss: fixed 123 labels. Loss 0.01408. Accuracy 0.996.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21863961
Train loss (w/o reg) on all data: 0.21033587
Test loss (w/o reg) on all data: 0.14360717
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.138292e-05
Norm of the params: 12.88701
              Random: fixed  27 labels. Loss 0.14361. Accuracy 0.973.
### Flips: 312, rs: 2, checks: 260
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025033034
Train loss (w/o reg) on all data: 0.017852584
Test loss (w/o reg) on all data: 0.021538366
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.315351e-07
Norm of the params: 11.983697
     Influence (LOO): fixed 128 labels. Loss 0.02154. Accuracy 0.989.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016201837
Train loss (w/o reg) on all data: 0.0075374427
Test loss (w/o reg) on all data: 0.021508168
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0857693e-07
Norm of the params: 13.163885
                Loss: fixed 125 labels. Loss 0.02151. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20934902
Train loss (w/o reg) on all data: 0.20140778
Test loss (w/o reg) on all data: 0.13861173
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.283492e-05
Norm of the params: 12.602577
              Random: fixed  36 labels. Loss 0.13861. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 312
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021325603
Train loss (w/o reg) on all data: 0.014980602
Test loss (w/o reg) on all data: 0.020502724
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6162336e-06
Norm of the params: 11.264992
     Influence (LOO): fixed 130 labels. Loss 0.02050. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01219276
Train loss (w/o reg) on all data: 0.005112449
Test loss (w/o reg) on all data: 0.016321434
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7569813e-06
Norm of the params: 11.899841
                Loss: fixed 129 labels. Loss 0.01632. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20305623
Train loss (w/o reg) on all data: 0.1948317
Test loss (w/o reg) on all data: 0.13342385
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3508613e-05
Norm of the params: 12.82539
              Random: fixed  41 labels. Loss 0.13342. Accuracy 0.977.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26269293
Train loss (w/o reg) on all data: 0.25543407
Test loss (w/o reg) on all data: 0.2102686
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.2321634e-05
Norm of the params: 12.048952
Flipped loss: 0.21027. Accuracy: 0.920
### Flips: 312, rs: 3, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21712059
Train loss (w/o reg) on all data: 0.20690843
Test loss (w/o reg) on all data: 0.18677603
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.688724e-05
Norm of the params: 14.291364
     Influence (LOO): fixed  32 labels. Loss 0.18678. Accuracy 0.927.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161072
Train loss (w/o reg) on all data: 0.14834282
Test loss (w/o reg) on all data: 0.18779281
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.5867841e-05
Norm of the params: 15.955678
                Loss: fixed  52 labels. Loss 0.18779. Accuracy 0.916.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2546712
Train loss (w/o reg) on all data: 0.24777213
Test loss (w/o reg) on all data: 0.19519238
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.726656e-05
Norm of the params: 11.746549
              Random: fixed   9 labels. Loss 0.19519. Accuracy 0.920.
### Flips: 312, rs: 3, checks: 104
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1651108
Train loss (w/o reg) on all data: 0.15387644
Test loss (w/o reg) on all data: 0.14655977
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.58508e-06
Norm of the params: 14.989565
     Influence (LOO): fixed  63 labels. Loss 0.14656. Accuracy 0.950.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077638954
Train loss (w/o reg) on all data: 0.059642956
Test loss (w/o reg) on all data: 0.17763086
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0685604e-05
Norm of the params: 18.971556
                Loss: fixed  95 labels. Loss 0.17763. Accuracy 0.943.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25125423
Train loss (w/o reg) on all data: 0.24423413
Test loss (w/o reg) on all data: 0.19162774
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.522139e-05
Norm of the params: 11.849132
              Random: fixed  14 labels. Loss 0.19163. Accuracy 0.920.
### Flips: 312, rs: 3, checks: 156
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13079046
Train loss (w/o reg) on all data: 0.11891327
Test loss (w/o reg) on all data: 0.12750517
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 7.834938e-06
Norm of the params: 15.412455
     Influence (LOO): fixed  83 labels. Loss 0.12751. Accuracy 0.954.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042766362
Train loss (w/o reg) on all data: 0.027664227
Test loss (w/o reg) on all data: 0.08759006
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.0270216e-06
Norm of the params: 17.379375
                Loss: fixed 121 labels. Loss 0.08759. Accuracy 0.958.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24707748
Train loss (w/o reg) on all data: 0.24004385
Test loss (w/o reg) on all data: 0.19259644
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 2.0061576e-05
Norm of the params: 11.860547
              Random: fixed  20 labels. Loss 0.19260. Accuracy 0.912.
### Flips: 312, rs: 3, checks: 208
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10162613
Train loss (w/o reg) on all data: 0.09077036
Test loss (w/o reg) on all data: 0.103774875
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1835415e-05
Norm of the params: 14.734836
     Influence (LOO): fixed 106 labels. Loss 0.10377. Accuracy 0.950.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028200455
Train loss (w/o reg) on all data: 0.01673977
Test loss (w/o reg) on all data: 0.03353471
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.534164e-06
Norm of the params: 15.139806
                Loss: fixed 136 labels. Loss 0.03353. Accuracy 0.985.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24215941
Train loss (w/o reg) on all data: 0.23534967
Test loss (w/o reg) on all data: 0.1748559
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.4808577e-05
Norm of the params: 11.670249
              Random: fixed  27 labels. Loss 0.17486. Accuracy 0.924.
### Flips: 312, rs: 3, checks: 260
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07047552
Train loss (w/o reg) on all data: 0.061134323
Test loss (w/o reg) on all data: 0.07403037
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.673584e-06
Norm of the params: 13.668354
     Influence (LOO): fixed 123 labels. Loss 0.07403. Accuracy 0.969.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018621787
Train loss (w/o reg) on all data: 0.009311679
Test loss (w/o reg) on all data: 0.029365595
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0814471e-06
Norm of the params: 13.645592
                Loss: fixed 143 labels. Loss 0.02937. Accuracy 0.985.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2299329
Train loss (w/o reg) on all data: 0.22289181
Test loss (w/o reg) on all data: 0.15516466
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 7.0024257e-06
Norm of the params: 11.8668375
              Random: fixed  38 labels. Loss 0.15516. Accuracy 0.947.
### Flips: 312, rs: 3, checks: 312
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03716804
Train loss (w/o reg) on all data: 0.028112326
Test loss (w/o reg) on all data: 0.04009468
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3792472e-06
Norm of the params: 13.457873
     Influence (LOO): fixed 140 labels. Loss 0.04009. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016287183
Train loss (w/o reg) on all data: 0.0077980384
Test loss (w/o reg) on all data: 0.027872523
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.140075e-07
Norm of the params: 13.030077
                Loss: fixed 147 labels. Loss 0.02787. Accuracy 0.989.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21895732
Train loss (w/o reg) on all data: 0.2115191
Test loss (w/o reg) on all data: 0.15315835
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4143146e-05
Norm of the params: 12.196894
              Random: fixed  46 labels. Loss 0.15316. Accuracy 0.943.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27603647
Train loss (w/o reg) on all data: 0.26968768
Test loss (w/o reg) on all data: 0.22834675
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.0746234e-05
Norm of the params: 11.268348
Flipped loss: 0.22835. Accuracy: 0.924
### Flips: 312, rs: 4, checks: 52
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2274943
Train loss (w/o reg) on all data: 0.21757151
Test loss (w/o reg) on all data: 0.18870871
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.081727e-05
Norm of the params: 14.087435
     Influence (LOO): fixed  29 labels. Loss 0.18871. Accuracy 0.939.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18129419
Train loss (w/o reg) on all data: 0.16927737
Test loss (w/o reg) on all data: 0.17460944
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9752446e-05
Norm of the params: 15.502784
                Loss: fixed  50 labels. Loss 0.17461. Accuracy 0.947.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2641262
Train loss (w/o reg) on all data: 0.25726572
Test loss (w/o reg) on all data: 0.20757432
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 8.351244e-05
Norm of the params: 11.713668
              Random: fixed  12 labels. Loss 0.20757. Accuracy 0.939.
### Flips: 312, rs: 4, checks: 104
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260573
Train loss (w/o reg) on all data: 0.18213007
Test loss (w/o reg) on all data: 0.14103882
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.4717664e-05
Norm of the params: 14.474578
     Influence (LOO): fixed  55 labels. Loss 0.14104. Accuracy 0.954.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0969456
Train loss (w/o reg) on all data: 0.080531985
Test loss (w/o reg) on all data: 0.1317328
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.352707e-06
Norm of the params: 18.118284
                Loss: fixed  98 labels. Loss 0.13173. Accuracy 0.947.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25595686
Train loss (w/o reg) on all data: 0.24897438
Test loss (w/o reg) on all data: 0.19997154
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 0.00015822047
Norm of the params: 11.8173275
              Random: fixed  22 labels. Loss 0.19997. Accuracy 0.954.
### Flips: 312, rs: 4, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14141595
Train loss (w/o reg) on all data: 0.12976746
Test loss (w/o reg) on all data: 0.10781814
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7211697e-06
Norm of the params: 15.263347
     Influence (LOO): fixed  87 labels. Loss 0.10782. Accuracy 0.962.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05154613
Train loss (w/o reg) on all data: 0.035338886
Test loss (w/o reg) on all data: 0.09312161
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0131285e-05
Norm of the params: 18.004025
                Loss: fixed 127 labels. Loss 0.09312. Accuracy 0.966.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25251892
Train loss (w/o reg) on all data: 0.24546541
Test loss (w/o reg) on all data: 0.18858513
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.7653834e-05
Norm of the params: 11.8773
              Random: fixed  27 labels. Loss 0.18859. Accuracy 0.958.
### Flips: 312, rs: 4, checks: 208
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09701476
Train loss (w/o reg) on all data: 0.083583966
Test loss (w/o reg) on all data: 0.0936224
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5144017e-05
Norm of the params: 16.389507
     Influence (LOO): fixed 110 labels. Loss 0.09362. Accuracy 0.981.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037003834
Train loss (w/o reg) on all data: 0.02361812
Test loss (w/o reg) on all data: 0.07158299
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0263147e-06
Norm of the params: 16.361977
                Loss: fixed 137 labels. Loss 0.07158. Accuracy 0.973.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23592642
Train loss (w/o reg) on all data: 0.2277689
Test loss (w/o reg) on all data: 0.181511
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 8.833119e-05
Norm of the params: 12.773039
              Random: fixed  35 labels. Loss 0.18151. Accuracy 0.939.
### Flips: 312, rs: 4, checks: 260
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065039404
Train loss (w/o reg) on all data: 0.055073258
Test loss (w/o reg) on all data: 0.050691098
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.106567e-06
Norm of the params: 14.118178
     Influence (LOO): fixed 128 labels. Loss 0.05069. Accuracy 0.985.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029511943
Train loss (w/o reg) on all data: 0.01816223
Test loss (w/o reg) on all data: 0.05814022
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.016761e-06
Norm of the params: 15.066329
                Loss: fixed 142 labels. Loss 0.05814. Accuracy 0.985.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23478986
Train loss (w/o reg) on all data: 0.22681618
Test loss (w/o reg) on all data: 0.17471673
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.8728103e-05
Norm of the params: 12.628291
              Random: fixed  38 labels. Loss 0.17472. Accuracy 0.950.
### Flips: 312, rs: 4, checks: 312
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04250917
Train loss (w/o reg) on all data: 0.03252495
Test loss (w/o reg) on all data: 0.034516275
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2225227e-06
Norm of the params: 14.13097
     Influence (LOO): fixed 141 labels. Loss 0.03452. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020558914
Train loss (w/o reg) on all data: 0.01084545
Test loss (w/o reg) on all data: 0.035405185
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4739932e-06
Norm of the params: 13.938052
                Loss: fixed 148 labels. Loss 0.03541. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2248382
Train loss (w/o reg) on all data: 0.21686345
Test loss (w/o reg) on all data: 0.16445002
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.113208e-05
Norm of the params: 12.629128
              Random: fixed  46 labels. Loss 0.16445. Accuracy 0.954.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26185194
Train loss (w/o reg) on all data: 0.25417814
Test loss (w/o reg) on all data: 0.21980867
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.2833891e-05
Norm of the params: 12.388534
Flipped loss: 0.21981. Accuracy: 0.920
### Flips: 312, rs: 5, checks: 52
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20969374
Train loss (w/o reg) on all data: 0.19972843
Test loss (w/o reg) on all data: 0.17702456
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.84089e-06
Norm of the params: 14.117584
     Influence (LOO): fixed  36 labels. Loss 0.17702. Accuracy 0.939.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15278448
Train loss (w/o reg) on all data: 0.13836977
Test loss (w/o reg) on all data: 0.21224518
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.9215032e-05
Norm of the params: 16.979233
                Loss: fixed  52 labels. Loss 0.21225. Accuracy 0.908.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2586819
Train loss (w/o reg) on all data: 0.25102687
Test loss (w/o reg) on all data: 0.21586451
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.980557e-05
Norm of the params: 12.373378
              Random: fixed   4 labels. Loss 0.21586. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 104
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16575086
Train loss (w/o reg) on all data: 0.15327077
Test loss (w/o reg) on all data: 0.15931827
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.247941e-05
Norm of the params: 15.79879
     Influence (LOO): fixed  62 labels. Loss 0.15932. Accuracy 0.935.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084249444
Train loss (w/o reg) on all data: 0.06533836
Test loss (w/o reg) on all data: 0.14632016
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.3408075e-06
Norm of the params: 19.447924
                Loss: fixed  90 labels. Loss 0.14632. Accuracy 0.943.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25481376
Train loss (w/o reg) on all data: 0.24704367
Test loss (w/o reg) on all data: 0.2150758
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.5740108e-05
Norm of the params: 12.466016
              Random: fixed   8 labels. Loss 0.21508. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 156
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10942132
Train loss (w/o reg) on all data: 0.09551139
Test loss (w/o reg) on all data: 0.12296493
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2881917e-05
Norm of the params: 16.679285
     Influence (LOO): fixed  94 labels. Loss 0.12296. Accuracy 0.962.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046055242
Train loss (w/o reg) on all data: 0.029764242
Test loss (w/o reg) on all data: 0.07313728
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0368557e-06
Norm of the params: 18.050486
                Loss: fixed 119 labels. Loss 0.07314. Accuracy 0.973.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25049323
Train loss (w/o reg) on all data: 0.24290438
Test loss (w/o reg) on all data: 0.20366503
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.5300118e-05
Norm of the params: 12.319778
              Random: fixed  13 labels. Loss 0.20367. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 208
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08203895
Train loss (w/o reg) on all data: 0.07043852
Test loss (w/o reg) on all data: 0.08064499
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9527159e-05
Norm of the params: 15.231826
     Influence (LOO): fixed 114 labels. Loss 0.08064. Accuracy 0.966.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02797432
Train loss (w/o reg) on all data: 0.015445387
Test loss (w/o reg) on all data: 0.031890173
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.022261e-06
Norm of the params: 15.829676
                Loss: fixed 132 labels. Loss 0.03189. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24383363
Train loss (w/o reg) on all data: 0.23594902
Test loss (w/o reg) on all data: 0.19297528
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.859447e-05
Norm of the params: 12.557555
              Random: fixed  21 labels. Loss 0.19298. Accuracy 0.939.
### Flips: 312, rs: 5, checks: 260
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053523257
Train loss (w/o reg) on all data: 0.04314513
Test loss (w/o reg) on all data: 0.05270972
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.2035003e-07
Norm of the params: 14.407029
     Influence (LOO): fixed 129 labels. Loss 0.05271. Accuracy 0.981.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02174235
Train loss (w/o reg) on all data: 0.010489628
Test loss (w/o reg) on all data: 0.025753511
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2353298e-06
Norm of the params: 15.001814
                Loss: fixed 137 labels. Loss 0.02575. Accuracy 0.989.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23396985
Train loss (w/o reg) on all data: 0.2259586
Test loss (w/o reg) on all data: 0.18651196
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.788429e-05
Norm of the params: 12.657997
              Random: fixed  29 labels. Loss 0.18651. Accuracy 0.943.
### Flips: 312, rs: 5, checks: 312
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034540005
Train loss (w/o reg) on all data: 0.025297932
Test loss (w/o reg) on all data: 0.023756223
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.786106e-06
Norm of the params: 13.595642
     Influence (LOO): fixed 139 labels. Loss 0.02376. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017377982
Train loss (w/o reg) on all data: 0.0080880085
Test loss (w/o reg) on all data: 0.02093416
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1812496e-06
Norm of the params: 13.630829
                Loss: fixed 141 labels. Loss 0.02093. Accuracy 0.992.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22486305
Train loss (w/o reg) on all data: 0.21637285
Test loss (w/o reg) on all data: 0.1700487
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.4595228e-05
Norm of the params: 13.030892
              Random: fixed  37 labels. Loss 0.17005. Accuracy 0.943.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25838166
Train loss (w/o reg) on all data: 0.25158027
Test loss (w/o reg) on all data: 0.19266118
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.120683e-05
Norm of the params: 11.663102
Flipped loss: 0.19266. Accuracy: 0.958
### Flips: 312, rs: 6, checks: 52
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2070826
Train loss (w/o reg) on all data: 0.19837435
Test loss (w/o reg) on all data: 0.16446729
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6553908e-05
Norm of the params: 13.197167
     Influence (LOO): fixed  32 labels. Loss 0.16447. Accuracy 0.958.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14550693
Train loss (w/o reg) on all data: 0.13146059
Test loss (w/o reg) on all data: 0.1667653
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.796117e-05
Norm of the params: 16.760876
                Loss: fixed  49 labels. Loss 0.16677. Accuracy 0.924.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24804205
Train loss (w/o reg) on all data: 0.24134684
Test loss (w/o reg) on all data: 0.18441741
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.0094167e-05
Norm of the params: 11.571703
              Random: fixed   9 labels. Loss 0.18442. Accuracy 0.943.
### Flips: 312, rs: 6, checks: 104
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15416084
Train loss (w/o reg) on all data: 0.14285806
Test loss (w/o reg) on all data: 0.14092399
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9313935e-05
Norm of the params: 15.035151
     Influence (LOO): fixed  65 labels. Loss 0.14092. Accuracy 0.947.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0762731
Train loss (w/o reg) on all data: 0.056936033
Test loss (w/o reg) on all data: 0.098296344
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.7418483e-06
Norm of the params: 19.665743
                Loss: fixed  91 labels. Loss 0.09830. Accuracy 0.969.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24168047
Train loss (w/o reg) on all data: 0.234952
Test loss (w/o reg) on all data: 0.16219151
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2645587e-05
Norm of the params: 11.600408
              Random: fixed  17 labels. Loss 0.16219. Accuracy 0.962.
### Flips: 312, rs: 6, checks: 156
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10784522
Train loss (w/o reg) on all data: 0.09666583
Test loss (w/o reg) on all data: 0.08604742
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1098415e-05
Norm of the params: 14.952852
     Influence (LOO): fixed 100 labels. Loss 0.08605. Accuracy 0.981.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03724722
Train loss (w/o reg) on all data: 0.022228295
Test loss (w/o reg) on all data: 0.037774466
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9765707e-06
Norm of the params: 17.331429
                Loss: fixed 118 labels. Loss 0.03777. Accuracy 0.985.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23496121
Train loss (w/o reg) on all data: 0.2282001
Test loss (w/o reg) on all data: 0.15447682
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6572741e-05
Norm of the params: 11.628519
              Random: fixed  24 labels. Loss 0.15448. Accuracy 0.962.
### Flips: 312, rs: 6, checks: 208
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0616915
Train loss (w/o reg) on all data: 0.051630575
Test loss (w/o reg) on all data: 0.04462364
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0171993e-05
Norm of the params: 14.185152
     Influence (LOO): fixed 124 labels. Loss 0.04462. Accuracy 0.989.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027496768
Train loss (w/o reg) on all data: 0.014742172
Test loss (w/o reg) on all data: 0.029464502
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.870635e-06
Norm of the params: 15.971597
                Loss: fixed 128 labels. Loss 0.02946. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22163266
Train loss (w/o reg) on all data: 0.21508117
Test loss (w/o reg) on all data: 0.14363475
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.6404745e-05
Norm of the params: 11.446821
              Random: fixed  35 labels. Loss 0.14363. Accuracy 0.954.
### Flips: 312, rs: 6, checks: 260
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032733455
Train loss (w/o reg) on all data: 0.02296895
Test loss (w/o reg) on all data: 0.027132817
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.814431e-06
Norm of the params: 13.974623
     Influence (LOO): fixed 136 labels. Loss 0.02713. Accuracy 0.989.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020276912
Train loss (w/o reg) on all data: 0.010476931
Test loss (w/o reg) on all data: 0.023388267
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1483166e-06
Norm of the params: 13.999987
                Loss: fixed 137 labels. Loss 0.02339. Accuracy 0.985.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21228266
Train loss (w/o reg) on all data: 0.2055172
Test loss (w/o reg) on all data: 0.13250604
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4785039e-05
Norm of the params: 11.632245
              Random: fixed  42 labels. Loss 0.13251. Accuracy 0.969.
### Flips: 312, rs: 6, checks: 312
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015054225
Train loss (w/o reg) on all data: 0.0086665265
Test loss (w/o reg) on all data: 0.011675679
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.807028e-07
Norm of the params: 11.302831
     Influence (LOO): fixed 144 labels. Loss 0.01168. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015450183
Train loss (w/o reg) on all data: 0.007312069
Test loss (w/o reg) on all data: 0.020927588
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.5161973e-07
Norm of the params: 12.757832
                Loss: fixed 141 labels. Loss 0.02093. Accuracy 0.989.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20506157
Train loss (w/o reg) on all data: 0.1979849
Test loss (w/o reg) on all data: 0.120400034
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2716394e-05
Norm of the params: 11.896775
              Random: fixed  49 labels. Loss 0.12040. Accuracy 0.992.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26930723
Train loss (w/o reg) on all data: 0.26240024
Test loss (w/o reg) on all data: 0.23159707
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.2392969e-05
Norm of the params: 11.7532835
Flipped loss: 0.23160. Accuracy: 0.924
### Flips: 312, rs: 7, checks: 52
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22043175
Train loss (w/o reg) on all data: 0.21032965
Test loss (w/o reg) on all data: 0.19877018
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.491863e-05
Norm of the params: 14.2141485
     Influence (LOO): fixed  33 labels. Loss 0.19877. Accuracy 0.935.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1655746
Train loss (w/o reg) on all data: 0.1530408
Test loss (w/o reg) on all data: 0.18307848
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.333128e-05
Norm of the params: 15.8327465
                Loss: fixed  52 labels. Loss 0.18308. Accuracy 0.920.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26676014
Train loss (w/o reg) on all data: 0.2598986
Test loss (w/o reg) on all data: 0.21737146
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.5817253e-05
Norm of the params: 11.714542
              Random: fixed   9 labels. Loss 0.21737. Accuracy 0.939.
### Flips: 312, rs: 7, checks: 104
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17529207
Train loss (w/o reg) on all data: 0.16434698
Test loss (w/o reg) on all data: 0.16324927
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.2107016e-05
Norm of the params: 14.795332
     Influence (LOO): fixed  65 labels. Loss 0.16325. Accuracy 0.943.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09077994
Train loss (w/o reg) on all data: 0.07419969
Test loss (w/o reg) on all data: 0.11892278
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.613787e-06
Norm of the params: 18.210024
                Loss: fixed  92 labels. Loss 0.11892. Accuracy 0.943.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25994483
Train loss (w/o reg) on all data: 0.25287297
Test loss (w/o reg) on all data: 0.19580905
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9583656e-05
Norm of the params: 11.892722
              Random: fixed  18 labels. Loss 0.19581. Accuracy 0.947.
### Flips: 312, rs: 7, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13508259
Train loss (w/o reg) on all data: 0.12348008
Test loss (w/o reg) on all data: 0.13077249
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.8127916e-06
Norm of the params: 15.233188
     Influence (LOO): fixed  91 labels. Loss 0.13077. Accuracy 0.962.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05439233
Train loss (w/o reg) on all data: 0.03677868
Test loss (w/o reg) on all data: 0.08577475
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.2377486e-06
Norm of the params: 18.768936
                Loss: fixed 120 labels. Loss 0.08577. Accuracy 0.966.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25447482
Train loss (w/o reg) on all data: 0.24682559
Test loss (w/o reg) on all data: 0.196967
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.675547e-05
Norm of the params: 12.368705
              Random: fixed  23 labels. Loss 0.19697. Accuracy 0.950.
### Flips: 312, rs: 7, checks: 208
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10718803
Train loss (w/o reg) on all data: 0.09669337
Test loss (w/o reg) on all data: 0.09806724
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8006893e-05
Norm of the params: 14.487694
     Influence (LOO): fixed 110 labels. Loss 0.09807. Accuracy 0.962.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03664801
Train loss (w/o reg) on all data: 0.02153435
Test loss (w/o reg) on all data: 0.0474553
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.783999e-06
Norm of the params: 17.386005
                Loss: fixed 134 labels. Loss 0.04746. Accuracy 0.985.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24481647
Train loss (w/o reg) on all data: 0.2372606
Test loss (w/o reg) on all data: 0.17722033
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.97967e-05
Norm of the params: 12.292982
              Random: fixed  33 labels. Loss 0.17722. Accuracy 0.962.
### Flips: 312, rs: 7, checks: 260
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07198415
Train loss (w/o reg) on all data: 0.061937667
Test loss (w/o reg) on all data: 0.05287793
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9559636e-05
Norm of the params: 14.174964
     Influence (LOO): fixed 131 labels. Loss 0.05288. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025281345
Train loss (w/o reg) on all data: 0.013384106
Test loss (w/o reg) on all data: 0.029058533
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.153088e-07
Norm of the params: 15.425459
                Loss: fixed 142 labels. Loss 0.02906. Accuracy 0.989.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22490473
Train loss (w/o reg) on all data: 0.21628666
Test loss (w/o reg) on all data: 0.1591208
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1435128e-05
Norm of the params: 13.128647
              Random: fixed  46 labels. Loss 0.15912. Accuracy 0.973.
### Flips: 312, rs: 7, checks: 312
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042063493
Train loss (w/o reg) on all data: 0.033027463
Test loss (w/o reg) on all data: 0.029972756
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4593899e-05
Norm of the params: 13.443236
     Influence (LOO): fixed 145 labels. Loss 0.02997. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020298457
Train loss (w/o reg) on all data: 0.009905775
Test loss (w/o reg) on all data: 0.02466114
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0704985e-06
Norm of the params: 14.41713
                Loss: fixed 146 labels. Loss 0.02466. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21990848
Train loss (w/o reg) on all data: 0.21149248
Test loss (w/o reg) on all data: 0.14537816
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.852135e-05
Norm of the params: 12.973814
              Random: fixed  51 labels. Loss 0.14538. Accuracy 0.973.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25021106
Train loss (w/o reg) on all data: 0.24314341
Test loss (w/o reg) on all data: 0.17939574
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.2246087e-05
Norm of the params: 11.889199
Flipped loss: 0.17940. Accuracy: 0.935
### Flips: 312, rs: 8, checks: 52
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20506427
Train loss (w/o reg) on all data: 0.19487119
Test loss (w/o reg) on all data: 0.14848413
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0986842e-05
Norm of the params: 14.278009
     Influence (LOO): fixed  32 labels. Loss 0.14848. Accuracy 0.943.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1411029
Train loss (w/o reg) on all data: 0.12655832
Test loss (w/o reg) on all data: 0.1308941
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.2362381e-05
Norm of the params: 17.055546
                Loss: fixed  52 labels. Loss 0.13089. Accuracy 0.931.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2489924
Train loss (w/o reg) on all data: 0.24191554
Test loss (w/o reg) on all data: 0.16991957
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3735718e-05
Norm of the params: 11.896945
              Random: fixed   3 labels. Loss 0.16992. Accuracy 0.950.
### Flips: 312, rs: 8, checks: 104
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14722694
Train loss (w/o reg) on all data: 0.13603006
Test loss (w/o reg) on all data: 0.106555074
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.424408e-05
Norm of the params: 14.964548
     Influence (LOO): fixed  66 labels. Loss 0.10656. Accuracy 0.966.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06364354
Train loss (w/o reg) on all data: 0.04622959
Test loss (w/o reg) on all data: 0.06999716
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.1459475e-06
Norm of the params: 18.662235
                Loss: fixed  98 labels. Loss 0.07000. Accuracy 0.969.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23610166
Train loss (w/o reg) on all data: 0.22857688
Test loss (w/o reg) on all data: 0.15038693
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.7749358e-05
Norm of the params: 12.267657
              Random: fixed  14 labels. Loss 0.15039. Accuracy 0.973.
### Flips: 312, rs: 8, checks: 156
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09557426
Train loss (w/o reg) on all data: 0.08309025
Test loss (w/o reg) on all data: 0.079324484
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8382547e-05
Norm of the params: 15.801272
     Influence (LOO): fixed  94 labels. Loss 0.07932. Accuracy 0.962.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037232663
Train loss (w/o reg) on all data: 0.022767814
Test loss (w/o reg) on all data: 0.03922196
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8182234e-06
Norm of the params: 17.008734
                Loss: fixed 116 labels. Loss 0.03922. Accuracy 0.989.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22382884
Train loss (w/o reg) on all data: 0.21641526
Test loss (w/o reg) on all data: 0.12954211
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4462053e-05
Norm of the params: 12.176689
              Random: fixed  26 labels. Loss 0.12954. Accuracy 0.985.
### Flips: 312, rs: 8, checks: 208
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06633183
Train loss (w/o reg) on all data: 0.055161893
Test loss (w/o reg) on all data: 0.049760193
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.9989283e-06
Norm of the params: 14.946529
     Influence (LOO): fixed 112 labels. Loss 0.04976. Accuracy 0.981.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024329849
Train loss (w/o reg) on all data: 0.013113199
Test loss (w/o reg) on all data: 0.029547539
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0800178e-06
Norm of the params: 14.97775
                Loss: fixed 125 labels. Loss 0.02955. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21526767
Train loss (w/o reg) on all data: 0.2071323
Test loss (w/o reg) on all data: 0.12336978
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.3420485e-05
Norm of the params: 12.755683
              Random: fixed  32 labels. Loss 0.12337. Accuracy 0.977.
### Flips: 312, rs: 8, checks: 260
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04029439
Train loss (w/o reg) on all data: 0.031439167
Test loss (w/o reg) on all data: 0.031300586
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5533922e-06
Norm of the params: 13.308062
     Influence (LOO): fixed 126 labels. Loss 0.03130. Accuracy 0.996.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018171906
Train loss (w/o reg) on all data: 0.008793147
Test loss (w/o reg) on all data: 0.019312913
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8119945e-06
Norm of the params: 13.69581
                Loss: fixed 129 labels. Loss 0.01931. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20549148
Train loss (w/o reg) on all data: 0.19754037
Test loss (w/o reg) on all data: 0.115594536
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1940832e-05
Norm of the params: 12.610407
              Random: fixed  41 labels. Loss 0.11559. Accuracy 0.989.
### Flips: 312, rs: 8, checks: 312
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026465131
Train loss (w/o reg) on all data: 0.019315016
Test loss (w/o reg) on all data: 0.016318057
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2036234e-06
Norm of the params: 11.958359
     Influence (LOO): fixed 132 labels. Loss 0.01632. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012964828
Train loss (w/o reg) on all data: 0.0054835314
Test loss (w/o reg) on all data: 0.015670983
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3676581e-06
Norm of the params: 12.232168
                Loss: fixed 132 labels. Loss 0.01567. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19838844
Train loss (w/o reg) on all data: 0.19088314
Test loss (w/o reg) on all data: 0.1112562
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4462328e-05
Norm of the params: 12.251769
              Random: fixed  48 labels. Loss 0.11126. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2612924
Train loss (w/o reg) on all data: 0.25428703
Test loss (w/o reg) on all data: 0.2129267
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.46839975e-05
Norm of the params: 11.836686
Flipped loss: 0.21293. Accuracy: 0.931
### Flips: 312, rs: 9, checks: 52
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20593973
Train loss (w/o reg) on all data: 0.19667888
Test loss (w/o reg) on all data: 0.17921934
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.7647615e-05
Norm of the params: 13.609447
     Influence (LOO): fixed  34 labels. Loss 0.17922. Accuracy 0.912.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1593766
Train loss (w/o reg) on all data: 0.14740297
Test loss (w/o reg) on all data: 0.18672523
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.284185e-05
Norm of the params: 15.474908
                Loss: fixed  50 labels. Loss 0.18673. Accuracy 0.908.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25858924
Train loss (w/o reg) on all data: 0.25160125
Test loss (w/o reg) on all data: 0.20546596
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.4604426e-05
Norm of the params: 11.822003
              Random: fixed   3 labels. Loss 0.20547. Accuracy 0.935.
### Flips: 312, rs: 9, checks: 104
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16719641
Train loss (w/o reg) on all data: 0.1574596
Test loss (w/o reg) on all data: 0.14947703
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 7.912583e-06
Norm of the params: 13.954785
     Influence (LOO): fixed  58 labels. Loss 0.14948. Accuracy 0.935.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07962276
Train loss (w/o reg) on all data: 0.06409188
Test loss (w/o reg) on all data: 0.08202924
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.220827e-05
Norm of the params: 17.624346
                Loss: fixed  94 labels. Loss 0.08203. Accuracy 0.973.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25436738
Train loss (w/o reg) on all data: 0.24686226
Test loss (w/o reg) on all data: 0.19543423
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.9897073e-05
Norm of the params: 12.251623
              Random: fixed  10 labels. Loss 0.19543. Accuracy 0.962.
### Flips: 312, rs: 9, checks: 156
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121287815
Train loss (w/o reg) on all data: 0.11075404
Test loss (w/o reg) on all data: 0.10217024
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.15171515e-05
Norm of the params: 14.514663
     Influence (LOO): fixed  89 labels. Loss 0.10217. Accuracy 0.962.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04356125
Train loss (w/o reg) on all data: 0.029771619
Test loss (w/o reg) on all data: 0.038073465
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.678031e-06
Norm of the params: 16.607006
                Loss: fixed 118 labels. Loss 0.03807. Accuracy 0.989.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24872914
Train loss (w/o reg) on all data: 0.24113192
Test loss (w/o reg) on all data: 0.18539578
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.342786e-05
Norm of the params: 12.326575
              Random: fixed  16 labels. Loss 0.18540. Accuracy 0.966.
### Flips: 312, rs: 9, checks: 208
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085927516
Train loss (w/o reg) on all data: 0.07653893
Test loss (w/o reg) on all data: 0.063098565
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5361003e-06
Norm of the params: 13.702983
     Influence (LOO): fixed 111 labels. Loss 0.06310. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024032649
Train loss (w/o reg) on all data: 0.0133063495
Test loss (w/o reg) on all data: 0.03122522
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4238316e-06
Norm of the params: 14.646706
                Loss: fixed 131 labels. Loss 0.03123. Accuracy 0.985.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23613349
Train loss (w/o reg) on all data: 0.22800282
Test loss (w/o reg) on all data: 0.17598322
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5291105e-05
Norm of the params: 12.751997
              Random: fixed  25 labels. Loss 0.17598. Accuracy 0.962.
### Flips: 312, rs: 9, checks: 260
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049917176
Train loss (w/o reg) on all data: 0.04004379
Test loss (w/o reg) on all data: 0.046221018
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.520163e-06
Norm of the params: 14.052321
     Influence (LOO): fixed 128 labels. Loss 0.04622. Accuracy 0.985.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018102195
Train loss (w/o reg) on all data: 0.008291801
Test loss (w/o reg) on all data: 0.020740766
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7719921e-06
Norm of the params: 14.0074215
                Loss: fixed 137 labels. Loss 0.02074. Accuracy 0.989.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22765496
Train loss (w/o reg) on all data: 0.21976586
Test loss (w/o reg) on all data: 0.16837949
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.0255323e-05
Norm of the params: 12.561135
              Random: fixed  32 labels. Loss 0.16838. Accuracy 0.950.
### Flips: 312, rs: 9, checks: 312
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018490791
Train loss (w/o reg) on all data: 0.011253924
Test loss (w/o reg) on all data: 0.020080483
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.206156e-07
Norm of the params: 12.030685
     Influence (LOO): fixed 141 labels. Loss 0.02008. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015241179
Train loss (w/o reg) on all data: 0.0069042556
Test loss (w/o reg) on all data: 0.023648802
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6612117e-06
Norm of the params: 12.9127245
                Loss: fixed 139 labels. Loss 0.02365. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2209621
Train loss (w/o reg) on all data: 0.21323134
Test loss (w/o reg) on all data: 0.15034936
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.0399247e-05
Norm of the params: 12.434439
              Random: fixed  39 labels. Loss 0.15035. Accuracy 0.966.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24645473
Train loss (w/o reg) on all data: 0.23849429
Test loss (w/o reg) on all data: 0.20166332
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.2281852e-05
Norm of the params: 12.617794
Flipped loss: 0.20166. Accuracy: 0.924
### Flips: 312, rs: 10, checks: 52
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19812028
Train loss (w/o reg) on all data: 0.18885224
Test loss (w/o reg) on all data: 0.17581674
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 8.662683e-06
Norm of the params: 13.614728
     Influence (LOO): fixed  31 labels. Loss 0.17582. Accuracy 0.939.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13906315
Train loss (w/o reg) on all data: 0.12689018
Test loss (w/o reg) on all data: 0.16784637
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 8.492491e-06
Norm of the params: 15.603185
                Loss: fixed  49 labels. Loss 0.16785. Accuracy 0.920.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23460466
Train loss (w/o reg) on all data: 0.22688295
Test loss (w/o reg) on all data: 0.18043077
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.8400383e-05
Norm of the params: 12.427151
              Random: fixed  13 labels. Loss 0.18043. Accuracy 0.943.
### Flips: 312, rs: 10, checks: 104
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14352232
Train loss (w/o reg) on all data: 0.1331822
Test loss (w/o reg) on all data: 0.14973275
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2852147e-05
Norm of the params: 14.380626
     Influence (LOO): fixed  59 labels. Loss 0.14973. Accuracy 0.950.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056768153
Train loss (w/o reg) on all data: 0.04248549
Test loss (w/o reg) on all data: 0.09792327
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7329745e-06
Norm of the params: 16.90128
                Loss: fixed  94 labels. Loss 0.09792. Accuracy 0.962.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22621362
Train loss (w/o reg) on all data: 0.21831955
Test loss (w/o reg) on all data: 0.16040386
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4242496e-05
Norm of the params: 12.565089
              Random: fixed  21 labels. Loss 0.16040. Accuracy 0.966.
### Flips: 312, rs: 10, checks: 156
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094389826
Train loss (w/o reg) on all data: 0.08189064
Test loss (w/o reg) on all data: 0.10597009
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.457583e-06
Norm of the params: 15.810873
     Influence (LOO): fixed  85 labels. Loss 0.10597. Accuracy 0.969.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034383312
Train loss (w/o reg) on all data: 0.023404479
Test loss (w/o reg) on all data: 0.035667896
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0256848e-06
Norm of the params: 14.818119
                Loss: fixed 114 labels. Loss 0.03567. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22085793
Train loss (w/o reg) on all data: 0.21311401
Test loss (w/o reg) on all data: 0.1548408
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.0212414e-05
Norm of the params: 12.445023
              Random: fixed  26 labels. Loss 0.15484. Accuracy 0.962.
### Flips: 312, rs: 10, checks: 208
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047323488
Train loss (w/o reg) on all data: 0.03453817
Test loss (w/o reg) on all data: 0.07087509
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9169474e-05
Norm of the params: 15.990821
     Influence (LOO): fixed 108 labels. Loss 0.07088. Accuracy 0.977.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018018954
Train loss (w/o reg) on all data: 0.009421358
Test loss (w/o reg) on all data: 0.021098282
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.032297e-06
Norm of the params: 13.113045
                Loss: fixed 123 labels. Loss 0.02110. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20863229
Train loss (w/o reg) on all data: 0.20061037
Test loss (w/o reg) on all data: 0.1418473
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.0861315e-05
Norm of the params: 12.666431
              Random: fixed  36 labels. Loss 0.14185. Accuracy 0.962.
### Flips: 312, rs: 10, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030740226
Train loss (w/o reg) on all data: 0.021466468
Test loss (w/o reg) on all data: 0.04293749
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.0360806e-06
Norm of the params: 13.618926
     Influence (LOO): fixed 120 labels. Loss 0.04294. Accuracy 0.973.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010907302
Train loss (w/o reg) on all data: 0.0046071718
Test loss (w/o reg) on all data: 0.014509575
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.1736816e-07
Norm of the params: 11.225089
                Loss: fixed 127 labels. Loss 0.01451. Accuracy 0.996.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20008188
Train loss (w/o reg) on all data: 0.19210632
Test loss (w/o reg) on all data: 0.13636614
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.48655545e-05
Norm of the params: 12.629771
              Random: fixed  41 labels. Loss 0.13637. Accuracy 0.966.
### Flips: 312, rs: 10, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013616028
Train loss (w/o reg) on all data: 0.0074463123
Test loss (w/o reg) on all data: 0.011244353
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0010507e-06
Norm of the params: 11.108299
     Influence (LOO): fixed 129 labels. Loss 0.01124. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009952482
Train loss (w/o reg) on all data: 0.004076233
Test loss (w/o reg) on all data: 0.012441685
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2781366e-07
Norm of the params: 10.840894
                Loss: fixed 128 labels. Loss 0.01244. Accuracy 0.996.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1849915
Train loss (w/o reg) on all data: 0.1768431
Test loss (w/o reg) on all data: 0.13229606
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8744513e-05
Norm of the params: 12.765887
              Random: fixed  49 labels. Loss 0.13230. Accuracy 0.962.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2604372
Train loss (w/o reg) on all data: 0.2522326
Test loss (w/o reg) on all data: 0.25063166
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.8002954e-05
Norm of the params: 12.809819
Flipped loss: 0.25063. Accuracy: 0.920
### Flips: 312, rs: 11, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20505756
Train loss (w/o reg) on all data: 0.19343162
Test loss (w/o reg) on all data: 0.21774612
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.8225466e-05
Norm of the params: 15.248572
     Influence (LOO): fixed  36 labels. Loss 0.21775. Accuracy 0.939.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15789114
Train loss (w/o reg) on all data: 0.14049216
Test loss (w/o reg) on all data: 0.24203597
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 8.445659e-06
Norm of the params: 18.654215
                Loss: fixed  50 labels. Loss 0.24204. Accuracy 0.912.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25276
Train loss (w/o reg) on all data: 0.24491452
Test loss (w/o reg) on all data: 0.22718517
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.0501425e-05
Norm of the params: 12.526351
              Random: fixed  11 labels. Loss 0.22719. Accuracy 0.935.
### Flips: 312, rs: 11, checks: 104
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15351754
Train loss (w/o reg) on all data: 0.13824213
Test loss (w/o reg) on all data: 0.21752988
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.1888349e-05
Norm of the params: 17.478802
     Influence (LOO): fixed  63 labels. Loss 0.21753. Accuracy 0.916.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083293825
Train loss (w/o reg) on all data: 0.06700283
Test loss (w/o reg) on all data: 0.17829119
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.292452e-06
Norm of the params: 18.050482
                Loss: fixed  95 labels. Loss 0.17829. Accuracy 0.943.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24857642
Train loss (w/o reg) on all data: 0.2405108
Test loss (w/o reg) on all data: 0.2135894
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.6896196e-05
Norm of the params: 12.700872
              Random: fixed  17 labels. Loss 0.21359. Accuracy 0.950.
### Flips: 312, rs: 11, checks: 156
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12953863
Train loss (w/o reg) on all data: 0.11673055
Test loss (w/o reg) on all data: 0.17860128
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.13292e-05
Norm of the params: 16.005049
     Influence (LOO): fixed  81 labels. Loss 0.17860. Accuracy 0.931.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04048217
Train loss (w/o reg) on all data: 0.027658891
Test loss (w/o reg) on all data: 0.12481075
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.6058665e-06
Norm of the params: 16.014542
                Loss: fixed 124 labels. Loss 0.12481. Accuracy 0.954.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24290568
Train loss (w/o reg) on all data: 0.2347482
Test loss (w/o reg) on all data: 0.20186286
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.157361e-05
Norm of the params: 12.773001
              Random: fixed  23 labels. Loss 0.20186. Accuracy 0.954.
### Flips: 312, rs: 11, checks: 208
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09181023
Train loss (w/o reg) on all data: 0.07881368
Test loss (w/o reg) on all data: 0.11892717
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.219225e-06
Norm of the params: 16.122375
     Influence (LOO): fixed 106 labels. Loss 0.11893. Accuracy 0.958.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028371267
Train loss (w/o reg) on all data: 0.016522342
Test loss (w/o reg) on all data: 0.08153269
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0828036e-06
Norm of the params: 15.394106
                Loss: fixed 133 labels. Loss 0.08153. Accuracy 0.973.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23352242
Train loss (w/o reg) on all data: 0.2251595
Test loss (w/o reg) on all data: 0.19280386
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4896547e-05
Norm of the params: 12.932841
              Random: fixed  31 labels. Loss 0.19280. Accuracy 0.958.
### Flips: 312, rs: 11, checks: 260
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060017582
Train loss (w/o reg) on all data: 0.04867516
Test loss (w/o reg) on all data: 0.055315822
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0048684e-05
Norm of the params: 15.061488
     Influence (LOO): fixed 128 labels. Loss 0.05532. Accuracy 0.989.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01960558
Train loss (w/o reg) on all data: 0.009707721
Test loss (w/o reg) on all data: 0.039953843
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0708037e-06
Norm of the params: 14.069726
                Loss: fixed 139 labels. Loss 0.03995. Accuracy 0.989.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22549118
Train loss (w/o reg) on all data: 0.2174313
Test loss (w/o reg) on all data: 0.18281709
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5749878e-05
Norm of the params: 12.696356
              Random: fixed  39 labels. Loss 0.18282. Accuracy 0.962.
### Flips: 312, rs: 11, checks: 312
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026686069
Train loss (w/o reg) on all data: 0.019119985
Test loss (w/o reg) on all data: 0.024972996
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.2266497e-07
Norm of the params: 12.301286
     Influence (LOO): fixed 144 labels. Loss 0.02497. Accuracy 0.996.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019482478
Train loss (w/o reg) on all data: 0.009765221
Test loss (w/o reg) on all data: 0.036160633
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.0128145e-07
Norm of the params: 13.940774
                Loss: fixed 140 labels. Loss 0.03616. Accuracy 0.989.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21346241
Train loss (w/o reg) on all data: 0.20536377
Test loss (w/o reg) on all data: 0.1617068
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.999288e-05
Norm of the params: 12.726865
              Random: fixed  50 labels. Loss 0.16171. Accuracy 0.973.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26247692
Train loss (w/o reg) on all data: 0.25651795
Test loss (w/o reg) on all data: 0.22760618
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.3152348e-05
Norm of the params: 10.916926
Flipped loss: 0.22761. Accuracy: 0.901
### Flips: 312, rs: 12, checks: 52
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22243102
Train loss (w/o reg) on all data: 0.2144705
Test loss (w/o reg) on all data: 0.18276055
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.8976086e-05
Norm of the params: 12.617859
     Influence (LOO): fixed  30 labels. Loss 0.18276. Accuracy 0.920.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16591261
Train loss (w/o reg) on all data: 0.15458246
Test loss (w/o reg) on all data: 0.19331822
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.125314e-05
Norm of the params: 15.053344
                Loss: fixed  47 labels. Loss 0.19332. Accuracy 0.908.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2605911
Train loss (w/o reg) on all data: 0.25447235
Test loss (w/o reg) on all data: 0.22226934
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.9425026e-05
Norm of the params: 11.062307
              Random: fixed   3 labels. Loss 0.22227. Accuracy 0.908.
### Flips: 312, rs: 12, checks: 104
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16984145
Train loss (w/o reg) on all data: 0.16111825
Test loss (w/o reg) on all data: 0.14867039
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 8.522266e-05
Norm of the params: 13.208487
     Influence (LOO): fixed  65 labels. Loss 0.14867. Accuracy 0.927.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09097435
Train loss (w/o reg) on all data: 0.075041786
Test loss (w/o reg) on all data: 0.15006979
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.06335e-06
Norm of the params: 17.85081
                Loss: fixed  90 labels. Loss 0.15007. Accuracy 0.939.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25220755
Train loss (w/o reg) on all data: 0.2456912
Test loss (w/o reg) on all data: 0.20595153
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.6770164e-05
Norm of the params: 11.416092
              Random: fixed  12 labels. Loss 0.20595. Accuracy 0.935.
### Flips: 312, rs: 12, checks: 156
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12256773
Train loss (w/o reg) on all data: 0.11244984
Test loss (w/o reg) on all data: 0.12448238
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.109271e-05
Norm of the params: 14.225251
     Influence (LOO): fixed  93 labels. Loss 0.12448. Accuracy 0.966.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044965316
Train loss (w/o reg) on all data: 0.031726904
Test loss (w/o reg) on all data: 0.108016066
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.5980737e-06
Norm of the params: 16.2717
                Loss: fixed 119 labels. Loss 0.10802. Accuracy 0.962.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24175414
Train loss (w/o reg) on all data: 0.23494533
Test loss (w/o reg) on all data: 0.18372916
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.4727353e-05
Norm of the params: 11.669467
              Random: fixed  22 labels. Loss 0.18373. Accuracy 0.950.
### Flips: 312, rs: 12, checks: 208
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07881928
Train loss (w/o reg) on all data: 0.06832845
Test loss (w/o reg) on all data: 0.08106512
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.790774e-05
Norm of the params: 14.48505
     Influence (LOO): fixed 119 labels. Loss 0.08107. Accuracy 0.981.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030447096
Train loss (w/o reg) on all data: 0.019047726
Test loss (w/o reg) on all data: 0.06334509
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1196918e-06
Norm of the params: 15.099252
                Loss: fixed 133 labels. Loss 0.06335. Accuracy 0.977.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23872648
Train loss (w/o reg) on all data: 0.23217522
Test loss (w/o reg) on all data: 0.17338172
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.6840531e-05
Norm of the params: 11.446635
              Random: fixed  26 labels. Loss 0.17338. Accuracy 0.966.
### Flips: 312, rs: 12, checks: 260
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030436723
Train loss (w/o reg) on all data: 0.02273373
Test loss (w/o reg) on all data: 0.037222527
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6476638e-06
Norm of the params: 12.412086
     Influence (LOO): fixed 141 labels. Loss 0.03722. Accuracy 0.989.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017919907
Train loss (w/o reg) on all data: 0.009419592
Test loss (w/o reg) on all data: 0.03520173
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0395302e-06
Norm of the params: 13.038647
                Loss: fixed 141 labels. Loss 0.03520. Accuracy 0.985.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23261032
Train loss (w/o reg) on all data: 0.22662877
Test loss (w/o reg) on all data: 0.15919945
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.5515586e-05
Norm of the params: 10.937597
              Random: fixed  33 labels. Loss 0.15920. Accuracy 0.962.
### Flips: 312, rs: 12, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014498443
Train loss (w/o reg) on all data: 0.00834328
Test loss (w/o reg) on all data: 0.022085859
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5050612e-06
Norm of the params: 11.095191
     Influence (LOO): fixed 146 labels. Loss 0.02209. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009413426
Train loss (w/o reg) on all data: 0.0037069384
Test loss (w/o reg) on all data: 0.015213051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.274474e-06
Norm of the params: 10.683153
                Loss: fixed 147 labels. Loss 0.01521. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2205416
Train loss (w/o reg) on all data: 0.21399017
Test loss (w/o reg) on all data: 0.13715233
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.0638027e-05
Norm of the params: 11.446771
              Random: fixed  43 labels. Loss 0.13715. Accuracy 0.969.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2632055
Train loss (w/o reg) on all data: 0.25643516
Test loss (w/o reg) on all data: 0.20938711
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 9.304353e-05
Norm of the params: 11.636457
Flipped loss: 0.20939. Accuracy: 0.908
### Flips: 312, rs: 13, checks: 52
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20641315
Train loss (w/o reg) on all data: 0.19673078
Test loss (w/o reg) on all data: 0.18304403
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 0.00010560779
Norm of the params: 13.915726
     Influence (LOO): fixed  33 labels. Loss 0.18304. Accuracy 0.927.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15473929
Train loss (w/o reg) on all data: 0.13995515
Test loss (w/o reg) on all data: 0.18616955
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.8087316e-05
Norm of the params: 17.19543
                Loss: fixed  51 labels. Loss 0.18617. Accuracy 0.920.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26296625
Train loss (w/o reg) on all data: 0.25617865
Test loss (w/o reg) on all data: 0.20555589
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.992579e-05
Norm of the params: 11.651273
              Random: fixed   2 labels. Loss 0.20556. Accuracy 0.924.
### Flips: 312, rs: 13, checks: 104
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15228772
Train loss (w/o reg) on all data: 0.14057598
Test loss (w/o reg) on all data: 0.15638094
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.6907454e-05
Norm of the params: 15.304737
     Influence (LOO): fixed  65 labels. Loss 0.15638. Accuracy 0.943.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09155688
Train loss (w/o reg) on all data: 0.0741355
Test loss (w/o reg) on all data: 0.129907
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.5422955e-05
Norm of the params: 18.666214
                Loss: fixed  92 labels. Loss 0.12991. Accuracy 0.935.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25923973
Train loss (w/o reg) on all data: 0.25255835
Test loss (w/o reg) on all data: 0.19790582
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.3089115e-05
Norm of the params: 11.559754
              Random: fixed   9 labels. Loss 0.19791. Accuracy 0.931.
### Flips: 312, rs: 13, checks: 156
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11488016
Train loss (w/o reg) on all data: 0.101975955
Test loss (w/o reg) on all data: 0.12314751
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.418937e-05
Norm of the params: 16.064993
     Influence (LOO): fixed  86 labels. Loss 0.12315. Accuracy 0.954.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04015657
Train loss (w/o reg) on all data: 0.024951683
Test loss (w/o reg) on all data: 0.052782375
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.8428216e-06
Norm of the params: 17.438398
                Loss: fixed 124 labels. Loss 0.05278. Accuracy 0.977.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25053078
Train loss (w/o reg) on all data: 0.24421033
Test loss (w/o reg) on all data: 0.17842788
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.737925e-05
Norm of the params: 11.243177
              Random: fixed  21 labels. Loss 0.17843. Accuracy 0.954.
### Flips: 312, rs: 13, checks: 208
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08599819
Train loss (w/o reg) on all data: 0.07265222
Test loss (w/o reg) on all data: 0.0962103
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.412645e-06
Norm of the params: 16.337671
     Influence (LOO): fixed 107 labels. Loss 0.09621. Accuracy 0.969.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025234029
Train loss (w/o reg) on all data: 0.013007534
Test loss (w/o reg) on all data: 0.03417051
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.054063e-06
Norm of the params: 15.637451
                Loss: fixed 136 labels. Loss 0.03417. Accuracy 0.989.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24461994
Train loss (w/o reg) on all data: 0.23814602
Test loss (w/o reg) on all data: 0.16904095
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3392492e-05
Norm of the params: 11.378856
              Random: fixed  28 labels. Loss 0.16904. Accuracy 0.958.
### Flips: 312, rs: 13, checks: 260
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063908346
Train loss (w/o reg) on all data: 0.0526604
Test loss (w/o reg) on all data: 0.05570743
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8055426e-06
Norm of the params: 14.998631
     Influence (LOO): fixed 125 labels. Loss 0.05571. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022131816
Train loss (w/o reg) on all data: 0.01107726
Test loss (w/o reg) on all data: 0.03035561
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.929706e-07
Norm of the params: 14.869133
                Loss: fixed 141 labels. Loss 0.03036. Accuracy 0.989.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23583183
Train loss (w/o reg) on all data: 0.22927013
Test loss (w/o reg) on all data: 0.15306568
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2024251e-05
Norm of the params: 11.455742
              Random: fixed  38 labels. Loss 0.15307. Accuracy 0.969.
### Flips: 312, rs: 13, checks: 312
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03865346
Train loss (w/o reg) on all data: 0.029416978
Test loss (w/o reg) on all data: 0.03384083
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.514276e-06
Norm of the params: 13.591527
     Influence (LOO): fixed 139 labels. Loss 0.03384. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013594531
Train loss (w/o reg) on all data: 0.0058168867
Test loss (w/o reg) on all data: 0.020702206
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1242391e-06
Norm of the params: 12.472085
                Loss: fixed 145 labels. Loss 0.02070. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22836451
Train loss (w/o reg) on all data: 0.22176318
Test loss (w/o reg) on all data: 0.14281747
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.305632e-05
Norm of the params: 11.490292
              Random: fixed  44 labels. Loss 0.14282. Accuracy 0.973.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25714502
Train loss (w/o reg) on all data: 0.24984278
Test loss (w/o reg) on all data: 0.17445116
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4400858e-05
Norm of the params: 12.08491
Flipped loss: 0.17445. Accuracy: 0.943
### Flips: 312, rs: 14, checks: 52
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19869602
Train loss (w/o reg) on all data: 0.18794188
Test loss (w/o reg) on all data: 0.13165523
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6622145e-05
Norm of the params: 14.665704
     Influence (LOO): fixed  36 labels. Loss 0.13166. Accuracy 0.958.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14834699
Train loss (w/o reg) on all data: 0.1319623
Test loss (w/o reg) on all data: 0.12656116
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0089571e-05
Norm of the params: 18.102318
                Loss: fixed  51 labels. Loss 0.12656. Accuracy 0.935.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25044164
Train loss (w/o reg) on all data: 0.24294123
Test loss (w/o reg) on all data: 0.17329432
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.035186e-05
Norm of the params: 12.247783
              Random: fixed   6 labels. Loss 0.17329. Accuracy 0.943.
### Flips: 312, rs: 14, checks: 104
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15082401
Train loss (w/o reg) on all data: 0.1378779
Test loss (w/o reg) on all data: 0.09563177
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1602695e-05
Norm of the params: 16.091063
     Influence (LOO): fixed  62 labels. Loss 0.09563. Accuracy 0.966.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068114825
Train loss (w/o reg) on all data: 0.049557596
Test loss (w/o reg) on all data: 0.08540862
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.1957876e-06
Norm of the params: 19.265112
                Loss: fixed  97 labels. Loss 0.08541. Accuracy 0.962.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2460849
Train loss (w/o reg) on all data: 0.23832615
Test loss (w/o reg) on all data: 0.16632359
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.7318464e-05
Norm of the params: 12.456931
              Random: fixed  10 labels. Loss 0.16632. Accuracy 0.939.
### Flips: 312, rs: 14, checks: 156
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10165447
Train loss (w/o reg) on all data: 0.08912236
Test loss (w/o reg) on all data: 0.059224166
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8780725e-05
Norm of the params: 15.831683
     Influence (LOO): fixed  92 labels. Loss 0.05922. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037283078
Train loss (w/o reg) on all data: 0.022233956
Test loss (w/o reg) on all data: 0.047301084
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0838422e-06
Norm of the params: 17.348846
                Loss: fixed 117 labels. Loss 0.04730. Accuracy 0.977.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23743543
Train loss (w/o reg) on all data: 0.22969241
Test loss (w/o reg) on all data: 0.1545768
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.0490696e-05
Norm of the params: 12.444286
              Random: fixed  16 labels. Loss 0.15458. Accuracy 0.954.
### Flips: 312, rs: 14, checks: 208
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060304873
Train loss (w/o reg) on all data: 0.049066864
Test loss (w/o reg) on all data: 0.03681176
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0616963e-05
Norm of the params: 14.992005
     Influence (LOO): fixed 115 labels. Loss 0.03681. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02108334
Train loss (w/o reg) on all data: 0.010935338
Test loss (w/o reg) on all data: 0.018778697
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4704187e-06
Norm of the params: 14.246405
                Loss: fixed 131 labels. Loss 0.01878. Accuracy 0.996.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2306821
Train loss (w/o reg) on all data: 0.222404
Test loss (w/o reg) on all data: 0.14915846
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2144901e-05
Norm of the params: 12.867093
              Random: fixed  21 labels. Loss 0.14916. Accuracy 0.954.
### Flips: 312, rs: 14, checks: 260
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046217315
Train loss (w/o reg) on all data: 0.036147825
Test loss (w/o reg) on all data: 0.033983737
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0361742e-06
Norm of the params: 14.191188
     Influence (LOO): fixed 124 labels. Loss 0.03398. Accuracy 0.989.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015895
Train loss (w/o reg) on all data: 0.0077113383
Test loss (w/o reg) on all data: 0.016036183
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.734235e-07
Norm of the params: 12.793485
                Loss: fixed 134 labels. Loss 0.01604. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22631577
Train loss (w/o reg) on all data: 0.21800257
Test loss (w/o reg) on all data: 0.14315827
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.224624e-05
Norm of the params: 12.894329
              Random: fixed  25 labels. Loss 0.14316. Accuracy 0.969.
### Flips: 312, rs: 14, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018176753
Train loss (w/o reg) on all data: 0.010978009
Test loss (w/o reg) on all data: 0.023422543
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3503877e-06
Norm of the params: 11.998954
     Influence (LOO): fixed 137 labels. Loss 0.02342. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015161058
Train loss (w/o reg) on all data: 0.007223571
Test loss (w/o reg) on all data: 0.016767241
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.830516e-07
Norm of the params: 12.599593
                Loss: fixed 135 labels. Loss 0.01677. Accuracy 0.992.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21675642
Train loss (w/o reg) on all data: 0.20818016
Test loss (w/o reg) on all data: 0.12893675
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9567702e-05
Norm of the params: 13.096761
              Random: fixed  34 labels. Loss 0.12894. Accuracy 0.973.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27154332
Train loss (w/o reg) on all data: 0.26533926
Test loss (w/o reg) on all data: 0.22511292
Train acc on all data:  0.8586437440305635
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.3916645e-05
Norm of the params: 11.139181
Flipped loss: 0.22511. Accuracy: 0.924
### Flips: 312, rs: 15, checks: 52
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231487
Train loss (w/o reg) on all data: 0.22309977
Test loss (w/o reg) on all data: 0.19053248
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.1393051e-05
Norm of the params: 12.951625
     Influence (LOO): fixed  28 labels. Loss 0.19053. Accuracy 0.931.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17564268
Train loss (w/o reg) on all data: 0.16388357
Test loss (w/o reg) on all data: 0.18859339
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 9.391061e-06
Norm of the params: 15.335653
                Loss: fixed  52 labels. Loss 0.18859. Accuracy 0.924.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26456827
Train loss (w/o reg) on all data: 0.2580765
Test loss (w/o reg) on all data: 0.21172155
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5308693e-05
Norm of the params: 11.394553
              Random: fixed  10 labels. Loss 0.21172. Accuracy 0.931.
### Flips: 312, rs: 15, checks: 104
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19867985
Train loss (w/o reg) on all data: 0.18883541
Test loss (w/o reg) on all data: 0.16620566
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6739774e-05
Norm of the params: 14.031702
     Influence (LOO): fixed  51 labels. Loss 0.16621. Accuracy 0.958.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10409496
Train loss (w/o reg) on all data: 0.08780436
Test loss (w/o reg) on all data: 0.15822223
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.5340794e-05
Norm of the params: 18.050264
                Loss: fixed  90 labels. Loss 0.15822. Accuracy 0.935.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26313114
Train loss (w/o reg) on all data: 0.25650707
Test loss (w/o reg) on all data: 0.20193112
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.3585246e-05
Norm of the params: 11.510067
              Random: fixed  16 labels. Loss 0.20193. Accuracy 0.931.
### Flips: 312, rs: 15, checks: 156
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158163
Train loss (w/o reg) on all data: 0.14903244
Test loss (w/o reg) on all data: 0.1386446
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.0161971e-05
Norm of the params: 13.5133705
     Influence (LOO): fixed  80 labels. Loss 0.13864. Accuracy 0.947.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069567606
Train loss (w/o reg) on all data: 0.05371434
Test loss (w/o reg) on all data: 0.12134004
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.6526025e-06
Norm of the params: 17.806328
                Loss: fixed 116 labels. Loss 0.12134. Accuracy 0.943.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25629675
Train loss (w/o reg) on all data: 0.2496205
Test loss (w/o reg) on all data: 0.18364489
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.3331765e-05
Norm of the params: 11.555299
              Random: fixed  24 labels. Loss 0.18364. Accuracy 0.947.
### Flips: 312, rs: 15, checks: 208
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11416991
Train loss (w/o reg) on all data: 0.103843145
Test loss (w/o reg) on all data: 0.112279415
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.2695713e-06
Norm of the params: 14.371337
     Influence (LOO): fixed 103 labels. Loss 0.11228. Accuracy 0.954.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03831001
Train loss (w/o reg) on all data: 0.024328103
Test loss (w/o reg) on all data: 0.048358463
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6550968e-06
Norm of the params: 16.722385
                Loss: fixed 139 labels. Loss 0.04836. Accuracy 0.985.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24427989
Train loss (w/o reg) on all data: 0.2375138
Test loss (w/o reg) on all data: 0.16524601
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.261354e-05
Norm of the params: 11.632796
              Random: fixed  36 labels. Loss 0.16525. Accuracy 0.950.
### Flips: 312, rs: 15, checks: 260
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08456047
Train loss (w/o reg) on all data: 0.074776106
Test loss (w/o reg) on all data: 0.06859935
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0986499e-05
Norm of the params: 13.988825
     Influence (LOO): fixed 125 labels. Loss 0.06860. Accuracy 0.977.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025649505
Train loss (w/o reg) on all data: 0.015960183
Test loss (w/o reg) on all data: 0.028724633
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5925607e-06
Norm of the params: 13.92072
                Loss: fixed 150 labels. Loss 0.02872. Accuracy 0.985.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23891489
Train loss (w/o reg) on all data: 0.23215386
Test loss (w/o reg) on all data: 0.14832766
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1801566e-05
Norm of the params: 11.628441
              Random: fixed  42 labels. Loss 0.14833. Accuracy 0.958.
### Flips: 312, rs: 15, checks: 312
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047923155
Train loss (w/o reg) on all data: 0.038365945
Test loss (w/o reg) on all data: 0.03794908
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.821071e-05
Norm of the params: 13.825491
     Influence (LOO): fixed 144 labels. Loss 0.03795. Accuracy 0.989.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018501803
Train loss (w/o reg) on all data: 0.010549023
Test loss (w/o reg) on all data: 0.02403873
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1387459e-06
Norm of the params: 12.611727
                Loss: fixed 154 labels. Loss 0.02404. Accuracy 0.985.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23021533
Train loss (w/o reg) on all data: 0.22361535
Test loss (w/o reg) on all data: 0.13447107
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 0.00013654561
Norm of the params: 11.489107
              Random: fixed  50 labels. Loss 0.13447. Accuracy 0.966.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25859657
Train loss (w/o reg) on all data: 0.25123197
Test loss (w/o reg) on all data: 0.17939869
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4784333e-05
Norm of the params: 12.1364
Flipped loss: 0.17940. Accuracy: 0.958
### Flips: 312, rs: 16, checks: 52
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20118028
Train loss (w/o reg) on all data: 0.1918801
Test loss (w/o reg) on all data: 0.13505071
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3241137e-05
Norm of the params: 13.638309
     Influence (LOO): fixed  36 labels. Loss 0.13505. Accuracy 0.962.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562856
Train loss (w/o reg) on all data: 0.14335325
Test loss (w/o reg) on all data: 0.13900435
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.187353e-06
Norm of the params: 15.668634
                Loss: fixed  51 labels. Loss 0.13900. Accuracy 0.958.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25543076
Train loss (w/o reg) on all data: 0.24845357
Test loss (w/o reg) on all data: 0.1600648
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2485722e-05
Norm of the params: 11.812858
              Random: fixed   7 labels. Loss 0.16006. Accuracy 0.962.
### Flips: 312, rs: 16, checks: 104
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14494084
Train loss (w/o reg) on all data: 0.13497208
Test loss (w/o reg) on all data: 0.09865458
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7587057e-05
Norm of the params: 14.1200285
     Influence (LOO): fixed  69 labels. Loss 0.09865. Accuracy 0.966.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07786943
Train loss (w/o reg) on all data: 0.06225095
Test loss (w/o reg) on all data: 0.06893384
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.900691e-06
Norm of the params: 17.673983
                Loss: fixed 100 labels. Loss 0.06893. Accuracy 0.973.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24349736
Train loss (w/o reg) on all data: 0.23662607
Test loss (w/o reg) on all data: 0.14315827
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.8155635e-05
Norm of the params: 11.722865
              Random: fixed  17 labels. Loss 0.14316. Accuracy 0.981.
### Flips: 312, rs: 16, checks: 156
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10129114
Train loss (w/o reg) on all data: 0.09008062
Test loss (w/o reg) on all data: 0.09609863
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7477876e-05
Norm of the params: 14.9736595
     Influence (LOO): fixed  91 labels. Loss 0.09610. Accuracy 0.966.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02358849
Train loss (w/o reg) on all data: 0.013165573
Test loss (w/o reg) on all data: 0.022386646
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0988095e-06
Norm of the params: 14.438087
                Loss: fixed 128 labels. Loss 0.02239. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23664826
Train loss (w/o reg) on all data: 0.22987035
Test loss (w/o reg) on all data: 0.13371982
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.818669e-05
Norm of the params: 11.642942
              Random: fixed  25 labels. Loss 0.13372. Accuracy 0.977.
### Flips: 312, rs: 16, checks: 208
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07196315
Train loss (w/o reg) on all data: 0.061476693
Test loss (w/o reg) on all data: 0.07183158
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.07529395e-05
Norm of the params: 14.482027
     Influence (LOO): fixed 110 labels. Loss 0.07183. Accuracy 0.969.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014785012
Train loss (w/o reg) on all data: 0.0066626514
Test loss (w/o reg) on all data: 0.02225345
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5216689e-06
Norm of the params: 12.745479
                Loss: fixed 133 labels. Loss 0.02225. Accuracy 0.992.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22941647
Train loss (w/o reg) on all data: 0.22245029
Test loss (w/o reg) on all data: 0.13133533
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.2324992e-05
Norm of the params: 11.803554
              Random: fixed  30 labels. Loss 0.13134. Accuracy 0.973.
### Flips: 312, rs: 16, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037189215
Train loss (w/o reg) on all data: 0.029885955
Test loss (w/o reg) on all data: 0.028354555
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.4069203e-06
Norm of the params: 12.085744
     Influence (LOO): fixed 129 labels. Loss 0.02835. Accuracy 0.996.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013465669
Train loss (w/o reg) on all data: 0.0059691817
Test loss (w/o reg) on all data: 0.013221939
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0375676e-07
Norm of the params: 12.244581
                Loss: fixed 134 labels. Loss 0.01322. Accuracy 0.996.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2142602
Train loss (w/o reg) on all data: 0.2067247
Test loss (w/o reg) on all data: 0.119737774
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1045926e-05
Norm of the params: 12.2764015
              Random: fixed  38 labels. Loss 0.11974. Accuracy 0.977.
### Flips: 312, rs: 16, checks: 312
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018814132
Train loss (w/o reg) on all data: 0.013495993
Test loss (w/o reg) on all data: 0.018706795
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.443816e-06
Norm of the params: 10.313232
     Influence (LOO): fixed 137 labels. Loss 0.01871. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012418412
Train loss (w/o reg) on all data: 0.0052846624
Test loss (w/o reg) on all data: 0.014176776
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7935813e-07
Norm of the params: 11.944664
                Loss: fixed 135 labels. Loss 0.01418. Accuracy 0.996.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20703506
Train loss (w/o reg) on all data: 0.20034102
Test loss (w/o reg) on all data: 0.11028676
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5731128e-05
Norm of the params: 11.570701
              Random: fixed  45 labels. Loss 0.11029. Accuracy 0.981.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2582849
Train loss (w/o reg) on all data: 0.25018275
Test loss (w/o reg) on all data: 0.19702932
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.3042348e-05
Norm of the params: 12.729617
Flipped loss: 0.19703. Accuracy: 0.931
### Flips: 312, rs: 17, checks: 52
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20484522
Train loss (w/o reg) on all data: 0.19485801
Test loss (w/o reg) on all data: 0.14895079
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.2179366e-05
Norm of the params: 14.133084
     Influence (LOO): fixed  36 labels. Loss 0.14895. Accuracy 0.977.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14806913
Train loss (w/o reg) on all data: 0.13300045
Test loss (w/o reg) on all data: 0.12801245
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.668313e-05
Norm of the params: 17.360111
                Loss: fixed  52 labels. Loss 0.12801. Accuracy 0.943.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25503746
Train loss (w/o reg) on all data: 0.24696288
Test loss (w/o reg) on all data: 0.176075
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6983447e-05
Norm of the params: 12.707934
              Random: fixed   8 labels. Loss 0.17607. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 104
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1525748
Train loss (w/o reg) on all data: 0.14107272
Test loss (w/o reg) on all data: 0.11624369
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.4481435e-06
Norm of the params: 15.167122
     Influence (LOO): fixed  68 labels. Loss 0.11624. Accuracy 0.969.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068462
Train loss (w/o reg) on all data: 0.050075594
Test loss (w/o reg) on all data: 0.05686812
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.590277e-06
Norm of the params: 19.176237
                Loss: fixed 100 labels. Loss 0.05687. Accuracy 0.985.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24197216
Train loss (w/o reg) on all data: 0.23367381
Test loss (w/o reg) on all data: 0.15433772
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1630798e-05
Norm of the params: 12.882816
              Random: fixed  21 labels. Loss 0.15434. Accuracy 0.969.
### Flips: 312, rs: 17, checks: 156
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11090599
Train loss (w/o reg) on all data: 0.09915355
Test loss (w/o reg) on all data: 0.08250786
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.4037918e-06
Norm of the params: 15.3313
     Influence (LOO): fixed  93 labels. Loss 0.08251. Accuracy 0.977.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03812467
Train loss (w/o reg) on all data: 0.023115305
Test loss (w/o reg) on all data: 0.023408191
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4011173e-06
Norm of the params: 17.325914
                Loss: fixed 123 labels. Loss 0.02341. Accuracy 0.996.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23082955
Train loss (w/o reg) on all data: 0.22230926
Test loss (w/o reg) on all data: 0.14057735
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9339052e-05
Norm of the params: 13.053959
              Random: fixed  30 labels. Loss 0.14058. Accuracy 0.977.
### Flips: 312, rs: 17, checks: 208
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07600887
Train loss (w/o reg) on all data: 0.06634192
Test loss (w/o reg) on all data: 0.05318869
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4694491e-05
Norm of the params: 13.904641
     Influence (LOO): fixed 116 labels. Loss 0.05319. Accuracy 0.985.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027538374
Train loss (w/o reg) on all data: 0.014494237
Test loss (w/o reg) on all data: 0.01756163
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1732332e-06
Norm of the params: 16.151865
                Loss: fixed 129 labels. Loss 0.01756. Accuracy 0.996.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22389421
Train loss (w/o reg) on all data: 0.21565172
Test loss (w/o reg) on all data: 0.13259612
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3983964e-05
Norm of the params: 12.839378
              Random: fixed  37 labels. Loss 0.13260. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 260
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048374377
Train loss (w/o reg) on all data: 0.039958943
Test loss (w/o reg) on all data: 0.032318994
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5024037e-06
Norm of the params: 12.973384
     Influence (LOO): fixed 129 labels. Loss 0.03232. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022689175
Train loss (w/o reg) on all data: 0.0112153
Test loss (w/o reg) on all data: 0.015721012
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7284025e-06
Norm of the params: 15.148515
                Loss: fixed 133 labels. Loss 0.01572. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21710476
Train loss (w/o reg) on all data: 0.2091154
Test loss (w/o reg) on all data: 0.1285988
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7430116e-05
Norm of the params: 12.640696
              Random: fixed  45 labels. Loss 0.12860. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021128526
Train loss (w/o reg) on all data: 0.014800331
Test loss (w/o reg) on all data: 0.019834967
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1326017e-06
Norm of the params: 11.250063
     Influence (LOO): fixed 140 labels. Loss 0.01983. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017814096
Train loss (w/o reg) on all data: 0.008176812
Test loss (w/o reg) on all data: 0.014883451
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.297801e-06
Norm of the params: 13.883289
                Loss: fixed 136 labels. Loss 0.01488. Accuracy 0.996.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20875771
Train loss (w/o reg) on all data: 0.20091447
Test loss (w/o reg) on all data: 0.11867804
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2579501e-05
Norm of the params: 12.524571
              Random: fixed  53 labels. Loss 0.11868. Accuracy 0.973.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26209554
Train loss (w/o reg) on all data: 0.25533867
Test loss (w/o reg) on all data: 0.21475093
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 4.154987e-05
Norm of the params: 11.624874
Flipped loss: 0.21475. Accuracy: 0.916
### Flips: 312, rs: 18, checks: 52
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922912
Train loss (w/o reg) on all data: 0.18733095
Test loss (w/o reg) on all data: 0.17974795
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 8.094817e-06
Norm of the params: 15.426068
     Influence (LOO): fixed  33 labels. Loss 0.17975. Accuracy 0.935.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15644084
Train loss (w/o reg) on all data: 0.14249751
Test loss (w/o reg) on all data: 0.1612955
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.2305627e-05
Norm of the params: 16.699299
                Loss: fixed  50 labels. Loss 0.16130. Accuracy 0.947.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25814885
Train loss (w/o reg) on all data: 0.25130418
Test loss (w/o reg) on all data: 0.20140898
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 7.60193e-05
Norm of the params: 11.700156
              Random: fixed   8 labels. Loss 0.20141. Accuracy 0.935.
### Flips: 312, rs: 18, checks: 104
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16447312
Train loss (w/o reg) on all data: 0.15260354
Test loss (w/o reg) on all data: 0.14280768
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.498019e-06
Norm of the params: 15.407514
     Influence (LOO): fixed  61 labels. Loss 0.14281. Accuracy 0.950.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0902239
Train loss (w/o reg) on all data: 0.07256765
Test loss (w/o reg) on all data: 0.10431667
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1724547e-05
Norm of the params: 18.791624
                Loss: fixed  90 labels. Loss 0.10432. Accuracy 0.954.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25077617
Train loss (w/o reg) on all data: 0.24377172
Test loss (w/o reg) on all data: 0.1916954
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.0919762e-05
Norm of the params: 11.835913
              Random: fixed  16 labels. Loss 0.19170. Accuracy 0.947.
### Flips: 312, rs: 18, checks: 156
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1319322
Train loss (w/o reg) on all data: 0.11946916
Test loss (w/o reg) on all data: 0.106760636
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0735333e-05
Norm of the params: 15.787993
     Influence (LOO): fixed  83 labels. Loss 0.10676. Accuracy 0.966.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05054934
Train loss (w/o reg) on all data: 0.035841007
Test loss (w/o reg) on all data: 0.053803645
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.224888e-06
Norm of the params: 17.151287
                Loss: fixed 119 labels. Loss 0.05380. Accuracy 0.981.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24352427
Train loss (w/o reg) on all data: 0.2360014
Test loss (w/o reg) on all data: 0.18389386
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.0478997e-05
Norm of the params: 12.2661085
              Random: fixed  22 labels. Loss 0.18389. Accuracy 0.947.
### Flips: 312, rs: 18, checks: 208
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08733848
Train loss (w/o reg) on all data: 0.075781114
Test loss (w/o reg) on all data: 0.07814181
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6336241e-06
Norm of the params: 15.203528
     Influence (LOO): fixed 110 labels. Loss 0.07814. Accuracy 0.981.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030742882
Train loss (w/o reg) on all data: 0.017965239
Test loss (w/o reg) on all data: 0.045205925
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.1011285e-06
Norm of the params: 15.986021
                Loss: fixed 133 labels. Loss 0.04521. Accuracy 0.985.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773466
Train loss (w/o reg) on all data: 0.23035266
Test loss (w/o reg) on all data: 0.17522357
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.852126e-05
Norm of the params: 12.150719
              Random: fixed  30 labels. Loss 0.17522. Accuracy 0.958.
### Flips: 312, rs: 18, checks: 260
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055618547
Train loss (w/o reg) on all data: 0.045847762
Test loss (w/o reg) on all data: 0.041478153
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.962158e-06
Norm of the params: 13.979117
     Influence (LOO): fixed 126 labels. Loss 0.04148. Accuracy 0.989.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022901617
Train loss (w/o reg) on all data: 0.012143263
Test loss (w/o reg) on all data: 0.02787032
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4061435e-06
Norm of the params: 14.668575
                Loss: fixed 138 labels. Loss 0.02787. Accuracy 0.992.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2371852
Train loss (w/o reg) on all data: 0.2299628
Test loss (w/o reg) on all data: 0.16502829
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.9103174e-05
Norm of the params: 12.018646
              Random: fixed  34 labels. Loss 0.16503. Accuracy 0.966.
### Flips: 312, rs: 18, checks: 312
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040577784
Train loss (w/o reg) on all data: 0.031338133
Test loss (w/o reg) on all data: 0.037804868
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7529138e-06
Norm of the params: 13.593862
     Influence (LOO): fixed 136 labels. Loss 0.03780. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017797517
Train loss (w/o reg) on all data: 0.00846783
Test loss (w/o reg) on all data: 0.016801154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4677195e-06
Norm of the params: 13.659933
                Loss: fixed 142 labels. Loss 0.01680. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22606856
Train loss (w/o reg) on all data: 0.2189768
Test loss (w/o reg) on all data: 0.15112153
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4804471e-05
Norm of the params: 11.909453
              Random: fixed  44 labels. Loss 0.15112. Accuracy 0.973.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24691248
Train loss (w/o reg) on all data: 0.23897819
Test loss (w/o reg) on all data: 0.18267193
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8149816e-05
Norm of the params: 12.597052
Flipped loss: 0.18267. Accuracy: 0.958
### Flips: 312, rs: 19, checks: 52
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18549281
Train loss (w/o reg) on all data: 0.17499618
Test loss (w/o reg) on all data: 0.14112642
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.0306781e-05
Norm of the params: 14.48905
     Influence (LOO): fixed  37 labels. Loss 0.14113. Accuracy 0.947.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13644558
Train loss (w/o reg) on all data: 0.12057884
Test loss (w/o reg) on all data: 0.1355128
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.861259e-05
Norm of the params: 17.813894
                Loss: fixed  52 labels. Loss 0.13551. Accuracy 0.943.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24360546
Train loss (w/o reg) on all data: 0.23577994
Test loss (w/o reg) on all data: 0.17741653
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.3163185e-05
Norm of the params: 12.510411
              Random: fixed   8 labels. Loss 0.17742. Accuracy 0.947.
### Flips: 312, rs: 19, checks: 104
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13170426
Train loss (w/o reg) on all data: 0.119732454
Test loss (w/o reg) on all data: 0.1092369
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.636156e-05
Norm of the params: 15.473724
     Influence (LOO): fixed  65 labels. Loss 0.10924. Accuracy 0.969.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056830153
Train loss (w/o reg) on all data: 0.039912008
Test loss (w/o reg) on all data: 0.0615064
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.130699e-06
Norm of the params: 18.394642
                Loss: fixed  97 labels. Loss 0.06151. Accuracy 0.973.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24028203
Train loss (w/o reg) on all data: 0.2323391
Test loss (w/o reg) on all data: 0.17082347
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.841102e-05
Norm of the params: 12.603915
              Random: fixed  12 labels. Loss 0.17082. Accuracy 0.947.
### Flips: 312, rs: 19, checks: 156
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09415228
Train loss (w/o reg) on all data: 0.08264611
Test loss (w/o reg) on all data: 0.08316293
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6465086e-05
Norm of the params: 15.169819
     Influence (LOO): fixed  87 labels. Loss 0.08316. Accuracy 0.969.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032645598
Train loss (w/o reg) on all data: 0.020117931
Test loss (w/o reg) on all data: 0.026755804
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1089752e-06
Norm of the params: 15.828877
                Loss: fixed 117 labels. Loss 0.02676. Accuracy 0.989.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23502362
Train loss (w/o reg) on all data: 0.22726919
Test loss (w/o reg) on all data: 0.15640102
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.15178e-05
Norm of the params: 12.453458
              Random: fixed  17 labels. Loss 0.15640. Accuracy 0.969.
### Flips: 312, rs: 19, checks: 208
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050859507
Train loss (w/o reg) on all data: 0.04064005
Test loss (w/o reg) on all data: 0.044600867
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.790703e-06
Norm of the params: 14.2964735
     Influence (LOO): fixed 111 labels. Loss 0.04460. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025309809
Train loss (w/o reg) on all data: 0.013957385
Test loss (w/o reg) on all data: 0.022844803
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.272242e-06
Norm of the params: 15.068127
                Loss: fixed 121 labels. Loss 0.02284. Accuracy 0.992.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22732963
Train loss (w/o reg) on all data: 0.21894954
Test loss (w/o reg) on all data: 0.15288311
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.51092e-05
Norm of the params: 12.946106
              Random: fixed  21 labels. Loss 0.15288. Accuracy 0.962.
### Flips: 312, rs: 19, checks: 260
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022844508
Train loss (w/o reg) on all data: 0.014476214
Test loss (w/o reg) on all data: 0.021247229
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.726601e-06
Norm of the params: 12.9369955
     Influence (LOO): fixed 124 labels. Loss 0.02125. Accuracy 0.992.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01823264
Train loss (w/o reg) on all data: 0.0088839475
Test loss (w/o reg) on all data: 0.021481318
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3536038e-06
Norm of the params: 13.67384
                Loss: fixed 124 labels. Loss 0.02148. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21588296
Train loss (w/o reg) on all data: 0.20743594
Test loss (w/o reg) on all data: 0.13044271
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7752438e-05
Norm of the params: 12.99771
              Random: fixed  32 labels. Loss 0.13044. Accuracy 0.977.
### Flips: 312, rs: 19, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014129013
Train loss (w/o reg) on all data: 0.007382169
Test loss (w/o reg) on all data: 0.013939095
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.704705e-07
Norm of the params: 11.616234
     Influence (LOO): fixed 130 labels. Loss 0.01394. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013827704
Train loss (w/o reg) on all data: 0.0066759856
Test loss (w/o reg) on all data: 0.018275352
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.74124e-07
Norm of the params: 11.959698
                Loss: fixed 129 labels. Loss 0.01828. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21202552
Train loss (w/o reg) on all data: 0.20376989
Test loss (w/o reg) on all data: 0.12707424
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1013503e-05
Norm of the params: 12.84962
              Random: fixed  36 labels. Loss 0.12707. Accuracy 0.977.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2783451
Train loss (w/o reg) on all data: 0.27142477
Test loss (w/o reg) on all data: 0.21270432
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.8401066e-05
Norm of the params: 11.764628
Flipped loss: 0.21270. Accuracy: 0.954
### Flips: 312, rs: 20, checks: 52
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2348985
Train loss (w/o reg) on all data: 0.22517362
Test loss (w/o reg) on all data: 0.17730346
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6363912e-05
Norm of the params: 13.946233
     Influence (LOO): fixed  31 labels. Loss 0.17730. Accuracy 0.954.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17083484
Train loss (w/o reg) on all data: 0.15934013
Test loss (w/o reg) on all data: 0.18599728
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 6.9936636e-06
Norm of the params: 15.162263
                Loss: fixed  52 labels. Loss 0.18600. Accuracy 0.920.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26566657
Train loss (w/o reg) on all data: 0.25811455
Test loss (w/o reg) on all data: 0.19717088
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.583983e-05
Norm of the params: 12.289864
              Random: fixed  12 labels. Loss 0.19717. Accuracy 0.966.
### Flips: 312, rs: 20, checks: 104
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1774042
Train loss (w/o reg) on all data: 0.16577245
Test loss (w/o reg) on all data: 0.15529183
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.002003e-05
Norm of the params: 15.252368
     Influence (LOO): fixed  65 labels. Loss 0.15529. Accuracy 0.950.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08160093
Train loss (w/o reg) on all data: 0.06492994
Test loss (w/o reg) on all data: 0.124387085
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.214314e-06
Norm of the params: 18.259785
                Loss: fixed 100 labels. Loss 0.12439. Accuracy 0.950.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25891963
Train loss (w/o reg) on all data: 0.2508903
Test loss (w/o reg) on all data: 0.1911392
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.649988e-05
Norm of the params: 12.672255
              Random: fixed  18 labels. Loss 0.19114. Accuracy 0.962.
### Flips: 312, rs: 20, checks: 156
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1242443
Train loss (w/o reg) on all data: 0.111914486
Test loss (w/o reg) on all data: 0.11699778
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.030465e-06
Norm of the params: 15.703383
     Influence (LOO): fixed  93 labels. Loss 0.11700. Accuracy 0.954.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0339053
Train loss (w/o reg) on all data: 0.020477852
Test loss (w/o reg) on all data: 0.029402304
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3622508e-06
Norm of the params: 16.387466
                Loss: fixed 132 labels. Loss 0.02940. Accuracy 0.992.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25645882
Train loss (w/o reg) on all data: 0.24852146
Test loss (w/o reg) on all data: 0.17891434
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8748127e-05
Norm of the params: 12.599499
              Random: fixed  23 labels. Loss 0.17891. Accuracy 0.973.
### Flips: 312, rs: 20, checks: 208
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08862733
Train loss (w/o reg) on all data: 0.07701868
Test loss (w/o reg) on all data: 0.084958754
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.711782e-06
Norm of the params: 15.237225
     Influence (LOO): fixed 115 labels. Loss 0.08496. Accuracy 0.969.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017691435
Train loss (w/o reg) on all data: 0.008421168
Test loss (w/o reg) on all data: 0.015207697
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6966519e-06
Norm of the params: 13.616364
                Loss: fixed 146 labels. Loss 0.01521. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663098
Train loss (w/o reg) on all data: 0.23816772
Test loss (w/o reg) on all data: 0.1545154
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.150654e-05
Norm of the params: 13.010201
              Random: fixed  32 labels. Loss 0.15452. Accuracy 0.985.
### Flips: 312, rs: 20, checks: 260
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05935903
Train loss (w/o reg) on all data: 0.048858378
Test loss (w/o reg) on all data: 0.06080939
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8922426e-06
Norm of the params: 14.491827
     Influence (LOO): fixed 135 labels. Loss 0.06081. Accuracy 0.989.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013404092
Train loss (w/o reg) on all data: 0.0057990127
Test loss (w/o reg) on all data: 0.01790716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.552241e-07
Norm of the params: 12.332948
                Loss: fixed 150 labels. Loss 0.01791. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23965335
Train loss (w/o reg) on all data: 0.23173612
Test loss (w/o reg) on all data: 0.13842203
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.356383e-05
Norm of the params: 12.583504
              Random: fixed  38 labels. Loss 0.13842. Accuracy 0.996.
### Flips: 312, rs: 20, checks: 312
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032546565
Train loss (w/o reg) on all data: 0.024339654
Test loss (w/o reg) on all data: 0.033331294
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.404015e-06
Norm of the params: 12.811643
     Influence (LOO): fixed 147 labels. Loss 0.03333. Accuracy 0.992.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012456067
Train loss (w/o reg) on all data: 0.005373302
Test loss (w/o reg) on all data: 0.019560898
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9557633e-06
Norm of the params: 11.901903
                Loss: fixed 152 labels. Loss 0.01956. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22872728
Train loss (w/o reg) on all data: 0.22126135
Test loss (w/o reg) on all data: 0.12762669
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.3577303e-05
Norm of the params: 12.219603
              Random: fixed  48 labels. Loss 0.12763. Accuracy 0.996.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271552
Train loss (w/o reg) on all data: 0.26342478
Test loss (w/o reg) on all data: 0.23285724
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.5329196e-05
Norm of the params: 12.749284
Flipped loss: 0.23286. Accuracy: 0.927
### Flips: 312, rs: 21, checks: 52
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23145123
Train loss (w/o reg) on all data: 0.22157863
Test loss (w/o reg) on all data: 0.21102129
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.4510213e-05
Norm of the params: 14.051766
     Influence (LOO): fixed  33 labels. Loss 0.21102. Accuracy 0.943.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17269324
Train loss (w/o reg) on all data: 0.15867816
Test loss (w/o reg) on all data: 0.17772369
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.591651e-05
Norm of the params: 16.742212
                Loss: fixed  51 labels. Loss 0.17772. Accuracy 0.935.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2624884
Train loss (w/o reg) on all data: 0.25431222
Test loss (w/o reg) on all data: 0.22402357
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.154443e-05
Norm of the params: 12.787633
              Random: fixed  10 labels. Loss 0.22402. Accuracy 0.935.
### Flips: 312, rs: 21, checks: 104
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18232895
Train loss (w/o reg) on all data: 0.17093053
Test loss (w/o reg) on all data: 0.17466779
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.752849e-05
Norm of the params: 15.0986185
     Influence (LOO): fixed  63 labels. Loss 0.17467. Accuracy 0.950.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10457396
Train loss (w/o reg) on all data: 0.087728225
Test loss (w/o reg) on all data: 0.13889219
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.154235e-06
Norm of the params: 18.355232
                Loss: fixed  95 labels. Loss 0.13889. Accuracy 0.954.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2578703
Train loss (w/o reg) on all data: 0.24948946
Test loss (w/o reg) on all data: 0.22249597
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 8.3407744e-05
Norm of the params: 12.946681
              Random: fixed  14 labels. Loss 0.22250. Accuracy 0.931.
### Flips: 312, rs: 21, checks: 156
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1455446
Train loss (w/o reg) on all data: 0.13322213
Test loss (w/o reg) on all data: 0.15431373
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.047161e-06
Norm of the params: 15.698709
     Influence (LOO): fixed  84 labels. Loss 0.15431. Accuracy 0.950.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05470207
Train loss (w/o reg) on all data: 0.040035367
Test loss (w/o reg) on all data: 0.08405
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.520908e-06
Norm of the params: 17.126997
                Loss: fixed 125 labels. Loss 0.08405. Accuracy 0.977.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24862722
Train loss (w/o reg) on all data: 0.23980469
Test loss (w/o reg) on all data: 0.20396486
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9967716e-05
Norm of the params: 13.283474
              Random: fixed  24 labels. Loss 0.20396. Accuracy 0.947.
### Flips: 312, rs: 21, checks: 208
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10147643
Train loss (w/o reg) on all data: 0.09015993
Test loss (w/o reg) on all data: 0.10006564
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.624204e-06
Norm of the params: 15.0442705
     Influence (LOO): fixed 113 labels. Loss 0.10007. Accuracy 0.973.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034071885
Train loss (w/o reg) on all data: 0.021464642
Test loss (w/o reg) on all data: 0.050028928
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9474135e-06
Norm of the params: 15.879068
                Loss: fixed 139 labels. Loss 0.05003. Accuracy 0.985.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24803145
Train loss (w/o reg) on all data: 0.23955296
Test loss (w/o reg) on all data: 0.19521092
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1122052e-05
Norm of the params: 13.0219
              Random: fixed  28 labels. Loss 0.19521. Accuracy 0.962.
### Flips: 312, rs: 21, checks: 260
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072640546
Train loss (w/o reg) on all data: 0.06258053
Test loss (w/o reg) on all data: 0.070312835
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5362444e-06
Norm of the params: 14.184508
     Influence (LOO): fixed 130 labels. Loss 0.07031. Accuracy 0.989.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023781043
Train loss (w/o reg) on all data: 0.013164743
Test loss (w/o reg) on all data: 0.021071931
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.796314e-06
Norm of the params: 14.571411
                Loss: fixed 147 labels. Loss 0.02107. Accuracy 0.996.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2380646
Train loss (w/o reg) on all data: 0.22997916
Test loss (w/o reg) on all data: 0.1782924
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.1104173e-05
Norm of the params: 12.716485
              Random: fixed  39 labels. Loss 0.17829. Accuracy 0.950.
### Flips: 312, rs: 21, checks: 312
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03714018
Train loss (w/o reg) on all data: 0.029770073
Test loss (w/o reg) on all data: 0.035473146
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.336105e-06
Norm of the params: 12.140929
     Influence (LOO): fixed 147 labels. Loss 0.03547. Accuracy 0.992.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018584102
Train loss (w/o reg) on all data: 0.00913099
Test loss (w/o reg) on all data: 0.017699163
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4843464e-06
Norm of the params: 13.749991
                Loss: fixed 149 labels. Loss 0.01770. Accuracy 0.996.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22354817
Train loss (w/o reg) on all data: 0.21492577
Test loss (w/o reg) on all data: 0.16381525
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1398061e-05
Norm of the params: 13.131949
              Random: fixed  50 labels. Loss 0.16382. Accuracy 0.950.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27331012
Train loss (w/o reg) on all data: 0.26612115
Test loss (w/o reg) on all data: 0.21040292
Train acc on all data:  0.8595988538681948
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.087361e-05
Norm of the params: 11.990805
Flipped loss: 0.21040. Accuracy: 0.939
### Flips: 312, rs: 22, checks: 52
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23373379
Train loss (w/o reg) on all data: 0.2240365
Test loss (w/o reg) on all data: 0.16474926
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1407208e-05
Norm of the params: 13.92644
     Influence (LOO): fixed  30 labels. Loss 0.16475. Accuracy 0.954.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17758097
Train loss (w/o reg) on all data: 0.16493614
Test loss (w/o reg) on all data: 0.14501217
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.3353987e-05
Norm of the params: 15.9027195
                Loss: fixed  52 labels. Loss 0.14501. Accuracy 0.935.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27160966
Train loss (w/o reg) on all data: 0.26476586
Test loss (w/o reg) on all data: 0.19400197
Train acc on all data:  0.8634192932187201
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.8151037e-05
Norm of the params: 11.699397
              Random: fixed   6 labels. Loss 0.19400. Accuracy 0.950.
### Flips: 312, rs: 22, checks: 104
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1920879
Train loss (w/o reg) on all data: 0.18198825
Test loss (w/o reg) on all data: 0.13100757
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3132984e-05
Norm of the params: 14.212425
     Influence (LOO): fixed  58 labels. Loss 0.13101. Accuracy 0.962.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106439754
Train loss (w/o reg) on all data: 0.087997526
Test loss (w/o reg) on all data: 0.09115687
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0104062e-05
Norm of the params: 19.20533
                Loss: fixed  96 labels. Loss 0.09116. Accuracy 0.962.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2676796
Train loss (w/o reg) on all data: 0.26086408
Test loss (w/o reg) on all data: 0.18869136
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.4608744e-05
Norm of the params: 11.675202
              Random: fixed  12 labels. Loss 0.18869. Accuracy 0.947.
### Flips: 312, rs: 22, checks: 156
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1422457
Train loss (w/o reg) on all data: 0.13114926
Test loss (w/o reg) on all data: 0.08023916
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0920951e-05
Norm of the params: 14.897273
     Influence (LOO): fixed  91 labels. Loss 0.08024. Accuracy 0.981.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063718095
Train loss (w/o reg) on all data: 0.046616133
Test loss (w/o reg) on all data: 0.054652426
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1016292e-05
Norm of the params: 18.494303
                Loss: fixed 123 labels. Loss 0.05465. Accuracy 0.981.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26280302
Train loss (w/o reg) on all data: 0.25567862
Test loss (w/o reg) on all data: 0.18300316
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.4728876e-05
Norm of the params: 11.936821
              Random: fixed  18 labels. Loss 0.18300. Accuracy 0.966.
### Flips: 312, rs: 22, checks: 208
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08984319
Train loss (w/o reg) on all data: 0.077940896
Test loss (w/o reg) on all data: 0.06754117
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.7870016e-06
Norm of the params: 15.428736
     Influence (LOO): fixed 120 labels. Loss 0.06754. Accuracy 0.977.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037220366
Train loss (w/o reg) on all data: 0.02301699
Test loss (w/o reg) on all data: 0.018329391
Train acc on all data:  0.9923591212989494
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7836951e-06
Norm of the params: 16.854303
                Loss: fixed 142 labels. Loss 0.01833. Accuracy 1.000.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25278622
Train loss (w/o reg) on all data: 0.2452223
Test loss (w/o reg) on all data: 0.17925264
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.168647e-05
Norm of the params: 12.299532
              Random: fixed  26 labels. Loss 0.17925. Accuracy 0.954.
### Flips: 312, rs: 22, checks: 260
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07323177
Train loss (w/o reg) on all data: 0.062146664
Test loss (w/o reg) on all data: 0.06131607
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.040754e-06
Norm of the params: 14.889668
     Influence (LOO): fixed 132 labels. Loss 0.06132. Accuracy 0.981.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031392846
Train loss (w/o reg) on all data: 0.01818438
Test loss (w/o reg) on all data: 0.0182974
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.5278864e-06
Norm of the params: 16.253286
                Loss: fixed 145 labels. Loss 0.01830. Accuracy 0.996.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24531452
Train loss (w/o reg) on all data: 0.23781787
Test loss (w/o reg) on all data: 0.1608237
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.278744e-05
Norm of the params: 12.244719
              Random: fixed  35 labels. Loss 0.16082. Accuracy 0.977.
### Flips: 312, rs: 22, checks: 312
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050614838
Train loss (w/o reg) on all data: 0.041655205
Test loss (w/o reg) on all data: 0.036906864
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.843223e-06
Norm of the params: 13.386287
     Influence (LOO): fixed 145 labels. Loss 0.03691. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020398822
Train loss (w/o reg) on all data: 0.010138043
Test loss (w/o reg) on all data: 0.011202597
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2291986e-06
Norm of the params: 14.325347
                Loss: fixed 151 labels. Loss 0.01120. Accuracy 0.996.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23006152
Train loss (w/o reg) on all data: 0.22215292
Test loss (w/o reg) on all data: 0.14560704
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5221663e-05
Norm of the params: 12.576646
              Random: fixed  47 labels. Loss 0.14561. Accuracy 0.969.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27119425
Train loss (w/o reg) on all data: 0.2653886
Test loss (w/o reg) on all data: 0.18958023
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0050956e-05
Norm of the params: 10.775564
Flipped loss: 0.18958. Accuracy: 0.973
### Flips: 312, rs: 23, checks: 52
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21034735
Train loss (w/o reg) on all data: 0.20001763
Test loss (w/o reg) on all data: 0.15497357
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3074479e-05
Norm of the params: 14.373395
     Influence (LOO): fixed  33 labels. Loss 0.15497. Accuracy 0.962.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16510257
Train loss (w/o reg) on all data: 0.15249264
Test loss (w/o reg) on all data: 0.11583872
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.873179e-05
Norm of the params: 15.880765
                Loss: fixed  52 labels. Loss 0.11584. Accuracy 0.954.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26682904
Train loss (w/o reg) on all data: 0.26069725
Test loss (w/o reg) on all data: 0.18187045
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6267007e-05
Norm of the params: 11.074122
              Random: fixed   5 labels. Loss 0.18187. Accuracy 0.973.
### Flips: 312, rs: 23, checks: 104
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16969891
Train loss (w/o reg) on all data: 0.15943964
Test loss (w/o reg) on all data: 0.12941301
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.719255e-05
Norm of the params: 14.324297
     Influence (LOO): fixed  61 labels. Loss 0.12941. Accuracy 0.973.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08561134
Train loss (w/o reg) on all data: 0.06831806
Test loss (w/o reg) on all data: 0.077218875
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.928043e-06
Norm of the params: 18.597462
                Loss: fixed  99 labels. Loss 0.07722. Accuracy 0.966.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26215097
Train loss (w/o reg) on all data: 0.25599188
Test loss (w/o reg) on all data: 0.17393258
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1636983e-05
Norm of the params: 11.098736
              Random: fixed  10 labels. Loss 0.17393. Accuracy 0.977.
### Flips: 312, rs: 23, checks: 156
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12307088
Train loss (w/o reg) on all data: 0.111438334
Test loss (w/o reg) on all data: 0.09244371
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5518268e-05
Norm of the params: 15.2529
     Influence (LOO): fixed  85 labels. Loss 0.09244. Accuracy 0.985.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02172833
Train loss (w/o reg) on all data: 0.011980855
Test loss (w/o reg) on all data: 0.03327115
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8100148e-06
Norm of the params: 13.962432
                Loss: fixed 133 labels. Loss 0.03327. Accuracy 0.989.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26026276
Train loss (w/o reg) on all data: 0.25416553
Test loss (w/o reg) on all data: 0.16914168
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.104957e-05
Norm of the params: 11.042844
              Random: fixed  14 labels. Loss 0.16914. Accuracy 0.985.
### Flips: 312, rs: 23, checks: 208
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09204913
Train loss (w/o reg) on all data: 0.0818641
Test loss (w/o reg) on all data: 0.063254006
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.1910273e-06
Norm of the params: 14.2723675
     Influence (LOO): fixed 104 labels. Loss 0.06325. Accuracy 0.985.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016511787
Train loss (w/o reg) on all data: 0.007848874
Test loss (w/o reg) on all data: 0.024987351
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1062647e-06
Norm of the params: 13.16276
                Loss: fixed 139 labels. Loss 0.02499. Accuracy 0.992.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24861135
Train loss (w/o reg) on all data: 0.24253608
Test loss (w/o reg) on all data: 0.15170534
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3092506e-05
Norm of the params: 11.022943
              Random: fixed  28 labels. Loss 0.15171. Accuracy 0.989.
### Flips: 312, rs: 23, checks: 260
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04783302
Train loss (w/o reg) on all data: 0.03858265
Test loss (w/o reg) on all data: 0.033218604
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1768989e-05
Norm of the params: 13.601745
     Influence (LOO): fixed 128 labels. Loss 0.03322. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0095543265
Train loss (w/o reg) on all data: 0.003986993
Test loss (w/o reg) on all data: 0.017192353
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9453386e-07
Norm of the params: 10.552093
                Loss: fixed 144 labels. Loss 0.01719. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23909841
Train loss (w/o reg) on all data: 0.23287447
Test loss (w/o reg) on all data: 0.14585517
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.059204e-05
Norm of the params: 11.157015
              Random: fixed  33 labels. Loss 0.14586. Accuracy 0.992.
### Flips: 312, rs: 23, checks: 312
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034561574
Train loss (w/o reg) on all data: 0.026133321
Test loss (w/o reg) on all data: 0.018966526
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4318365e-06
Norm of the params: 12.98326
     Influence (LOO): fixed 136 labels. Loss 0.01897. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729318
Test loss (w/o reg) on all data: 0.01205575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8296532e-07
Norm of the params: 9.153236
                Loss: fixed 146 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22099176
Train loss (w/o reg) on all data: 0.21452321
Test loss (w/o reg) on all data: 0.12681
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.449757e-05
Norm of the params: 11.37414
              Random: fixed  47 labels. Loss 0.12681. Accuracy 0.996.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27394018
Train loss (w/o reg) on all data: 0.26695755
Test loss (w/o reg) on all data: 0.20352976
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.1927422e-05
Norm of the params: 11.817465
Flipped loss: 0.20353. Accuracy: 0.954
### Flips: 312, rs: 24, checks: 52
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23958029
Train loss (w/o reg) on all data: 0.23091224
Test loss (w/o reg) on all data: 0.17871507
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.841709e-05
Norm of the params: 13.166666
     Influence (LOO): fixed  26 labels. Loss 0.17872. Accuracy 0.947.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18708625
Train loss (w/o reg) on all data: 0.17452018
Test loss (w/o reg) on all data: 0.14481984
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9991907e-05
Norm of the params: 15.853125
                Loss: fixed  49 labels. Loss 0.14482. Accuracy 0.958.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26860604
Train loss (w/o reg) on all data: 0.26176476
Test loss (w/o reg) on all data: 0.20363878
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 7.724888e-05
Norm of the params: 11.697247
              Random: fixed   5 labels. Loss 0.20364. Accuracy 0.954.
### Flips: 312, rs: 24, checks: 104
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1917075
Train loss (w/o reg) on all data: 0.18164243
Test loss (w/o reg) on all data: 0.1391419
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.5223853e-05
Norm of the params: 14.188075
     Influence (LOO): fixed  59 labels. Loss 0.13914. Accuracy 0.962.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11331588
Train loss (w/o reg) on all data: 0.09710183
Test loss (w/o reg) on all data: 0.08669996
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5185607e-05
Norm of the params: 18.007809
                Loss: fixed  96 labels. Loss 0.08670. Accuracy 0.973.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26334924
Train loss (w/o reg) on all data: 0.25679672
Test loss (w/o reg) on all data: 0.18866794
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.111125e-05
Norm of the params: 11.447716
              Random: fixed  15 labels. Loss 0.18867. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 156
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13904478
Train loss (w/o reg) on all data: 0.1296443
Test loss (w/o reg) on all data: 0.10219552
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.885107e-05
Norm of the params: 13.711652
     Influence (LOO): fixed  93 labels. Loss 0.10220. Accuracy 0.962.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056988902
Train loss (w/o reg) on all data: 0.040167887
Test loss (w/o reg) on all data: 0.051070005
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.752809e-06
Norm of the params: 18.341764
                Loss: fixed 127 labels. Loss 0.05107. Accuracy 0.977.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26201108
Train loss (w/o reg) on all data: 0.2556072
Test loss (w/o reg) on all data: 0.18279265
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3460425e-05
Norm of the params: 11.317155
              Random: fixed  19 labels. Loss 0.18279. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 208
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09547663
Train loss (w/o reg) on all data: 0.086697266
Test loss (w/o reg) on all data: 0.06856915
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4840492e-05
Norm of the params: 13.250933
     Influence (LOO): fixed 116 labels. Loss 0.06857. Accuracy 0.969.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03658761
Train loss (w/o reg) on all data: 0.022104876
Test loss (w/o reg) on all data: 0.042587187
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2577904e-06
Norm of the params: 17.019243
                Loss: fixed 140 labels. Loss 0.04259. Accuracy 0.977.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2584615
Train loss (w/o reg) on all data: 0.25199145
Test loss (w/o reg) on all data: 0.17871095
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.6593992e-05
Norm of the params: 11.3754635
              Random: fixed  23 labels. Loss 0.17871. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 260
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06586832
Train loss (w/o reg) on all data: 0.056659084
Test loss (w/o reg) on all data: 0.04770401
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.968828e-06
Norm of the params: 13.571467
     Influence (LOO): fixed 134 labels. Loss 0.04770. Accuracy 0.985.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021255685
Train loss (w/o reg) on all data: 0.009894348
Test loss (w/o reg) on all data: 0.027905416
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7891715e-06
Norm of the params: 15.074042
                Loss: fixed 150 labels. Loss 0.02791. Accuracy 0.985.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2524391
Train loss (w/o reg) on all data: 0.24575336
Test loss (w/o reg) on all data: 0.16930059
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7651531e-05
Norm of the params: 11.563526
              Random: fixed  29 labels. Loss 0.16930. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 312
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027443409
Train loss (w/o reg) on all data: 0.020435235
Test loss (w/o reg) on all data: 0.018997526
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.443987e-06
Norm of the params: 11.839067
     Influence (LOO): fixed 152 labels. Loss 0.01900. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01871147
Train loss (w/o reg) on all data: 0.008355867
Test loss (w/o reg) on all data: 0.025823213
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1671224e-05
Norm of the params: 14.391388
                Loss: fixed 152 labels. Loss 0.02582. Accuracy 0.985.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24799189
Train loss (w/o reg) on all data: 0.242206
Test loss (w/o reg) on all data: 0.15754233
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.4485092e-05
Norm of the params: 10.75721
              Random: fixed  37 labels. Loss 0.15754. Accuracy 0.966.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26719028
Train loss (w/o reg) on all data: 0.25869605
Test loss (w/o reg) on all data: 0.2118175
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3499824e-05
Norm of the params: 13.033981
Flipped loss: 0.21182. Accuracy: 0.954
### Flips: 312, rs: 25, checks: 52
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21224724
Train loss (w/o reg) on all data: 0.20006998
Test loss (w/o reg) on all data: 0.18850173
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.482559e-05
Norm of the params: 15.605938
     Influence (LOO): fixed  32 labels. Loss 0.18850. Accuracy 0.924.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16758412
Train loss (w/o reg) on all data: 0.15397559
Test loss (w/o reg) on all data: 0.15544268
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.1050236e-05
Norm of the params: 16.497595
                Loss: fixed  50 labels. Loss 0.15544. Accuracy 0.927.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25966907
Train loss (w/o reg) on all data: 0.25122553
Test loss (w/o reg) on all data: 0.205494
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3049945e-05
Norm of the params: 12.995034
              Random: fixed   8 labels. Loss 0.20549. Accuracy 0.950.
### Flips: 312, rs: 25, checks: 104
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16722204
Train loss (w/o reg) on all data: 0.15339692
Test loss (w/o reg) on all data: 0.15416436
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1541488e-05
Norm of the params: 16.628363
     Influence (LOO): fixed  58 labels. Loss 0.15416. Accuracy 0.947.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09415404
Train loss (w/o reg) on all data: 0.07932052
Test loss (w/o reg) on all data: 0.12685661
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.441594e-06
Norm of the params: 17.224121
                Loss: fixed  93 labels. Loss 0.12686. Accuracy 0.954.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25533012
Train loss (w/o reg) on all data: 0.24670798
Test loss (w/o reg) on all data: 0.19250543
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 0.00012382297
Norm of the params: 13.131747
              Random: fixed  14 labels. Loss 0.19251. Accuracy 0.962.
### Flips: 312, rs: 25, checks: 156
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13629901
Train loss (w/o reg) on all data: 0.12457016
Test loss (w/o reg) on all data: 0.10519246
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.578536e-06
Norm of the params: 15.3159075
     Influence (LOO): fixed  83 labels. Loss 0.10519. Accuracy 0.966.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053200692
Train loss (w/o reg) on all data: 0.03863104
Test loss (w/o reg) on all data: 0.0843465
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0541604e-05
Norm of the params: 17.070238
                Loss: fixed 122 labels. Loss 0.08435. Accuracy 0.962.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24504231
Train loss (w/o reg) on all data: 0.23631422
Test loss (w/o reg) on all data: 0.1803015
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.291131e-05
Norm of the params: 13.21218
              Random: fixed  23 labels. Loss 0.18030. Accuracy 0.958.
### Flips: 312, rs: 25, checks: 208
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09753152
Train loss (w/o reg) on all data: 0.08474157
Test loss (w/o reg) on all data: 0.0803682
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4532746e-05
Norm of the params: 15.993719
     Influence (LOO): fixed 105 labels. Loss 0.08037. Accuracy 0.977.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026999969
Train loss (w/o reg) on all data: 0.0150254555
Test loss (w/o reg) on all data: 0.06472952
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.1978913e-06
Norm of the params: 15.475474
                Loss: fixed 141 labels. Loss 0.06473. Accuracy 0.977.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24213468
Train loss (w/o reg) on all data: 0.2339162
Test loss (w/o reg) on all data: 0.16026384
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.8186816e-05
Norm of the params: 12.820668
              Random: fixed  31 labels. Loss 0.16026. Accuracy 0.958.
### Flips: 312, rs: 25, checks: 260
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071429424
Train loss (w/o reg) on all data: 0.05864619
Test loss (w/o reg) on all data: 0.05494253
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.08929e-06
Norm of the params: 15.989515
     Influence (LOO): fixed 122 labels. Loss 0.05494. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015457558
Train loss (w/o reg) on all data: 0.0065284297
Test loss (w/o reg) on all data: 0.038514666
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.552038e-06
Norm of the params: 13.36348
                Loss: fixed 146 labels. Loss 0.03851. Accuracy 0.985.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2364456
Train loss (w/o reg) on all data: 0.22818668
Test loss (w/o reg) on all data: 0.15625884
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3928684e-05
Norm of the params: 12.852175
              Random: fixed  36 labels. Loss 0.15626. Accuracy 0.962.
### Flips: 312, rs: 25, checks: 312
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04798367
Train loss (w/o reg) on all data: 0.038872328
Test loss (w/o reg) on all data: 0.028166
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0136194e-06
Norm of the params: 13.499143
     Influence (LOO): fixed 139 labels. Loss 0.02817. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015923407
Train loss (w/o reg) on all data: 0.0069887056
Test loss (w/o reg) on all data: 0.03543625
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.4376115e-07
Norm of the params: 13.367649
                Loss: fixed 147 labels. Loss 0.03544. Accuracy 0.985.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2311272
Train loss (w/o reg) on all data: 0.22335382
Test loss (w/o reg) on all data: 0.15015769
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2931291e-05
Norm of the params: 12.468669
              Random: fixed  43 labels. Loss 0.15016. Accuracy 0.969.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27552545
Train loss (w/o reg) on all data: 0.26826754
Test loss (w/o reg) on all data: 0.20484018
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.1865685e-05
Norm of the params: 12.048152
Flipped loss: 0.20484. Accuracy: 0.947
### Flips: 312, rs: 26, checks: 52
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173457
Train loss (w/o reg) on all data: 0.21063867
Test loss (w/o reg) on all data: 0.1902926
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.3820772e-05
Norm of the params: 14.896909
     Influence (LOO): fixed  33 labels. Loss 0.19029. Accuracy 0.931.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18003061
Train loss (w/o reg) on all data: 0.16529864
Test loss (w/o reg) on all data: 0.1531545
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.6849243e-05
Norm of the params: 17.165066
                Loss: fixed  50 labels. Loss 0.15315. Accuracy 0.947.
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26467866
Train loss (w/o reg) on all data: 0.256796
Test loss (w/o reg) on all data: 0.19430865
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.5959365e-05
Norm of the params: 12.55599
              Random: fixed  12 labels. Loss 0.19431. Accuracy 0.954.
### Flips: 312, rs: 26, checks: 104
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19185084
Train loss (w/o reg) on all data: 0.1802118
Test loss (w/o reg) on all data: 0.1406427
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.3314132e-05
Norm of the params: 15.25716
     Influence (LOO): fixed  59 labels. Loss 0.14064. Accuracy 0.966.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10695931
Train loss (w/o reg) on all data: 0.08956697
Test loss (w/o reg) on all data: 0.09279966
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8371544e-05
Norm of the params: 18.650656
                Loss: fixed  97 labels. Loss 0.09280. Accuracy 0.966.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25851792
Train loss (w/o reg) on all data: 0.2509326
Test loss (w/o reg) on all data: 0.18519136
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2066332e-05
Norm of the params: 12.316904
              Random: fixed  19 labels. Loss 0.18519. Accuracy 0.958.
### Flips: 312, rs: 26, checks: 156
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14181486
Train loss (w/o reg) on all data: 0.12986904
Test loss (w/o reg) on all data: 0.083398975
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9614346e-05
Norm of the params: 15.456914
     Influence (LOO): fixed  90 labels. Loss 0.08340. Accuracy 0.989.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06680851
Train loss (w/o reg) on all data: 0.050235007
Test loss (w/o reg) on all data: 0.06757647
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.8275047e-06
Norm of the params: 18.206318
                Loss: fixed 122 labels. Loss 0.06758. Accuracy 0.977.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2541747
Train loss (w/o reg) on all data: 0.24645238
Test loss (w/o reg) on all data: 0.17667781
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.708619e-05
Norm of the params: 12.427655
              Random: fixed  27 labels. Loss 0.17668. Accuracy 0.969.
### Flips: 312, rs: 26, checks: 208
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12171775
Train loss (w/o reg) on all data: 0.1110847
Test loss (w/o reg) on all data: 0.058687773
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3865836e-05
Norm of the params: 14.582902
     Influence (LOO): fixed 105 labels. Loss 0.05869. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033607528
Train loss (w/o reg) on all data: 0.020801526
Test loss (w/o reg) on all data: 0.05092461
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.774258e-06
Norm of the params: 16.00375
                Loss: fixed 139 labels. Loss 0.05092. Accuracy 0.981.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24563964
Train loss (w/o reg) on all data: 0.23782693
Test loss (w/o reg) on all data: 0.1678947
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2850425e-05
Norm of the params: 12.500169
              Random: fixed  34 labels. Loss 0.16789. Accuracy 0.962.
### Flips: 312, rs: 26, checks: 260
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079213776
Train loss (w/o reg) on all data: 0.06962078
Test loss (w/o reg) on all data: 0.037036255
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3885776e-06
Norm of the params: 13.851351
     Influence (LOO): fixed 130 labels. Loss 0.03704. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017776156
Train loss (w/o reg) on all data: 0.009395122
Test loss (w/o reg) on all data: 0.02311054
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1134532e-06
Norm of the params: 12.946841
                Loss: fixed 152 labels. Loss 0.02311. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23722018
Train loss (w/o reg) on all data: 0.22943847
Test loss (w/o reg) on all data: 0.15102427
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.533573e-05
Norm of the params: 12.475353
              Random: fixed  42 labels. Loss 0.15102. Accuracy 0.969.
### Flips: 312, rs: 26, checks: 312
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04702221
Train loss (w/o reg) on all data: 0.038500473
Test loss (w/o reg) on all data: 0.024556065
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.924663e-06
Norm of the params: 13.055065
     Influence (LOO): fixed 146 labels. Loss 0.02456. Accuracy 0.996.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012467487
Train loss (w/o reg) on all data: 0.0056645726
Test loss (w/o reg) on all data: 0.019856172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8884066e-07
Norm of the params: 11.664403
                Loss: fixed 156 labels. Loss 0.01986. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23178826
Train loss (w/o reg) on all data: 0.22449382
Test loss (w/o reg) on all data: 0.14226612
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1942563e-05
Norm of the params: 12.078449
              Random: fixed  49 labels. Loss 0.14227. Accuracy 0.977.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26100078
Train loss (w/o reg) on all data: 0.25369373
Test loss (w/o reg) on all data: 0.20172699
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.5642144e-05
Norm of the params: 12.088871
Flipped loss: 0.20173. Accuracy: 0.950
### Flips: 312, rs: 27, checks: 52
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21972692
Train loss (w/o reg) on all data: 0.21156679
Test loss (w/o reg) on all data: 0.1561324
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3508534e-05
Norm of the params: 12.775074
     Influence (LOO): fixed  33 labels. Loss 0.15613. Accuracy 0.962.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16290972
Train loss (w/o reg) on all data: 0.1520681
Test loss (w/o reg) on all data: 0.150831
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.50186825e-05
Norm of the params: 14.725235
                Loss: fixed  51 labels. Loss 0.15083. Accuracy 0.931.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2573643
Train loss (w/o reg) on all data: 0.2502381
Test loss (w/o reg) on all data: 0.19341774
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.6298769e-05
Norm of the params: 11.93836
              Random: fixed   5 labels. Loss 0.19342. Accuracy 0.947.
### Flips: 312, rs: 27, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17213382
Train loss (w/o reg) on all data: 0.16316676
Test loss (w/o reg) on all data: 0.12287054
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.9496633e-06
Norm of the params: 13.39183
     Influence (LOO): fixed  65 labels. Loss 0.12287. Accuracy 0.962.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083451346
Train loss (w/o reg) on all data: 0.067398496
Test loss (w/o reg) on all data: 0.09356314
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6030035e-05
Norm of the params: 17.918062
                Loss: fixed  96 labels. Loss 0.09356. Accuracy 0.954.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25369906
Train loss (w/o reg) on all data: 0.24680279
Test loss (w/o reg) on all data: 0.18935928
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.6197583e-05
Norm of the params: 11.744176
              Random: fixed  12 labels. Loss 0.18936. Accuracy 0.954.
### Flips: 312, rs: 27, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11742036
Train loss (w/o reg) on all data: 0.10771074
Test loss (w/o reg) on all data: 0.08871397
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.6918536e-05
Norm of the params: 13.935292
     Influence (LOO): fixed  93 labels. Loss 0.08871. Accuracy 0.962.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035639394
Train loss (w/o reg) on all data: 0.02198383
Test loss (w/o reg) on all data: 0.054771226
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.261875e-06
Norm of the params: 16.52608
                Loss: fixed 126 labels. Loss 0.05477. Accuracy 0.985.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24700458
Train loss (w/o reg) on all data: 0.24039538
Test loss (w/o reg) on all data: 0.18350814
Train acc on all data:  0.87774594078319
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.5958102e-05
Norm of the params: 11.497132
              Random: fixed  18 labels. Loss 0.18351. Accuracy 0.950.
### Flips: 312, rs: 27, checks: 208
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08678789
Train loss (w/o reg) on all data: 0.077910006
Test loss (w/o reg) on all data: 0.065593146
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.148164e-06
Norm of the params: 13.325078
     Influence (LOO): fixed 110 labels. Loss 0.06559. Accuracy 0.981.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020118348
Train loss (w/o reg) on all data: 0.010468096
Test loss (w/o reg) on all data: 0.037271097
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.816829e-06
Norm of the params: 13.892625
                Loss: fixed 138 labels. Loss 0.03727. Accuracy 0.985.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23847179
Train loss (w/o reg) on all data: 0.23197126
Test loss (w/o reg) on all data: 0.16179144
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.201894e-05
Norm of the params: 11.402215
              Random: fixed  27 labels. Loss 0.16179. Accuracy 0.985.
### Flips: 312, rs: 27, checks: 260
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055214144
Train loss (w/o reg) on all data: 0.04591553
Test loss (w/o reg) on all data: 0.05707113
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.014042e-06
Norm of the params: 13.637167
     Influence (LOO): fixed 126 labels. Loss 0.05707. Accuracy 0.981.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016844047
Train loss (w/o reg) on all data: 0.008210963
Test loss (w/o reg) on all data: 0.027728084
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.22501e-07
Norm of the params: 13.1400795
                Loss: fixed 140 labels. Loss 0.02773. Accuracy 0.989.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23445697
Train loss (w/o reg) on all data: 0.22804795
Test loss (w/o reg) on all data: 0.16147391
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1395146e-05
Norm of the params: 11.321676
              Random: fixed  31 labels. Loss 0.16147. Accuracy 0.981.
### Flips: 312, rs: 27, checks: 312
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028732732
Train loss (w/o reg) on all data: 0.01967546
Test loss (w/o reg) on all data: 0.02726478
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4339366e-06
Norm of the params: 13.459029
     Influence (LOO): fixed 138 labels. Loss 0.02726. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012368919
Train loss (w/o reg) on all data: 0.0050371843
Test loss (w/o reg) on all data: 0.019326044
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2825566e-07
Norm of the params: 12.109282
                Loss: fixed 142 labels. Loss 0.01933. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22587663
Train loss (w/o reg) on all data: 0.21924251
Test loss (w/o reg) on all data: 0.16384761
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6417532e-05
Norm of the params: 11.518786
              Random: fixed  36 labels. Loss 0.16385. Accuracy 0.981.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27145502
Train loss (w/o reg) on all data: 0.26310465
Test loss (w/o reg) on all data: 0.22921965
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.239824e-05
Norm of the params: 12.923128
Flipped loss: 0.22922. Accuracy: 0.931
### Flips: 312, rs: 28, checks: 52
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23218052
Train loss (w/o reg) on all data: 0.22264342
Test loss (w/o reg) on all data: 0.19443879
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.6933375e-05
Norm of the params: 13.810937
     Influence (LOO): fixed  29 labels. Loss 0.19444. Accuracy 0.931.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18020815
Train loss (w/o reg) on all data: 0.16702284
Test loss (w/o reg) on all data: 0.19240145
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.8633057e-05
Norm of the params: 16.23903
                Loss: fixed  50 labels. Loss 0.19240. Accuracy 0.931.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26874384
Train loss (w/o reg) on all data: 0.2605381
Test loss (w/o reg) on all data: 0.21760784
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.73818e-05
Norm of the params: 12.8107395
              Random: fixed   5 labels. Loss 0.21761. Accuracy 0.943.
### Flips: 312, rs: 28, checks: 104
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1935703
Train loss (w/o reg) on all data: 0.18372792
Test loss (w/o reg) on all data: 0.15302719
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.769771e-05
Norm of the params: 14.030244
     Influence (LOO): fixed  57 labels. Loss 0.15303. Accuracy 0.958.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10305192
Train loss (w/o reg) on all data: 0.08552262
Test loss (w/o reg) on all data: 0.116338804
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3483603e-05
Norm of the params: 18.723944
                Loss: fixed  96 labels. Loss 0.11634. Accuracy 0.954.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26500562
Train loss (w/o reg) on all data: 0.25656813
Test loss (w/o reg) on all data: 0.20502336
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.4772412e-05
Norm of the params: 12.99036
              Random: fixed   9 labels. Loss 0.20502. Accuracy 0.943.
### Flips: 312, rs: 28, checks: 156
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13762005
Train loss (w/o reg) on all data: 0.12717053
Test loss (w/o reg) on all data: 0.10891942
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8111285e-05
Norm of the params: 14.4565
     Influence (LOO): fixed  93 labels. Loss 0.10892. Accuracy 0.958.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05146474
Train loss (w/o reg) on all data: 0.036916085
Test loss (w/o reg) on all data: 0.060673874
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.9854326e-06
Norm of the params: 17.057934
                Loss: fixed 126 labels. Loss 0.06067. Accuracy 0.977.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2551489
Train loss (w/o reg) on all data: 0.24623097
Test loss (w/o reg) on all data: 0.19674799
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.5231363e-05
Norm of the params: 13.355077
              Random: fixed  18 labels. Loss 0.19675. Accuracy 0.950.
### Flips: 312, rs: 28, checks: 208
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08533534
Train loss (w/o reg) on all data: 0.07341783
Test loss (w/o reg) on all data: 0.08109025
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9526582e-05
Norm of the params: 15.438593
     Influence (LOO): fixed 121 labels. Loss 0.08109. Accuracy 0.966.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036261484
Train loss (w/o reg) on all data: 0.022280397
Test loss (w/o reg) on all data: 0.04828301
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3228124e-06
Norm of the params: 16.721895
                Loss: fixed 135 labels. Loss 0.04828. Accuracy 0.985.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24530125
Train loss (w/o reg) on all data: 0.23638289
Test loss (w/o reg) on all data: 0.18142566
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.692887e-05
Norm of the params: 13.355416
              Random: fixed  28 labels. Loss 0.18143. Accuracy 0.954.
### Flips: 312, rs: 28, checks: 260
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059233226
Train loss (w/o reg) on all data: 0.049290944
Test loss (w/o reg) on all data: 0.05851358
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3804856e-06
Norm of the params: 14.101263
     Influence (LOO): fixed 138 labels. Loss 0.05851. Accuracy 0.985.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028844088
Train loss (w/o reg) on all data: 0.016465846
Test loss (w/o reg) on all data: 0.03834267
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6016924e-05
Norm of the params: 15.734193
                Loss: fixed 141 labels. Loss 0.03834. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23097973
Train loss (w/o reg) on all data: 0.22214857
Test loss (w/o reg) on all data: 0.17094956
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.770243e-05
Norm of the params: 13.289967
              Random: fixed  37 labels. Loss 0.17095. Accuracy 0.954.
### Flips: 312, rs: 28, checks: 312
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038700536
Train loss (w/o reg) on all data: 0.030392634
Test loss (w/o reg) on all data: 0.03762214
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9544132e-06
Norm of the params: 12.890232
     Influence (LOO): fixed 149 labels. Loss 0.03762. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018351387
Train loss (w/o reg) on all data: 0.008847377
Test loss (w/o reg) on all data: 0.009623768
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1345214e-06
Norm of the params: 13.786958
                Loss: fixed 149 labels. Loss 0.00962. Accuracy 1.000.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21825324
Train loss (w/o reg) on all data: 0.20934533
Test loss (w/o reg) on all data: 0.14762765
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.1492952e-05
Norm of the params: 13.347594
              Random: fixed  46 labels. Loss 0.14763. Accuracy 0.966.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26828083
Train loss (w/o reg) on all data: 0.26022384
Test loss (w/o reg) on all data: 0.2321953
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.983978e-05
Norm of the params: 12.69409
Flipped loss: 0.23220. Accuracy: 0.920
### Flips: 312, rs: 29, checks: 52
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22008151
Train loss (w/o reg) on all data: 0.20942546
Test loss (w/o reg) on all data: 0.19499378
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.78586e-05
Norm of the params: 14.598659
     Influence (LOO): fixed  36 labels. Loss 0.19499. Accuracy 0.931.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17152426
Train loss (w/o reg) on all data: 0.15621093
Test loss (w/o reg) on all data: 0.19964069
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 4.7077185e-05
Norm of the params: 17.500475
                Loss: fixed  50 labels. Loss 0.19964. Accuracy 0.916.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25792536
Train loss (w/o reg) on all data: 0.24955238
Test loss (w/o reg) on all data: 0.22097175
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 9.308037e-05
Norm of the params: 12.940609
              Random: fixed  10 labels. Loss 0.22097. Accuracy 0.924.
### Flips: 312, rs: 29, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18685605
Train loss (w/o reg) on all data: 0.17644934
Test loss (w/o reg) on all data: 0.16284397
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.6286918e-05
Norm of the params: 14.426851
     Influence (LOO): fixed  59 labels. Loss 0.16284. Accuracy 0.947.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09006638
Train loss (w/o reg) on all data: 0.07102175
Test loss (w/o reg) on all data: 0.14587599
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0574391e-05
Norm of the params: 19.516472
                Loss: fixed  97 labels. Loss 0.14588. Accuracy 0.935.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25154227
Train loss (w/o reg) on all data: 0.24368636
Test loss (w/o reg) on all data: 0.21020263
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.1433541e-05
Norm of the params: 12.534666
              Random: fixed  19 labels. Loss 0.21020. Accuracy 0.920.
### Flips: 312, rs: 29, checks: 156
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14247611
Train loss (w/o reg) on all data: 0.13116033
Test loss (w/o reg) on all data: 0.124333054
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0280058e-05
Norm of the params: 15.043786
     Influence (LOO): fixed  87 labels. Loss 0.12433. Accuracy 0.966.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04913878
Train loss (w/o reg) on all data: 0.033674445
Test loss (w/o reg) on all data: 0.08149082
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5548463e-06
Norm of the params: 17.58655
                Loss: fixed 124 labels. Loss 0.08149. Accuracy 0.977.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24782076
Train loss (w/o reg) on all data: 0.24019699
Test loss (w/o reg) on all data: 0.19073461
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5223966e-05
Norm of the params: 12.3481045
              Random: fixed  27 labels. Loss 0.19073. Accuracy 0.943.
### Flips: 312, rs: 29, checks: 208
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10442925
Train loss (w/o reg) on all data: 0.09376835
Test loss (w/o reg) on all data: 0.097925164
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4007556e-05
Norm of the params: 14.601989
     Influence (LOO): fixed 113 labels. Loss 0.09793. Accuracy 0.973.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036711775
Train loss (w/o reg) on all data: 0.022190941
Test loss (w/o reg) on all data: 0.043561757
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3154954e-06
Norm of the params: 17.041616
                Loss: fixed 136 labels. Loss 0.04356. Accuracy 0.985.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23597838
Train loss (w/o reg) on all data: 0.22793142
Test loss (w/o reg) on all data: 0.17087416
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.4115524e-05
Norm of the params: 12.686173
              Random: fixed  38 labels. Loss 0.17087. Accuracy 0.958.
### Flips: 312, rs: 29, checks: 260
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06809686
Train loss (w/o reg) on all data: 0.05675465
Test loss (w/o reg) on all data: 0.06915905
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.666871e-06
Norm of the params: 15.061351
     Influence (LOO): fixed 131 labels. Loss 0.06916. Accuracy 0.969.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027044512
Train loss (w/o reg) on all data: 0.014580507
Test loss (w/o reg) on all data: 0.041683786
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.3350356e-06
Norm of the params: 15.788607
                Loss: fixed 143 labels. Loss 0.04168. Accuracy 0.985.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23353904
Train loss (w/o reg) on all data: 0.22563556
Test loss (w/o reg) on all data: 0.1623864
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5960435e-05
Norm of the params: 12.572581
              Random: fixed  42 labels. Loss 0.16239. Accuracy 0.962.
### Flips: 312, rs: 29, checks: 312
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044011652
Train loss (w/o reg) on all data: 0.03602972
Test loss (w/o reg) on all data: 0.03029812
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.3351665e-06
Norm of the params: 12.634822
     Influence (LOO): fixed 148 labels. Loss 0.03030. Accuracy 0.996.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020025883
Train loss (w/o reg) on all data: 0.009433615
Test loss (w/o reg) on all data: 0.04045505
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1736334e-06
Norm of the params: 14.554909
                Loss: fixed 148 labels. Loss 0.04046. Accuracy 0.985.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22847372
Train loss (w/o reg) on all data: 0.22055583
Test loss (w/o reg) on all data: 0.16136871
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.2994772e-05
Norm of the params: 12.584031
              Random: fixed  46 labels. Loss 0.16137. Accuracy 0.958.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25300625
Train loss (w/o reg) on all data: 0.2441728
Test loss (w/o reg) on all data: 0.18110566
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.2233413e-05
Norm of the params: 13.29169
Flipped loss: 0.18111. Accuracy: 0.966
### Flips: 312, rs: 30, checks: 52
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19898817
Train loss (w/o reg) on all data: 0.18903859
Test loss (w/o reg) on all data: 0.15760206
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.1577354e-05
Norm of the params: 14.10644
     Influence (LOO): fixed  33 labels. Loss 0.15760. Accuracy 0.950.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15205728
Train loss (w/o reg) on all data: 0.13713105
Test loss (w/o reg) on all data: 0.117763355
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7219232e-05
Norm of the params: 17.277863
                Loss: fixed  50 labels. Loss 0.11776. Accuracy 0.958.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24590267
Train loss (w/o reg) on all data: 0.23688903
Test loss (w/o reg) on all data: 0.17148873
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0553523e-05
Norm of the params: 13.426568
              Random: fixed   7 labels. Loss 0.17149. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15460101
Train loss (w/o reg) on all data: 0.1430147
Test loss (w/o reg) on all data: 0.12788492
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3487252e-05
Norm of the params: 15.222551
     Influence (LOO): fixed  58 labels. Loss 0.12788. Accuracy 0.966.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086287186
Train loss (w/o reg) on all data: 0.071525216
Test loss (w/o reg) on all data: 0.062195405
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4146547e-05
Norm of the params: 17.182531
                Loss: fixed  91 labels. Loss 0.06220. Accuracy 0.977.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23856741
Train loss (w/o reg) on all data: 0.22918333
Test loss (w/o reg) on all data: 0.16343617
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8836252e-05
Norm of the params: 13.699697
              Random: fixed  13 labels. Loss 0.16344. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10739905
Train loss (w/o reg) on all data: 0.096892394
Test loss (w/o reg) on all data: 0.08484671
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.3025153e-05
Norm of the params: 14.495967
     Influence (LOO): fixed  87 labels. Loss 0.08485. Accuracy 0.969.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03519681
Train loss (w/o reg) on all data: 0.02402616
Test loss (w/o reg) on all data: 0.028553449
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.801465e-06
Norm of the params: 14.947007
                Loss: fixed 120 labels. Loss 0.02855. Accuracy 0.989.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23645143
Train loss (w/o reg) on all data: 0.22692649
Test loss (w/o reg) on all data: 0.16228451
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.848415e-05
Norm of the params: 13.802133
              Random: fixed  17 labels. Loss 0.16228. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 208
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08043206
Train loss (w/o reg) on all data: 0.07045766
Test loss (w/o reg) on all data: 0.06563913
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.2509556e-06
Norm of the params: 14.124021
     Influence (LOO): fixed 104 labels. Loss 0.06564. Accuracy 0.981.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012044645
Train loss (w/o reg) on all data: 0.0052880533
Test loss (w/o reg) on all data: 0.01344372
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.156851e-06
Norm of the params: 11.624621
                Loss: fixed 135 labels. Loss 0.01344. Accuracy 0.992.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23275015
Train loss (w/o reg) on all data: 0.22308467
Test loss (w/o reg) on all data: 0.15991677
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6013086e-05
Norm of the params: 13.903583
              Random: fixed  21 labels. Loss 0.15992. Accuracy 0.973.
### Flips: 312, rs: 30, checks: 260
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04628995
Train loss (w/o reg) on all data: 0.03826727
Test loss (w/o reg) on all data: 0.03462937
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.203239e-06
Norm of the params: 12.667029
     Influence (LOO): fixed 124 labels. Loss 0.03463. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007394776
Train loss (w/o reg) on all data: 0.0026356303
Test loss (w/o reg) on all data: 0.013663706
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8372852e-07
Norm of the params: 9.756173
                Loss: fixed 137 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22667526
Train loss (w/o reg) on all data: 0.217657
Test loss (w/o reg) on all data: 0.15286024
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.335257e-05
Norm of the params: 13.430013
              Random: fixed  26 labels. Loss 0.15286. Accuracy 0.981.
### Flips: 312, rs: 30, checks: 312
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019120185
Train loss (w/o reg) on all data: 0.013724099
Test loss (w/o reg) on all data: 0.019027036
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.028561e-07
Norm of the params: 10.388537
     Influence (LOO): fixed 135 labels. Loss 0.01903. Accuracy 0.996.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073947744
Train loss (w/o reg) on all data: 0.0026356052
Test loss (w/o reg) on all data: 0.013664851
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5017064e-07
Norm of the params: 9.756197
                Loss: fixed 137 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2216321
Train loss (w/o reg) on all data: 0.2127202
Test loss (w/o reg) on all data: 0.14690173
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.307939e-05
Norm of the params: 13.350576
              Random: fixed  30 labels. Loss 0.14690. Accuracy 0.981.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25921082
Train loss (w/o reg) on all data: 0.25111073
Test loss (w/o reg) on all data: 0.18815313
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.419823e-05
Norm of the params: 12.727996
Flipped loss: 0.18815. Accuracy: 0.962
### Flips: 312, rs: 31, checks: 52
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20665652
Train loss (w/o reg) on all data: 0.194081
Test loss (w/o reg) on all data: 0.15468235
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.9813185e-05
Norm of the params: 15.85908
     Influence (LOO): fixed  33 labels. Loss 0.15468. Accuracy 0.958.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15580694
Train loss (w/o reg) on all data: 0.14093034
Test loss (w/o reg) on all data: 0.14353439
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.396968e-05
Norm of the params: 17.249119
                Loss: fixed  49 labels. Loss 0.14353. Accuracy 0.939.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25528374
Train loss (w/o reg) on all data: 0.24730495
Test loss (w/o reg) on all data: 0.17010386
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.762998e-05
Norm of the params: 12.632342
              Random: fixed  11 labels. Loss 0.17010. Accuracy 0.973.
### Flips: 312, rs: 31, checks: 104
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17696041
Train loss (w/o reg) on all data: 0.163804
Test loss (w/o reg) on all data: 0.14793125
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.2365395e-05
Norm of the params: 16.221226
     Influence (LOO): fixed  55 labels. Loss 0.14793. Accuracy 0.977.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090401486
Train loss (w/o reg) on all data: 0.07304416
Test loss (w/o reg) on all data: 0.09841447
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4266182e-05
Norm of the params: 18.631868
                Loss: fixed  91 labels. Loss 0.09841. Accuracy 0.969.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244582
Train loss (w/o reg) on all data: 0.23587176
Test loss (w/o reg) on all data: 0.15955575
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5499586e-05
Norm of the params: 13.198663
              Random: fixed  19 labels. Loss 0.15956. Accuracy 0.973.
### Flips: 312, rs: 31, checks: 156
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12648343
Train loss (w/o reg) on all data: 0.112770006
Test loss (w/o reg) on all data: 0.10314442
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1670513e-05
Norm of the params: 16.561052
     Influence (LOO): fixed  87 labels. Loss 0.10314. Accuracy 0.985.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053971644
Train loss (w/o reg) on all data: 0.038862802
Test loss (w/o reg) on all data: 0.06323284
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.964909e-06
Norm of the params: 17.383236
                Loss: fixed 115 labels. Loss 0.06323. Accuracy 0.973.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239748
Train loss (w/o reg) on all data: 0.23116165
Test loss (w/o reg) on all data: 0.15162335
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9026316e-05
Norm of the params: 13.104462
              Random: fixed  25 labels. Loss 0.15162. Accuracy 0.966.
### Flips: 312, rs: 31, checks: 208
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09178237
Train loss (w/o reg) on all data: 0.080733135
Test loss (w/o reg) on all data: 0.0736057
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6464553e-05
Norm of the params: 14.865553
     Influence (LOO): fixed 112 labels. Loss 0.07361. Accuracy 0.985.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034132365
Train loss (w/o reg) on all data: 0.021486504
Test loss (w/o reg) on all data: 0.033923622
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.251663e-06
Norm of the params: 15.903372
                Loss: fixed 130 labels. Loss 0.03392. Accuracy 0.985.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2323031
Train loss (w/o reg) on all data: 0.22404933
Test loss (w/o reg) on all data: 0.13905393
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.389522e-05
Norm of the params: 12.848171
              Random: fixed  32 labels. Loss 0.13905. Accuracy 0.977.
### Flips: 312, rs: 31, checks: 260
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04823239
Train loss (w/o reg) on all data: 0.038712632
Test loss (w/o reg) on all data: 0.043424394
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.22306e-06
Norm of the params: 13.798376
     Influence (LOO): fixed 134 labels. Loss 0.04342. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021126825
Train loss (w/o reg) on all data: 0.011277441
Test loss (w/o reg) on all data: 0.02688766
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9898062e-06
Norm of the params: 14.035232
                Loss: fixed 140 labels. Loss 0.02689. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720852
Train loss (w/o reg) on all data: 0.21884309
Test loss (w/o reg) on all data: 0.13293113
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5664104e-05
Norm of the params: 12.9347925
              Random: fixed  40 labels. Loss 0.13293. Accuracy 0.977.
### Flips: 312, rs: 31, checks: 312
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026618082
Train loss (w/o reg) on all data: 0.018564539
Test loss (w/o reg) on all data: 0.024602737
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0301076e-06
Norm of the params: 12.69137
     Influence (LOO): fixed 143 labels. Loss 0.02460. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017046787
Train loss (w/o reg) on all data: 0.008458231
Test loss (w/o reg) on all data: 0.021486962
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0933944e-07
Norm of the params: 13.10615
                Loss: fixed 143 labels. Loss 0.02149. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21230944
Train loss (w/o reg) on all data: 0.20415258
Test loss (w/o reg) on all data: 0.12055487
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3908592e-05
Norm of the params: 12.772507
              Random: fixed  49 labels. Loss 0.12055. Accuracy 0.977.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26353464
Train loss (w/o reg) on all data: 0.25725853
Test loss (w/o reg) on all data: 0.19560967
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.711338e-05
Norm of the params: 11.203672
Flipped loss: 0.19561. Accuracy: 0.954
### Flips: 312, rs: 32, checks: 52
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21733359
Train loss (w/o reg) on all data: 0.20811608
Test loss (w/o reg) on all data: 0.1670769
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.192987e-06
Norm of the params: 13.577554
     Influence (LOO): fixed  30 labels. Loss 0.16708. Accuracy 0.962.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16538014
Train loss (w/o reg) on all data: 0.1550429
Test loss (w/o reg) on all data: 0.13697441
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3225206e-05
Norm of the params: 14.378614
                Loss: fixed  50 labels. Loss 0.13697. Accuracy 0.962.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25971183
Train loss (w/o reg) on all data: 0.2534862
Test loss (w/o reg) on all data: 0.18919085
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8594761e-05
Norm of the params: 11.158532
              Random: fixed   5 labels. Loss 0.18919. Accuracy 0.962.
### Flips: 312, rs: 32, checks: 104
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16497643
Train loss (w/o reg) on all data: 0.15485816
Test loss (w/o reg) on all data: 0.12350716
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4527626e-05
Norm of the params: 14.225525
     Influence (LOO): fixed  63 labels. Loss 0.12351. Accuracy 0.969.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085149586
Train loss (w/o reg) on all data: 0.071816094
Test loss (w/o reg) on all data: 0.08217857
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.631238e-06
Norm of the params: 16.330029
                Loss: fixed  94 labels. Loss 0.08218. Accuracy 0.985.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25203612
Train loss (w/o reg) on all data: 0.24586771
Test loss (w/o reg) on all data: 0.17063397
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9298575e-05
Norm of the params: 11.107112
              Random: fixed  13 labels. Loss 0.17063. Accuracy 0.969.
### Flips: 312, rs: 32, checks: 156
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13065054
Train loss (w/o reg) on all data: 0.12054759
Test loss (w/o reg) on all data: 0.088189974
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2828724e-05
Norm of the params: 14.214742
     Influence (LOO): fixed  86 labels. Loss 0.08819. Accuracy 0.981.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030804515
Train loss (w/o reg) on all data: 0.018804548
Test loss (w/o reg) on all data: 0.0550604
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0478028e-06
Norm of the params: 15.491912
                Loss: fixed 125 labels. Loss 0.05506. Accuracy 0.977.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24514142
Train loss (w/o reg) on all data: 0.23907673
Test loss (w/o reg) on all data: 0.15321079
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7873426e-05
Norm of the params: 11.013335
              Random: fixed  23 labels. Loss 0.15321. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 208
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08904383
Train loss (w/o reg) on all data: 0.0804549
Test loss (w/o reg) on all data: 0.050073806
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4796571e-05
Norm of the params: 13.106438
     Influence (LOO): fixed 110 labels. Loss 0.05007. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017785508
Train loss (w/o reg) on all data: 0.009136196
Test loss (w/o reg) on all data: 0.04287595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0527386e-06
Norm of the params: 13.152424
                Loss: fixed 135 labels. Loss 0.04288. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2360417
Train loss (w/o reg) on all data: 0.2295228
Test loss (w/o reg) on all data: 0.1439749
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.469761e-05
Norm of the params: 11.418314
              Random: fixed  33 labels. Loss 0.14397. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 260
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038585946
Train loss (w/o reg) on all data: 0.030035459
Test loss (w/o reg) on all data: 0.03765234
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8054561e-06
Norm of the params: 13.077068
     Influence (LOO): fixed 130 labels. Loss 0.03765. Accuracy 0.985.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015598589
Train loss (w/o reg) on all data: 0.0077930004
Test loss (w/o reg) on all data: 0.03761474
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.505738e-06
Norm of the params: 12.49447
                Loss: fixed 138 labels. Loss 0.03761. Accuracy 0.989.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2280721
Train loss (w/o reg) on all data: 0.2219108
Test loss (w/o reg) on all data: 0.13697748
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.071213e-05
Norm of the params: 11.10073
              Random: fixed  40 labels. Loss 0.13698. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020757684
Train loss (w/o reg) on all data: 0.014751412
Test loss (w/o reg) on all data: 0.023589669
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8640432e-06
Norm of the params: 10.960175
     Influence (LOO): fixed 139 labels. Loss 0.02359. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008834362
Train loss (w/o reg) on all data: 0.003571542
Test loss (w/o reg) on all data: 0.024816968
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.606747e-07
Norm of the params: 10.259455
                Loss: fixed 141 labels. Loss 0.02482. Accuracy 0.989.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21609771
Train loss (w/o reg) on all data: 0.20985065
Test loss (w/o reg) on all data: 0.12402446
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.868765e-05
Norm of the params: 11.177714
              Random: fixed  50 labels. Loss 0.12402. Accuracy 0.981.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27393284
Train loss (w/o reg) on all data: 0.26783457
Test loss (w/o reg) on all data: 0.22614963
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.4342275e-05
Norm of the params: 11.043796
Flipped loss: 0.22615. Accuracy: 0.924
### Flips: 312, rs: 33, checks: 52
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21889892
Train loss (w/o reg) on all data: 0.20969847
Test loss (w/o reg) on all data: 0.184009
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.91415e-05
Norm of the params: 13.564994
     Influence (LOO): fixed  36 labels. Loss 0.18401. Accuracy 0.939.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1647749
Train loss (w/o reg) on all data: 0.15266567
Test loss (w/o reg) on all data: 0.19672488
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.8921935e-05
Norm of the params: 15.56228
                Loss: fixed  51 labels. Loss 0.19672. Accuracy 0.908.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2656025
Train loss (w/o reg) on all data: 0.25963163
Test loss (w/o reg) on all data: 0.2076527
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.3991888e-05
Norm of the params: 10.92783
              Random: fixed  11 labels. Loss 0.20765. Accuracy 0.935.
### Flips: 312, rs: 33, checks: 104
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18432055
Train loss (w/o reg) on all data: 0.17368752
Test loss (w/o reg) on all data: 0.15432188
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.9663814e-05
Norm of the params: 14.582889
     Influence (LOO): fixed  62 labels. Loss 0.15432. Accuracy 0.966.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084844485
Train loss (w/o reg) on all data: 0.0689547
Test loss (w/o reg) on all data: 0.1269475
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.047091e-06
Norm of the params: 17.826826
                Loss: fixed  97 labels. Loss 0.12695. Accuracy 0.950.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25052807
Train loss (w/o reg) on all data: 0.24379933
Test loss (w/o reg) on all data: 0.2027587
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.6477053e-05
Norm of the params: 11.600632
              Random: fixed  22 labels. Loss 0.20276. Accuracy 0.927.
### Flips: 312, rs: 33, checks: 156
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13351375
Train loss (w/o reg) on all data: 0.12184861
Test loss (w/o reg) on all data: 0.09971719
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8897537e-05
Norm of the params: 15.274248
     Influence (LOO): fixed  93 labels. Loss 0.09972. Accuracy 0.985.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04282748
Train loss (w/o reg) on all data: 0.028960686
Test loss (w/o reg) on all data: 0.06518946
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.39931535e-05
Norm of the params: 16.653406
                Loss: fixed 125 labels. Loss 0.06519. Accuracy 0.973.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24208587
Train loss (w/o reg) on all data: 0.23506302
Test loss (w/o reg) on all data: 0.18980603
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.1021103e-05
Norm of the params: 11.851462
              Random: fixed  30 labels. Loss 0.18981. Accuracy 0.935.
### Flips: 312, rs: 33, checks: 208
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08726114
Train loss (w/o reg) on all data: 0.07530249
Test loss (w/o reg) on all data: 0.07820957
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2196221e-05
Norm of the params: 15.465222
     Influence (LOO): fixed 118 labels. Loss 0.07821. Accuracy 0.981.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03059447
Train loss (w/o reg) on all data: 0.019107034
Test loss (w/o reg) on all data: 0.033890426
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9790398e-06
Norm of the params: 15.157465
                Loss: fixed 135 labels. Loss 0.03389. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24015287
Train loss (w/o reg) on all data: 0.23304352
Test loss (w/o reg) on all data: 0.18645789
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.073014e-05
Norm of the params: 11.924208
              Random: fixed  32 labels. Loss 0.18646. Accuracy 0.939.
### Flips: 312, rs: 33, checks: 260
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0421823
Train loss (w/o reg) on all data: 0.032086875
Test loss (w/o reg) on all data: 0.03646232
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0131506e-06
Norm of the params: 14.209451
     Influence (LOO): fixed 138 labels. Loss 0.03646. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014485337
Train loss (w/o reg) on all data: 0.006656959
Test loss (w/o reg) on all data: 0.018547563
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.88932e-07
Norm of the params: 12.512696
                Loss: fixed 144 labels. Loss 0.01855. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2291809
Train loss (w/o reg) on all data: 0.22174439
Test loss (w/o reg) on all data: 0.18045452
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1281333e-05
Norm of the params: 12.1954975
              Random: fixed  39 labels. Loss 0.18045. Accuracy 0.950.
### Flips: 312, rs: 33, checks: 312
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029422648
Train loss (w/o reg) on all data: 0.021050252
Test loss (w/o reg) on all data: 0.023754386
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3433355e-06
Norm of the params: 12.940168
     Influence (LOO): fixed 143 labels. Loss 0.02375. Accuracy 0.992.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012158865
Train loss (w/o reg) on all data: 0.0054603377
Test loss (w/o reg) on all data: 0.01637491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.177316e-07
Norm of the params: 11.574565
                Loss: fixed 146 labels. Loss 0.01637. Accuracy 0.992.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21238963
Train loss (w/o reg) on all data: 0.2045516
Test loss (w/o reg) on all data: 0.15980677
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4812284e-05
Norm of the params: 12.520408
              Random: fixed  50 labels. Loss 0.15981. Accuracy 0.954.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25906318
Train loss (w/o reg) on all data: 0.25044987
Test loss (w/o reg) on all data: 0.22666271
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.7167024e-05
Norm of the params: 13.125021
Flipped loss: 0.22666. Accuracy: 0.931
### Flips: 312, rs: 34, checks: 52
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2091521
Train loss (w/o reg) on all data: 0.19878313
Test loss (w/o reg) on all data: 0.2003221
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.2341966e-05
Norm of the params: 14.40068
     Influence (LOO): fixed  32 labels. Loss 0.20032. Accuracy 0.920.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14824048
Train loss (w/o reg) on all data: 0.13384974
Test loss (w/o reg) on all data: 0.21765634
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 4.2613214e-05
Norm of the params: 16.965103
                Loss: fixed  52 labels. Loss 0.21766. Accuracy 0.912.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2504597
Train loss (w/o reg) on all data: 0.24239565
Test loss (w/o reg) on all data: 0.2032951
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.6336147e-05
Norm of the params: 12.699643
              Random: fixed  11 labels. Loss 0.20330. Accuracy 0.950.
### Flips: 312, rs: 34, checks: 104
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15808056
Train loss (w/o reg) on all data: 0.1461102
Test loss (w/o reg) on all data: 0.17934859
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.1903706e-05
Norm of the params: 15.472786
     Influence (LOO): fixed  59 labels. Loss 0.17935. Accuracy 0.943.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07649095
Train loss (w/o reg) on all data: 0.05918849
Test loss (w/o reg) on all data: 0.1327174
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.2527303e-06
Norm of the params: 18.602402
                Loss: fixed  99 labels. Loss 0.13272. Accuracy 0.931.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242943
Train loss (w/o reg) on all data: 0.234793
Test loss (w/o reg) on all data: 0.19282681
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.5996858e-05
Norm of the params: 12.767143
              Random: fixed  17 labels. Loss 0.19283. Accuracy 0.950.
### Flips: 312, rs: 34, checks: 156
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10991312
Train loss (w/o reg) on all data: 0.09652622
Test loss (w/o reg) on all data: 0.13104968
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.42131585e-05
Norm of the params: 16.3627
     Influence (LOO): fixed  87 labels. Loss 0.13105. Accuracy 0.962.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04300636
Train loss (w/o reg) on all data: 0.027269451
Test loss (w/o reg) on all data: 0.09390531
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.617668e-06
Norm of the params: 17.740864
                Loss: fixed 121 labels. Loss 0.09391. Accuracy 0.958.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23048565
Train loss (w/o reg) on all data: 0.22238354
Test loss (w/o reg) on all data: 0.18096215
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.643696e-05
Norm of the params: 12.729578
              Random: fixed  29 labels. Loss 0.18096. Accuracy 0.962.
### Flips: 312, rs: 34, checks: 208
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06405243
Train loss (w/o reg) on all data: 0.051543094
Test loss (w/o reg) on all data: 0.09106192
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.123224e-06
Norm of the params: 15.817293
     Influence (LOO): fixed 110 labels. Loss 0.09106. Accuracy 0.969.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023087945
Train loss (w/o reg) on all data: 0.012670249
Test loss (w/o reg) on all data: 0.028079787
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8357966e-06
Norm of the params: 14.434469
                Loss: fixed 132 labels. Loss 0.02808. Accuracy 0.989.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2242065
Train loss (w/o reg) on all data: 0.21569249
Test loss (w/o reg) on all data: 0.17130709
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.367459e-05
Norm of the params: 13.049149
              Random: fixed  34 labels. Loss 0.17131. Accuracy 0.962.
### Flips: 312, rs: 34, checks: 260
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04859216
Train loss (w/o reg) on all data: 0.03737765
Test loss (w/o reg) on all data: 0.0684769
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.449058e-06
Norm of the params: 14.97632
     Influence (LOO): fixed 123 labels. Loss 0.06848. Accuracy 0.969.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014835923
Train loss (w/o reg) on all data: 0.00653407
Test loss (w/o reg) on all data: 0.014187271
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.41813e-07
Norm of the params: 12.885537
                Loss: fixed 137 labels. Loss 0.01419. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21689163
Train loss (w/o reg) on all data: 0.208382
Test loss (w/o reg) on all data: 0.16857482
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.593518e-05
Norm of the params: 13.045788
              Random: fixed  38 labels. Loss 0.16857. Accuracy 0.958.
### Flips: 312, rs: 34, checks: 312
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028475383
Train loss (w/o reg) on all data: 0.01890882
Test loss (w/o reg) on all data: 0.031504728
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7813563e-06
Norm of the params: 13.832255
     Influence (LOO): fixed 136 labels. Loss 0.03150. Accuracy 0.989.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012077497
Train loss (w/o reg) on all data: 0.0048378753
Test loss (w/o reg) on all data: 0.015721533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2555544e-06
Norm of the params: 12.032974
                Loss: fixed 139 labels. Loss 0.01572. Accuracy 0.992.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21294482
Train loss (w/o reg) on all data: 0.20460725
Test loss (w/o reg) on all data: 0.15751643
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1804452e-05
Norm of the params: 12.913229
              Random: fixed  44 labels. Loss 0.15752. Accuracy 0.969.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27312553
Train loss (w/o reg) on all data: 0.2651181
Test loss (w/o reg) on all data: 0.26903546
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 0.000103216655
Norm of the params: 12.6549835
Flipped loss: 0.26904. Accuracy: 0.901
### Flips: 312, rs: 35, checks: 52
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2288199
Train loss (w/o reg) on all data: 0.2184965
Test loss (w/o reg) on all data: 0.2137472
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.1338536e-05
Norm of the params: 14.369003
     Influence (LOO): fixed  33 labels. Loss 0.21375. Accuracy 0.916.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17259203
Train loss (w/o reg) on all data: 0.1573421
Test loss (w/o reg) on all data: 0.24994937
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 4.8622e-05
Norm of the params: 17.464207
                Loss: fixed  51 labels. Loss 0.24995. Accuracy 0.905.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26375487
Train loss (w/o reg) on all data: 0.25545698
Test loss (w/o reg) on all data: 0.24288669
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 5.1988263e-05
Norm of the params: 12.882462
              Random: fixed  11 labels. Loss 0.24289. Accuracy 0.905.
### Flips: 312, rs: 35, checks: 104
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18399903
Train loss (w/o reg) on all data: 0.17197376
Test loss (w/o reg) on all data: 0.19419728
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.4882973e-05
Norm of the params: 15.508229
     Influence (LOO): fixed  59 labels. Loss 0.19420. Accuracy 0.927.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111965194
Train loss (w/o reg) on all data: 0.09351009
Test loss (w/o reg) on all data: 0.17961398
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.04844e-05
Norm of the params: 19.212029
                Loss: fixed  87 labels. Loss 0.17961. Accuracy 0.935.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25817624
Train loss (w/o reg) on all data: 0.24974409
Test loss (w/o reg) on all data: 0.23718056
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 5.4184173e-05
Norm of the params: 12.986264
              Random: fixed  19 labels. Loss 0.23718. Accuracy 0.905.
### Flips: 312, rs: 35, checks: 156
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14072207
Train loss (w/o reg) on all data: 0.1279315
Test loss (w/o reg) on all data: 0.12287947
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.7105817e-06
Norm of the params: 15.994103
     Influence (LOO): fixed  89 labels. Loss 0.12288. Accuracy 0.954.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07348781
Train loss (w/o reg) on all data: 0.055332936
Test loss (w/o reg) on all data: 0.14142986
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.5727186e-06
Norm of the params: 19.055115
                Loss: fixed 116 labels. Loss 0.14143. Accuracy 0.954.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25678882
Train loss (w/o reg) on all data: 0.24843605
Test loss (w/o reg) on all data: 0.21905437
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.282268e-05
Norm of the params: 12.924988
              Random: fixed  24 labels. Loss 0.21905. Accuracy 0.927.
### Flips: 312, rs: 35, checks: 208
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102139115
Train loss (w/o reg) on all data: 0.08916999
Test loss (w/o reg) on all data: 0.09003061
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.7433044e-06
Norm of the params: 16.105358
     Influence (LOO): fixed 113 labels. Loss 0.09003. Accuracy 0.981.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055720508
Train loss (w/o reg) on all data: 0.03825158
Test loss (w/o reg) on all data: 0.10998457
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3139709e-05
Norm of the params: 18.691673
                Loss: fixed 131 labels. Loss 0.10998. Accuracy 0.954.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24997345
Train loss (w/o reg) on all data: 0.24156465
Test loss (w/o reg) on all data: 0.1957542
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.010709e-05
Norm of the params: 12.96827
              Random: fixed  34 labels. Loss 0.19575. Accuracy 0.943.
### Flips: 312, rs: 35, checks: 260
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066588715
Train loss (w/o reg) on all data: 0.05436703
Test loss (w/o reg) on all data: 0.05624154
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.8711804e-06
Norm of the params: 15.634379
     Influence (LOO): fixed 133 labels. Loss 0.05624. Accuracy 0.977.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03720852
Train loss (w/o reg) on all data: 0.022019302
Test loss (w/o reg) on all data: 0.055265844
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1387756e-06
Norm of the params: 17.42941
                Loss: fixed 147 labels. Loss 0.05527. Accuracy 0.977.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24014401
Train loss (w/o reg) on all data: 0.23163582
Test loss (w/o reg) on all data: 0.17372924
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.367673e-05
Norm of the params: 13.044683
              Random: fixed  44 labels. Loss 0.17373. Accuracy 0.958.
### Flips: 312, rs: 35, checks: 312
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050017428
Train loss (w/o reg) on all data: 0.03925803
Test loss (w/o reg) on all data: 0.039560225
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5400882e-06
Norm of the params: 14.669286
     Influence (LOO): fixed 148 labels. Loss 0.03956. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031756006
Train loss (w/o reg) on all data: 0.017993608
Test loss (w/o reg) on all data: 0.041096803
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8530284e-06
Norm of the params: 16.5906
                Loss: fixed 153 labels. Loss 0.04110. Accuracy 0.985.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23041515
Train loss (w/o reg) on all data: 0.22255783
Test loss (w/o reg) on all data: 0.15422052
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2168277e-05
Norm of the params: 12.53581
              Random: fixed  55 labels. Loss 0.15422. Accuracy 0.969.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24206604
Train loss (w/o reg) on all data: 0.23374766
Test loss (w/o reg) on all data: 0.18932138
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.322387e-05
Norm of the params: 12.898357
Flipped loss: 0.18932. Accuracy: 0.924
### Flips: 312, rs: 36, checks: 52
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18684569
Train loss (w/o reg) on all data: 0.17581858
Test loss (w/o reg) on all data: 0.16566758
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.332661e-05
Norm of the params: 14.85067
     Influence (LOO): fixed  34 labels. Loss 0.16567. Accuracy 0.931.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14185762
Train loss (w/o reg) on all data: 0.12702696
Test loss (w/o reg) on all data: 0.15541676
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.844515e-05
Norm of the params: 17.222467
                Loss: fixed  48 labels. Loss 0.15542. Accuracy 0.931.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23626584
Train loss (w/o reg) on all data: 0.22740577
Test loss (w/o reg) on all data: 0.18646426
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.7100901e-05
Norm of the params: 13.311696
              Random: fixed   5 labels. Loss 0.18646. Accuracy 0.912.
### Flips: 312, rs: 36, checks: 104
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14319417
Train loss (w/o reg) on all data: 0.13002421
Test loss (w/o reg) on all data: 0.1453435
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 7.413461e-06
Norm of the params: 16.229578
     Influence (LOO): fixed  56 labels. Loss 0.14534. Accuracy 0.931.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08614025
Train loss (w/o reg) on all data: 0.0679949
Test loss (w/o reg) on all data: 0.106968656
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.0358834e-06
Norm of the params: 19.050121
                Loss: fixed  82 labels. Loss 0.10697. Accuracy 0.947.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22745597
Train loss (w/o reg) on all data: 0.21832415
Test loss (w/o reg) on all data: 0.17495377
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.7148267e-05
Norm of the params: 13.5142975
              Random: fixed  14 labels. Loss 0.17495. Accuracy 0.931.
### Flips: 312, rs: 36, checks: 156
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11352204
Train loss (w/o reg) on all data: 0.0992178
Test loss (w/o reg) on all data: 0.11832153
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4592273e-05
Norm of the params: 16.914042
     Influence (LOO): fixed  79 labels. Loss 0.11832. Accuracy 0.954.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049357884
Train loss (w/o reg) on all data: 0.03303286
Test loss (w/o reg) on all data: 0.04635999
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.322156e-06
Norm of the params: 18.069324
                Loss: fixed 109 labels. Loss 0.04636. Accuracy 0.985.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22335358
Train loss (w/o reg) on all data: 0.21423069
Test loss (w/o reg) on all data: 0.16629152
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.76906e-05
Norm of the params: 13.507702
              Random: fixed  20 labels. Loss 0.16629. Accuracy 0.939.
### Flips: 312, rs: 36, checks: 208
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07309538
Train loss (w/o reg) on all data: 0.060755905
Test loss (w/o reg) on all data: 0.078013755
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.628828e-06
Norm of the params: 15.709537
     Influence (LOO): fixed 101 labels. Loss 0.07801. Accuracy 0.962.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029486582
Train loss (w/o reg) on all data: 0.016969305
Test loss (w/o reg) on all data: 0.016853184
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1177823e-06
Norm of the params: 15.822312
                Loss: fixed 127 labels. Loss 0.01685. Accuracy 0.992.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22032323
Train loss (w/o reg) on all data: 0.21140663
Test loss (w/o reg) on all data: 0.15853599
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7808467e-05
Norm of the params: 13.354106
              Random: fixed  25 labels. Loss 0.15854. Accuracy 0.958.
### Flips: 312, rs: 36, checks: 260
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039086
Train loss (w/o reg) on all data: 0.028796205
Test loss (w/o reg) on all data: 0.045703866
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7092239e-06
Norm of the params: 14.345589
     Influence (LOO): fixed 124 labels. Loss 0.04570. Accuracy 0.977.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02335572
Train loss (w/o reg) on all data: 0.012990205
Test loss (w/o reg) on all data: 0.017198749
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3899406e-07
Norm of the params: 14.398275
                Loss: fixed 131 labels. Loss 0.01720. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21077053
Train loss (w/o reg) on all data: 0.20127355
Test loss (w/o reg) on all data: 0.15155964
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.354731e-06
Norm of the params: 13.781866
              Random: fixed  33 labels. Loss 0.15156. Accuracy 0.962.
### Flips: 312, rs: 36, checks: 312
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024882661
Train loss (w/o reg) on all data: 0.016502334
Test loss (w/o reg) on all data: 0.022748904
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2461484e-06
Norm of the params: 12.946295
     Influence (LOO): fixed 131 labels. Loss 0.02275. Accuracy 0.989.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017952392
Train loss (w/o reg) on all data: 0.00892507
Test loss (w/o reg) on all data: 0.012662273
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.419516e-07
Norm of the params: 13.436757
                Loss: fixed 133 labels. Loss 0.01266. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20409998
Train loss (w/o reg) on all data: 0.19437838
Test loss (w/o reg) on all data: 0.14227213
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.5065e-05
Norm of the params: 13.943889
              Random: fixed  38 labels. Loss 0.14227. Accuracy 0.943.
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26621658
Train loss (w/o reg) on all data: 0.25708872
Test loss (w/o reg) on all data: 0.23547995
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.179041e-05
Norm of the params: 13.511358
Flipped loss: 0.23548. Accuracy: 0.924
### Flips: 312, rs: 37, checks: 52
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21856977
Train loss (w/o reg) on all data: 0.20579228
Test loss (w/o reg) on all data: 0.20807087
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 5.1846677e-05
Norm of the params: 15.985923
     Influence (LOO): fixed  31 labels. Loss 0.20807. Accuracy 0.920.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17322515
Train loss (w/o reg) on all data: 0.15764932
Test loss (w/o reg) on all data: 0.19189982
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.0411837e-05
Norm of the params: 17.649828
                Loss: fixed  52 labels. Loss 0.19190. Accuracy 0.939.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25956297
Train loss (w/o reg) on all data: 0.24999359
Test loss (w/o reg) on all data: 0.23115246
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.9593428e-05
Norm of the params: 13.834285
              Random: fixed   5 labels. Loss 0.23115. Accuracy 0.927.
### Flips: 312, rs: 37, checks: 104
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18018948
Train loss (w/o reg) on all data: 0.16593972
Test loss (w/o reg) on all data: 0.17765976
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.3702916e-05
Norm of the params: 16.881798
     Influence (LOO): fixed  57 labels. Loss 0.17766. Accuracy 0.939.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10466866
Train loss (w/o reg) on all data: 0.08512607
Test loss (w/o reg) on all data: 0.13307303
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.9174247e-05
Norm of the params: 19.76997
                Loss: fixed  94 labels. Loss 0.13307. Accuracy 0.943.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25877112
Train loss (w/o reg) on all data: 0.24925959
Test loss (w/o reg) on all data: 0.22391923
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.9353563e-05
Norm of the params: 13.792411
              Random: fixed   9 labels. Loss 0.22392. Accuracy 0.931.
### Flips: 312, rs: 37, checks: 156
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13952674
Train loss (w/o reg) on all data: 0.12657876
Test loss (w/o reg) on all data: 0.14757867
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.437566e-06
Norm of the params: 16.092222
     Influence (LOO): fixed  83 labels. Loss 0.14758. Accuracy 0.950.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06803952
Train loss (w/o reg) on all data: 0.051963832
Test loss (w/o reg) on all data: 0.08574854
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9988086e-06
Norm of the params: 17.930803
                Loss: fixed 122 labels. Loss 0.08575. Accuracy 0.973.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25545704
Train loss (w/o reg) on all data: 0.24602704
Test loss (w/o reg) on all data: 0.21484426
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.141606e-05
Norm of the params: 13.733165
              Random: fixed  13 labels. Loss 0.21484. Accuracy 0.954.
### Flips: 312, rs: 37, checks: 208
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09980068
Train loss (w/o reg) on all data: 0.086965084
Test loss (w/o reg) on all data: 0.10594034
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.30336375e-05
Norm of the params: 16.022236
     Influence (LOO): fixed 105 labels. Loss 0.10594. Accuracy 0.969.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037027426
Train loss (w/o reg) on all data: 0.02380834
Test loss (w/o reg) on all data: 0.047792397
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.148285e-06
Norm of the params: 16.25982
                Loss: fixed 138 labels. Loss 0.04779. Accuracy 0.989.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2504597
Train loss (w/o reg) on all data: 0.24133174
Test loss (w/o reg) on all data: 0.20496024
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8686303e-05
Norm of the params: 13.511453
              Random: fixed  20 labels. Loss 0.20496. Accuracy 0.969.
### Flips: 312, rs: 37, checks: 260
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06872517
Train loss (w/o reg) on all data: 0.057576526
Test loss (w/o reg) on all data: 0.06373808
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2280845e-05
Norm of the params: 14.932277
     Influence (LOO): fixed 127 labels. Loss 0.06374. Accuracy 0.981.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019139756
Train loss (w/o reg) on all data: 0.010250155
Test loss (w/o reg) on all data: 0.029015206
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.3109286e-07
Norm of the params: 13.333867
                Loss: fixed 147 labels. Loss 0.02902. Accuracy 0.989.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2391145
Train loss (w/o reg) on all data: 0.22891092
Test loss (w/o reg) on all data: 0.20157039
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.3489756e-05
Norm of the params: 14.285359
              Random: fixed  28 labels. Loss 0.20157. Accuracy 0.962.
### Flips: 312, rs: 37, checks: 312
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049424663
Train loss (w/o reg) on all data: 0.03948493
Test loss (w/o reg) on all data: 0.044286653
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0020873e-06
Norm of the params: 14.099458
     Influence (LOO): fixed 138 labels. Loss 0.04429. Accuracy 0.985.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01585654
Train loss (w/o reg) on all data: 0.007373234
Test loss (w/o reg) on all data: 0.028746542
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0490927e-06
Norm of the params: 13.025595
                Loss: fixed 149 labels. Loss 0.02875. Accuracy 0.989.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23009574
Train loss (w/o reg) on all data: 0.21950804
Test loss (w/o reg) on all data: 0.19088465
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9955194e-05
Norm of the params: 14.55178
              Random: fixed  37 labels. Loss 0.19088. Accuracy 0.969.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27535054
Train loss (w/o reg) on all data: 0.26703584
Test loss (w/o reg) on all data: 0.21135317
Train acc on all data:  0.8624641833810889
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.593257e-05
Norm of the params: 12.895493
Flipped loss: 0.21135. Accuracy: 0.947
### Flips: 312, rs: 38, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23444366
Train loss (w/o reg) on all data: 0.22312276
Test loss (w/o reg) on all data: 0.17906956
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3362616e-05
Norm of the params: 15.047194
     Influence (LOO): fixed  27 labels. Loss 0.17907. Accuracy 0.947.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18331476
Train loss (w/o reg) on all data: 0.17032948
Test loss (w/o reg) on all data: 0.15355746
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.084818e-05
Norm of the params: 16.115383
                Loss: fixed  48 labels. Loss 0.15356. Accuracy 0.950.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27302715
Train loss (w/o reg) on all data: 0.26461604
Test loss (w/o reg) on all data: 0.20531769
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.7302121e-05
Norm of the params: 12.97005
              Random: fixed   5 labels. Loss 0.20532. Accuracy 0.943.
### Flips: 312, rs: 38, checks: 104
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19305637
Train loss (w/o reg) on all data: 0.18104644
Test loss (w/o reg) on all data: 0.15128234
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.7928865e-05
Norm of the params: 15.498341
     Influence (LOO): fixed  55 labels. Loss 0.15128. Accuracy 0.947.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10349431
Train loss (w/o reg) on all data: 0.088888764
Test loss (w/o reg) on all data: 0.10788036
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.732668e-06
Norm of the params: 17.091253
                Loss: fixed  96 labels. Loss 0.10788. Accuracy 0.962.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27118233
Train loss (w/o reg) on all data: 0.2630044
Test loss (w/o reg) on all data: 0.19570309
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.4353356e-05
Norm of the params: 12.789016
              Random: fixed  10 labels. Loss 0.19570. Accuracy 0.943.
### Flips: 312, rs: 38, checks: 156
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15032364
Train loss (w/o reg) on all data: 0.13807623
Test loss (w/o reg) on all data: 0.12038278
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4437937e-05
Norm of the params: 15.650825
     Influence (LOO): fixed  82 labels. Loss 0.12038. Accuracy 0.966.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063215524
Train loss (w/o reg) on all data: 0.046591677
Test loss (w/o reg) on all data: 0.083864726
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.966114e-05
Norm of the params: 18.233948
                Loss: fixed 125 labels. Loss 0.08386. Accuracy 0.958.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26900208
Train loss (w/o reg) on all data: 0.2606629
Test loss (w/o reg) on all data: 0.19337611
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.1996248e-05
Norm of the params: 12.914452
              Random: fixed  14 labels. Loss 0.19338. Accuracy 0.950.
### Flips: 312, rs: 38, checks: 208
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11492423
Train loss (w/o reg) on all data: 0.100935385
Test loss (w/o reg) on all data: 0.0886816
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0077831e-05
Norm of the params: 16.726534
     Influence (LOO): fixed 105 labels. Loss 0.08868. Accuracy 0.966.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423311
Train loss (w/o reg) on all data: 0.026882432
Test loss (w/o reg) on all data: 0.0585046
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2221243e-05
Norm of the params: 17.577637
                Loss: fixed 137 labels. Loss 0.05850. Accuracy 0.981.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26357305
Train loss (w/o reg) on all data: 0.25468522
Test loss (w/o reg) on all data: 0.17974728
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.7822752e-05
Norm of the params: 13.332541
              Random: fixed  22 labels. Loss 0.17975. Accuracy 0.962.
### Flips: 312, rs: 38, checks: 260
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09279559
Train loss (w/o reg) on all data: 0.08152181
Test loss (w/o reg) on all data: 0.07282621
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6602338e-05
Norm of the params: 15.015845
     Influence (LOO): fixed 122 labels. Loss 0.07283. Accuracy 0.973.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03223722
Train loss (w/o reg) on all data: 0.019667283
Test loss (w/o reg) on all data: 0.036055274
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8501229e-06
Norm of the params: 15.85556
                Loss: fixed 148 labels. Loss 0.03606. Accuracy 0.992.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25837317
Train loss (w/o reg) on all data: 0.24922773
Test loss (w/o reg) on all data: 0.17476204
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.266276e-05
Norm of the params: 13.524378
              Random: fixed  28 labels. Loss 0.17476. Accuracy 0.962.
### Flips: 312, rs: 38, checks: 312
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06411696
Train loss (w/o reg) on all data: 0.054347374
Test loss (w/o reg) on all data: 0.04871726
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.961774e-06
Norm of the params: 13.978259
     Influence (LOO): fixed 139 labels. Loss 0.04872. Accuracy 0.981.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022803694
Train loss (w/o reg) on all data: 0.012000283
Test loss (w/o reg) on all data: 0.027406216
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2984474e-06
Norm of the params: 14.699259
                Loss: fixed 155 labels. Loss 0.02741. Accuracy 0.992.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24876915
Train loss (w/o reg) on all data: 0.24015778
Test loss (w/o reg) on all data: 0.1580767
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.751255e-05
Norm of the params: 13.123539
              Random: fixed  40 labels. Loss 0.15808. Accuracy 0.962.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25811562
Train loss (w/o reg) on all data: 0.2517549
Test loss (w/o reg) on all data: 0.20971018
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.0200461e-05
Norm of the params: 11.278916
Flipped loss: 0.20971. Accuracy: 0.939
### Flips: 312, rs: 39, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20395441
Train loss (w/o reg) on all data: 0.1947361
Test loss (w/o reg) on all data: 0.1709605
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.301465e-05
Norm of the params: 13.5781555
     Influence (LOO): fixed  35 labels. Loss 0.17096. Accuracy 0.947.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15128425
Train loss (w/o reg) on all data: 0.13689622
Test loss (w/o reg) on all data: 0.18451828
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.579731e-06
Norm of the params: 16.963503
                Loss: fixed  49 labels. Loss 0.18452. Accuracy 0.943.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25003862
Train loss (w/o reg) on all data: 0.24342729
Test loss (w/o reg) on all data: 0.18806209
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 0.00010735522
Norm of the params: 11.498974
              Random: fixed  10 labels. Loss 0.18806. Accuracy 0.947.
### Flips: 312, rs: 39, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15558213
Train loss (w/o reg) on all data: 0.14574055
Test loss (w/o reg) on all data: 0.12710442
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1029043e-05
Norm of the params: 14.029663
     Influence (LOO): fixed  66 labels. Loss 0.12710. Accuracy 0.954.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08601816
Train loss (w/o reg) on all data: 0.06870963
Test loss (w/o reg) on all data: 0.14788799
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5332289e-05
Norm of the params: 18.605665
                Loss: fixed  90 labels. Loss 0.14789. Accuracy 0.943.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24547559
Train loss (w/o reg) on all data: 0.23865761
Test loss (w/o reg) on all data: 0.1834282
Train acc on all data:  0.894937917860554
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.11266745e-05
Norm of the params: 11.67731
              Random: fixed  16 labels. Loss 0.18343. Accuracy 0.950.
### Flips: 312, rs: 39, checks: 156
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10737634
Train loss (w/o reg) on all data: 0.09637642
Test loss (w/o reg) on all data: 0.09860962
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 7.996842e-06
Norm of the params: 14.83234
     Influence (LOO): fixed  94 labels. Loss 0.09861. Accuracy 0.947.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048733983
Train loss (w/o reg) on all data: 0.03304313
Test loss (w/o reg) on all data: 0.08432227
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9584206e-06
Norm of the params: 17.714882
                Loss: fixed 118 labels. Loss 0.08432. Accuracy 0.958.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2358301
Train loss (w/o reg) on all data: 0.22932883
Test loss (w/o reg) on all data: 0.15659565
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 0.000100605226
Norm of the params: 11.402869
              Random: fixed  26 labels. Loss 0.15660. Accuracy 0.958.
### Flips: 312, rs: 39, checks: 208
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07969347
Train loss (w/o reg) on all data: 0.06829677
Test loss (w/o reg) on all data: 0.09826684
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.196697e-06
Norm of the params: 15.097485
     Influence (LOO): fixed 114 labels. Loss 0.09827. Accuracy 0.966.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029209413
Train loss (w/o reg) on all data: 0.016898934
Test loss (w/o reg) on all data: 0.037977125
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4831369e-06
Norm of the params: 15.691068
                Loss: fixed 131 labels. Loss 0.03798. Accuracy 0.985.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23076159
Train loss (w/o reg) on all data: 0.22455701
Test loss (w/o reg) on all data: 0.14561869
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.019623e-06
Norm of the params: 11.139636
              Random: fixed  33 labels. Loss 0.14562. Accuracy 0.973.
### Flips: 312, rs: 39, checks: 260
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049315326
Train loss (w/o reg) on all data: 0.03970746
Test loss (w/o reg) on all data: 0.05800393
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.709155e-06
Norm of the params: 13.8620825
     Influence (LOO): fixed 129 labels. Loss 0.05800. Accuracy 0.969.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019803345
Train loss (w/o reg) on all data: 0.009304854
Test loss (w/o reg) on all data: 0.014899698
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.5833693e-07
Norm of the params: 14.490335
                Loss: fixed 136 labels. Loss 0.01490. Accuracy 0.996.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22233637
Train loss (w/o reg) on all data: 0.21604782
Test loss (w/o reg) on all data: 0.13671608
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1421839e-05
Norm of the params: 11.2147665
              Random: fixed  40 labels. Loss 0.13672. Accuracy 0.969.
### Flips: 312, rs: 39, checks: 312
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037199758
Train loss (w/o reg) on all data: 0.027840076
Test loss (w/o reg) on all data: 0.044605605
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.834762e-06
Norm of the params: 13.681873
     Influence (LOO): fixed 138 labels. Loss 0.04461. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019095074
Train loss (w/o reg) on all data: 0.00949415
Test loss (w/o reg) on all data: 0.012837538
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.752906e-06
Norm of the params: 13.857074
                Loss: fixed 140 labels. Loss 0.01284. Accuracy 1.000.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21308446
Train loss (w/o reg) on all data: 0.20656653
Test loss (w/o reg) on all data: 0.13080867
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9432837e-05
Norm of the params: 11.417475
              Random: fixed  50 labels. Loss 0.13081. Accuracy 0.973.
