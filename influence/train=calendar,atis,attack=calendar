nohup: ignoring input
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-07 13:40:51.429341: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-07 13:40:57.644799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 460.12MiB
2018-02-07 13:40:57.644857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Total number of parameters: 359
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6562918e-08
Norm of the params: 6.092818
Orig loss: 0.00266. Accuracy: 1.000
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031635165
Train loss (w/o reg) on all data: 0.02810424
Test loss (w/o reg) on all data: 0.014134563
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2656486e-06
Norm of the params: 8.403481
Flipped loss: 0.01413. Accuracy: 0.999
### Flips: 205, rs: 0, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601031
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2032716e-08
Norm of the params: 6.0928426
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9748251e-08
Norm of the params: 6.0928164
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031339653
Train loss (w/o reg) on all data: 0.027779067
Test loss (w/o reg) on all data: 0.01354386
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8698659e-06
Norm of the params: 8.4387045
              Random: fixed   1 labels. Loss 0.01354. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560877
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.551497e-08
Norm of the params: 6.092806
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601121
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.075104e-08
Norm of the params: 6.092828
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029876215
Train loss (w/o reg) on all data: 0.026420603
Test loss (w/o reg) on all data: 0.012322728
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7267281e-06
Norm of the params: 8.313378
              Random: fixed   3 labels. Loss 0.01232. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096009776
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9924814e-08
Norm of the params: 6.0928516
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.42342e-08
Norm of the params: 6.092814
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02928638
Train loss (w/o reg) on all data: 0.025886508
Test loss (w/o reg) on all data: 0.012392683
Train acc on all data:  0.9927060539752006
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2233878e-06
Norm of the params: 8.2460575
              Random: fixed   4 labels. Loss 0.01239. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600957
Test loss (w/o reg) on all data: 0.0026560368
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.513224e-08
Norm of the params: 6.092855
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6881596e-08
Norm of the params: 6.092809
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026538491
Train loss (w/o reg) on all data: 0.02306414
Test loss (w/o reg) on all data: 0.011377581
Train acc on all data:  0.9934354485776805
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4427272e-06
Norm of the params: 8.335887
              Random: fixed   7 labels. Loss 0.01138. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2222403e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601256
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8106016e-08
Norm of the params: 6.0928054
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026173448
Train loss (w/o reg) on all data: 0.02279522
Test loss (w/o reg) on all data: 0.010818052
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4242029e-06
Norm of the params: 8.219766
              Random: fixed   8 labels. Loss 0.01082. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009600593
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4282959e-07
Norm of the params: 6.0929136
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010906
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6517391e-08
Norm of the params: 6.0928335
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026173448
Train loss (w/o reg) on all data: 0.02279583
Test loss (w/o reg) on all data: 0.010818331
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 5.877988e-07
Norm of the params: 8.219025
              Random: fixed   8 labels. Loss 0.01082. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018427212
Train loss (w/o reg) on all data: 0.013939227
Test loss (w/o reg) on all data: 0.013969123
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2949349e-06
Norm of the params: 9.47416
Flipped loss: 0.01397. Accuracy: 0.998
### Flips: 205, rs: 1, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012733
Test loss (w/o reg) on all data: 0.0026560968
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8483633e-08
Norm of the params: 6.092803
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601069
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5800976e-08
Norm of the params: 6.092836
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018130165
Train loss (w/o reg) on all data: 0.013735349
Test loss (w/o reg) on all data: 0.013235646
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.9124094e-07
Norm of the params: 9.375303
              Random: fixed   1 labels. Loss 0.01324. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3703338e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.298281e-08
Norm of the params: 6.0928116
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016962409
Train loss (w/o reg) on all data: 0.012546253
Test loss (w/o reg) on all data: 0.011877685
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.470992e-07
Norm of the params: 9.398039
              Random: fixed   3 labels. Loss 0.01188. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012343
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5424889e-08
Norm of the params: 6.0928087
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601159
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8513715e-08
Norm of the params: 6.092821
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01696241
Train loss (w/o reg) on all data: 0.012545974
Test loss (w/o reg) on all data: 0.011877291
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.81722e-07
Norm of the params: 9.398337
              Random: fixed   3 labels. Loss 0.01188. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.533866e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8703357e-08
Norm of the params: 6.0928206
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016693696
Train loss (w/o reg) on all data: 0.012341659
Test loss (w/o reg) on all data: 0.011317032
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0498951e-06
Norm of the params: 9.329563
              Random: fixed   4 labels. Loss 0.01132. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9116758e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6050095e-08
Norm of the params: 6.092813
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015620094
Train loss (w/o reg) on all data: 0.011234349
Test loss (w/o reg) on all data: 0.011332432
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3467987e-06
Norm of the params: 9.365623
              Random: fixed   5 labels. Loss 0.01133. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5284251e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.234149e-08
Norm of the params: 6.092803
                Loss: fixed  21 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015620093
Train loss (w/o reg) on all data: 0.011234408
Test loss (w/o reg) on all data: 0.011333342
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7005374e-06
Norm of the params: 9.365559
              Random: fixed   5 labels. Loss 0.01133. Accuracy 0.998.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027075274
Train loss (w/o reg) on all data: 0.023486197
Test loss (w/o reg) on all data: 0.016306287
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2824772e-05
Norm of the params: 8.472398
Flipped loss: 0.01631. Accuracy: 0.996
### Flips: 205, rs: 2, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960068
Test loss (w/o reg) on all data: 0.0026560058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.372972e-08
Norm of the params: 6.092901
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1101823e-08
Norm of the params: 6.092832
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027075272
Train loss (w/o reg) on all data: 0.023486694
Test loss (w/o reg) on all data: 0.016309625
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6381753e-06
Norm of the params: 8.47181
              Random: fixed   0 labels. Loss 0.01631. Accuracy 0.996.
### Flips: 205, rs: 2, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096006715
Test loss (w/o reg) on all data: 0.0026560046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.517015e-08
Norm of the params: 6.0929027
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010976
Test loss (w/o reg) on all data: 0.0026560375
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.143281e-08
Norm of the params: 6.092832
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027075274
Train loss (w/o reg) on all data: 0.023487147
Test loss (w/o reg) on all data: 0.016308673
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1928437e-06
Norm of the params: 8.471276
              Random: fixed   0 labels. Loss 0.01631. Accuracy 0.996.
### Flips: 205, rs: 2, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600651
Test loss (w/o reg) on all data: 0.0026560032
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.7760134e-08
Norm of the params: 6.092905
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010923
Test loss (w/o reg) on all data: 0.002656037
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2228786e-08
Norm of the params: 6.092833
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024010707
Train loss (w/o reg) on all data: 0.020575076
Test loss (w/o reg) on all data: 0.013349561
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1640049e-06
Norm of the params: 8.289307
              Random: fixed   5 labels. Loss 0.01335. Accuracy 0.997.
### Flips: 205, rs: 2, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0250521e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.294317e-09
Norm of the params: 6.0928116
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022743506
Train loss (w/o reg) on all data: 0.019230898
Test loss (w/o reg) on all data: 0.012190325
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9003795e-06
Norm of the params: 8.381657
              Random: fixed   7 labels. Loss 0.01219. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3932716e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.719268e-09
Norm of the params: 6.092811
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02199995
Train loss (w/o reg) on all data: 0.018459244
Test loss (w/o reg) on all data: 0.012038352
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.0286661e-06
Norm of the params: 8.4151125
              Random: fixed   8 labels. Loss 0.01204. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5545638e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9652152e-08
Norm of the params: 6.092812
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0212389
Train loss (w/o reg) on all data: 0.017792933
Test loss (w/o reg) on all data: 0.011607982
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.997319e-07
Norm of the params: 8.301769
              Random: fixed   9 labels. Loss 0.01161. Accuracy 0.998.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029708153
Train loss (w/o reg) on all data: 0.025652826
Test loss (w/o reg) on all data: 0.013333974
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7915405e-06
Norm of the params: 9.005917
Flipped loss: 0.01333. Accuracy: 0.999
### Flips: 205, rs: 3, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5807402e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012605
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.866172e-08
Norm of the params: 6.0928044
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027077679
Train loss (w/o reg) on all data: 0.02318267
Test loss (w/o reg) on all data: 0.01213906
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9037775e-06
Norm of the params: 8.826109
              Random: fixed   3 labels. Loss 0.01214. Accuracy 0.999.
### Flips: 205, rs: 3, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601327
Test loss (w/o reg) on all data: 0.0026560968
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.700935e-08
Norm of the params: 6.092794
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601121
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0041165e-08
Norm of the params: 6.0928283
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025711233
Train loss (w/o reg) on all data: 0.021918843
Test loss (w/o reg) on all data: 0.011361774
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4862247e-06
Norm of the params: 8.7090645
              Random: fixed   5 labels. Loss 0.01136. Accuracy 0.999.
### Flips: 205, rs: 3, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.45501975e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.641154e-09
Norm of the params: 6.0928183
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025711233
Train loss (w/o reg) on all data: 0.021918803
Test loss (w/o reg) on all data: 0.011361377
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3423574e-07
Norm of the params: 8.709111
              Random: fixed   5 labels. Loss 0.01136. Accuracy 0.999.
### Flips: 205, rs: 3, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4526389e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6368e-09
Norm of the params: 6.0928187
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022209007
Train loss (w/o reg) on all data: 0.018245576
Test loss (w/o reg) on all data: 0.010277396
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.569934e-07
Norm of the params: 8.903293
              Random: fixed   9 labels. Loss 0.01028. Accuracy 0.999.
### Flips: 205, rs: 3, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960128
Test loss (w/o reg) on all data: 0.0026561022
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9400441e-08
Norm of the params: 6.0928016
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0868683e-08
Norm of the params: 6.092828
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021689147
Train loss (w/o reg) on all data: 0.017745754
Test loss (w/o reg) on all data: 0.010228201
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.405137e-07
Norm of the params: 8.880758
              Random: fixed  10 labels. Loss 0.01023. Accuracy 0.999.
### Flips: 205, rs: 3, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600995
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0900226e-08
Norm of the params: 6.0928483
     Influence (LOO): fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3831036e-08
Norm of the params: 6.0928125
                Loss: fixed  32 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021689147
Train loss (w/o reg) on all data: 0.017745474
Test loss (w/o reg) on all data: 0.010229143
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.297901e-06
Norm of the params: 8.881074
              Random: fixed  10 labels. Loss 0.01023. Accuracy 0.999.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02722891
Train loss (w/o reg) on all data: 0.023199106
Test loss (w/o reg) on all data: 0.013079468
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.0629943e-06
Norm of the params: 8.977532
Flipped loss: 0.01308. Accuracy: 0.997
### Flips: 205, rs: 4, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1664068e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8714662e-08
Norm of the params: 6.09281
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026448332
Train loss (w/o reg) on all data: 0.022551958
Test loss (w/o reg) on all data: 0.013293039
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.115709e-07
Norm of the params: 8.827656
              Random: fixed   1 labels. Loss 0.01329. Accuracy 0.997.
### Flips: 205, rs: 4, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8197529e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1155906e-08
Norm of the params: 6.0928235
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025888713
Train loss (w/o reg) on all data: 0.021919714
Test loss (w/o reg) on all data: 0.012966853
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2035692e-06
Norm of the params: 8.909544
              Random: fixed   2 labels. Loss 0.01297. Accuracy 0.999.
### Flips: 205, rs: 4, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7645784e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011185
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2352888e-08
Norm of the params: 6.0928288
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025196403
Train loss (w/o reg) on all data: 0.021190794
Test loss (w/o reg) on all data: 0.012900791
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6737592e-06
Norm of the params: 8.950541
              Random: fixed   3 labels. Loss 0.01290. Accuracy 0.999.
### Flips: 205, rs: 4, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012565
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6786599e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2158756e-08
Norm of the params: 6.092829
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024727609
Train loss (w/o reg) on all data: 0.020877328
Test loss (w/o reg) on all data: 0.012905202
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.918385e-06
Norm of the params: 8.775285
              Random: fixed   4 labels. Loss 0.01291. Accuracy 0.997.
### Flips: 205, rs: 4, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3443489e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7426432e-08
Norm of the params: 6.0928235
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024727607
Train loss (w/o reg) on all data: 0.020877441
Test loss (w/o reg) on all data: 0.012905677
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.9807486e-07
Norm of the params: 8.775152
              Random: fixed   4 labels. Loss 0.01291. Accuracy 0.997.
### Flips: 205, rs: 4, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3439987e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7425068e-08
Norm of the params: 6.0928235
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021416672
Train loss (w/o reg) on all data: 0.017562637
Test loss (w/o reg) on all data: 0.01121291
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.6769317e-06
Norm of the params: 8.77956
              Random: fixed   8 labels. Loss 0.01121. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019641228
Train loss (w/o reg) on all data: 0.01594713
Test loss (w/o reg) on all data: 0.011729995
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5768385e-07
Norm of the params: 8.595463
Flipped loss: 0.01173. Accuracy: 0.999
### Flips: 205, rs: 5, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4335291e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8477463e-08
Norm of the params: 6.0928164
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019350002
Train loss (w/o reg) on all data: 0.015586617
Test loss (w/o reg) on all data: 0.011070109
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2394562e-06
Norm of the params: 8.675695
              Random: fixed   1 labels. Loss 0.01107. Accuracy 0.998.
### Flips: 205, rs: 5, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601349
Test loss (w/o reg) on all data: 0.0026560894
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5494998e-08
Norm of the params: 6.09279
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0541705e-08
Norm of the params: 6.0928283
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018239934
Train loss (w/o reg) on all data: 0.01437668
Test loss (w/o reg) on all data: 0.010924494
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4216214e-06
Norm of the params: 8.790055
              Random: fixed   2 labels. Loss 0.01092. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5972063e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2665744e-08
Norm of the params: 6.092819
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017069472
Train loss (w/o reg) on all data: 0.013253778
Test loss (w/o reg) on all data: 0.00981151
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.586363e-07
Norm of the params: 8.735782
              Random: fixed   4 labels. Loss 0.00981. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010853
Test loss (w/o reg) on all data: 0.0026561092
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.212447e-08
Norm of the params: 6.0928345
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.675794e-08
Norm of the params: 6.0928173
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015427925
Train loss (w/o reg) on all data: 0.011625716
Test loss (w/o reg) on all data: 0.008205183
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5914554e-07
Norm of the params: 8.72033
              Random: fixed   6 labels. Loss 0.00821. Accuracy 1.000.
### Flips: 205, rs: 5, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2662825e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.416972e-09
Norm of the params: 6.092816
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015318888
Train loss (w/o reg) on all data: 0.01151822
Test loss (w/o reg) on all data: 0.007568169
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.539068e-07
Norm of the params: 8.718564
              Random: fixed   7 labels. Loss 0.00757. Accuracy 1.000.
### Flips: 205, rs: 5, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601352
Test loss (w/o reg) on all data: 0.002656102
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6015514e-08
Norm of the params: 6.0927896
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4228457e-08
Norm of the params: 6.092826
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013496123
Train loss (w/o reg) on all data: 0.009872942
Test loss (w/o reg) on all data: 0.006978773
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0983858e-07
Norm of the params: 8.512556
              Random: fixed   9 labels. Loss 0.00698. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027459713
Train loss (w/o reg) on all data: 0.02325669
Test loss (w/o reg) on all data: 0.011922192
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 1.140162e-06
Norm of the params: 9.16845
Flipped loss: 0.01192. Accuracy: 1.000
### Flips: 205, rs: 6, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560891
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2996784e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1268492e-08
Norm of the params: 6.0928063
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025221722
Train loss (w/o reg) on all data: 0.021030044
Test loss (w/o reg) on all data: 0.010853168
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7168475e-06
Norm of the params: 9.156067
              Random: fixed   3 labels. Loss 0.01085. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7945e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2853698e-08
Norm of the params: 6.092812
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025221724
Train loss (w/o reg) on all data: 0.021028787
Test loss (w/o reg) on all data: 0.010852719
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9644874e-06
Norm of the params: 9.157442
              Random: fixed   3 labels. Loss 0.01085. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7937222e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.285643e-08
Norm of the params: 6.092812
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0236561
Train loss (w/o reg) on all data: 0.01944333
Test loss (w/o reg) on all data: 0.01070564
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5770194e-06
Norm of the params: 9.179073
              Random: fixed   5 labels. Loss 0.01071. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5949544e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5389046e-08
Norm of the params: 6.0928116
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022612866
Train loss (w/o reg) on all data: 0.018389491
Test loss (w/o reg) on all data: 0.010017065
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 7.7801326e-07
Norm of the params: 9.1906185
              Random: fixed   6 labels. Loss 0.01002. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0438423e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.717234e-09
Norm of the params: 6.092822
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022612866
Train loss (w/o reg) on all data: 0.01838982
Test loss (w/o reg) on all data: 0.010016794
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0602693e-06
Norm of the params: 9.190263
              Random: fixed   6 labels. Loss 0.01002. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.039868e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.718854e-09
Norm of the params: 6.092822
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022174211
Train loss (w/o reg) on all data: 0.017962895
Test loss (w/o reg) on all data: 0.009978137
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1899381e-06
Norm of the params: 9.17749
              Random: fixed   7 labels. Loss 0.00998. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026945513
Train loss (w/o reg) on all data: 0.023050759
Test loss (w/o reg) on all data: 0.013153897
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.0149906e-07
Norm of the params: 8.825819
Flipped loss: 0.01315. Accuracy: 0.999
### Flips: 205, rs: 7, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4955372e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.711722e-09
Norm of the params: 6.092814
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024893802
Train loss (w/o reg) on all data: 0.021107038
Test loss (w/o reg) on all data: 0.0118092755
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2047237e-06
Norm of the params: 8.7026005
              Random: fixed   3 labels. Loss 0.01181. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8440791e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0585027e-08
Norm of the params: 6.092825
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024893798
Train loss (w/o reg) on all data: 0.021106806
Test loss (w/o reg) on all data: 0.011809042
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7730723e-06
Norm of the params: 8.702865
              Random: fixed   3 labels. Loss 0.01181. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8415005e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0991298e-08
Norm of the params: 6.092825
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024893802
Train loss (w/o reg) on all data: 0.02110675
Test loss (w/o reg) on all data: 0.011808854
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.8361037e-07
Norm of the params: 8.702931
              Random: fixed   3 labels. Loss 0.01181. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8389275e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0923625e-08
Norm of the params: 6.092825
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023626357
Train loss (w/o reg) on all data: 0.01988114
Test loss (w/o reg) on all data: 0.011372587
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6232572e-06
Norm of the params: 8.654731
              Random: fixed   5 labels. Loss 0.01137. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3243254e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3491707e-08
Norm of the params: 6.0928082
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021691442
Train loss (w/o reg) on all data: 0.018184379
Test loss (w/o reg) on all data: 0.011651185
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2307602e-06
Norm of the params: 8.375038
              Random: fixed   7 labels. Loss 0.01165. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.050452e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.118449e-09
Norm of the params: 6.092815
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021279395
Train loss (w/o reg) on all data: 0.017736843
Test loss (w/o reg) on all data: 0.010984038
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5730008e-06
Norm of the params: 8.417306
              Random: fixed   8 labels. Loss 0.01098. Accuracy 0.999.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025174523
Train loss (w/o reg) on all data: 0.020846248
Test loss (w/o reg) on all data: 0.013207813
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.507662e-07
Norm of the params: 9.304058
Flipped loss: 0.01321. Accuracy: 0.997
### Flips: 205, rs: 8, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.002656094
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8148952e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5690282e-08
Norm of the params: 6.0928187
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02457838
Train loss (w/o reg) on all data: 0.020224428
Test loss (w/o reg) on all data: 0.012131755
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.857018e-07
Norm of the params: 9.3316145
              Random: fixed   1 labels. Loss 0.01213. Accuracy 0.998.
### Flips: 205, rs: 8, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560952
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6080498e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7790157e-08
Norm of the params: 6.0928283
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024034465
Train loss (w/o reg) on all data: 0.019795194
Test loss (w/o reg) on all data: 0.011808328
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6071384e-06
Norm of the params: 9.207899
              Random: fixed   2 labels. Loss 0.01181. Accuracy 0.998.
### Flips: 205, rs: 8, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0207229e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.860781e-09
Norm of the params: 6.0928216
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02403446
Train loss (w/o reg) on all data: 0.019795256
Test loss (w/o reg) on all data: 0.011807755
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.009614e-06
Norm of the params: 9.207828
              Random: fixed   2 labels. Loss 0.01181. Accuracy 0.998.
### Flips: 205, rs: 8, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0241098e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.85614e-09
Norm of the params: 6.0928216
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023511518
Train loss (w/o reg) on all data: 0.0192757
Test loss (w/o reg) on all data: 0.011628533
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5116387e-06
Norm of the params: 9.204147
              Random: fixed   3 labels. Loss 0.01163. Accuracy 0.998.
### Flips: 205, rs: 8, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011994
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2132595e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6145723e-08
Norm of the params: 6.09282
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021571679
Train loss (w/o reg) on all data: 0.01733712
Test loss (w/o reg) on all data: 0.011132644
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5248023e-06
Norm of the params: 9.202783
              Random: fixed   6 labels. Loss 0.01113. Accuracy 0.997.
### Flips: 205, rs: 8, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601166
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3864284e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2098935e-08
Norm of the params: 6.092814
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021149725
Train loss (w/o reg) on all data: 0.016945222
Test loss (w/o reg) on all data: 0.0109516345
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5309157e-06
Norm of the params: 9.170063
              Random: fixed   7 labels. Loss 0.01095. Accuracy 0.998.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02425889
Train loss (w/o reg) on all data: 0.020147474
Test loss (w/o reg) on all data: 0.012437557
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5272678e-06
Norm of the params: 9.067983
Flipped loss: 0.01244. Accuracy: 0.999
### Flips: 205, rs: 9, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0147263e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7582845e-08
Norm of the params: 6.092819
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02425889
Train loss (w/o reg) on all data: 0.020148214
Test loss (w/o reg) on all data: 0.012437425
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.9018714e-07
Norm of the params: 9.067167
              Random: fixed   0 labels. Loss 0.01244. Accuracy 0.999.
### Flips: 205, rs: 9, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0208685e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7573486e-08
Norm of the params: 6.092819
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02425889
Train loss (w/o reg) on all data: 0.020147791
Test loss (w/o reg) on all data: 0.0124382945
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1127088e-06
Norm of the params: 9.067634
              Random: fixed   0 labels. Loss 0.01244. Accuracy 0.999.
### Flips: 205, rs: 9, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0139762e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7557674e-08
Norm of the params: 6.092819
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024258891
Train loss (w/o reg) on all data: 0.020148229
Test loss (w/o reg) on all data: 0.012436536
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.242103e-06
Norm of the params: 9.067152
              Random: fixed   0 labels. Loss 0.01244. Accuracy 0.999.
### Flips: 205, rs: 9, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0143334e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7576365e-08
Norm of the params: 6.092819
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023660725
Train loss (w/o reg) on all data: 0.019551748
Test loss (w/o reg) on all data: 0.012591675
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0459794e-06
Norm of the params: 9.065292
              Random: fixed   1 labels. Loss 0.01259. Accuracy 0.999.
### Flips: 205, rs: 9, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601298
Test loss (w/o reg) on all data: 0.002656095
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1390978e-08
Norm of the params: 6.092798
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.394255e-09
Norm of the params: 6.092819
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022469824
Train loss (w/o reg) on all data: 0.018478988
Test loss (w/o reg) on all data: 0.011700854
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.007202e-07
Norm of the params: 8.934023
              Random: fixed   3 labels. Loss 0.01170. Accuracy 0.998.
### Flips: 205, rs: 9, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011243
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3140313e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8702142e-08
Norm of the params: 6.0928063
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02078981
Train loss (w/o reg) on all data: 0.016714418
Test loss (w/o reg) on all data: 0.010995595
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.245765e-06
Norm of the params: 9.028169
              Random: fixed   5 labels. Loss 0.01100. Accuracy 0.998.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027048927
Train loss (w/o reg) on all data: 0.023255236
Test loss (w/o reg) on all data: 0.013681923
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.796345e-06
Norm of the params: 8.710559
Flipped loss: 0.01368. Accuracy: 0.999
### Flips: 205, rs: 10, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.036399e-08
Norm of the params: 6.092808
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560402
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7760553e-08
Norm of the params: 6.0928335
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02612819
Train loss (w/o reg) on all data: 0.022194723
Test loss (w/o reg) on all data: 0.013430667
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4793287e-06
Norm of the params: 8.869575
              Random: fixed   1 labels. Loss 0.01343. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096014584
Test loss (w/o reg) on all data: 0.0026561108
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.833885e-08
Norm of the params: 6.092774
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2114532e-08
Norm of the params: 6.092819
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025857026
Train loss (w/o reg) on all data: 0.021970512
Test loss (w/o reg) on all data: 0.012932403
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3748581e-06
Norm of the params: 8.816478
              Random: fixed   2 labels. Loss 0.01293. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560803
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6337028e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4384669e-08
Norm of the params: 6.092824
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02492975
Train loss (w/o reg) on all data: 0.020959372
Test loss (w/o reg) on all data: 0.012299595
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.392268e-06
Norm of the params: 8.911094
              Random: fixed   3 labels. Loss 0.01230. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601434
Test loss (w/o reg) on all data: 0.0026560936
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7502918e-08
Norm of the params: 6.0927753
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4433877e-08
Norm of the params: 6.092816
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023172488
Train loss (w/o reg) on all data: 0.019141858
Test loss (w/o reg) on all data: 0.012187292
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.008842e-07
Norm of the params: 8.978452
              Random: fixed   5 labels. Loss 0.01219. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9972643e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3678095e-08
Norm of the params: 6.092811
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021741834
Train loss (w/o reg) on all data: 0.017625889
Test loss (w/o reg) on all data: 0.010536886
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.153285e-07
Norm of the params: 9.072975
              Random: fixed   7 labels. Loss 0.01054. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560938
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.786569e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560803
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.336101e-08
Norm of the params: 6.092809
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02174183
Train loss (w/o reg) on all data: 0.017626291
Test loss (w/o reg) on all data: 0.010537192
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6052808e-06
Norm of the params: 9.07253
              Random: fixed   7 labels. Loss 0.01054. Accuracy 0.999.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023348073
Train loss (w/o reg) on all data: 0.019103546
Test loss (w/o reg) on all data: 0.011707252
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.3171866e-07
Norm of the params: 9.213606
Flipped loss: 0.01171. Accuracy: 0.997
### Flips: 205, rs: 11, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.615162e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9464048e-08
Norm of the params: 6.0928097
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022289013
Train loss (w/o reg) on all data: 0.018184891
Test loss (w/o reg) on all data: 0.009245473
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.899915e-07
Norm of the params: 9.0599375
              Random: fixed   1 labels. Loss 0.00925. Accuracy 0.999.
### Flips: 205, rs: 11, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560833
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.707194e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.254892e-09
Norm of the params: 6.0928183
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021505984
Train loss (w/o reg) on all data: 0.017469876
Test loss (w/o reg) on all data: 0.00906268
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3768482e-06
Norm of the params: 8.984552
              Random: fixed   2 labels. Loss 0.00906. Accuracy 0.999.
### Flips: 205, rs: 11, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.49672e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960129
Test loss (w/o reg) on all data: 0.002656087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2798144e-08
Norm of the params: 6.0928
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019233618
Train loss (w/o reg) on all data: 0.015504416
Test loss (w/o reg) on all data: 0.008212695
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1159218e-06
Norm of the params: 8.636205
              Random: fixed   5 labels. Loss 0.00821. Accuracy 0.999.
### Flips: 205, rs: 11, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096013397
Test loss (w/o reg) on all data: 0.0026560964
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.703609e-08
Norm of the params: 6.092793
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4717885e-08
Norm of the params: 6.092826
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019233618
Train loss (w/o reg) on all data: 0.015504468
Test loss (w/o reg) on all data: 0.008213318
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9434616e-07
Norm of the params: 8.636145
              Random: fixed   5 labels. Loss 0.00821. Accuracy 0.999.
### Flips: 205, rs: 11, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601341
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6849756e-08
Norm of the params: 6.0927925
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4642042e-08
Norm of the params: 6.0928254
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018104594
Train loss (w/o reg) on all data: 0.014369279
Test loss (w/o reg) on all data: 0.007047653
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 9.654489e-07
Norm of the params: 8.643281
              Random: fixed   6 labels. Loss 0.00705. Accuracy 1.000.
### Flips: 205, rs: 11, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601284
Test loss (w/o reg) on all data: 0.002656107
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.433832e-08
Norm of the params: 6.0928016
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010155
Test loss (w/o reg) on all data: 0.002656034
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9154722e-08
Norm of the params: 6.092845
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017607814
Train loss (w/o reg) on all data: 0.014053125
Test loss (w/o reg) on all data: 0.006847492
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2981868e-07
Norm of the params: 8.431712
              Random: fixed   7 labels. Loss 0.00685. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026746377
Train loss (w/o reg) on all data: 0.022748703
Test loss (w/o reg) on all data: 0.011774068
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.726153e-07
Norm of the params: 8.941672
Flipped loss: 0.01177. Accuracy: 0.999
### Flips: 205, rs: 12, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7314505e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.577801e-09
Norm of the params: 6.0928197
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025796454
Train loss (w/o reg) on all data: 0.021756174
Test loss (w/o reg) on all data: 0.011413307
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.051112e-07
Norm of the params: 8.989194
              Random: fixed   1 labels. Loss 0.01141. Accuracy 0.999.
### Flips: 205, rs: 12, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5459808e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05737685e-08
Norm of the params: 6.092831
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025796456
Train loss (w/o reg) on all data: 0.021755906
Test loss (w/o reg) on all data: 0.011412962
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0919204e-06
Norm of the params: 8.989494
              Random: fixed   1 labels. Loss 0.01141. Accuracy 0.999.
### Flips: 205, rs: 12, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5271196e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601099
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.056516e-08
Norm of the params: 6.092831
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024761142
Train loss (w/o reg) on all data: 0.020653047
Test loss (w/o reg) on all data: 0.011007611
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6746968e-06
Norm of the params: 9.064322
              Random: fixed   3 labels. Loss 0.01101. Accuracy 0.999.
### Flips: 205, rs: 12, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096014
Test loss (w/o reg) on all data: 0.0026560773
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2037665e-08
Norm of the params: 6.0927815
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601298
Test loss (w/o reg) on all data: 0.002656092
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.990549e-08
Norm of the params: 6.092798
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023363594
Train loss (w/o reg) on all data: 0.019112652
Test loss (w/o reg) on all data: 0.010401632
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.0530447e-07
Norm of the params: 9.220566
              Random: fixed   5 labels. Loss 0.01040. Accuracy 0.999.
### Flips: 205, rs: 12, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2322776e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9122495e-09
Norm of the params: 6.092814
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022774218
Train loss (w/o reg) on all data: 0.018558772
Test loss (w/o reg) on all data: 0.009829638
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2603668e-06
Norm of the params: 9.181988
              Random: fixed   6 labels. Loss 0.00983. Accuracy 1.000.
### Flips: 205, rs: 12, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2075025e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.517873e-09
Norm of the params: 6.0928183
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022036064
Train loss (w/o reg) on all data: 0.017833136
Test loss (w/o reg) on all data: 0.009592081
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 7.184027e-07
Norm of the params: 9.168345
              Random: fixed   7 labels. Loss 0.00959. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020842724
Train loss (w/o reg) on all data: 0.016402785
Test loss (w/o reg) on all data: 0.013410192
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.1153146e-07
Norm of the params: 9.423309
Flipped loss: 0.01341. Accuracy: 0.996
### Flips: 205, rs: 13, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.03772555e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2106503e-08
Norm of the params: 6.092819
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020842722
Train loss (w/o reg) on all data: 0.016402934
Test loss (w/o reg) on all data: 0.013408837
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.6338747e-07
Norm of the params: 9.423151
              Random: fixed   0 labels. Loss 0.01341. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0401908e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.21142e-08
Norm of the params: 6.092819
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019524535
Train loss (w/o reg) on all data: 0.0151001755
Test loss (w/o reg) on all data: 0.012957966
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.607055e-07
Norm of the params: 9.406763
              Random: fixed   2 labels. Loss 0.01296. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601137
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.935449e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4625827e-08
Norm of the params: 6.0928135
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0175318
Train loss (w/o reg) on all data: 0.013145932
Test loss (w/o reg) on all data: 0.011982334
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6559583e-06
Norm of the params: 9.365754
              Random: fixed   4 labels. Loss 0.01198. Accuracy 0.997.
### Flips: 205, rs: 13, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5364683e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4457261e-08
Norm of the params: 6.092821
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014734423
Train loss (w/o reg) on all data: 0.010750642
Test loss (w/o reg) on all data: 0.010501967
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.2966384e-07
Norm of the params: 8.92612
              Random: fixed   8 labels. Loss 0.01050. Accuracy 0.997.
### Flips: 205, rs: 13, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.803853e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960124
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6043966e-08
Norm of the params: 6.092809
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014734425
Train loss (w/o reg) on all data: 0.010751243
Test loss (w/o reg) on all data: 0.010501395
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.4701634e-07
Norm of the params: 8.925449
              Random: fixed   8 labels. Loss 0.01050. Accuracy 0.997.
### Flips: 205, rs: 13, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8064755e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6028043e-08
Norm of the params: 6.092809
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014734425
Train loss (w/o reg) on all data: 0.010751224
Test loss (w/o reg) on all data: 0.010501429
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.207277e-07
Norm of the params: 8.925469
              Random: fixed   8 labels. Loss 0.01050. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027661119
Train loss (w/o reg) on all data: 0.023541378
Test loss (w/o reg) on all data: 0.018724214
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.393983e-07
Norm of the params: 9.07716
Flipped loss: 0.01872. Accuracy: 0.993
### Flips: 205, rs: 14, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013193
Test loss (w/o reg) on all data: 0.0026560936
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.876993e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4393116e-08
Norm of the params: 6.0928254
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02578178
Train loss (w/o reg) on all data: 0.021795671
Test loss (w/o reg) on all data: 0.016991502
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.6941217e-07
Norm of the params: 8.928729
              Random: fixed   2 labels. Loss 0.01699. Accuracy 0.994.
### Flips: 205, rs: 14, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601315
Test loss (w/o reg) on all data: 0.002656093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7274936e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.549744e-08
Norm of the params: 6.0928254
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025781784
Train loss (w/o reg) on all data: 0.021796005
Test loss (w/o reg) on all data: 0.016991328
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.2848897e-07
Norm of the params: 8.92836
              Random: fixed   2 labels. Loss 0.01699. Accuracy 0.994.
### Flips: 205, rs: 14, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601315
Test loss (w/o reg) on all data: 0.002656093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7277922e-08
Norm of the params: 6.092796
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5495612e-08
Norm of the params: 6.0928254
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024590857
Train loss (w/o reg) on all data: 0.020611756
Test loss (w/o reg) on all data: 0.01696251
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5458585e-06
Norm of the params: 8.920875
              Random: fixed   3 labels. Loss 0.01696. Accuracy 0.994.
### Flips: 205, rs: 14, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601305
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.728354e-08
Norm of the params: 6.0927978
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011034
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8545018e-08
Norm of the params: 6.09283
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024590861
Train loss (w/o reg) on all data: 0.020611988
Test loss (w/o reg) on all data: 0.016962994
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.6055118e-06
Norm of the params: 8.920619
              Random: fixed   3 labels. Loss 0.01696. Accuracy 0.994.
### Flips: 205, rs: 14, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601305
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7282557e-08
Norm of the params: 6.0927978
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011034
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.85503e-08
Norm of the params: 6.09283
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024095353
Train loss (w/o reg) on all data: 0.02031139
Test loss (w/o reg) on all data: 0.01610712
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.4842027e-07
Norm of the params: 8.699384
              Random: fixed   5 labels. Loss 0.01611. Accuracy 0.995.
### Flips: 205, rs: 14, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601026
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.55067e-08
Norm of the params: 6.092844
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4737303e-08
Norm of the params: 6.092814
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02312091
Train loss (w/o reg) on all data: 0.019360859
Test loss (w/o reg) on all data: 0.015231484
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.3409377e-06
Norm of the params: 8.671852
              Random: fixed   7 labels. Loss 0.01523. Accuracy 0.995.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024723083
Train loss (w/o reg) on all data: 0.020785695
Test loss (w/o reg) on all data: 0.013583446
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2323721e-06
Norm of the params: 8.873994
Flipped loss: 0.01358. Accuracy: 0.995
### Flips: 205, rs: 15, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601084
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2016556e-08
Norm of the params: 6.0928335
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8808054e-08
Norm of the params: 6.0928206
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023258708
Train loss (w/o reg) on all data: 0.0192195
Test loss (w/o reg) on all data: 0.013436563
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.8318034e-07
Norm of the params: 8.988002
              Random: fixed   1 labels. Loss 0.01344. Accuracy 0.995.
### Flips: 205, rs: 15, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.975355e-09
Norm of the params: 6.092817
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02281e-08
Norm of the params: 6.092819
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021176057
Train loss (w/o reg) on all data: 0.017073957
Test loss (w/o reg) on all data: 0.013294754
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.8427416e-07
Norm of the params: 9.057704
              Random: fixed   4 labels. Loss 0.01329. Accuracy 0.995.
### Flips: 205, rs: 15, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601129
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.751693e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601277
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.995497e-08
Norm of the params: 6.0928025
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01943621
Train loss (w/o reg) on all data: 0.015473719
Test loss (w/o reg) on all data: 0.010149439
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1367375e-06
Norm of the params: 8.902238
              Random: fixed   6 labels. Loss 0.01015. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1037278e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1025419e-08
Norm of the params: 6.0928173
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018988397
Train loss (w/o reg) on all data: 0.0150910625
Test loss (w/o reg) on all data: 0.010177778
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.0610477e-07
Norm of the params: 8.828742
              Random: fixed   7 labels. Loss 0.01018. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2323167e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012506
Test loss (w/o reg) on all data: 0.0026560836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3918002e-08
Norm of the params: 6.0928063
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016917545
Train loss (w/o reg) on all data: 0.013017296
Test loss (w/o reg) on all data: 0.009510927
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.8301017e-07
Norm of the params: 8.832044
              Random: fixed   9 labels. Loss 0.00951. Accuracy 0.998.
### Flips: 205, rs: 15, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9549142e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.131796e-08
Norm of the params: 6.092831
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016695976
Train loss (w/o reg) on all data: 0.012883867
Test loss (w/o reg) on all data: 0.009307758
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.2486266e-07
Norm of the params: 8.731677
              Random: fixed  10 labels. Loss 0.00931. Accuracy 0.998.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817923
Train loss (w/o reg) on all data: 0.019798897
Test loss (w/o reg) on all data: 0.01082089
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7156043e-06
Norm of the params: 8.965517
Flipped loss: 0.01082. Accuracy: 0.999
### Flips: 205, rs: 16, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4010521e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02509645e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817923
Train loss (w/o reg) on all data: 0.019798758
Test loss (w/o reg) on all data: 0.010820141
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.467946e-07
Norm of the params: 8.965674
              Random: fixed   0 labels. Loss 0.01082. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4021434e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0264648e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817923
Train loss (w/o reg) on all data: 0.019799098
Test loss (w/o reg) on all data: 0.010820882
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0712063e-06
Norm of the params: 8.965295
              Random: fixed   0 labels. Loss 0.01082. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4003705e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0247442e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817925
Train loss (w/o reg) on all data: 0.019799039
Test loss (w/o reg) on all data: 0.010820296
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.998179e-07
Norm of the params: 8.965362
              Random: fixed   0 labels. Loss 0.01082. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4014931e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0256905e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817923
Train loss (w/o reg) on all data: 0.019798962
Test loss (w/o reg) on all data: 0.010820792
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2797892e-06
Norm of the params: 8.9654455
              Random: fixed   0 labels. Loss 0.01082. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4007082e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0237279e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023817923
Train loss (w/o reg) on all data: 0.019798381
Test loss (w/o reg) on all data: 0.010820561
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.149138e-06
Norm of the params: 8.966094
              Random: fixed   0 labels. Loss 0.01082. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4024976e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02740705e-08
Norm of the params: 6.092823
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023338286
Train loss (w/o reg) on all data: 0.019299114
Test loss (w/o reg) on all data: 0.010695737
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2597558e-06
Norm of the params: 8.987961
              Random: fixed   1 labels. Loss 0.01070. Accuracy 0.999.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023704495
Train loss (w/o reg) on all data: 0.01968623
Test loss (w/o reg) on all data: 0.009910649
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3501542e-06
Norm of the params: 8.964669
Flipped loss: 0.00991. Accuracy: 1.000
### Flips: 205, rs: 17, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096010073
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.894987e-08
Norm of the params: 6.0928473
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4314092e-08
Norm of the params: 6.092813
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022180224
Train loss (w/o reg) on all data: 0.018125761
Test loss (w/o reg) on all data: 0.009400446
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.208347e-07
Norm of the params: 9.004957
              Random: fixed   1 labels. Loss 0.00940. Accuracy 0.999.
### Flips: 205, rs: 17, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6036159e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0085288e-08
Norm of the params: 6.0928173
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021757092
Train loss (w/o reg) on all data: 0.017837537
Test loss (w/o reg) on all data: 0.009250552
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.3099856e-07
Norm of the params: 8.853873
              Random: fixed   2 labels. Loss 0.00925. Accuracy 0.999.
### Flips: 205, rs: 17, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601001
Test loss (w/o reg) on all data: 0.0026560454
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1361598e-08
Norm of the params: 6.092848
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5360731e-08
Norm of the params: 6.0928144
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020139035
Train loss (w/o reg) on all data: 0.01637379
Test loss (w/o reg) on all data: 0.008565068
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1746755e-06
Norm of the params: 8.677838
              Random: fixed   4 labels. Loss 0.00857. Accuracy 0.999.
### Flips: 205, rs: 17, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1115007e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.541466e-09
Norm of the params: 6.092812
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020224577
Train loss (w/o reg) on all data: 0.016548155
Test loss (w/o reg) on all data: 0.008622404
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9019391e-06
Norm of the params: 8.574873
              Random: fixed   5 labels. Loss 0.00862. Accuracy 0.999.
### Flips: 205, rs: 17, checks: 1025
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8759987e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2709832e-08
Norm of the params: 6.092825
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019640775
Train loss (w/o reg) on all data: 0.016121395
Test loss (w/o reg) on all data: 0.008035372
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9567885e-06
Norm of the params: 8.389732
              Random: fixed   6 labels. Loss 0.00804. Accuracy 0.999.
### Flips: 205, rs: 17, checks: 1230
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7030848e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3796616e-08
Norm of the params: 6.0928154
                Loss: fixed  24 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019640774
Train loss (w/o reg) on all data: 0.016121322
Test loss (w/o reg) on all data: 0.008034918
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.938165e-07
Norm of the params: 8.389817
              Random: fixed   6 labels. Loss 0.00803. Accuracy 0.999.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026048154
Train loss (w/o reg) on all data: 0.021960799
Test loss (w/o reg) on all data: 0.015305169
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.605835e-07
Norm of the params: 9.04141
Flipped loss: 0.01531. Accuracy: 0.996
### Flips: 205, rs: 18, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5838489e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2903914e-08
Norm of the params: 6.0928116
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026048154
Train loss (w/o reg) on all data: 0.021961737
Test loss (w/o reg) on all data: 0.015303516
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7019554e-06
Norm of the params: 9.040373
              Random: fixed   0 labels. Loss 0.01530. Accuracy 0.996.
### Flips: 205, rs: 18, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5674431e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.288658e-08
Norm of the params: 6.0928116
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026003763
Train loss (w/o reg) on all data: 0.021858273
Test loss (w/o reg) on all data: 0.014675876
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.560829e-07
Norm of the params: 9.105482
              Random: fixed   1 labels. Loss 0.01468. Accuracy 0.997.
### Flips: 205, rs: 18, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5580891e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1498367e-08
Norm of the params: 6.092809
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02529148
Train loss (w/o reg) on all data: 0.021137962
Test loss (w/o reg) on all data: 0.014682436
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5827462e-06
Norm of the params: 9.114296
              Random: fixed   2 labels. Loss 0.01468. Accuracy 0.997.
### Flips: 205, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.591407e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.251048e-09
Norm of the params: 6.0928187
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023814205
Train loss (w/o reg) on all data: 0.019437062
Test loss (w/o reg) on all data: 0.014247589
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.543474e-06
Norm of the params: 9.356435
              Random: fixed   5 labels. Loss 0.01425. Accuracy 0.997.
### Flips: 205, rs: 18, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3244782e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5347638e-08
Norm of the params: 6.092818
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023131499
Train loss (w/o reg) on all data: 0.019003307
Test loss (w/o reg) on all data: 0.013471247
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.309573e-07
Norm of the params: 9.086464
              Random: fixed   6 labels. Loss 0.01347. Accuracy 0.998.
### Flips: 205, rs: 18, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.905219e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601077
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2588673e-08
Norm of the params: 6.092834
                Loss: fixed  30 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022616556
Train loss (w/o reg) on all data: 0.018497627
Test loss (w/o reg) on all data: 0.013045317
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.8001436e-07
Norm of the params: 9.076263
              Random: fixed   7 labels. Loss 0.01305. Accuracy 0.999.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030055111
Train loss (w/o reg) on all data: 0.025207626
Test loss (w/o reg) on all data: 0.020280251
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9638776e-06
Norm of the params: 9.846305
Flipped loss: 0.02028. Accuracy: 0.995
### Flips: 205, rs: 19, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560856
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3618924e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560465
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7427993e-08
Norm of the params: 6.092829
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028908929
Train loss (w/o reg) on all data: 0.024157459
Test loss (w/o reg) on all data: 0.018921517
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2127201e-06
Norm of the params: 9.748302
              Random: fixed   3 labels. Loss 0.01892. Accuracy 0.995.
### Flips: 205, rs: 19, checks: 410
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.508548e-09
Norm of the params: 6.092822
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.734476e-09
Norm of the params: 6.092817
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02632474
Train loss (w/o reg) on all data: 0.02165479
Test loss (w/o reg) on all data: 0.016958106
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6888094e-06
Norm of the params: 9.664314
              Random: fixed   7 labels. Loss 0.01696. Accuracy 0.995.
### Flips: 205, rs: 19, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3979755e-08
Norm of the params: 6.0928307
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5149225e-08
Norm of the params: 6.0928116
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025425535
Train loss (w/o reg) on all data: 0.020850789
Test loss (w/o reg) on all data: 0.015536708
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.119776e-06
Norm of the params: 9.565297
              Random: fixed   8 labels. Loss 0.01554. Accuracy 0.996.
### Flips: 205, rs: 19, checks: 820
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2130747e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.696158e-09
Norm of the params: 6.0928173
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025425533
Train loss (w/o reg) on all data: 0.020850902
Test loss (w/o reg) on all data: 0.015536837
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4086427e-06
Norm of the params: 9.565178
              Random: fixed   8 labels. Loss 0.01554. Accuracy 0.996.
### Flips: 205, rs: 19, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.239289e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5490247e-09
Norm of the params: 6.0928173
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023710798
Train loss (w/o reg) on all data: 0.01941623
Test loss (w/o reg) on all data: 0.013963906
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4786182e-06
Norm of the params: 9.26776
              Random: fixed  11 labels. Loss 0.01396. Accuracy 0.997.
### Flips: 205, rs: 19, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7246137e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6949564e-08
Norm of the params: 6.092813
                Loss: fixed  34 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023710798
Train loss (w/o reg) on all data: 0.019416539
Test loss (w/o reg) on all data: 0.013962498
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3469108e-06
Norm of the params: 9.2674265
              Random: fixed  11 labels. Loss 0.01396. Accuracy 0.997.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028304894
Train loss (w/o reg) on all data: 0.023605317
Test loss (w/o reg) on all data: 0.012875355
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1287522e-06
Norm of the params: 9.694923
Flipped loss: 0.01288. Accuracy: 0.998
### Flips: 205, rs: 20, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8082758e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.251246e-08
Norm of the params: 6.0928245
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027873771
Train loss (w/o reg) on all data: 0.023260355
Test loss (w/o reg) on all data: 0.012744153
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2439696e-06
Norm of the params: 9.60564
              Random: fixed   1 labels. Loss 0.01274. Accuracy 0.999.
### Flips: 205, rs: 20, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096013
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4142776e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012343
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1069386e-08
Norm of the params: 6.092809
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026225463
Train loss (w/o reg) on all data: 0.021496216
Test loss (w/o reg) on all data: 0.012436221
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.257941e-07
Norm of the params: 9.725477
              Random: fixed   3 labels. Loss 0.01244. Accuracy 0.999.
### Flips: 205, rs: 20, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7844021e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2105628e-08
Norm of the params: 6.0928144
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024692003
Train loss (w/o reg) on all data: 0.019953342
Test loss (w/o reg) on all data: 0.012216628
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0580378e-06
Norm of the params: 9.735154
              Random: fixed   5 labels. Loss 0.01222. Accuracy 0.999.
### Flips: 205, rs: 20, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3588094e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.032462e-09
Norm of the params: 6.092824
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024037795
Train loss (w/o reg) on all data: 0.019412542
Test loss (w/o reg) on all data: 0.0118859075
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5106104e-06
Norm of the params: 9.617956
              Random: fixed   7 labels. Loss 0.01189. Accuracy 0.999.
### Flips: 205, rs: 20, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.089249e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9723295e-09
Norm of the params: 6.0928206
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022737466
Train loss (w/o reg) on all data: 0.018453429
Test loss (w/o reg) on all data: 0.011457433
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4316935e-06
Norm of the params: 9.256391
              Random: fixed   9 labels. Loss 0.01146. Accuracy 0.998.
### Flips: 205, rs: 20, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009600902
Test loss (w/o reg) on all data: 0.002655996
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5610844e-08
Norm of the params: 6.092863
     Influence (LOO): fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.00265605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5994948e-08
Norm of the params: 6.0928183
                Loss: fixed  33 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020326447
Train loss (w/o reg) on all data: 0.015937462
Test loss (w/o reg) on all data: 0.010969966
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.030397e-07
Norm of the params: 9.369083
              Random: fixed  11 labels. Loss 0.01097. Accuracy 0.998.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026661098
Train loss (w/o reg) on all data: 0.023022737
Test loss (w/o reg) on all data: 0.01443636
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4023257e-06
Norm of the params: 8.53037
Flipped loss: 0.01444. Accuracy: 0.997
### Flips: 205, rs: 21, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601257
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2706113e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1079996e-08
Norm of the params: 6.0928173
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025503056
Train loss (w/o reg) on all data: 0.021865038
Test loss (w/o reg) on all data: 0.014039621
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.3968804e-07
Norm of the params: 8.529968
              Random: fixed   2 labels. Loss 0.01404. Accuracy 0.996.
### Flips: 205, rs: 21, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8104137e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6772233e-08
Norm of the params: 6.0928082
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025503054
Train loss (w/o reg) on all data: 0.021864895
Test loss (w/o reg) on all data: 0.014039968
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.5012e-07
Norm of the params: 8.530133
              Random: fixed   2 labels. Loss 0.01404. Accuracy 0.996.
### Flips: 205, rs: 21, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.81097e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6781131e-08
Norm of the params: 6.0928082
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024078686
Train loss (w/o reg) on all data: 0.02048264
Test loss (w/o reg) on all data: 0.0131894
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.227175e-07
Norm of the params: 8.480619
              Random: fixed   5 labels. Loss 0.01319. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960152
Test loss (w/o reg) on all data: 0.0026561134
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.836116e-08
Norm of the params: 6.092763
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.753938e-08
Norm of the params: 6.0928183
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022814488
Train loss (w/o reg) on all data: 0.019367736
Test loss (w/o reg) on all data: 0.012354839
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3813124e-06
Norm of the params: 8.302713
              Random: fixed   7 labels. Loss 0.01235. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096014567
Test loss (w/o reg) on all data: 0.0026561012
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.285427e-08
Norm of the params: 6.092773
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9960764e-08
Norm of the params: 6.092812
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020874437
Train loss (w/o reg) on all data: 0.017471625
Test loss (w/o reg) on all data: 0.011032636
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.747313e-07
Norm of the params: 8.24962
              Random: fixed   9 labels. Loss 0.01103. Accuracy 0.999.
### Flips: 205, rs: 21, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6775758e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.708666e-09
Norm of the params: 6.09282
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020726416
Train loss (w/o reg) on all data: 0.017328978
Test loss (w/o reg) on all data: 0.010611126
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9224352e-06
Norm of the params: 8.243105
              Random: fixed  10 labels. Loss 0.01061. Accuracy 0.999.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025917973
Train loss (w/o reg) on all data: 0.02168639
Test loss (w/o reg) on all data: 0.012709656
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2169481e-06
Norm of the params: 9.199547
Flipped loss: 0.01271. Accuracy: 0.998
### Flips: 205, rs: 22, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601072
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.798776e-08
Norm of the params: 6.092836
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012524
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1287029e-08
Norm of the params: 6.0928054
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024545364
Train loss (w/o reg) on all data: 0.0203676
Test loss (w/o reg) on all data: 0.011963413
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7683377e-06
Norm of the params: 9.140859
              Random: fixed   2 labels. Loss 0.01196. Accuracy 0.998.
### Flips: 205, rs: 22, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6596745e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3204972e-08
Norm of the params: 6.0928173
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023782488
Train loss (w/o reg) on all data: 0.019493539
Test loss (w/o reg) on all data: 0.0118040955
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5798026e-06
Norm of the params: 9.261695
              Random: fixed   3 labels. Loss 0.01180. Accuracy 0.998.
### Flips: 205, rs: 22, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.38288785e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.403911e-09
Norm of the params: 6.0928226
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023365136
Train loss (w/o reg) on all data: 0.019030891
Test loss (w/o reg) on all data: 0.0109971985
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1638778e-06
Norm of the params: 9.3104725
              Random: fixed   4 labels. Loss 0.01100. Accuracy 0.998.
### Flips: 205, rs: 22, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013054
Test loss (w/o reg) on all data: 0.002656094
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1993282e-08
Norm of the params: 6.0927978
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2900261e-08
Norm of the params: 6.0928216
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022265954
Train loss (w/o reg) on all data: 0.018019643
Test loss (w/o reg) on all data: 0.010714601
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5950556e-06
Norm of the params: 9.215543
              Random: fixed   6 labels. Loss 0.01071. Accuracy 0.998.
### Flips: 205, rs: 22, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7544235e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.379188e-09
Norm of the params: 6.0928197
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022265954
Train loss (w/o reg) on all data: 0.01801929
Test loss (w/o reg) on all data: 0.0107140625
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6354375e-06
Norm of the params: 9.215926
              Random: fixed   6 labels. Loss 0.01071. Accuracy 0.998.
### Flips: 205, rs: 22, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.759469e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.434322e-09
Norm of the params: 6.0928197
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021223694
Train loss (w/o reg) on all data: 0.01703651
Test loss (w/o reg) on all data: 0.009878264
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9483393e-06
Norm of the params: 9.151157
              Random: fixed   8 labels. Loss 0.00988. Accuracy 0.998.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025638636
Train loss (w/o reg) on all data: 0.021378845
Test loss (w/o reg) on all data: 0.010980295
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0492439e-06
Norm of the params: 9.23016
Flipped loss: 0.01098. Accuracy: 0.999
### Flips: 205, rs: 23, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012815
Test loss (w/o reg) on all data: 0.0026560877
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1395385e-08
Norm of the params: 6.0928016
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601122
Test loss (w/o reg) on all data: 0.002656051
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4098667e-08
Norm of the params: 6.092828
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023885608
Train loss (w/o reg) on all data: 0.019692505
Test loss (w/o reg) on all data: 0.010278305
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2998136e-06
Norm of the params: 9.157623
              Random: fixed   2 labels. Loss 0.01028. Accuracy 0.999.
### Flips: 205, rs: 23, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601123
Test loss (w/o reg) on all data: 0.0026560451
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3293277e-08
Norm of the params: 6.092828
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2579421e-08
Norm of the params: 6.092819
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022687234
Train loss (w/o reg) on all data: 0.018587831
Test loss (w/o reg) on all data: 0.009565048
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8490075e-06
Norm of the params: 9.054726
              Random: fixed   4 labels. Loss 0.00957. Accuracy 0.999.
### Flips: 205, rs: 23, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600894
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6484994e-08
Norm of the params: 6.092865
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1624108e-08
Norm of the params: 6.09282
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021723399
Train loss (w/o reg) on all data: 0.017447742
Test loss (w/o reg) on all data: 0.009373361
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.5500045e-07
Norm of the params: 9.247332
              Random: fixed   5 labels. Loss 0.00937. Accuracy 0.999.
### Flips: 205, rs: 23, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601166
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7251747e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7154726e-08
Norm of the params: 6.092812
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021723397
Train loss (w/o reg) on all data: 0.017446818
Test loss (w/o reg) on all data: 0.00937375
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5236525e-06
Norm of the params: 9.24833
              Random: fixed   5 labels. Loss 0.00937. Accuracy 0.999.
### Flips: 205, rs: 23, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7278719e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.723637e-08
Norm of the params: 6.092812
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021723397
Train loss (w/o reg) on all data: 0.017447494
Test loss (w/o reg) on all data: 0.009373389
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.84254e-07
Norm of the params: 9.247599
              Random: fixed   5 labels. Loss 0.00937. Accuracy 0.999.
### Flips: 205, rs: 23, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7249464e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7164156e-08
Norm of the params: 6.092812
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0211378
Train loss (w/o reg) on all data: 0.016828751
Test loss (w/o reg) on all data: 0.0091711925
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1102704e-06
Norm of the params: 9.28337
              Random: fixed   6 labels. Loss 0.00917. Accuracy 0.999.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024007097
Train loss (w/o reg) on all data: 0.019900795
Test loss (w/o reg) on all data: 0.011685213
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1178766e-06
Norm of the params: 9.062342
Flipped loss: 0.01169. Accuracy: 1.000
### Flips: 205, rs: 24, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4592626e-08
Norm of the params: 6.092806
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.422018e-08
Norm of the params: 6.0928197
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023496848
Train loss (w/o reg) on all data: 0.019511016
Test loss (w/o reg) on all data: 0.011011827
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3363946e-06
Norm of the params: 8.928417
              Random: fixed   1 labels. Loss 0.01101. Accuracy 1.000.
### Flips: 205, rs: 24, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012553
Test loss (w/o reg) on all data: 0.002656075
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2138032e-08
Norm of the params: 6.092806
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3716797e-09
Norm of the params: 6.092819
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022797847
Train loss (w/o reg) on all data: 0.019023415
Test loss (w/o reg) on all data: 0.0096224435
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6958746e-06
Norm of the params: 8.68842
              Random: fixed   3 labels. Loss 0.00962. Accuracy 1.000.
### Flips: 205, rs: 24, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012343
Test loss (w/o reg) on all data: 0.0026560826
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8584604e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.307358e-08
Norm of the params: 6.0928254
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02210686
Train loss (w/o reg) on all data: 0.018321613
Test loss (w/o reg) on all data: 0.0092627285
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1347865e-07
Norm of the params: 8.700858
              Random: fixed   4 labels. Loss 0.00926. Accuracy 1.000.
### Flips: 205, rs: 24, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012955
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.312946e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.136354e-08
Norm of the params: 6.0928144
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020791642
Train loss (w/o reg) on all data: 0.017139174
Test loss (w/o reg) on all data: 0.008483822
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 8.01474e-07
Norm of the params: 8.546892
              Random: fixed   6 labels. Loss 0.00848. Accuracy 1.000.
### Flips: 205, rs: 24, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4482407e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8053589e-08
Norm of the params: 6.092816
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019543273
Train loss (w/o reg) on all data: 0.01597146
Test loss (w/o reg) on all data: 0.007567772
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1810237e-06
Norm of the params: 8.452
              Random: fixed   8 labels. Loss 0.00757. Accuracy 1.000.
### Flips: 205, rs: 24, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011173
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2079242e-08
Norm of the params: 6.092828
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3947675e-09
Norm of the params: 6.092812
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018562488
Train loss (w/o reg) on all data: 0.015047683
Test loss (w/o reg) on all data: 0.007562959
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1403627e-07
Norm of the params: 8.384275
              Random: fixed   9 labels. Loss 0.00756. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030224089
Train loss (w/o reg) on all data: 0.026138226
Test loss (w/o reg) on all data: 0.015068837
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7997032e-06
Norm of the params: 9.039761
Flipped loss: 0.01507. Accuracy: 0.999
### Flips: 205, rs: 25, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7604837e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560833
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5970114e-08
Norm of the params: 6.092813
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028390486
Train loss (w/o reg) on all data: 0.024023686
Test loss (w/o reg) on all data: 0.015265486
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.23454e-06
Norm of the params: 9.345373
              Random: fixed   2 labels. Loss 0.01527. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2293935e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.76974e-09
Norm of the params: 6.0928183
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028390486
Train loss (w/o reg) on all data: 0.024023792
Test loss (w/o reg) on all data: 0.015263249
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7294866e-06
Norm of the params: 9.34526
              Random: fixed   2 labels. Loss 0.01526. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.23035235e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.72589e-09
Norm of the params: 6.0928183
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027586963
Train loss (w/o reg) on all data: 0.023222802
Test loss (w/o reg) on all data: 0.015119321
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.160273e-06
Norm of the params: 9.342549
              Random: fixed   3 labels. Loss 0.01512. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1952678e-08
Norm of the params: 6.092815
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.4295e-09
Norm of the params: 6.0928164
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027586965
Train loss (w/o reg) on all data: 0.023222558
Test loss (w/o reg) on all data: 0.015120187
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.1434134e-07
Norm of the params: 9.342811
              Random: fixed   3 labels. Loss 0.01512. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1958915e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.4359435e-09
Norm of the params: 6.0928164
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024975978
Train loss (w/o reg) on all data: 0.020503592
Test loss (w/o reg) on all data: 0.013562397
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2385461e-06
Norm of the params: 9.457682
              Random: fixed   7 labels. Loss 0.01356. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2009038e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.787266e-09
Norm of the params: 6.092822
                Loss: fixed  31 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023457756
Train loss (w/o reg) on all data: 0.01897864
Test loss (w/o reg) on all data: 0.013223466
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9762557e-06
Norm of the params: 9.464794
              Random: fixed   9 labels. Loss 0.01322. Accuracy 0.999.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02691235
Train loss (w/o reg) on all data: 0.022262802
Test loss (w/o reg) on all data: 0.01056286
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7436348e-06
Norm of the params: 9.643183
Flipped loss: 0.01056. Accuracy: 0.999
### Flips: 205, rs: 26, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6783492e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7853448e-08
Norm of the params: 6.0928197
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02691235
Train loss (w/o reg) on all data: 0.02226292
Test loss (w/o reg) on all data: 0.010563173
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5611905e-06
Norm of the params: 9.643061
              Random: fixed   0 labels. Loss 0.01056. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012506
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6649537e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7617763e-08
Norm of the params: 6.092819
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02691235
Train loss (w/o reg) on all data: 0.022263382
Test loss (w/o reg) on all data: 0.010563189
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.510756e-07
Norm of the params: 9.642582
              Random: fixed   0 labels. Loss 0.01056. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6523687e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7563645e-08
Norm of the params: 6.092819
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025158726
Train loss (w/o reg) on all data: 0.020587156
Test loss (w/o reg) on all data: 0.0105102565
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.4564883e-07
Norm of the params: 9.561976
              Random: fixed   2 labels. Loss 0.01051. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4403167e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.441596e-09
Norm of the params: 6.092822
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025158726
Train loss (w/o reg) on all data: 0.020586176
Test loss (w/o reg) on all data: 0.010510029
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3840591e-06
Norm of the params: 9.563002
              Random: fixed   2 labels. Loss 0.01051. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4326213e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.025256e-09
Norm of the params: 6.092822
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023132693
Train loss (w/o reg) on all data: 0.01854096
Test loss (w/o reg) on all data: 0.010210838
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.1052083e-07
Norm of the params: 9.583041
              Random: fixed   4 labels. Loss 0.01021. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601337
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.93028e-08
Norm of the params: 6.0927925
     Influence (LOO): fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026560866
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2880917e-08
Norm of the params: 6.092804
                Loss: fixed  27 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020314686
Train loss (w/o reg) on all data: 0.015978497
Test loss (w/o reg) on all data: 0.008852724
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.6173144e-07
Norm of the params: 9.31256
              Random: fixed   8 labels. Loss 0.00885. Accuracy 0.999.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023776237
Train loss (w/o reg) on all data: 0.01965365
Test loss (w/o reg) on all data: 0.015177705
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.7065087e-07
Norm of the params: 9.080296
Flipped loss: 0.01518. Accuracy: 0.996
### Flips: 205, rs: 27, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.967972e-09
Norm of the params: 6.09282
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.222873e-09
Norm of the params: 6.092813
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02333077
Train loss (w/o reg) on all data: 0.01924238
Test loss (w/o reg) on all data: 0.0148875825
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5467093e-06
Norm of the params: 9.042557
              Random: fixed   1 labels. Loss 0.01489. Accuracy 0.995.
### Flips: 205, rs: 27, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2719776e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.292152e-09
Norm of the params: 6.092812
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022398613
Train loss (w/o reg) on all data: 0.0182931
Test loss (w/o reg) on all data: 0.014304268
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4127107e-06
Norm of the params: 9.061471
              Random: fixed   2 labels. Loss 0.01430. Accuracy 0.996.
### Flips: 205, rs: 27, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3111861e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3447769e-08
Norm of the params: 6.092814
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020617876
Train loss (w/o reg) on all data: 0.01660509
Test loss (w/o reg) on all data: 0.013734561
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.080565e-07
Norm of the params: 8.958556
              Random: fixed   5 labels. Loss 0.01373. Accuracy 0.996.
### Flips: 205, rs: 27, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.002656087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9414449e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011214
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0118422e-08
Norm of the params: 6.0928273
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020617876
Train loss (w/o reg) on all data: 0.016604818
Test loss (w/o reg) on all data: 0.013734245
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0055044e-06
Norm of the params: 8.958859
              Random: fixed   5 labels. Loss 0.01373. Accuracy 0.996.
### Flips: 205, rs: 27, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601256
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9421808e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011214
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0118847e-08
Norm of the params: 6.0928273
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018557655
Train loss (w/o reg) on all data: 0.014526121
Test loss (w/o reg) on all data: 0.012846955
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.340136e-07
Norm of the params: 8.979459
              Random: fixed   7 labels. Loss 0.01285. Accuracy 0.996.
### Flips: 205, rs: 27, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6245929e-08
Norm of the params: 6.092815
     Influence (LOO): fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8664133e-08
Norm of the params: 6.0928183
                Loss: fixed  26 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017794829
Train loss (w/o reg) on all data: 0.013642372
Test loss (w/o reg) on all data: 0.0123831425
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.793753e-07
Norm of the params: 9.1131315
              Random: fixed   8 labels. Loss 0.01238. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02075433
Train loss (w/o reg) on all data: 0.016676912
Test loss (w/o reg) on all data: 0.012493997
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.443273e-07
Norm of the params: 9.030414
Flipped loss: 0.01249. Accuracy: 0.998
### Flips: 205, rs: 28, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0557292e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.274662e-09
Norm of the params: 6.0928125
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018701795
Train loss (w/o reg) on all data: 0.014457368
Test loss (w/o reg) on all data: 0.011716004
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.211626e-06
Norm of the params: 9.213498
              Random: fixed   2 labels. Loss 0.01172. Accuracy 0.998.
### Flips: 205, rs: 28, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.54832e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.002656087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.632579e-08
Norm of the params: 6.0928154
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017987587
Train loss (w/o reg) on all data: 0.013764907
Test loss (w/o reg) on all data: 0.011860513
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.522062e-07
Norm of the params: 9.189864
              Random: fixed   3 labels. Loss 0.01186. Accuracy 0.997.
### Flips: 205, rs: 28, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560852
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2550624e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.00265608
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.363276e-08
Norm of the params: 6.0928164
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017987587
Train loss (w/o reg) on all data: 0.013764935
Test loss (w/o reg) on all data: 0.011860685
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.6574968e-07
Norm of the params: 9.189834
              Random: fixed   3 labels. Loss 0.01186. Accuracy 0.997.
### Flips: 205, rs: 28, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.367819e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4043851e-08
Norm of the params: 6.092817
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015441458
Train loss (w/o reg) on all data: 0.011716657
Test loss (w/o reg) on all data: 0.009772076
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8084956e-07
Norm of the params: 8.631107
              Random: fixed   7 labels. Loss 0.00977. Accuracy 0.998.
### Flips: 205, rs: 28, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1759177e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2112666e-08
Norm of the params: 6.09282
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0131026395
Train loss (w/o reg) on all data: 0.009366851
Test loss (w/o reg) on all data: 0.009145096
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8723787e-07
Norm of the params: 8.643829
              Random: fixed   9 labels. Loss 0.00915. Accuracy 0.999.
### Flips: 205, rs: 28, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.832912e-09
Norm of the params: 6.092824
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0002267e-08
Norm of the params: 6.0928154
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0131026395
Train loss (w/o reg) on all data: 0.009366898
Test loss (w/o reg) on all data: 0.009144881
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.5208644e-07
Norm of the params: 8.643774
              Random: fixed   9 labels. Loss 0.00914. Accuracy 0.999.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628055
Train loss (w/o reg) on all data: 0.016446562
Test loss (w/o reg) on all data: 0.008403752
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 9.994512e-07
Norm of the params: 7.9768314
Flipped loss: 0.00840. Accuracy: 1.000
### Flips: 205, rs: 29, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8226098e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5526618e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628054
Train loss (w/o reg) on all data: 0.016446872
Test loss (w/o reg) on all data: 0.008404828
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0086563e-06
Norm of the params: 7.9764433
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
### Flips: 205, rs: 29, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8213289e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5530247e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628054
Train loss (w/o reg) on all data: 0.016446723
Test loss (w/o reg) on all data: 0.008404344
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 8.979984e-07
Norm of the params: 7.9766293
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
### Flips: 205, rs: 29, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.814809e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5493999e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628052
Train loss (w/o reg) on all data: 0.016446529
Test loss (w/o reg) on all data: 0.008404581
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3117284e-06
Norm of the params: 7.9768715
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
### Flips: 205, rs: 29, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8170002e-08
Norm of the params: 6.0928206
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.550196e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628055
Train loss (w/o reg) on all data: 0.01644655
Test loss (w/o reg) on all data: 0.008404312
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 6.970768e-07
Norm of the params: 7.976848
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
### Flips: 205, rs: 29, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7916157e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5393057e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628057
Train loss (w/o reg) on all data: 0.016446486
Test loss (w/o reg) on all data: 0.008404063
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5807367e-06
Norm of the params: 7.9769306
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
### Flips: 205, rs: 29, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8000897e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5398381e-08
Norm of the params: 6.092814
                Loss: fixed  16 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019628055
Train loss (w/o reg) on all data: 0.016446445
Test loss (w/o reg) on all data: 0.008404519
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.300026e-06
Norm of the params: 7.97698
              Random: fixed   0 labels. Loss 0.00840. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026453989
Train loss (w/o reg) on all data: 0.022015497
Test loss (w/o reg) on all data: 0.012713698
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2844935e-06
Norm of the params: 9.421774
Flipped loss: 0.01271. Accuracy: 0.998
### Flips: 205, rs: 30, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096015143
Test loss (w/o reg) on all data: 0.0026561015
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4128872e-08
Norm of the params: 6.092763
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9268758e-08
Norm of the params: 6.0928245
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0247951
Train loss (w/o reg) on all data: 0.020454025
Test loss (w/o reg) on all data: 0.012497146
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.1950484e-07
Norm of the params: 9.317805
              Random: fixed   2 labels. Loss 0.01250. Accuracy 0.998.
### Flips: 205, rs: 30, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3599089e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7472814e-09
Norm of the params: 6.0928183
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02330749
Train loss (w/o reg) on all data: 0.018927995
Test loss (w/o reg) on all data: 0.010529785
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1979235e-06
Norm of the params: 9.358947
              Random: fixed   4 labels. Loss 0.01053. Accuracy 0.998.
### Flips: 205, rs: 30, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6722977e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.03173e-08
Norm of the params: 6.0928183
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021324048
Train loss (w/o reg) on all data: 0.016873233
Test loss (w/o reg) on all data: 0.008687631
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0355575e-06
Norm of the params: 9.434845
              Random: fixed   7 labels. Loss 0.00869. Accuracy 1.000.
### Flips: 205, rs: 30, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5336084e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012076
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0468621e-08
Norm of the params: 6.092813
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020460183
Train loss (w/o reg) on all data: 0.01596005
Test loss (w/o reg) on all data: 0.008214266
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 8.4816173e-07
Norm of the params: 9.486974
              Random: fixed   9 labels. Loss 0.00821. Accuracy 1.000.
### Flips: 205, rs: 30, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6637362e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0012541e-08
Norm of the params: 6.0928254
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019315667
Train loss (w/o reg) on all data: 0.014844517
Test loss (w/o reg) on all data: 0.00792719
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 1.006614e-06
Norm of the params: 9.456373
              Random: fixed  11 labels. Loss 0.00793. Accuracy 1.000.
### Flips: 205, rs: 30, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013286
Test loss (w/o reg) on all data: 0.0026560917
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9774046e-08
Norm of the params: 6.0927935
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1043017e-08
Norm of the params: 6.092824
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017657777
Train loss (w/o reg) on all data: 0.013210548
Test loss (w/o reg) on all data: 0.0078215925
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5532522e-07
Norm of the params: 9.431045
              Random: fixed  13 labels. Loss 0.00782. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026357196
Train loss (w/o reg) on all data: 0.022136878
Test loss (w/o reg) on all data: 0.015120976
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9589852e-06
Norm of the params: 9.187293
Flipped loss: 0.01512. Accuracy: 0.999
### Flips: 205, rs: 31, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8718309e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6148626e-08
Norm of the params: 6.0928183
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024533344
Train loss (w/o reg) on all data: 0.020147642
Test loss (w/o reg) on all data: 0.015449075
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.753203e-07
Norm of the params: 9.365579
              Random: fixed   2 labels. Loss 0.01545. Accuracy 0.997.
### Flips: 205, rs: 31, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.723289e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1859766e-08
Norm of the params: 6.092825
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02213775
Train loss (w/o reg) on all data: 0.01789321
Test loss (w/o reg) on all data: 0.013780766
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.1892306e-07
Norm of the params: 9.213621
              Random: fixed   5 labels. Loss 0.01378. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2183979e-08
Norm of the params: 6.092827
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.6072945e-09
Norm of the params: 6.092821
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019950643
Train loss (w/o reg) on all data: 0.015914876
Test loss (w/o reg) on all data: 0.011837698
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4560585e-06
Norm of the params: 8.984171
              Random: fixed   9 labels. Loss 0.01184. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4671279e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5895273e-08
Norm of the params: 6.0928216
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018286513
Train loss (w/o reg) on all data: 0.014333589
Test loss (w/o reg) on all data: 0.011599625
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.5459754e-07
Norm of the params: 8.891484
              Random: fixed  12 labels. Loss 0.01160. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9498643e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2105128e-08
Norm of the params: 6.092818
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016395846
Train loss (w/o reg) on all data: 0.0127719315
Test loss (w/o reg) on all data: 0.0100874165
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.257585e-07
Norm of the params: 8.51342
              Random: fixed  15 labels. Loss 0.01009. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6311065e-09
Norm of the params: 6.092814
     Influence (LOO): fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3187473e-09
Norm of the params: 6.092819
                Loss: fixed  29 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0151133835
Train loss (w/o reg) on all data: 0.011508441
Test loss (w/o reg) on all data: 0.009947499
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2995922e-06
Norm of the params: 8.491105
              Random: fixed  17 labels. Loss 0.00995. Accuracy 0.998.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032105066
Train loss (w/o reg) on all data: 0.028513346
Test loss (w/o reg) on all data: 0.015271578
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3937113e-06
Norm of the params: 8.475515
Flipped loss: 0.01527. Accuracy: 0.999
### Flips: 205, rs: 32, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5075314e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5130261e-08
Norm of the params: 6.092818
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030793501
Train loss (w/o reg) on all data: 0.027214142
Test loss (w/o reg) on all data: 0.0147419805
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2710885e-06
Norm of the params: 8.460921
              Random: fixed   2 labels. Loss 0.01474. Accuracy 0.999.
### Flips: 205, rs: 32, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6046097e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011546
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6425291e-08
Norm of the params: 6.092823
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029211037
Train loss (w/o reg) on all data: 0.02562044
Test loss (w/o reg) on all data: 0.014679881
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.767169e-06
Norm of the params: 8.474191
              Random: fixed   5 labels. Loss 0.01468. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1744003e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026561017
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.980837e-08
Norm of the params: 6.092808
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029211035
Train loss (w/o reg) on all data: 0.025620881
Test loss (w/o reg) on all data: 0.014678069
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3855154e-06
Norm of the params: 8.47367
              Random: fixed   5 labels. Loss 0.01468. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1523266e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026561003
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.92127e-08
Norm of the params: 6.0928087
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029211037
Train loss (w/o reg) on all data: 0.025621023
Test loss (w/o reg) on all data: 0.014678898
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3491214e-06
Norm of the params: 8.473503
              Random: fixed   5 labels. Loss 0.01468. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1612425e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.002656101
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.978154e-08
Norm of the params: 6.0928082
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026627304
Train loss (w/o reg) on all data: 0.023025002
Test loss (w/o reg) on all data: 0.013583353
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.5781573e-07
Norm of the params: 8.487993
              Random: fixed   8 labels. Loss 0.01358. Accuracy 0.998.
### Flips: 205, rs: 32, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012076
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.169754e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7786075e-09
Norm of the params: 6.09282
                Loss: fixed  38 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02442325
Train loss (w/o reg) on all data: 0.020783111
Test loss (w/o reg) on all data: 0.012874864
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.301461e-06
Norm of the params: 8.532455
              Random: fixed  11 labels. Loss 0.01287. Accuracy 0.999.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02549317
Train loss (w/o reg) on all data: 0.02141421
Test loss (w/o reg) on all data: 0.014147301
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.264012e-06
Norm of the params: 9.032122
Flipped loss: 0.01415. Accuracy: 0.998
### Flips: 205, rs: 33, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010306
Test loss (w/o reg) on all data: 0.0026560272
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0674493e-08
Norm of the params: 6.092844
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.01085e-09
Norm of the params: 6.0928154
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024936754
Train loss (w/o reg) on all data: 0.020976642
Test loss (w/o reg) on all data: 0.014337026
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0908343e-06
Norm of the params: 8.899563
              Random: fixed   1 labels. Loss 0.01434. Accuracy 0.997.
### Flips: 205, rs: 33, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7117955e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.643657e-09
Norm of the params: 6.092824
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024936754
Train loss (w/o reg) on all data: 0.020976722
Test loss (w/o reg) on all data: 0.014337827
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.445688e-07
Norm of the params: 8.899475
              Random: fixed   1 labels. Loss 0.01434. Accuracy 0.997.
### Flips: 205, rs: 33, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7221913e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.697304e-09
Norm of the params: 6.092825
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024936754
Train loss (w/o reg) on all data: 0.02097688
Test loss (w/o reg) on all data: 0.014337639
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0020273e-06
Norm of the params: 8.899296
              Random: fixed   1 labels. Loss 0.01434. Accuracy 0.997.
### Flips: 205, rs: 33, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7079705e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.574699e-09
Norm of the params: 6.092824
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023458853
Train loss (w/o reg) on all data: 0.019582212
Test loss (w/o reg) on all data: 0.013561485
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1098587e-06
Norm of the params: 8.805272
              Random: fixed   3 labels. Loss 0.01356. Accuracy 0.998.
### Flips: 205, rs: 33, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0236902e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6570738e-08
Norm of the params: 6.092818
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021734294
Train loss (w/o reg) on all data: 0.017861525
Test loss (w/o reg) on all data: 0.013188395
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.92125e-07
Norm of the params: 8.800873
              Random: fixed   4 labels. Loss 0.01319. Accuracy 0.997.
### Flips: 205, rs: 33, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0141508e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4934637e-08
Norm of the params: 6.09282
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02044676
Train loss (w/o reg) on all data: 0.016653948
Test loss (w/o reg) on all data: 0.010671764
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.105296e-06
Norm of the params: 8.709549
              Random: fixed   6 labels. Loss 0.01067. Accuracy 0.999.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022523075
Train loss (w/o reg) on all data: 0.018849831
Test loss (w/o reg) on all data: 0.009961402
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8852558e-06
Norm of the params: 8.571167
Flipped loss: 0.00996. Accuracy: 0.999
### Flips: 205, rs: 34, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013944
Test loss (w/o reg) on all data: 0.0026561057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2412046e-08
Norm of the params: 6.092783
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4578332e-08
Norm of the params: 6.092824
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022523079
Train loss (w/o reg) on all data: 0.018850628
Test loss (w/o reg) on all data: 0.0099615725
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.268709e-07
Norm of the params: 8.57024
              Random: fixed   0 labels. Loss 0.00996. Accuracy 0.999.
### Flips: 205, rs: 34, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013944
Test loss (w/o reg) on all data: 0.0026561057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.235624e-08
Norm of the params: 6.092783
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4571549e-08
Norm of the params: 6.092824
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022155588
Train loss (w/o reg) on all data: 0.018596007
Test loss (w/o reg) on all data: 0.0096247615
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6459797e-07
Norm of the params: 8.437514
              Random: fixed   1 labels. Loss 0.00962. Accuracy 0.999.
### Flips: 205, rs: 34, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011395
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.223096e-09
Norm of the params: 6.092825
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1409558e-08
Norm of the params: 6.0928164
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020951163
Train loss (w/o reg) on all data: 0.017362032
Test loss (w/o reg) on all data: 0.009041196
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7627495e-07
Norm of the params: 8.472464
              Random: fixed   2 labels. Loss 0.00904. Accuracy 1.000.
### Flips: 205, rs: 34, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601116
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1737195e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601279
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8712045e-08
Norm of the params: 6.092802
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020951165
Train loss (w/o reg) on all data: 0.017362798
Test loss (w/o reg) on all data: 0.009041937
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 6.8827757e-07
Norm of the params: 8.4715605
              Random: fixed   2 labels. Loss 0.00904. Accuracy 1.000.
### Flips: 205, rs: 34, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8029576e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3726e-08
Norm of the params: 6.092804
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020951163
Train loss (w/o reg) on all data: 0.017362483
Test loss (w/o reg) on all data: 0.009041697
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 6.430154e-07
Norm of the params: 8.4719305
              Random: fixed   2 labels. Loss 0.00904. Accuracy 1.000.
### Flips: 205, rs: 34, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9522853e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5550492e-08
Norm of the params: 6.092803
                Loss: fixed  22 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020420566
Train loss (w/o reg) on all data: 0.016918747
Test loss (w/o reg) on all data: 0.008945359
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7635764e-06
Norm of the params: 8.368773
              Random: fixed   3 labels. Loss 0.00895. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03387822
Train loss (w/o reg) on all data: 0.029878665
Test loss (w/o reg) on all data: 0.015237432
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.1996657e-06
Norm of the params: 8.943773
Flipped loss: 0.01524. Accuracy: 0.998
### Flips: 205, rs: 35, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1150189e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2366143e-09
Norm of the params: 6.0928235
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029126609
Train loss (w/o reg) on all data: 0.02498506
Test loss (w/o reg) on all data: 0.013446219
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6827728e-07
Norm of the params: 9.101152
              Random: fixed   5 labels. Loss 0.01345. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4742882e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.659266e-09
Norm of the params: 6.092814
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026809692
Train loss (w/o reg) on all data: 0.022473827
Test loss (w/o reg) on all data: 0.011709917
Train acc on all data:  0.9929491855093605
Test acc on all data:   1.0
Norm of the mean of gradients: 4.746829e-06
Norm of the params: 9.312214
              Random: fixed   8 labels. Loss 0.01171. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2768673e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4016846e-08
Norm of the params: 6.0928183
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026344627
Train loss (w/o reg) on all data: 0.022115026
Test loss (w/o reg) on all data: 0.011677794
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6742356e-06
Norm of the params: 9.197392
              Random: fixed  10 labels. Loss 0.01168. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4102488e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8622918e-08
Norm of the params: 6.0928116
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023327975
Train loss (w/o reg) on all data: 0.018971672
Test loss (w/o reg) on all data: 0.009851882
Train acc on all data:  0.9941648431801605
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3165958e-06
Norm of the params: 9.334134
              Random: fixed  14 labels. Loss 0.00985. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8387547e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1170408e-08
Norm of the params: 6.092818
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021379955
Train loss (w/o reg) on all data: 0.016922005
Test loss (w/o reg) on all data: 0.008659139
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6508303e-07
Norm of the params: 9.442407
              Random: fixed  16 labels. Loss 0.00866. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.24382105e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6974226e-09
Norm of the params: 6.092814
                Loss: fixed  40 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020415941
Train loss (w/o reg) on all data: 0.016098913
Test loss (w/o reg) on all data: 0.008499204
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0239263e-06
Norm of the params: 9.291963
              Random: fixed  18 labels. Loss 0.00850. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027097244
Train loss (w/o reg) on all data: 0.02301057
Test loss (w/o reg) on all data: 0.014144116
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.007676e-06
Norm of the params: 9.040657
Flipped loss: 0.01414. Accuracy: 0.998
### Flips: 205, rs: 36, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2502413e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0469275e-08
Norm of the params: 6.092821
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025633704
Train loss (w/o reg) on all data: 0.021413386
Test loss (w/o reg) on all data: 0.013723592
Train acc on all data:  0.9936785801118405
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3303085e-06
Norm of the params: 9.187294
              Random: fixed   2 labels. Loss 0.01372. Accuracy 0.998.
### Flips: 205, rs: 36, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560975
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9054962e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2262702e-08
Norm of the params: 6.092823
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024228862
Train loss (w/o reg) on all data: 0.019917231
Test loss (w/o reg) on all data: 0.012601293
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.7927183e-07
Norm of the params: 9.286153
              Random: fixed   4 labels. Loss 0.01260. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5989278e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4861378e-08
Norm of the params: 6.092814
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024228863
Train loss (w/o reg) on all data: 0.01991679
Test loss (w/o reg) on all data: 0.012600423
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5110622e-06
Norm of the params: 9.286629
              Random: fixed   4 labels. Loss 0.01260. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5963999e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4861749e-08
Norm of the params: 6.092814
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024228863
Train loss (w/o reg) on all data: 0.019917272
Test loss (w/o reg) on all data: 0.012600334
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.6382114e-07
Norm of the params: 9.286109
              Random: fixed   4 labels. Loss 0.01260. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5989234e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4863334e-08
Norm of the params: 6.092814
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023859415
Train loss (w/o reg) on all data: 0.019509982
Test loss (w/o reg) on all data: 0.01281569
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3453916e-06
Norm of the params: 9.326772
              Random: fixed   5 labels. Loss 0.01282. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010993
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.526736e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.093702e-08
Norm of the params: 6.092808
                Loss: fixed  28 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022799855
Train loss (w/o reg) on all data: 0.018462885
Test loss (w/o reg) on all data: 0.012799537
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3919711e-06
Norm of the params: 9.3134
              Random: fixed   6 labels. Loss 0.01280. Accuracy 0.998.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020224242
Train loss (w/o reg) on all data: 0.017126251
Test loss (w/o reg) on all data: 0.011164799
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0918372e-07
Norm of the params: 7.871455
Flipped loss: 0.01116. Accuracy: 0.997
### Flips: 205, rs: 37, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7090114e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8562941e-08
Norm of the params: 6.092819
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02022424
Train loss (w/o reg) on all data: 0.017126305
Test loss (w/o reg) on all data: 0.011164764
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.107887e-07
Norm of the params: 7.8713856
              Random: fixed   0 labels. Loss 0.01116. Accuracy 0.997.
### Flips: 205, rs: 37, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7086345e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8568727e-08
Norm of the params: 6.092819
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019141056
Train loss (w/o reg) on all data: 0.016105682
Test loss (w/o reg) on all data: 0.010502059
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1954697e-06
Norm of the params: 7.791501
              Random: fixed   2 labels. Loss 0.01050. Accuracy 0.997.
### Flips: 205, rs: 37, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012955
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9622167e-08
Norm of the params: 6.092799
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2512623e-08
Norm of the params: 6.092819
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018263359
Train loss (w/o reg) on all data: 0.015124045
Test loss (w/o reg) on all data: 0.010079119
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.9427645e-07
Norm of the params: 7.923779
              Random: fixed   3 labels. Loss 0.01008. Accuracy 0.997.
### Flips: 205, rs: 37, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1294813e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5988139e-08
Norm of the params: 6.0928235
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015345925
Train loss (w/o reg) on all data: 0.012035568
Test loss (w/o reg) on all data: 0.0074105454
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6678324e-07
Norm of the params: 8.136777
              Random: fixed   8 labels. Loss 0.00741. Accuracy 1.000.
### Flips: 205, rs: 37, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1137369e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.27955e-09
Norm of the params: 6.092823
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015345925
Train loss (w/o reg) on all data: 0.012035676
Test loss (w/o reg) on all data: 0.00741083
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 9.183325e-07
Norm of the params: 8.136645
              Random: fixed   8 labels. Loss 0.00741. Accuracy 1.000.
### Flips: 205, rs: 37, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11408065e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2688113e-09
Norm of the params: 6.092823
                Loss: fixed  20 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015345926
Train loss (w/o reg) on all data: 0.012035448
Test loss (w/o reg) on all data: 0.007410531
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0607946e-06
Norm of the params: 8.136926
              Random: fixed   8 labels. Loss 0.00741. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024297697
Train loss (w/o reg) on all data: 0.020187266
Test loss (w/o reg) on all data: 0.012293609
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.444402e-07
Norm of the params: 9.066896
Flipped loss: 0.01229. Accuracy: 0.999
### Flips: 205, rs: 38, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.926379e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.564499e-08
Norm of the params: 6.0928135
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024297697
Train loss (w/o reg) on all data: 0.02018798
Test loss (w/o reg) on all data: 0.012292394
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5918191e-06
Norm of the params: 9.06611
              Random: fixed   0 labels. Loss 0.01229. Accuracy 0.999.
### Flips: 205, rs: 38, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.936053e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5657061e-08
Norm of the params: 6.0928135
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02339249
Train loss (w/o reg) on all data: 0.019226573
Test loss (w/o reg) on all data: 0.0123473
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.49858e-07
Norm of the params: 9.127888
              Random: fixed   1 labels. Loss 0.01235. Accuracy 0.999.
### Flips: 205, rs: 38, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012687
Test loss (w/o reg) on all data: 0.0026560966
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.851031e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010393
Test loss (w/o reg) on all data: 0.0026560407
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5216897e-08
Norm of the params: 6.092841
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022118002
Train loss (w/o reg) on all data: 0.018038908
Test loss (w/o reg) on all data: 0.011477137
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1072328e-06
Norm of the params: 9.032269
              Random: fixed   2 labels. Loss 0.01148. Accuracy 1.000.
### Flips: 205, rs: 38, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.030918e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.15515e-08
Norm of the params: 6.09282
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021708285
Train loss (w/o reg) on all data: 0.017727086
Test loss (w/o reg) on all data: 0.011281728
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.795981e-06
Norm of the params: 8.923227
              Random: fixed   3 labels. Loss 0.01128. Accuracy 1.000.
### Flips: 205, rs: 38, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7815953e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0226707e-08
Norm of the params: 6.092819
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01882827
Train loss (w/o reg) on all data: 0.015152607
Test loss (w/o reg) on all data: 0.0097296545
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6343689e-06
Norm of the params: 8.573986
              Random: fixed   6 labels. Loss 0.00973. Accuracy 0.999.
### Flips: 205, rs: 38, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7781936e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960124
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.40540255e-08
Norm of the params: 6.0928082
                Loss: fixed  23 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016432881
Train loss (w/o reg) on all data: 0.0128127
Test loss (w/o reg) on all data: 0.009137561
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.173576e-07
Norm of the params: 8.509032
              Random: fixed   9 labels. Loss 0.00914. Accuracy 0.999.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023572642
Train loss (w/o reg) on all data: 0.019285327
Test loss (w/o reg) on all data: 0.017532222
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.1114786e-06
Norm of the params: 9.259931
Flipped loss: 0.01753. Accuracy: 0.995
### Flips: 205, rs: 39, checks: 205
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601033
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8019993e-08
Norm of the params: 6.0928426
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0147888e-08
Norm of the params: 6.0928164
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02357264
Train loss (w/o reg) on all data: 0.01928473
Test loss (w/o reg) on all data: 0.017532589
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8542522e-06
Norm of the params: 9.2605715
              Random: fixed   0 labels. Loss 0.01753. Accuracy 0.995.
### Flips: 205, rs: 39, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010463
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2983242e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.024624e-08
Norm of the params: 6.0928144
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02296087
Train loss (w/o reg) on all data: 0.018843021
Test loss (w/o reg) on all data: 0.017055843
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3960297e-06
Norm of the params: 9.075073
              Random: fixed   1 labels. Loss 0.01706. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.429045e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9058271e-08
Norm of the params: 6.0928273
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022960868
Train loss (w/o reg) on all data: 0.018842375
Test loss (w/o reg) on all data: 0.017055616
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5922324e-06
Norm of the params: 9.075784
              Random: fixed   1 labels. Loss 0.01706. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4157488e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9206226e-08
Norm of the params: 6.0928273
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02076098
Train loss (w/o reg) on all data: 0.016535593
Test loss (w/o reg) on all data: 0.014753615
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.0318234e-06
Norm of the params: 9.192809
              Random: fixed   5 labels. Loss 0.01475. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5825036e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8832355e-08
Norm of the params: 6.09281
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019740717
Train loss (w/o reg) on all data: 0.01551614
Test loss (w/o reg) on all data: 0.014566109
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0842976e-07
Norm of the params: 9.191929
              Random: fixed   6 labels. Loss 0.01457. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2325198e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3116541e-08
Norm of the params: 6.0928154
                Loss: fixed  25 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018930988
Train loss (w/o reg) on all data: 0.0146582965
Test loss (w/o reg) on all data: 0.013250638
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.3136016e-07
Norm of the params: 9.244124
              Random: fixed   7 labels. Loss 0.01325. Accuracy 0.997.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03583473
Train loss (w/o reg) on all data: 0.031393413
Test loss (w/o reg) on all data: 0.022107378
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4040118e-06
Norm of the params: 9.424772
Flipped loss: 0.02211. Accuracy: 0.993
### Flips: 410, rs: 0, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0024515e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8936885e-09
Norm of the params: 6.092813
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0347636
Train loss (w/o reg) on all data: 0.030203037
Test loss (w/o reg) on all data: 0.021451956
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.7978281e-06
Norm of the params: 9.550459
              Random: fixed   2 labels. Loss 0.02145. Accuracy 0.994.
### Flips: 410, rs: 0, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.29099025e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.547141e-09
Norm of the params: 6.092817
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032662902
Train loss (w/o reg) on all data: 0.028018499
Test loss (w/o reg) on all data: 0.020738255
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.19279e-06
Norm of the params: 9.637845
              Random: fixed   6 labels. Loss 0.02074. Accuracy 0.994.
### Flips: 410, rs: 0, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012646
Test loss (w/o reg) on all data: 0.0026560968
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3958134e-08
Norm of the params: 6.092804
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.923606e-08
Norm of the params: 6.092824
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031710267
Train loss (w/o reg) on all data: 0.02708575
Test loss (w/o reg) on all data: 0.01827666
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2882665e-06
Norm of the params: 9.61719
              Random: fixed   8 labels. Loss 0.01828. Accuracy 0.996.
### Flips: 410, rs: 0, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.18717e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5306594e-08
Norm of the params: 6.092823
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030615032
Train loss (w/o reg) on all data: 0.025977066
Test loss (w/o reg) on all data: 0.018022172
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3266814e-06
Norm of the params: 9.631165
              Random: fixed  10 labels. Loss 0.01802. Accuracy 0.996.
### Flips: 410, rs: 0, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9774945e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4846528e-08
Norm of the params: 6.09282
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028065437
Train loss (w/o reg) on all data: 0.023472738
Test loss (w/o reg) on all data: 0.01525708
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7304964e-06
Norm of the params: 9.584049
              Random: fixed  15 labels. Loss 0.01526. Accuracy 0.999.
### Flips: 410, rs: 0, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6033058e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3890405e-08
Norm of the params: 6.0928116
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027458293
Train loss (w/o reg) on all data: 0.022945663
Test loss (w/o reg) on all data: 0.014600472
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6800062e-06
Norm of the params: 9.500136
              Random: fixed  17 labels. Loss 0.01460. Accuracy 0.999.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038308263
Train loss (w/o reg) on all data: 0.034462605
Test loss (w/o reg) on all data: 0.018017769
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.976411e-06
Norm of the params: 8.770013
Flipped loss: 0.01802. Accuracy: 0.999
### Flips: 410, rs: 1, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601283
Test loss (w/o reg) on all data: 0.0026560903
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5181293e-08
Norm of the params: 6.092801
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16721965e-08
Norm of the params: 6.0928216
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038018465
Train loss (w/o reg) on all data: 0.034168106
Test loss (w/o reg) on all data: 0.017768322
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3597856e-07
Norm of the params: 8.775373
              Random: fixed   1 labels. Loss 0.01777. Accuracy 0.999.
### Flips: 410, rs: 1, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601274
Test loss (w/o reg) on all data: 0.0026560877
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3884992e-08
Norm of the params: 6.0928025
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0895836e-08
Norm of the params: 6.0928216
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03620205
Train loss (w/o reg) on all data: 0.032227222
Test loss (w/o reg) on all data: 0.016883707
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.750505e-06
Norm of the params: 8.916086
              Random: fixed   3 labels. Loss 0.01688. Accuracy 0.999.
### Flips: 410, rs: 1, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1161604e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0193755e-08
Norm of the params: 6.0928144
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0317554
Train loss (w/o reg) on all data: 0.027744487
Test loss (w/o reg) on all data: 0.015645346
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.991356e-07
Norm of the params: 8.956463
              Random: fixed   9 labels. Loss 0.01565. Accuracy 0.998.
### Flips: 410, rs: 1, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560859
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8887421e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.672973e-09
Norm of the params: 6.0928273
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030508757
Train loss (w/o reg) on all data: 0.026453458
Test loss (w/o reg) on all data: 0.014604891
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6028017e-06
Norm of the params: 9.005885
              Random: fixed  11 labels. Loss 0.01460. Accuracy 0.999.
### Flips: 410, rs: 1, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960119
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2098054e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5188851e-08
Norm of the params: 6.09282
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029567175
Train loss (w/o reg) on all data: 0.025599165
Test loss (w/o reg) on all data: 0.014388258
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.7960575e-07
Norm of the params: 8.908435
              Random: fixed  13 labels. Loss 0.01439. Accuracy 0.999.
### Flips: 410, rs: 1, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2206749e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.410484e-09
Norm of the params: 6.0928135
                Loss: fixed  46 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028333105
Train loss (w/o reg) on all data: 0.024315853
Test loss (w/o reg) on all data: 0.012848113
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3567858e-06
Norm of the params: 8.963539
              Random: fixed  14 labels. Loss 0.01285. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03977231
Train loss (w/o reg) on all data: 0.035633374
Test loss (w/o reg) on all data: 0.019279566
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.414061e-06
Norm of the params: 9.098283
Flipped loss: 0.01928. Accuracy: 0.997
### Flips: 410, rs: 2, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0546086e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2988822e-08
Norm of the params: 6.092811
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03905047
Train loss (w/o reg) on all data: 0.034832362
Test loss (w/o reg) on all data: 0.018695928
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.572685e-06
Norm of the params: 9.18489
              Random: fixed   1 labels. Loss 0.01870. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0450671e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4394669e-08
Norm of the params: 6.09281
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0377986
Train loss (w/o reg) on all data: 0.03350615
Test loss (w/o reg) on all data: 0.017763663
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.424999e-06
Norm of the params: 9.265472
              Random: fixed   4 labels. Loss 0.01776. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.546448e-09
Norm of the params: 6.092809
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.406951e-09
Norm of the params: 6.092819
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036848925
Train loss (w/o reg) on all data: 0.032408897
Test loss (w/o reg) on all data: 0.017595343
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.582665e-07
Norm of the params: 9.423404
              Random: fixed   5 labels. Loss 0.01760. Accuracy 0.998.
### Flips: 410, rs: 2, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1453504e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1501292e-08
Norm of the params: 6.0928197
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03629203
Train loss (w/o reg) on all data: 0.031910963
Test loss (w/o reg) on all data: 0.016697692
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.273035e-06
Norm of the params: 9.360626
              Random: fixed   7 labels. Loss 0.01670. Accuracy 0.998.
### Flips: 410, rs: 2, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2619456e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.374534e-08
Norm of the params: 6.09282
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03615451
Train loss (w/o reg) on all data: 0.031942476
Test loss (w/o reg) on all data: 0.016423663
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1631004e-06
Norm of the params: 9.178269
              Random: fixed   9 labels. Loss 0.01642. Accuracy 0.998.
### Flips: 410, rs: 2, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601074
Test loss (w/o reg) on all data: 0.0026560382
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.075138e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012716
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.37867e-08
Norm of the params: 6.092803
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032213636
Train loss (w/o reg) on all data: 0.028114095
Test loss (w/o reg) on all data: 0.014506057
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0546901e-06
Norm of the params: 9.054878
              Random: fixed  15 labels. Loss 0.01451. Accuracy 0.998.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042049292
Train loss (w/o reg) on all data: 0.038299043
Test loss (w/o reg) on all data: 0.019755065
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.8678606e-06
Norm of the params: 8.660543
Flipped loss: 0.01976. Accuracy: 0.999
### Flips: 410, rs: 3, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0629277e-08
Norm of the params: 6.092815
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8528421e-08
Norm of the params: 6.092823
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04035617
Train loss (w/o reg) on all data: 0.03651046
Test loss (w/o reg) on all data: 0.018469758
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6681485e-06
Norm of the params: 8.770073
              Random: fixed   3 labels. Loss 0.01847. Accuracy 0.999.
### Flips: 410, rs: 3, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1838134e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2331612e-08
Norm of the params: 6.092816
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03825978
Train loss (w/o reg) on all data: 0.034380212
Test loss (w/o reg) on all data: 0.016843716
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5643811e-06
Norm of the params: 8.808595
              Random: fixed   7 labels. Loss 0.01684. Accuracy 0.999.
### Flips: 410, rs: 3, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.00533315e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2217578e-08
Norm of the params: 6.0928164
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038259774
Train loss (w/o reg) on all data: 0.034379587
Test loss (w/o reg) on all data: 0.01684381
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2492308e-06
Norm of the params: 8.8093
              Random: fixed   7 labels. Loss 0.01684. Accuracy 0.999.
### Flips: 410, rs: 3, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0059563e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2223516e-08
Norm of the params: 6.0928164
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03563936
Train loss (w/o reg) on all data: 0.03164873
Test loss (w/o reg) on all data: 0.016043704
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.413184e-06
Norm of the params: 8.933792
              Random: fixed  11 labels. Loss 0.01604. Accuracy 0.998.
### Flips: 410, rs: 3, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601225
Test loss (w/o reg) on all data: 0.0026560947
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.826758e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.721713e-08
Norm of the params: 6.0928197
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035211857
Train loss (w/o reg) on all data: 0.031154942
Test loss (w/o reg) on all data: 0.015790114
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2301406e-06
Norm of the params: 9.007682
              Random: fixed  12 labels. Loss 0.01579. Accuracy 0.998.
### Flips: 410, rs: 3, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560912
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4560904e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1062892e-08
Norm of the params: 6.0928216
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03381613
Train loss (w/o reg) on all data: 0.02990479
Test loss (w/o reg) on all data: 0.014814737
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.061377e-06
Norm of the params: 8.84459
              Random: fixed  15 labels. Loss 0.01481. Accuracy 0.999.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04348616
Train loss (w/o reg) on all data: 0.039116994
Test loss (w/o reg) on all data: 0.030281844
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.4999038e-06
Norm of the params: 9.347905
Flipped loss: 0.03028. Accuracy: 0.992
### Flips: 410, rs: 4, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9704563e-08
Norm of the params: 6.092831
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004045902
Train loss (w/o reg) on all data: 0.0015994769
Test loss (w/o reg) on all data: 0.0046502743
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2913413e-07
Norm of the params: 6.994891
                Loss: fixed  61 labels. Loss 0.00465. Accuracy 0.999.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042016793
Train loss (w/o reg) on all data: 0.03753361
Test loss (w/o reg) on all data: 0.029163888
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3121121e-05
Norm of the params: 9.469089
              Random: fixed   3 labels. Loss 0.02916. Accuracy 0.992.
### Flips: 410, rs: 4, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960133
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8106222e-08
Norm of the params: 6.092794
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8612687e-08
Norm of the params: 6.092823
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041021638
Train loss (w/o reg) on all data: 0.036556136
Test loss (w/o reg) on all data: 0.027486846
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.2335766e-06
Norm of the params: 9.4504
              Random: fixed   5 labels. Loss 0.02749. Accuracy 0.993.
### Flips: 410, rs: 4, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601369
Test loss (w/o reg) on all data: 0.0026560933
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9479933e-08
Norm of the params: 6.0927877
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1070715e-08
Norm of the params: 6.0928235
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04035354
Train loss (w/o reg) on all data: 0.03578831
Test loss (w/o reg) on all data: 0.027142745
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.992154e-06
Norm of the params: 9.555345
              Random: fixed   7 labels. Loss 0.02714. Accuracy 0.993.
### Flips: 410, rs: 4, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013077
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1829182e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2412283e-08
Norm of the params: 6.0928164
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038998522
Train loss (w/o reg) on all data: 0.034376934
Test loss (w/o reg) on all data: 0.02556302
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.3838153e-06
Norm of the params: 9.614144
              Random: fixed   9 labels. Loss 0.02556. Accuracy 0.992.
### Flips: 410, rs: 4, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9506933e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.543809e-08
Norm of the params: 6.0928197
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034946226
Train loss (w/o reg) on all data: 0.030340368
Test loss (w/o reg) on all data: 0.022128284
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5133982e-06
Norm of the params: 9.59777
              Random: fixed  16 labels. Loss 0.02213. Accuracy 0.994.
### Flips: 410, rs: 4, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.450706e-09
Norm of the params: 6.092816
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.4273485e-09
Norm of the params: 6.0928164
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031232532
Train loss (w/o reg) on all data: 0.026833108
Test loss (w/o reg) on all data: 0.018548463
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.106685e-06
Norm of the params: 9.3802185
              Random: fixed  23 labels. Loss 0.01855. Accuracy 0.997.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03651449
Train loss (w/o reg) on all data: 0.032036904
Test loss (w/o reg) on all data: 0.019777453
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5634475e-06
Norm of the params: 9.46318
Flipped loss: 0.01978. Accuracy: 0.998
### Flips: 410, rs: 5, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601081
Test loss (w/o reg) on all data: 0.0026560426
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2286153e-08
Norm of the params: 6.092834
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0338899e-08
Norm of the params: 6.0928054
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034635678
Train loss (w/o reg) on all data: 0.03020523
Test loss (w/o reg) on all data: 0.019541929
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.281299e-06
Norm of the params: 9.413233
              Random: fixed   4 labels. Loss 0.01954. Accuracy 0.998.
### Flips: 410, rs: 5, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009945
Test loss (w/o reg) on all data: 0.00265602
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9843146e-08
Norm of the params: 6.0928493
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5309305e-08
Norm of the params: 6.092817
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033803362
Train loss (w/o reg) on all data: 0.02932189
Test loss (w/o reg) on all data: 0.019194104
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.013714e-06
Norm of the params: 9.467285
              Random: fixed   5 labels. Loss 0.01919. Accuracy 0.998.
### Flips: 410, rs: 5, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010935
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.408495e-08
Norm of the params: 6.0928326
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.535647e-09
Norm of the params: 6.092813
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033803366
Train loss (w/o reg) on all data: 0.029321408
Test loss (w/o reg) on all data: 0.019193629
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.346277e-06
Norm of the params: 9.467796
              Random: fixed   5 labels. Loss 0.01919. Accuracy 0.998.
### Flips: 410, rs: 5, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601096
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3633191e-08
Norm of the params: 6.092832
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.315041e-09
Norm of the params: 6.092813
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032739017
Train loss (w/o reg) on all data: 0.02834966
Test loss (w/o reg) on all data: 0.018224856
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.137969e-06
Norm of the params: 9.369479
              Random: fixed   8 labels. Loss 0.01822. Accuracy 0.998.
### Flips: 410, rs: 5, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8117529e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5380452e-08
Norm of the params: 6.0928173
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03131007
Train loss (w/o reg) on all data: 0.026923262
Test loss (w/o reg) on all data: 0.018254237
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.778307e-07
Norm of the params: 9.36676
              Random: fixed  10 labels. Loss 0.01825. Accuracy 0.998.
### Flips: 410, rs: 5, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026561029
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4776876e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3650577e-08
Norm of the params: 6.0928187
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030437868
Train loss (w/o reg) on all data: 0.02609058
Test loss (w/o reg) on all data: 0.016056933
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.165787e-07
Norm of the params: 9.3244705
              Random: fixed  11 labels. Loss 0.01606. Accuracy 0.998.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044907767
Train loss (w/o reg) on all data: 0.040762067
Test loss (w/o reg) on all data: 0.030455796
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.730156e-06
Norm of the params: 9.105713
Flipped loss: 0.03046. Accuracy: 0.990
### Flips: 410, rs: 6, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010347
Test loss (w/o reg) on all data: 0.002656031
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9932037e-08
Norm of the params: 6.0928416
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0158778e-08
Norm of the params: 6.0928116
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042756684
Train loss (w/o reg) on all data: 0.038541786
Test loss (w/o reg) on all data: 0.029429154
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9766626e-06
Norm of the params: 9.18139
              Random: fixed   4 labels. Loss 0.02943. Accuracy 0.993.
### Flips: 410, rs: 6, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.203791e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601266
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1942215e-08
Norm of the params: 6.092804
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042353608
Train loss (w/o reg) on all data: 0.038107056
Test loss (w/o reg) on all data: 0.028961312
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3720407e-06
Norm of the params: 9.215804
              Random: fixed   5 labels. Loss 0.02896. Accuracy 0.993.
### Flips: 410, rs: 6, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601001
Test loss (w/o reg) on all data: 0.0026560263
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7078024e-08
Norm of the params: 6.0928473
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.081445e-09
Norm of the params: 6.0928164
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042500924
Train loss (w/o reg) on all data: 0.038250115
Test loss (w/o reg) on all data: 0.027464604
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.426806e-06
Norm of the params: 9.220424
              Random: fixed   7 labels. Loss 0.02746. Accuracy 0.995.
### Flips: 410, rs: 6, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2909192e-08
Norm of the params: 6.0928125
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.518624e-09
Norm of the params: 6.092822
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04185503
Train loss (w/o reg) on all data: 0.037708398
Test loss (w/o reg) on all data: 0.026702806
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.2668493e-06
Norm of the params: 9.106734
              Random: fixed   9 labels. Loss 0.02670. Accuracy 0.994.
### Flips: 410, rs: 6, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6885627e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2950707e-08
Norm of the params: 6.0928173
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04030327
Train loss (w/o reg) on all data: 0.03619468
Test loss (w/o reg) on all data: 0.023502912
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.6472468e-06
Norm of the params: 9.06487
              Random: fixed  14 labels. Loss 0.02350. Accuracy 0.995.
### Flips: 410, rs: 6, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601343
Test loss (w/o reg) on all data: 0.0026560891
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7502115e-08
Norm of the params: 6.092792
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17128245e-08
Norm of the params: 6.092824
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03936099
Train loss (w/o reg) on all data: 0.03531465
Test loss (w/o reg) on all data: 0.022485523
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0132532e-06
Norm of the params: 8.995932
              Random: fixed  17 labels. Loss 0.02249. Accuracy 0.995.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040849917
Train loss (w/o reg) on all data: 0.03677334
Test loss (w/o reg) on all data: 0.019474497
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.341381e-06
Norm of the params: 9.029482
Flipped loss: 0.01947. Accuracy: 0.998
### Flips: 410, rs: 7, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9556266e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2558672e-08
Norm of the params: 6.092814
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040130153
Train loss (w/o reg) on all data: 0.03609959
Test loss (w/o reg) on all data: 0.018390857
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.0424825e-06
Norm of the params: 8.978378
              Random: fixed   2 labels. Loss 0.01839. Accuracy 0.998.
### Flips: 410, rs: 7, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7100265e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0808688e-08
Norm of the params: 6.0928135
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038953733
Train loss (w/o reg) on all data: 0.03494432
Test loss (w/o reg) on all data: 0.017328111
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.5894044e-06
Norm of the params: 8.954792
              Random: fixed   4 labels. Loss 0.01733. Accuracy 0.998.
### Flips: 410, rs: 7, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6263306e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.016046e-08
Norm of the params: 6.092811
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037923343
Train loss (w/o reg) on all data: 0.033926297
Test loss (w/o reg) on all data: 0.016997555
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4973452e-06
Norm of the params: 8.94097
              Random: fixed   6 labels. Loss 0.01700. Accuracy 0.998.
### Flips: 410, rs: 7, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601388
Test loss (w/o reg) on all data: 0.0026561017
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.970609e-08
Norm of the params: 6.0927844
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8319026e-08
Norm of the params: 6.0928173
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03609182
Train loss (w/o reg) on all data: 0.03210844
Test loss (w/o reg) on all data: 0.016351322
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.613419e-06
Norm of the params: 8.925672
              Random: fixed   8 labels. Loss 0.01635. Accuracy 0.998.
### Flips: 410, rs: 7, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9183638e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.32001725e-08
Norm of the params: 6.0928164
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035107084
Train loss (w/o reg) on all data: 0.031011479
Test loss (w/o reg) on all data: 0.016024057
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.956297e-06
Norm of the params: 9.050531
              Random: fixed   9 labels. Loss 0.01602. Accuracy 0.998.
### Flips: 410, rs: 7, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011115
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8455841e-08
Norm of the params: 6.092829
     Influence (LOO): fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.809779e-09
Norm of the params: 6.092816
                Loss: fixed  51 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03345704
Train loss (w/o reg) on all data: 0.029278444
Test loss (w/o reg) on all data: 0.014177488
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2216932e-06
Norm of the params: 9.141768
              Random: fixed  13 labels. Loss 0.01418. Accuracy 0.999.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03481004
Train loss (w/o reg) on all data: 0.030629873
Test loss (w/o reg) on all data: 0.017995892
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.236029e-06
Norm of the params: 9.143485
Flipped loss: 0.01800. Accuracy: 0.999
### Flips: 410, rs: 8, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8063469e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.006624e-09
Norm of the params: 6.092827
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033877283
Train loss (w/o reg) on all data: 0.029682571
Test loss (w/o reg) on all data: 0.017608572
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.5015846e-06
Norm of the params: 9.159382
              Random: fixed   1 labels. Loss 0.01761. Accuracy 0.999.
### Flips: 410, rs: 8, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5179134e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3105585e-08
Norm of the params: 6.092822
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032322757
Train loss (w/o reg) on all data: 0.027955545
Test loss (w/o reg) on all data: 0.01702802
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2999276e-06
Norm of the params: 9.345814
              Random: fixed   3 labels. Loss 0.01703. Accuracy 1.000.
### Flips: 410, rs: 8, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.00265608
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2184693e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2490296e-08
Norm of the params: 6.0928226
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03056318
Train loss (w/o reg) on all data: 0.026280204
Test loss (w/o reg) on all data: 0.015406695
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1110207e-06
Norm of the params: 9.255241
              Random: fixed   6 labels. Loss 0.01541. Accuracy 1.000.
### Flips: 410, rs: 8, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162408
Train loss (w/o reg) on all data: 0.0009601486
Test loss (w/o reg) on all data: 0.0026561029
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.854144e-08
Norm of the params: 6.09277
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2742734e-08
Norm of the params: 6.0928235
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027644165
Train loss (w/o reg) on all data: 0.023396255
Test loss (w/o reg) on all data: 0.0143927
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0085313e-06
Norm of the params: 9.217276
              Random: fixed  12 labels. Loss 0.01439. Accuracy 0.998.
### Flips: 410, rs: 8, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601328
Test loss (w/o reg) on all data: 0.0026561052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6836263e-08
Norm of the params: 6.092794
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601094
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9270065e-08
Norm of the params: 6.092832
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026591197
Train loss (w/o reg) on all data: 0.022387974
Test loss (w/o reg) on all data: 0.013217139
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.210787e-06
Norm of the params: 9.168668
              Random: fixed  13 labels. Loss 0.01322. Accuracy 0.999.
### Flips: 410, rs: 8, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.873813e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1299376e-08
Norm of the params: 6.092822
                Loss: fixed  42 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02487945
Train loss (w/o reg) on all data: 0.020729063
Test loss (w/o reg) on all data: 0.012723014
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.562144e-06
Norm of the params: 9.110857
              Random: fixed  16 labels. Loss 0.01272. Accuracy 0.998.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045882177
Train loss (w/o reg) on all data: 0.041490234
Test loss (w/o reg) on all data: 0.023009516
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9211545e-06
Norm of the params: 9.372236
Flipped loss: 0.02301. Accuracy: 0.997
### Flips: 410, rs: 9, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3655587e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012576
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2912562e-08
Norm of the params: 6.092806
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042650484
Train loss (w/o reg) on all data: 0.038232572
Test loss (w/o reg) on all data: 0.021610688
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.8971866e-06
Norm of the params: 9.399905
              Random: fixed   5 labels. Loss 0.02161. Accuracy 0.997.
### Flips: 410, rs: 9, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6670349e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3764791e-08
Norm of the params: 6.0928197
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040331054
Train loss (w/o reg) on all data: 0.035606194
Test loss (w/o reg) on all data: 0.020679586
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.8390077e-06
Norm of the params: 9.720967
              Random: fixed   8 labels. Loss 0.02068. Accuracy 0.998.
### Flips: 410, rs: 9, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6907977e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1588019e-08
Norm of the params: 6.092815
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03865087
Train loss (w/o reg) on all data: 0.03377904
Test loss (w/o reg) on all data: 0.019479817
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.236551e-06
Norm of the params: 9.870999
              Random: fixed  10 labels. Loss 0.01948. Accuracy 0.998.
### Flips: 410, rs: 9, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601258
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4554602e-08
Norm of the params: 6.092805
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0143938e-08
Norm of the params: 6.092821
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036880683
Train loss (w/o reg) on all data: 0.032025203
Test loss (w/o reg) on all data: 0.01926578
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2739243e-06
Norm of the params: 9.85442
              Random: fixed  13 labels. Loss 0.01927. Accuracy 0.998.
### Flips: 410, rs: 9, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601382
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7941085e-08
Norm of the params: 6.0927844
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560435
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2577234e-08
Norm of the params: 6.0928235
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03547827
Train loss (w/o reg) on all data: 0.030822285
Test loss (w/o reg) on all data: 0.01809063
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7617286e-06
Norm of the params: 9.649856
              Random: fixed  17 labels. Loss 0.01809. Accuracy 0.997.
### Flips: 410, rs: 9, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2151475e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.005272e-09
Norm of the params: 6.0928164
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033042163
Train loss (w/o reg) on all data: 0.02841198
Test loss (w/o reg) on all data: 0.015862338
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5517687e-06
Norm of the params: 9.623076
              Random: fixed  21 labels. Loss 0.01586. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04164438
Train loss (w/o reg) on all data: 0.03736253
Test loss (w/o reg) on all data: 0.023521474
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.036272e-06
Norm of the params: 9.254026
Flipped loss: 0.02352. Accuracy: 0.995
### Flips: 410, rs: 10, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7250592e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3627816e-08
Norm of the params: 6.0928173
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039452862
Train loss (w/o reg) on all data: 0.035273936
Test loss (w/o reg) on all data: 0.019970652
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6356473e-06
Norm of the params: 9.142131
              Random: fixed   4 labels. Loss 0.01997. Accuracy 0.996.
### Flips: 410, rs: 10, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012704
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0953816e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011074
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8765363e-08
Norm of the params: 6.09283
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034751303
Train loss (w/o reg) on all data: 0.030680524
Test loss (w/o reg) on all data: 0.017064119
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.815164e-06
Norm of the params: 9.023058
              Random: fixed  11 labels. Loss 0.01706. Accuracy 0.998.
### Flips: 410, rs: 10, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4780919e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011825
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1345723e-09
Norm of the params: 6.092818
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03217613
Train loss (w/o reg) on all data: 0.02800521
Test loss (w/o reg) on all data: 0.015921315
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3368331e-06
Norm of the params: 9.133367
              Random: fixed  15 labels. Loss 0.01592. Accuracy 0.998.
### Flips: 410, rs: 10, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601553
Test loss (w/o reg) on all data: 0.0026561436
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0140116e-07
Norm of the params: 6.0927567
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5294052e-08
Norm of the params: 6.09282
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02856841
Train loss (w/o reg) on all data: 0.024655294
Test loss (w/o reg) on all data: 0.013496725
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0570925e-06
Norm of the params: 8.8466
              Random: fixed  22 labels. Loss 0.01350. Accuracy 0.999.
### Flips: 410, rs: 10, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3481456e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.542175e-09
Norm of the params: 6.0928216
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028092802
Train loss (w/o reg) on all data: 0.024293058
Test loss (w/o reg) on all data: 0.013273304
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.796748e-07
Norm of the params: 8.717504
              Random: fixed  23 labels. Loss 0.01327. Accuracy 0.997.
### Flips: 410, rs: 10, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011383
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2429524e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.8956745e-09
Norm of the params: 6.092811
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027062336
Train loss (w/o reg) on all data: 0.023201423
Test loss (w/o reg) on all data: 0.012587589
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6629007e-06
Norm of the params: 8.787394
              Random: fixed  24 labels. Loss 0.01259. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045612868
Train loss (w/o reg) on all data: 0.04132223
Test loss (w/o reg) on all data: 0.020986233
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1640592e-06
Norm of the params: 9.263518
Flipped loss: 0.02099. Accuracy: 0.998
### Flips: 410, rs: 11, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5383481e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4328949e-08
Norm of the params: 6.0928073
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041043915
Train loss (w/o reg) on all data: 0.036563948
Test loss (w/o reg) on all data: 0.02006306
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3318673e-06
Norm of the params: 9.4656925
              Random: fixed   7 labels. Loss 0.02006. Accuracy 0.998.
### Flips: 410, rs: 11, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3057717e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4246008e-08
Norm of the params: 6.0928116
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03938731
Train loss (w/o reg) on all data: 0.034969382
Test loss (w/o reg) on all data: 0.01997545
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8325346e-06
Norm of the params: 9.399924
              Random: fixed   9 labels. Loss 0.01998. Accuracy 0.998.
### Flips: 410, rs: 11, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010737
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5837761e-08
Norm of the params: 6.092836
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.879608e-09
Norm of the params: 6.092813
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037077125
Train loss (w/o reg) on all data: 0.03269377
Test loss (w/o reg) on all data: 0.019441603
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.5300504e-06
Norm of the params: 9.363072
              Random: fixed  13 labels. Loss 0.01944. Accuracy 0.997.
### Flips: 410, rs: 11, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.002656088
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4452928e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.682976e-08
Norm of the params: 6.0928254
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034637775
Train loss (w/o reg) on all data: 0.030034507
Test loss (w/o reg) on all data: 0.01856363
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.2829583e-06
Norm of the params: 9.595071
              Random: fixed  17 labels. Loss 0.01856. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5588103e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0277171e-08
Norm of the params: 6.092813
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034392368
Train loss (w/o reg) on all data: 0.02979775
Test loss (w/o reg) on all data: 0.017877178
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.070454e-06
Norm of the params: 9.586053
              Random: fixed  18 labels. Loss 0.01788. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600997
Test loss (w/o reg) on all data: 0.0026560314
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.275441e-08
Norm of the params: 6.092848
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.002656089
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2333767e-08
Norm of the params: 6.092804
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033069346
Train loss (w/o reg) on all data: 0.028322108
Test loss (w/o reg) on all data: 0.017666098
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.2385794e-06
Norm of the params: 9.743958
              Random: fixed  20 labels. Loss 0.01767. Accuracy 0.996.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042585306
Train loss (w/o reg) on all data: 0.039019745
Test loss (w/o reg) on all data: 0.022647237
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8802561e-06
Norm of the params: 8.444598
Flipped loss: 0.02265. Accuracy: 0.997
### Flips: 410, rs: 12, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601223
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3807607e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.271117e-08
Norm of the params: 6.092824
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04143041
Train loss (w/o reg) on all data: 0.037788395
Test loss (w/o reg) on all data: 0.021997366
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8287451e-06
Norm of the params: 8.534651
              Random: fixed   2 labels. Loss 0.02200. Accuracy 0.996.
### Flips: 410, rs: 12, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.069059e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.144819e-09
Norm of the params: 6.092812
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039901033
Train loss (w/o reg) on all data: 0.036272567
Test loss (w/o reg) on all data: 0.021281786
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.7335037e-06
Norm of the params: 8.518761
              Random: fixed   5 labels. Loss 0.02128. Accuracy 0.996.
### Flips: 410, rs: 12, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2803308e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.339636e-09
Norm of the params: 6.0928116
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037925947
Train loss (w/o reg) on all data: 0.03419242
Test loss (w/o reg) on all data: 0.020153748
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.540101e-07
Norm of the params: 8.641212
              Random: fixed   8 labels. Loss 0.02015. Accuracy 0.998.
### Flips: 410, rs: 12, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5580243e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1018848e-08
Norm of the params: 6.0928183
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034039643
Train loss (w/o reg) on all data: 0.030062659
Test loss (w/o reg) on all data: 0.019377477
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.6513717e-06
Norm of the params: 8.918503
              Random: fixed  13 labels. Loss 0.01938. Accuracy 0.996.
### Flips: 410, rs: 12, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.336525e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.990092e-08
Norm of the params: 6.0928335
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032235682
Train loss (w/o reg) on all data: 0.02810343
Test loss (w/o reg) on all data: 0.018549312
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.6736086e-06
Norm of the params: 9.090934
              Random: fixed  16 labels. Loss 0.01855. Accuracy 0.995.
### Flips: 410, rs: 12, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5971354e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0516263e-08
Norm of the params: 6.0928235
                Loss: fixed  54 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031265665
Train loss (w/o reg) on all data: 0.02719162
Test loss (w/o reg) on all data: 0.017245108
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4497369e-06
Norm of the params: 9.026679
              Random: fixed  19 labels. Loss 0.01725. Accuracy 0.996.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03950054
Train loss (w/o reg) on all data: 0.0348696
Test loss (w/o reg) on all data: 0.01914025
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.0351184e-06
Norm of the params: 9.623865
Flipped loss: 0.01914. Accuracy: 0.998
### Flips: 410, rs: 13, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2277667e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4268339e-08
Norm of the params: 6.0928154
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038644344
Train loss (w/o reg) on all data: 0.033938702
Test loss (w/o reg) on all data: 0.019149488
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0954518e-06
Norm of the params: 9.701175
              Random: fixed   2 labels. Loss 0.01915. Accuracy 0.996.
### Flips: 410, rs: 13, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4251446e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.131723e-09
Norm of the params: 6.0928288
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036643066
Train loss (w/o reg) on all data: 0.031940706
Test loss (w/o reg) on all data: 0.01927643
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.7174416e-06
Norm of the params: 9.697795
              Random: fixed   6 labels. Loss 0.01928. Accuracy 0.996.
### Flips: 410, rs: 13, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.097542e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.63296e-09
Norm of the params: 6.0928144
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034906723
Train loss (w/o reg) on all data: 0.03020305
Test loss (w/o reg) on all data: 0.019320417
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4834638e-06
Norm of the params: 9.699147
              Random: fixed   9 labels. Loss 0.01932. Accuracy 0.995.
### Flips: 410, rs: 13, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010405
Test loss (w/o reg) on all data: 0.002656013
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.466888e-08
Norm of the params: 6.0928416
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6873135e-08
Norm of the params: 6.092829
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033186797
Train loss (w/o reg) on all data: 0.028529873
Test loss (w/o reg) on all data: 0.01903482
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.942458e-06
Norm of the params: 9.650828
              Random: fixed  13 labels. Loss 0.01903. Accuracy 0.995.
### Flips: 410, rs: 13, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011005
Test loss (w/o reg) on all data: 0.0026560442
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.849159e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3311484e-08
Norm of the params: 6.0928164
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03208384
Train loss (w/o reg) on all data: 0.027365832
Test loss (w/o reg) on all data: 0.017237002
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7947085e-06
Norm of the params: 9.713913
              Random: fixed  16 labels. Loss 0.01724. Accuracy 0.995.
### Flips: 410, rs: 13, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.872723e-09
Norm of the params: 6.0928135
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.23907515e-08
Norm of the params: 6.0928183
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02819823
Train loss (w/o reg) on all data: 0.023454443
Test loss (w/o reg) on all data: 0.016120555
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.668036e-06
Norm of the params: 9.740417
              Random: fixed  23 labels. Loss 0.01612. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042339165
Train loss (w/o reg) on all data: 0.03788916
Test loss (w/o reg) on all data: 0.025534865
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.9976823e-06
Norm of the params: 9.433986
Flipped loss: 0.02553. Accuracy: 0.993
### Flips: 410, rs: 14, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2573042e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7084758e-08
Norm of the params: 6.0928235
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039206237
Train loss (w/o reg) on all data: 0.034678806
Test loss (w/o reg) on all data: 0.02267468
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.2843658e-06
Norm of the params: 9.515704
              Random: fixed   4 labels. Loss 0.02267. Accuracy 0.995.
### Flips: 410, rs: 14, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4743297e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.0695334e-09
Norm of the params: 6.0928144
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037094
Train loss (w/o reg) on all data: 0.03268578
Test loss (w/o reg) on all data: 0.019259706
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.1584514e-06
Norm of the params: 9.389591
              Random: fixed   8 labels. Loss 0.01926. Accuracy 0.995.
### Flips: 410, rs: 14, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1803908e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4486512e-08
Norm of the params: 6.0928044
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035570253
Train loss (w/o reg) on all data: 0.03116383
Test loss (w/o reg) on all data: 0.018948687
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6518565e-06
Norm of the params: 9.387675
              Random: fixed  11 labels. Loss 0.01895. Accuracy 0.995.
### Flips: 410, rs: 14, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.399378e-09
Norm of the params: 6.092813
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.726003e-09
Norm of the params: 6.0928173
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034333833
Train loss (w/o reg) on all data: 0.02979309
Test loss (w/o reg) on all data: 0.018261809
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.5244816e-06
Norm of the params: 9.529683
              Random: fixed  13 labels. Loss 0.01826. Accuracy 0.995.
### Flips: 410, rs: 14, checks: 1025
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.782921e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6908787e-08
Norm of the params: 6.0928135
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032883003
Train loss (w/o reg) on all data: 0.028462226
Test loss (w/o reg) on all data: 0.017302627
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9880472e-06
Norm of the params: 9.402954
              Random: fixed  17 labels. Loss 0.01730. Accuracy 0.996.
### Flips: 410, rs: 14, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5426552e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.575194e-09
Norm of the params: 6.0928135
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030233163
Train loss (w/o reg) on all data: 0.025759416
Test loss (w/o reg) on all data: 0.016369058
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5308902e-06
Norm of the params: 9.45912
              Random: fixed  21 labels. Loss 0.01637. Accuracy 0.997.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049341246
Train loss (w/o reg) on all data: 0.044504948
Test loss (w/o reg) on all data: 0.034480777
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5208387e-06
Norm of the params: 9.834937
Flipped loss: 0.03448. Accuracy: 0.991
### Flips: 410, rs: 15, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010737
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1231175e-08
Norm of the params: 6.092835
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2754281e-08
Norm of the params: 6.092822
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048603393
Train loss (w/o reg) on all data: 0.043952134
Test loss (w/o reg) on all data: 0.033898916
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.995825e-06
Norm of the params: 9.644958
              Random: fixed   2 labels. Loss 0.03390. Accuracy 0.992.
### Flips: 410, rs: 15, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601091
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0832492e-08
Norm of the params: 6.092832
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7475346e-08
Norm of the params: 6.0928082
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045017384
Train loss (w/o reg) on all data: 0.0400266
Test loss (w/o reg) on all data: 0.03157886
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.609144e-06
Norm of the params: 9.990777
              Random: fixed   8 labels. Loss 0.03158. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5148057e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.259934e-09
Norm of the params: 6.0928144
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043808617
Train loss (w/o reg) on all data: 0.038940493
Test loss (w/o reg) on all data: 0.029982412
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.884858e-06
Norm of the params: 9.867242
              Random: fixed  12 labels. Loss 0.02998. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7343337e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1871037e-08
Norm of the params: 6.0928082
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042479802
Train loss (w/o reg) on all data: 0.037528835
Test loss (w/o reg) on all data: 0.029431624
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6474082e-06
Norm of the params: 9.950845
              Random: fixed  14 labels. Loss 0.02943. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601257
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.045435e-08
Norm of the params: 6.092805
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0305801e-08
Norm of the params: 6.0928144
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041296657
Train loss (w/o reg) on all data: 0.036610913
Test loss (w/o reg) on all data: 0.028564664
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.909077e-06
Norm of the params: 9.680645
              Random: fixed  18 labels. Loss 0.02856. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2870194e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7481655e-08
Norm of the params: 6.092819
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03962906
Train loss (w/o reg) on all data: 0.03479399
Test loss (w/o reg) on all data: 0.028265253
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.320382e-06
Norm of the params: 9.833684
              Random: fixed  21 labels. Loss 0.02827. Accuracy 0.994.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03861392
Train loss (w/o reg) on all data: 0.03387517
Test loss (w/o reg) on all data: 0.020468166
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7277513e-06
Norm of the params: 9.735244
Flipped loss: 0.02047. Accuracy: 0.997
### Flips: 410, rs: 16, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601015
Test loss (w/o reg) on all data: 0.002656042
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0563382e-08
Norm of the params: 6.0928454
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3005253e-08
Norm of the params: 6.0928154
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037887394
Train loss (w/o reg) on all data: 0.033174556
Test loss (w/o reg) on all data: 0.020267375
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.977418e-06
Norm of the params: 9.708591
              Random: fixed   1 labels. Loss 0.02027. Accuracy 0.997.
### Flips: 410, rs: 16, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4694116e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012483
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6171123e-08
Norm of the params: 6.0928073
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03613651
Train loss (w/o reg) on all data: 0.031400185
Test loss (w/o reg) on all data: 0.019407412
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.556107e-07
Norm of the params: 9.732755
              Random: fixed   4 labels. Loss 0.01941. Accuracy 0.998.
### Flips: 410, rs: 16, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7603491e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0300946e-08
Norm of the params: 6.0928135
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03570077
Train loss (w/o reg) on all data: 0.031016609
Test loss (w/o reg) on all data: 0.019165607
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7739001e-06
Norm of the params: 9.6790085
              Random: fixed   5 labels. Loss 0.01917. Accuracy 0.997.
### Flips: 410, rs: 16, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5406363e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.001688e-08
Norm of the params: 6.0928073
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03532867
Train loss (w/o reg) on all data: 0.030738033
Test loss (w/o reg) on all data: 0.019101262
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.9611767e-06
Norm of the params: 9.581899
              Random: fixed   6 labels. Loss 0.01910. Accuracy 0.998.
### Flips: 410, rs: 16, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.387219e-08
Norm of the params: 6.092833
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0201215e-08
Norm of the params: 6.092808
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03404102
Train loss (w/o reg) on all data: 0.029277965
Test loss (w/o reg) on all data: 0.018080711
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3426957e-06
Norm of the params: 9.7601795
              Random: fixed   8 labels. Loss 0.01808. Accuracy 0.997.
### Flips: 410, rs: 16, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0401035e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2799559e-08
Norm of the params: 6.092817
                Loss: fixed  48 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033127673
Train loss (w/o reg) on all data: 0.028571395
Test loss (w/o reg) on all data: 0.017883908
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4355022e-06
Norm of the params: 9.545971
              Random: fixed  10 labels. Loss 0.01788. Accuracy 0.998.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418408
Train loss (w/o reg) on all data: 0.037594292
Test loss (w/o reg) on all data: 0.020217055
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.096823e-06
Norm of the params: 9.2157545
Flipped loss: 0.02022. Accuracy: 0.999
### Flips: 410, rs: 17, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6669962e-08
Norm of the params: 6.092808
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0072724e-08
Norm of the params: 6.0928173
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040737968
Train loss (w/o reg) on all data: 0.036602285
Test loss (w/o reg) on all data: 0.019225655
Train acc on all data:  0.9883296863603209
Test acc on all data:   1.0
Norm of the mean of gradients: 7.979726e-06
Norm of the params: 9.094704
              Random: fixed   3 labels. Loss 0.01923. Accuracy 1.000.
### Flips: 410, rs: 17, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2492264e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9498977e-08
Norm of the params: 6.092818
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03944735
Train loss (w/o reg) on all data: 0.03550349
Test loss (w/o reg) on all data: 0.017950324
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6260717e-06
Norm of the params: 8.881282
              Random: fixed   5 labels. Loss 0.01795. Accuracy 0.999.
### Flips: 410, rs: 17, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1507383e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560454
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5474558e-08
Norm of the params: 6.092829
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039447352
Train loss (w/o reg) on all data: 0.03550275
Test loss (w/o reg) on all data: 0.01794968
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3983397e-06
Norm of the params: 8.882117
              Random: fixed   5 labels. Loss 0.01795. Accuracy 0.999.
### Flips: 410, rs: 17, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1586186e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5526228e-08
Norm of the params: 6.0928288
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037822187
Train loss (w/o reg) on all data: 0.033890128
Test loss (w/o reg) on all data: 0.016923202
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6917762e-06
Norm of the params: 8.867986
              Random: fixed   8 labels. Loss 0.01692. Accuracy 1.000.
### Flips: 410, rs: 17, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960127
Test loss (w/o reg) on all data: 0.0026560891
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.375914e-08
Norm of the params: 6.092804
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.696043e-09
Norm of the params: 6.092816
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036095157
Train loss (w/o reg) on all data: 0.032078702
Test loss (w/o reg) on all data: 0.016801098
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4729582e-06
Norm of the params: 8.962651
              Random: fixed  11 labels. Loss 0.01680. Accuracy 0.998.
### Flips: 410, rs: 17, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3063225e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1778316e-08
Norm of the params: 6.0928144
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03254868
Train loss (w/o reg) on all data: 0.02839162
Test loss (w/o reg) on all data: 0.014946459
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.042774e-07
Norm of the params: 9.11818
              Random: fixed  15 labels. Loss 0.01495. Accuracy 0.999.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039423767
Train loss (w/o reg) on all data: 0.03559489
Test loss (w/o reg) on all data: 0.022824055
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0560924e-06
Norm of the params: 8.750859
Flipped loss: 0.02282. Accuracy: 0.996
### Flips: 410, rs: 18, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1039604e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.6477395e-09
Norm of the params: 6.092819
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037679642
Train loss (w/o reg) on all data: 0.0338914
Test loss (w/o reg) on all data: 0.020775734
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.0886043e-06
Norm of the params: 8.704305
              Random: fixed   4 labels. Loss 0.02078. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601325
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.910522e-08
Norm of the params: 6.092795
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960115
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3544284e-08
Norm of the params: 6.0928226
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035826128
Train loss (w/o reg) on all data: 0.032018006
Test loss (w/o reg) on all data: 0.0195683
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5388588e-06
Norm of the params: 8.727108
              Random: fixed   7 labels. Loss 0.01957. Accuracy 0.997.
### Flips: 410, rs: 18, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601456
Test loss (w/o reg) on all data: 0.0026561073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2537424e-08
Norm of the params: 6.0927734
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601119
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2467036e-08
Norm of the params: 6.0928283
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035826128
Train loss (w/o reg) on all data: 0.03201729
Test loss (w/o reg) on all data: 0.019568648
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5409273e-06
Norm of the params: 8.727928
              Random: fixed   7 labels. Loss 0.01957. Accuracy 0.997.
### Flips: 410, rs: 18, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601456
Test loss (w/o reg) on all data: 0.0026561073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.303146e-08
Norm of the params: 6.0927734
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2635724e-08
Norm of the params: 6.092828
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03526032
Train loss (w/o reg) on all data: 0.031417705
Test loss (w/o reg) on all data: 0.019136542
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6621245e-06
Norm of the params: 8.766543
              Random: fixed   8 labels. Loss 0.01914. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8729407e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3858884e-08
Norm of the params: 6.0928097
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032952715
Train loss (w/o reg) on all data: 0.029108154
Test loss (w/o reg) on all data: 0.01737605
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.09916e-06
Norm of the params: 8.768763
              Random: fixed  12 labels. Loss 0.01738. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601665
Test loss (w/o reg) on all data: 0.0026561467
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3212596e-07
Norm of the params: 6.092739
     Influence (LOO): fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0334151e-08
Norm of the params: 6.0928197
                Loss: fixed  53 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032396562
Train loss (w/o reg) on all data: 0.028592376
Test loss (w/o reg) on all data: 0.016993405
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5565885e-06
Norm of the params: 8.722598
              Random: fixed  13 labels. Loss 0.01699. Accuracy 0.995.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036732286
Train loss (w/o reg) on all data: 0.032869518
Test loss (w/o reg) on all data: 0.018196262
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.331104e-06
Norm of the params: 8.789502
Flipped loss: 0.01820. Accuracy: 0.999
### Flips: 410, rs: 19, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011034
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9914486e-08
Norm of the params: 6.0928307
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.950667e-09
Norm of the params: 6.092816
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033727538
Train loss (w/o reg) on all data: 0.030065272
Test loss (w/o reg) on all data: 0.017153509
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.4209594e-07
Norm of the params: 8.558348
              Random: fixed   6 labels. Loss 0.01715. Accuracy 0.997.
### Flips: 410, rs: 19, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.060887e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9463043e-09
Norm of the params: 6.0928173
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033070117
Train loss (w/o reg) on all data: 0.02943161
Test loss (w/o reg) on all data: 0.016827337
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.404877e-06
Norm of the params: 8.530545
              Random: fixed   7 labels. Loss 0.01683. Accuracy 0.998.
### Flips: 410, rs: 19, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2667547e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.47029e-09
Norm of the params: 6.092821
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03234095
Train loss (w/o reg) on all data: 0.028711228
Test loss (w/o reg) on all data: 0.016626937
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.9251544e-06
Norm of the params: 8.520238
              Random: fixed   8 labels. Loss 0.01663. Accuracy 0.997.
### Flips: 410, rs: 19, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601269
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5122958e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16201875e-08
Norm of the params: 6.092823
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031926326
Train loss (w/o reg) on all data: 0.028284572
Test loss (w/o reg) on all data: 0.015891347
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2134565e-06
Norm of the params: 8.534346
              Random: fixed   9 labels. Loss 0.01589. Accuracy 0.998.
### Flips: 410, rs: 19, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4100598e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6877922e-08
Norm of the params: 6.092816
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030434784
Train loss (w/o reg) on all data: 0.026836865
Test loss (w/o reg) on all data: 0.015381138
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.2046255e-06
Norm of the params: 8.482829
              Random: fixed  11 labels. Loss 0.01538. Accuracy 0.996.
### Flips: 410, rs: 19, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9916522e-08
Norm of the params: 6.092827
     Influence (LOO): fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012483
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.538582e-08
Norm of the params: 6.0928073
                Loss: fixed  45 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029207198
Train loss (w/o reg) on all data: 0.025611065
Test loss (w/o reg) on all data: 0.013834704
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.2800416e-07
Norm of the params: 8.480723
              Random: fixed  13 labels. Loss 0.01383. Accuracy 0.998.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03733491
Train loss (w/o reg) on all data: 0.033078935
Test loss (w/o reg) on all data: 0.022375181
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.115988e-06
Norm of the params: 9.226026
Flipped loss: 0.02238. Accuracy: 0.993
### Flips: 410, rs: 20, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003069519
Train loss (w/o reg) on all data: 0.0011410397
Test loss (w/o reg) on all data: 0.002827048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8170107e-08
Norm of the params: 6.2104416
     Influence (LOO): fixed  54 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601231
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2371864e-08
Norm of the params: 6.09281
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03478057
Train loss (w/o reg) on all data: 0.030675476
Test loss (w/o reg) on all data: 0.020780012
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.318883e-06
Norm of the params: 9.0610075
              Random: fixed   4 labels. Loss 0.02078. Accuracy 0.992.
### Flips: 410, rs: 20, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096014165
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4030564e-08
Norm of the params: 6.0927806
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.002656038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5637883e-08
Norm of the params: 6.092831
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033583872
Train loss (w/o reg) on all data: 0.029604098
Test loss (w/o reg) on all data: 0.01838573
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.0039013e-06
Norm of the params: 8.921631
              Random: fixed   8 labels. Loss 0.01839. Accuracy 0.993.
### Flips: 410, rs: 20, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960131
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6539889e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.998092e-08
Norm of the params: 6.0928254
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033167146
Train loss (w/o reg) on all data: 0.029239671
Test loss (w/o reg) on all data: 0.017299227
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1683608e-06
Norm of the params: 8.862814
              Random: fixed  10 labels. Loss 0.01730. Accuracy 0.995.
### Flips: 410, rs: 20, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4137083e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8426764e-08
Norm of the params: 6.0928254
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032225788
Train loss (w/o reg) on all data: 0.028446749
Test loss (w/o reg) on all data: 0.015838876
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.7338857e-06
Norm of the params: 8.693723
              Random: fixed  12 labels. Loss 0.01584. Accuracy 0.995.
### Flips: 410, rs: 20, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8292846e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.47836925e-08
Norm of the params: 6.092811
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030746125
Train loss (w/o reg) on all data: 0.027027838
Test loss (w/o reg) on all data: 0.0149063
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0078862e-06
Norm of the params: 8.623558
              Random: fixed  16 labels. Loss 0.01491. Accuracy 0.996.
### Flips: 410, rs: 20, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601331
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1748677e-08
Norm of the params: 6.0927944
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.59848e-08
Norm of the params: 6.0928297
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027716914
Train loss (w/o reg) on all data: 0.02387532
Test loss (w/o reg) on all data: 0.01322415
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.568004e-07
Norm of the params: 8.765381
              Random: fixed  21 labels. Loss 0.01322. Accuracy 0.997.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039169915
Train loss (w/o reg) on all data: 0.0350451
Test loss (w/o reg) on all data: 0.017782789
Train acc on all data:  0.9885728178944809
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5386859e-06
Norm of the params: 9.08275
Flipped loss: 0.01778. Accuracy: 1.000
### Flips: 410, rs: 21, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013356
Test loss (w/o reg) on all data: 0.0026561029
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1599242e-08
Norm of the params: 6.092793
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010586
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2833378e-08
Norm of the params: 6.0928373
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037588704
Train loss (w/o reg) on all data: 0.03338251
Test loss (w/o reg) on all data: 0.0170621
Train acc on all data:  0.9897884755652808
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1266534e-06
Norm of the params: 9.17191
              Random: fixed   3 labels. Loss 0.01706. Accuracy 1.000.
### Flips: 410, rs: 21, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.416117e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5768906e-08
Norm of the params: 6.092829
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035200745
Train loss (w/o reg) on all data: 0.030923074
Test loss (w/o reg) on all data: 0.015365298
Train acc on all data:  0.9902747386336008
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4445136e-06
Norm of the params: 9.24951
              Random: fixed   6 labels. Loss 0.01537. Accuracy 1.000.
### Flips: 410, rs: 21, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2326683e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0283967e-08
Norm of the params: 6.092806
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034546494
Train loss (w/o reg) on all data: 0.030359443
Test loss (w/o reg) on all data: 0.015234623
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0471057e-06
Norm of the params: 9.151012
              Random: fixed   7 labels. Loss 0.01523. Accuracy 1.000.
### Flips: 410, rs: 21, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3956808e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601281
Test loss (w/o reg) on all data: 0.0026560826
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2719252e-08
Norm of the params: 6.092802
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031822443
Train loss (w/o reg) on all data: 0.027691586
Test loss (w/o reg) on all data: 0.0142116025
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2208718e-06
Norm of the params: 9.089396
              Random: fixed  11 labels. Loss 0.01421. Accuracy 0.998.
### Flips: 410, rs: 21, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7136095e-08
Norm of the params: 6.0928087
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601122
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7607297e-08
Norm of the params: 6.0928273
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030477721
Train loss (w/o reg) on all data: 0.02634012
Test loss (w/o reg) on all data: 0.0135790985
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.22337e-06
Norm of the params: 9.096814
              Random: fixed  13 labels. Loss 0.01358. Accuracy 0.999.
### Flips: 410, rs: 21, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9758504e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0428422e-08
Norm of the params: 6.092821
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03047772
Train loss (w/o reg) on all data: 0.026340727
Test loss (w/o reg) on all data: 0.013579639
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9015325e-06
Norm of the params: 9.096145
              Random: fixed  13 labels. Loss 0.01358. Accuracy 0.999.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04666903
Train loss (w/o reg) on all data: 0.042754877
Test loss (w/o reg) on all data: 0.023329766
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.738798e-06
Norm of the params: 8.847768
Flipped loss: 0.02333. Accuracy: 0.998
### Flips: 410, rs: 22, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.540681e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601272
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4899292e-08
Norm of the params: 6.092803
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045327194
Train loss (w/o reg) on all data: 0.041572392
Test loss (w/o reg) on all data: 0.022942182
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4710619e-06
Norm of the params: 8.665795
              Random: fixed   2 labels. Loss 0.02294. Accuracy 0.997.
### Flips: 410, rs: 22, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010964
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7562606e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.527906e-09
Norm of the params: 6.0928154
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04374109
Train loss (w/o reg) on all data: 0.03994798
Test loss (w/o reg) on all data: 0.022351319
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.820334e-06
Norm of the params: 8.709888
              Random: fixed   5 labels. Loss 0.02235. Accuracy 0.998.
### Flips: 410, rs: 22, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601245
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3738153e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1528624e-08
Norm of the params: 6.09282
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040690444
Train loss (w/o reg) on all data: 0.03675666
Test loss (w/o reg) on all data: 0.022278018
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.6494065e-06
Norm of the params: 8.869931
              Random: fixed  10 labels. Loss 0.02228. Accuracy 0.996.
### Flips: 410, rs: 22, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0156162e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3664838e-08
Norm of the params: 6.092815
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039914556
Train loss (w/o reg) on all data: 0.036032043
Test loss (w/o reg) on all data: 0.02183763
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.661122e-06
Norm of the params: 8.811936
              Random: fixed  12 labels. Loss 0.02184. Accuracy 0.995.
### Flips: 410, rs: 22, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0807951e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9969483e-09
Norm of the params: 6.0928154
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038081236
Train loss (w/o reg) on all data: 0.03424998
Test loss (w/o reg) on all data: 0.021779867
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.244593e-06
Norm of the params: 8.753579
              Random: fixed  15 labels. Loss 0.02178. Accuracy 0.995.
### Flips: 410, rs: 22, checks: 1230
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.140884e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012536
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6554395e-08
Norm of the params: 6.092806
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034679133
Train loss (w/o reg) on all data: 0.030666873
Test loss (w/o reg) on all data: 0.021396529
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2357327e-06
Norm of the params: 8.95797
              Random: fixed  19 labels. Loss 0.02140. Accuracy 0.994.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038305234
Train loss (w/o reg) on all data: 0.033815503
Test loss (w/o reg) on all data: 0.023089297
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.366501e-06
Norm of the params: 9.476004
Flipped loss: 0.02309. Accuracy: 0.996
### Flips: 410, rs: 23, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0508496e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8889168e-08
Norm of the params: 6.0928044
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037550926
Train loss (w/o reg) on all data: 0.033127055
Test loss (w/o reg) on all data: 0.022178665
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6625119e-06
Norm of the params: 9.406243
              Random: fixed   1 labels. Loss 0.02218. Accuracy 0.996.
### Flips: 410, rs: 23, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009724
Test loss (w/o reg) on all data: 0.0026560365
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0133987e-08
Norm of the params: 6.0928516
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.0026560856
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8644214e-08
Norm of the params: 6.0928087
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03557247
Train loss (w/o reg) on all data: 0.031134514
Test loss (w/o reg) on all data: 0.020706218
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1403498e-06
Norm of the params: 9.421206
              Random: fixed   4 labels. Loss 0.02071. Accuracy 0.996.
### Flips: 410, rs: 23, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013036
Test loss (w/o reg) on all data: 0.0026561148
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2603755e-08
Norm of the params: 6.0927978
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5161175e-08
Norm of the params: 6.092828
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032885533
Train loss (w/o reg) on all data: 0.028535949
Test loss (w/o reg) on all data: 0.019806901
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3217116e-06
Norm of the params: 9.326936
              Random: fixed   9 labels. Loss 0.01981. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601335
Test loss (w/o reg) on all data: 0.0026561166
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6115694e-08
Norm of the params: 6.092793
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6289176e-08
Norm of the params: 6.0928288
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032350324
Train loss (w/o reg) on all data: 0.028043207
Test loss (w/o reg) on all data: 0.019528974
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2154821e-06
Norm of the params: 9.281288
              Random: fixed  10 labels. Loss 0.01953. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601292
Test loss (w/o reg) on all data: 0.0026561099
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.440434e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960109
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.857909e-08
Norm of the params: 6.0928335
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029812064
Train loss (w/o reg) on all data: 0.025603551
Test loss (w/o reg) on all data: 0.018703774
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2817305e-06
Norm of the params: 9.174434
              Random: fixed  14 labels. Loss 0.01870. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5947496e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2747208e-08
Norm of the params: 6.092813
                Loss: fixed  50 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028947823
Train loss (w/o reg) on all data: 0.0248604
Test loss (w/o reg) on all data: 0.018170938
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.5883653e-06
Norm of the params: 9.041485
              Random: fixed  16 labels. Loss 0.01817. Accuracy 0.997.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04200051
Train loss (w/o reg) on all data: 0.038074993
Test loss (w/o reg) on all data: 0.022007313
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.8736343e-06
Norm of the params: 8.860608
Flipped loss: 0.02201. Accuracy: 0.997
### Flips: 410, rs: 24, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601299
Test loss (w/o reg) on all data: 0.002656089
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.840071e-08
Norm of the params: 6.092798
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035932385
Train loss (w/o reg) on all data: 0.0013895626
Test loss (w/o reg) on all data: 0.003160754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3117133e-08
Norm of the params: 6.638789
                Loss: fixed  54 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042249348
Train loss (w/o reg) on all data: 0.03835795
Test loss (w/o reg) on all data: 0.021298027
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.776745e-06
Norm of the params: 8.822014
              Random: fixed   1 labels. Loss 0.02130. Accuracy 0.998.
### Flips: 410, rs: 24, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4819467e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6519065e-08
Norm of the params: 6.092825
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041784935
Train loss (w/o reg) on all data: 0.037813347
Test loss (w/o reg) on all data: 0.021051528
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8054458e-05
Norm of the params: 8.912451
              Random: fixed   2 labels. Loss 0.02105. Accuracy 0.998.
### Flips: 410, rs: 24, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5922222e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8180685e-08
Norm of the params: 6.092812
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041021723
Train loss (w/o reg) on all data: 0.037114497
Test loss (w/o reg) on all data: 0.020636499
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2395794e-05
Norm of the params: 8.839937
              Random: fixed   3 labels. Loss 0.02064. Accuracy 0.996.
### Flips: 410, rs: 24, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560242
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.017615e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560859
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1702356e-08
Norm of the params: 6.0928044
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039620154
Train loss (w/o reg) on all data: 0.03577269
Test loss (w/o reg) on all data: 0.019073157
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6787819e-06
Norm of the params: 8.772077
              Random: fixed   7 labels. Loss 0.01907. Accuracy 0.998.
### Flips: 410, rs: 24, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012943
Test loss (w/o reg) on all data: 0.0026561033
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.827121e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560486
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6563374e-08
Norm of the params: 6.092829
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0371832
Train loss (w/o reg) on all data: 0.033285722
Test loss (w/o reg) on all data: 0.018712955
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1276779e-06
Norm of the params: 8.828903
              Random: fixed  11 labels. Loss 0.01871. Accuracy 0.997.
### Flips: 410, rs: 24, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.678018e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2850204e-08
Norm of the params: 6.0928173
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035569407
Train loss (w/o reg) on all data: 0.03163649
Test loss (w/o reg) on all data: 0.017814673
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.7112949e-06
Norm of the params: 8.868954
              Random: fixed  15 labels. Loss 0.01781. Accuracy 0.996.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04328925
Train loss (w/o reg) on all data: 0.039496254
Test loss (w/o reg) on all data: 0.02076798
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3898366e-06
Norm of the params: 8.709763
Flipped loss: 0.02077. Accuracy: 0.997
### Flips: 410, rs: 25, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601166
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1102196e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.851625e-08
Norm of the params: 6.09281
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042401157
Train loss (w/o reg) on all data: 0.038535874
Test loss (w/o reg) on all data: 0.020272318
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.4471385e-06
Norm of the params: 8.792362
              Random: fixed   1 labels. Loss 0.02027. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7351821e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3546488e-08
Norm of the params: 6.0928187
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041158825
Train loss (w/o reg) on all data: 0.03726752
Test loss (w/o reg) on all data: 0.019290455
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.438366e-06
Norm of the params: 8.821908
              Random: fixed   4 labels. Loss 0.01929. Accuracy 0.998.
### Flips: 410, rs: 25, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8877364e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3437845e-08
Norm of the params: 6.092811
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03916596
Train loss (w/o reg) on all data: 0.035272885
Test loss (w/o reg) on all data: 0.017952662
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.454469e-06
Norm of the params: 8.8239155
              Random: fixed   8 labels. Loss 0.01795. Accuracy 0.998.
### Flips: 410, rs: 25, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.876273e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601232
Test loss (w/o reg) on all data: 0.0026560852
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7618202e-08
Norm of the params: 6.09281
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03776645
Train loss (w/o reg) on all data: 0.033897873
Test loss (w/o reg) on all data: 0.016556066
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3054971e-06
Norm of the params: 8.796106
              Random: fixed  11 labels. Loss 0.01656. Accuracy 1.000.
### Flips: 410, rs: 25, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8631207e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6039433e-08
Norm of the params: 6.092816
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034670398
Train loss (w/o reg) on all data: 0.03051843
Test loss (w/o reg) on all data: 0.016201612
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.19363e-06
Norm of the params: 9.112594
              Random: fixed  15 labels. Loss 0.01620. Accuracy 0.999.
### Flips: 410, rs: 25, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5271927e-08
Norm of the params: 6.092807
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010655
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.751639e-08
Norm of the params: 6.0928373
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03318625
Train loss (w/o reg) on all data: 0.029038975
Test loss (w/o reg) on all data: 0.015772417
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.6724293e-07
Norm of the params: 9.107439
              Random: fixed  18 labels. Loss 0.01577. Accuracy 0.998.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047687102
Train loss (w/o reg) on all data: 0.04376856
Test loss (w/o reg) on all data: 0.023739634
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.548982e-06
Norm of the params: 8.852734
Flipped loss: 0.02374. Accuracy: 0.996
### Flips: 410, rs: 26, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.22262405e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6276917e-09
Norm of the params: 6.0928183
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045302782
Train loss (w/o reg) on all data: 0.041436303
Test loss (w/o reg) on all data: 0.022237116
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7060825e-06
Norm of the params: 8.793724
              Random: fixed   4 labels. Loss 0.02224. Accuracy 0.997.
### Flips: 410, rs: 26, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.00265603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.368337e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010615
Test loss (w/o reg) on all data: 0.0026560407
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2976008e-08
Norm of the params: 6.0928373
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044302322
Train loss (w/o reg) on all data: 0.04036388
Test loss (w/o reg) on all data: 0.021519339
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.2520073e-06
Norm of the params: 8.875181
              Random: fixed   6 labels. Loss 0.02152. Accuracy 0.998.
### Flips: 410, rs: 26, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.442572e-09
Norm of the params: 6.0928183
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.002513e-09
Norm of the params: 6.092814
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040721677
Train loss (w/o reg) on all data: 0.036466587
Test loss (w/o reg) on all data: 0.020611217
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.31498e-06
Norm of the params: 9.225065
              Random: fixed  12 labels. Loss 0.02061. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1462327e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5513874e-09
Norm of the params: 6.0928173
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040129025
Train loss (w/o reg) on all data: 0.03577979
Test loss (w/o reg) on all data: 0.020194583
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.961392e-06
Norm of the params: 9.326559
              Random: fixed  13 labels. Loss 0.02019. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0381438e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.506635e-09
Norm of the params: 6.092821
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03788025
Train loss (w/o reg) on all data: 0.033608053
Test loss (w/o reg) on all data: 0.018841011
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2320019e-06
Norm of the params: 9.243588
              Random: fixed  18 labels. Loss 0.01884. Accuracy 0.998.
### Flips: 410, rs: 26, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013135
Test loss (w/o reg) on all data: 0.0026560924
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.607022e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3622629e-08
Norm of the params: 6.0928254
                Loss: fixed  63 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03638146
Train loss (w/o reg) on all data: 0.031945933
Test loss (w/o reg) on all data: 0.018454438
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5107832e-06
Norm of the params: 9.418631
              Random: fixed  20 labels. Loss 0.01845. Accuracy 0.998.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035013936
Train loss (w/o reg) on all data: 0.031105043
Test loss (w/o reg) on all data: 0.023747114
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.31328e-06
Norm of the params: 8.8418255
Flipped loss: 0.02375. Accuracy: 0.994
### Flips: 410, rs: 27, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601088
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8588393e-08
Norm of the params: 6.092833
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6996626e-08
Norm of the params: 6.092813
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035013936
Train loss (w/o reg) on all data: 0.031104019
Test loss (w/o reg) on all data: 0.023748443
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0977388e-06
Norm of the params: 8.842983
              Random: fixed   0 labels. Loss 0.02375. Accuracy 0.994.
### Flips: 410, rs: 27, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601088
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.852924e-08
Norm of the params: 6.092833
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6967457e-08
Norm of the params: 6.092813
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033559203
Train loss (w/o reg) on all data: 0.029485557
Test loss (w/o reg) on all data: 0.020587187
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.4726758e-07
Norm of the params: 9.026237
              Random: fixed   4 labels. Loss 0.02059. Accuracy 0.997.
### Flips: 410, rs: 27, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1458561e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.20906e-09
Norm of the params: 6.092821
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03284439
Train loss (w/o reg) on all data: 0.028832134
Test loss (w/o reg) on all data: 0.020258997
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4308849e-06
Norm of the params: 8.957964
              Random: fixed   5 labels. Loss 0.02026. Accuracy 0.997.
### Flips: 410, rs: 27, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8906175e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.452826e-09
Norm of the params: 6.0928226
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031554226
Train loss (w/o reg) on all data: 0.027434852
Test loss (w/o reg) on all data: 0.019765263
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.2873193e-06
Norm of the params: 9.0767565
              Random: fixed   7 labels. Loss 0.01977. Accuracy 0.996.
### Flips: 410, rs: 27, checks: 1025
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601342
Test loss (w/o reg) on all data: 0.0026560863
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9618077e-08
Norm of the params: 6.092792
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8515439e-08
Norm of the params: 6.0928297
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03064933
Train loss (w/o reg) on all data: 0.02656314
Test loss (w/o reg) on all data: 0.018475128
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3137331e-06
Norm of the params: 9.040121
              Random: fixed   9 labels. Loss 0.01848. Accuracy 0.997.
### Flips: 410, rs: 27, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.453592e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560826
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5680277e-08
Norm of the params: 6.0928097
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03020795
Train loss (w/o reg) on all data: 0.026113987
Test loss (w/o reg) on all data: 0.017887572
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2535443e-06
Norm of the params: 9.048717
              Random: fixed  10 labels. Loss 0.01789. Accuracy 0.998.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043120284
Train loss (w/o reg) on all data: 0.038900934
Test loss (w/o reg) on all data: 0.023511872
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6497585e-06
Norm of the params: 9.18624
Flipped loss: 0.02351. Accuracy: 0.995
### Flips: 410, rs: 28, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3303234e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2335443e-08
Norm of the params: 6.0928264
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04123515
Train loss (w/o reg) on all data: 0.036997493
Test loss (w/o reg) on all data: 0.02290713
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.2545953e-06
Norm of the params: 9.206144
              Random: fixed   4 labels. Loss 0.02291. Accuracy 0.995.
### Flips: 410, rs: 28, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3030744e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1829138e-08
Norm of the params: 6.0928116
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038697593
Train loss (w/o reg) on all data: 0.034424856
Test loss (w/o reg) on all data: 0.022189932
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9162408e-06
Norm of the params: 9.244172
              Random: fixed   8 labels. Loss 0.02219. Accuracy 0.993.
### Flips: 410, rs: 28, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009945
Test loss (w/o reg) on all data: 0.0026560305
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.002665e-08
Norm of the params: 6.092849
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0100306e-08
Norm of the params: 6.0928183
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037024904
Train loss (w/o reg) on all data: 0.032949086
Test loss (w/o reg) on all data: 0.020723427
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.5964242e-06
Norm of the params: 9.028642
              Random: fixed  12 labels. Loss 0.02072. Accuracy 0.993.
### Flips: 410, rs: 28, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.882376e-09
Norm of the params: 6.09282
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1840392e-08
Norm of the params: 6.0928106
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03542759
Train loss (w/o reg) on all data: 0.031245941
Test loss (w/o reg) on all data: 0.020978207
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5508853e-06
Norm of the params: 9.145107
              Random: fixed  13 labels. Loss 0.02098. Accuracy 0.995.
### Flips: 410, rs: 28, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601486
Test loss (w/o reg) on all data: 0.0026561096
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.952912e-08
Norm of the params: 6.0927677
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.576038e-08
Norm of the params: 6.092814
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03306024
Train loss (w/o reg) on all data: 0.029066313
Test loss (w/o reg) on all data: 0.018540181
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2920606e-06
Norm of the params: 8.937483
              Random: fixed  17 labels. Loss 0.01854. Accuracy 0.994.
### Flips: 410, rs: 28, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0024255e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.0578e-09
Norm of the params: 6.092814
                Loss: fixed  58 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031435393
Train loss (w/o reg) on all data: 0.027302662
Test loss (w/o reg) on all data: 0.016968345
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.1532385e-06
Norm of the params: 9.091456
              Random: fixed  20 labels. Loss 0.01697. Accuracy 0.995.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04349867
Train loss (w/o reg) on all data: 0.039333526
Test loss (w/o reg) on all data: 0.020752564
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7381276e-06
Norm of the params: 9.127042
Flipped loss: 0.02075. Accuracy: 0.998
### Flips: 410, rs: 29, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1575556e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1006152e-08
Norm of the params: 6.0928164
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041689668
Train loss (w/o reg) on all data: 0.037541263
Test loss (w/o reg) on all data: 0.020376189
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.954087e-06
Norm of the params: 9.108684
              Random: fixed   3 labels. Loss 0.02038. Accuracy 0.998.
### Flips: 410, rs: 29, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601067
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6999317e-08
Norm of the params: 6.0928364
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1687766e-08
Norm of the params: 6.0928073
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03944479
Train loss (w/o reg) on all data: 0.03511191
Test loss (w/o reg) on all data: 0.01994608
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0343385e-05
Norm of the params: 9.309002
              Random: fixed   6 labels. Loss 0.01995. Accuracy 0.998.
### Flips: 410, rs: 29, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4466354e-08
Norm of the params: 6.092817
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8764005e-08
Norm of the params: 6.09282
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03881641
Train loss (w/o reg) on all data: 0.03442742
Test loss (w/o reg) on all data: 0.019387037
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.818056e-06
Norm of the params: 9.369088
              Random: fixed   8 labels. Loss 0.01939. Accuracy 0.998.
### Flips: 410, rs: 29, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.387839e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0189956e-08
Norm of the params: 6.0928035
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036669742
Train loss (w/o reg) on all data: 0.03236886
Test loss (w/o reg) on all data: 0.017652472
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.16825e-06
Norm of the params: 9.27457
              Random: fixed  12 labels. Loss 0.01765. Accuracy 0.998.
### Flips: 410, rs: 29, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4689639e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5271712e-09
Norm of the params: 6.092813
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035734408
Train loss (w/o reg) on all data: 0.031429064
Test loss (w/o reg) on all data: 0.016702732
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5766608e-06
Norm of the params: 9.27938
              Random: fixed  14 labels. Loss 0.01670. Accuracy 0.998.
### Flips: 410, rs: 29, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.581301e-09
Norm of the params: 6.092822
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0487322e-08
Norm of the params: 6.092814
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033577576
Train loss (w/o reg) on all data: 0.029281458
Test loss (w/o reg) on all data: 0.014969456
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4137609e-06
Norm of the params: 9.269431
              Random: fixed  17 labels. Loss 0.01497. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037693672
Train loss (w/o reg) on all data: 0.03330564
Test loss (w/o reg) on all data: 0.022259627
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2138128e-06
Norm of the params: 9.368063
Flipped loss: 0.02226. Accuracy: 0.996
### Flips: 410, rs: 30, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601312
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.651482e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.977551e-08
Norm of the params: 6.092826
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036266897
Train loss (w/o reg) on all data: 0.03194929
Test loss (w/o reg) on all data: 0.019649664
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6557514e-06
Norm of the params: 9.292586
              Random: fixed   3 labels. Loss 0.01965. Accuracy 0.998.
### Flips: 410, rs: 30, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.34313725e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0179157e-08
Norm of the params: 6.0928216
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03485837
Train loss (w/o reg) on all data: 0.030556053
Test loss (w/o reg) on all data: 0.018358383
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.314504e-06
Norm of the params: 9.276116
              Random: fixed   5 labels. Loss 0.01836. Accuracy 0.996.
### Flips: 410, rs: 30, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4310957e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601053
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4665793e-08
Norm of the params: 6.0928392
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034063414
Train loss (w/o reg) on all data: 0.02976544
Test loss (w/o reg) on all data: 0.018775554
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.918856e-07
Norm of the params: 9.271435
              Random: fixed   6 labels. Loss 0.01878. Accuracy 0.996.
### Flips: 410, rs: 30, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.006277e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9709633e-08
Norm of the params: 6.0928164
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032977946
Train loss (w/o reg) on all data: 0.02864322
Test loss (w/o reg) on all data: 0.017940115
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.937819e-06
Norm of the params: 9.310989
              Random: fixed   8 labels. Loss 0.01794. Accuracy 0.996.
### Flips: 410, rs: 30, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601313
Test loss (w/o reg) on all data: 0.002656088
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2462634e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601014
Test loss (w/o reg) on all data: 0.0026560302
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8977593e-08
Norm of the params: 6.0928454
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030837916
Train loss (w/o reg) on all data: 0.026355036
Test loss (w/o reg) on all data: 0.016341021
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3931766e-06
Norm of the params: 9.46877
              Random: fixed  12 labels. Loss 0.01634. Accuracy 0.998.
### Flips: 410, rs: 30, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4041185e-08
Norm of the params: 6.0928297
     Influence (LOO): fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.397416e-09
Norm of the params: 6.0928183
                Loss: fixed  47 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030609189
Train loss (w/o reg) on all data: 0.026161684
Test loss (w/o reg) on all data: 0.015975237
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1283242e-06
Norm of the params: 9.431335
              Random: fixed  13 labels. Loss 0.01598. Accuracy 0.999.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041280005
Train loss (w/o reg) on all data: 0.037152156
Test loss (w/o reg) on all data: 0.022245247
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7086824e-06
Norm of the params: 9.086088
Flipped loss: 0.02225. Accuracy: 0.997
### Flips: 410, rs: 31, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7343114e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5370198e-08
Norm of the params: 6.0928144
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038628772
Train loss (w/o reg) on all data: 0.034484614
Test loss (w/o reg) on all data: 0.020739686
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.225526e-06
Norm of the params: 9.10402
              Random: fixed   5 labels. Loss 0.02074. Accuracy 0.999.
### Flips: 410, rs: 31, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010446
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.591632e-08
Norm of the params: 6.09284
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.389644e-09
Norm of the params: 6.0928164
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03690308
Train loss (w/o reg) on all data: 0.03278958
Test loss (w/o reg) on all data: 0.019314734
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0949054e-06
Norm of the params: 9.070281
              Random: fixed   9 labels. Loss 0.01931. Accuracy 0.998.
### Flips: 410, rs: 31, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601121
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3128443e-08
Norm of the params: 6.0928288
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1588445e-08
Norm of the params: 6.092813
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03630795
Train loss (w/o reg) on all data: 0.03228738
Test loss (w/o reg) on all data: 0.019050013
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3510413e-06
Norm of the params: 8.967237
              Random: fixed  11 labels. Loss 0.01905. Accuracy 0.998.
### Flips: 410, rs: 31, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960144
Test loss (w/o reg) on all data: 0.0026561287
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0279184e-08
Norm of the params: 6.092776
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0012381e-08
Norm of the params: 6.092827
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035230406
Train loss (w/o reg) on all data: 0.031227967
Test loss (w/o reg) on all data: 0.017763244
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.216306e-06
Norm of the params: 8.9470005
              Random: fixed  14 labels. Loss 0.01776. Accuracy 0.997.
### Flips: 410, rs: 31, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960091
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.399668e-08
Norm of the params: 6.0928617
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601149
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4933052e-08
Norm of the params: 6.092823
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03330937
Train loss (w/o reg) on all data: 0.029142352
Test loss (w/o reg) on all data: 0.017754983
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.107202e-06
Norm of the params: 9.129097
              Random: fixed  16 labels. Loss 0.01775. Accuracy 0.998.
### Flips: 410, rs: 31, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601379
Test loss (w/o reg) on all data: 0.0026560898
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.84429e-08
Norm of the params: 6.0927844
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6153056e-08
Norm of the params: 6.0928144
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031792294
Train loss (w/o reg) on all data: 0.027731227
Test loss (w/o reg) on all data: 0.017848376
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.6350726e-06
Norm of the params: 9.012289
              Random: fixed  18 labels. Loss 0.01785. Accuracy 0.997.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040257122
Train loss (w/o reg) on all data: 0.035813164
Test loss (w/o reg) on all data: 0.021858577
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0620882e-05
Norm of the params: 9.427575
Flipped loss: 0.02186. Accuracy: 0.996
### Flips: 410, rs: 32, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600933
Test loss (w/o reg) on all data: 0.002656024
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4092523e-08
Norm of the params: 6.0928593
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601157
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6358392e-08
Norm of the params: 6.092822
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039603677
Train loss (w/o reg) on all data: 0.03526859
Test loss (w/o reg) on all data: 0.02070142
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.799743e-06
Norm of the params: 9.3113785
              Random: fixed   2 labels. Loss 0.02070. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601053
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5148452e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601177
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.631492e-09
Norm of the params: 6.0928197
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03929696
Train loss (w/o reg) on all data: 0.03499518
Test loss (w/o reg) on all data: 0.02062509
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.7673013e-06
Norm of the params: 9.275536
              Random: fixed   3 labels. Loss 0.02063. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601085
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1580743e-08
Norm of the params: 6.0928345
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3386847e-08
Norm of the params: 6.092814
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03900736
Train loss (w/o reg) on all data: 0.034640156
Test loss (w/o reg) on all data: 0.02071054
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9534118e-06
Norm of the params: 9.345805
              Random: fixed   4 labels. Loss 0.02071. Accuracy 0.996.
### Flips: 410, rs: 32, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5377102e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1809779e-08
Norm of the params: 6.092813
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03877249
Train loss (w/o reg) on all data: 0.034511056
Test loss (w/o reg) on all data: 0.019629622
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8612855e-06
Norm of the params: 9.231939
              Random: fixed   6 labels. Loss 0.01963. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.40209e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.052252e-08
Norm of the params: 6.092808
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036692664
Train loss (w/o reg) on all data: 0.032484755
Test loss (w/o reg) on all data: 0.01795143
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5418645e-06
Norm of the params: 9.173779
              Random: fixed  11 labels. Loss 0.01795. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096014544
Test loss (w/o reg) on all data: 0.0026561173
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7736025e-08
Norm of the params: 6.0927734
     Influence (LOO): fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601113
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5519059e-08
Norm of the params: 6.0928297
                Loss: fixed  57 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034959674
Train loss (w/o reg) on all data: 0.030769255
Test loss (w/o reg) on all data: 0.016764153
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.6880755e-07
Norm of the params: 9.154692
              Random: fixed  14 labels. Loss 0.01676. Accuracy 0.999.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045079466
Train loss (w/o reg) on all data: 0.040462583
Test loss (w/o reg) on all data: 0.023804897
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.430169e-06
Norm of the params: 9.609248
Flipped loss: 0.02380. Accuracy: 0.997
### Flips: 410, rs: 33, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560496
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7967512e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4612974e-08
Norm of the params: 6.0928063
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044127095
Train loss (w/o reg) on all data: 0.039460342
Test loss (w/o reg) on all data: 0.023601087
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.6293494e-06
Norm of the params: 9.661006
              Random: fixed   2 labels. Loss 0.02360. Accuracy 0.997.
### Flips: 410, rs: 33, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601258
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2260058e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5381627e-08
Norm of the params: 6.0928164
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042862628
Train loss (w/o reg) on all data: 0.03817964
Test loss (w/o reg) on all data: 0.022575812
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.543135e-06
Norm of the params: 9.677795
              Random: fixed   4 labels. Loss 0.02258. Accuracy 0.996.
### Flips: 410, rs: 33, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601257
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.637129e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.739884e-08
Norm of the params: 6.0928183
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042189978
Train loss (w/o reg) on all data: 0.037607994
Test loss (w/o reg) on all data: 0.021903588
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3925162e-06
Norm of the params: 9.572863
              Random: fixed   6 labels. Loss 0.02190. Accuracy 0.996.
### Flips: 410, rs: 33, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960109
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4229383e-08
Norm of the params: 6.092833
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0625237e-08
Norm of the params: 6.0928073
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03921337
Train loss (w/o reg) on all data: 0.034582194
Test loss (w/o reg) on all data: 0.020715509
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5885537e-06
Norm of the params: 9.624112
              Random: fixed  11 labels. Loss 0.02072. Accuracy 0.997.
### Flips: 410, rs: 33, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600985
Test loss (w/o reg) on all data: 0.002656045
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4109874e-08
Norm of the params: 6.09285
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960136
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5062726e-08
Norm of the params: 6.092789
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03871896
Train loss (w/o reg) on all data: 0.0340308
Test loss (w/o reg) on all data: 0.019743469
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.324324e-06
Norm of the params: 9.683141
              Random: fixed  13 labels. Loss 0.01974. Accuracy 0.997.
### Flips: 410, rs: 33, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2791797e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9433216e-08
Norm of the params: 6.0928283
                Loss: fixed  59 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037278965
Train loss (w/o reg) on all data: 0.03255727
Test loss (w/o reg) on all data: 0.019142084
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2203023e-06
Norm of the params: 9.717708
              Random: fixed  15 labels. Loss 0.01914. Accuracy 0.997.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042613637
Train loss (w/o reg) on all data: 0.037986714
Test loss (w/o reg) on all data: 0.022800151
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1189887e-06
Norm of the params: 9.619692
Flipped loss: 0.02280. Accuracy: 0.998
### Flips: 410, rs: 34, checks: 205
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013426
Test loss (w/o reg) on all data: 0.0026560917
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.293732e-08
Norm of the params: 6.0927916
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3237957e-08
Norm of the params: 6.0928173
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041003227
Train loss (w/o reg) on all data: 0.036412172
Test loss (w/o reg) on all data: 0.021794068
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1208298e-06
Norm of the params: 9.5823345
              Random: fixed   3 labels. Loss 0.02179. Accuracy 0.997.
### Flips: 410, rs: 34, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7986501e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.545854e-09
Norm of the params: 6.0928173
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040001843
Train loss (w/o reg) on all data: 0.035403036
Test loss (w/o reg) on all data: 0.020616554
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.935972e-06
Norm of the params: 9.59042
              Random: fixed   6 labels. Loss 0.02062. Accuracy 0.998.
### Flips: 410, rs: 34, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9135113e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.562738e-08
Norm of the params: 6.0928135
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03845695
Train loss (w/o reg) on all data: 0.03393418
Test loss (w/o reg) on all data: 0.019778363
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.677407e-07
Norm of the params: 9.510805
              Random: fixed   9 labels. Loss 0.01978. Accuracy 0.998.
### Flips: 410, rs: 34, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601063
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9048513e-08
Norm of the params: 6.0928364
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601284
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.275722e-08
Norm of the params: 6.092801
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03648115
Train loss (w/o reg) on all data: 0.032028064
Test loss (w/o reg) on all data: 0.017352393
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.3422618e-06
Norm of the params: 9.437251
              Random: fixed  13 labels. Loss 0.01735. Accuracy 0.998.
### Flips: 410, rs: 34, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960124
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2418861e-08
Norm of the params: 6.0928087
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601062
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3388173e-08
Norm of the params: 6.0928364
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033516053
Train loss (w/o reg) on all data: 0.029088004
Test loss (w/o reg) on all data: 0.016609844
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0541531e-06
Norm of the params: 9.410686
              Random: fixed  17 labels. Loss 0.01661. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3772102e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.12015e-09
Norm of the params: 6.0928154
                Loss: fixed  55 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03258672
Train loss (w/o reg) on all data: 0.028264252
Test loss (w/o reg) on all data: 0.01622782
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2044359e-06
Norm of the params: 9.297813
              Random: fixed  19 labels. Loss 0.01623. Accuracy 0.999.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03833521
Train loss (w/o reg) on all data: 0.03389269
Test loss (w/o reg) on all data: 0.025171537
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2444816e-06
Norm of the params: 9.426049
Flipped loss: 0.02517. Accuracy: 0.992
### Flips: 410, rs: 35, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8223491e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601275
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0696653e-08
Norm of the params: 6.092803
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036559306
Train loss (w/o reg) on all data: 0.032186113
Test loss (w/o reg) on all data: 0.02329348
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.824245e-06
Norm of the params: 9.352213
              Random: fixed   4 labels. Loss 0.02329. Accuracy 0.992.
### Flips: 410, rs: 35, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5300243e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.732835e-09
Norm of the params: 6.0928206
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035071142
Train loss (w/o reg) on all data: 0.030626113
Test loss (w/o reg) on all data: 0.021734646
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9015575e-06
Norm of the params: 9.428709
              Random: fixed   7 labels. Loss 0.02173. Accuracy 0.996.
### Flips: 410, rs: 35, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6574708e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8682854e-08
Norm of the params: 6.0928183
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03424216
Train loss (w/o reg) on all data: 0.029737504
Test loss (w/o reg) on all data: 0.021763206
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4474499e-06
Norm of the params: 9.49174
              Random: fixed   9 labels. Loss 0.02176. Accuracy 0.996.
### Flips: 410, rs: 35, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7263316e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3174295e-08
Norm of the params: 6.092817
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03108519
Train loss (w/o reg) on all data: 0.026495093
Test loss (w/o reg) on all data: 0.019023132
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2284535e-06
Norm of the params: 9.581331
              Random: fixed  14 labels. Loss 0.01902. Accuracy 0.996.
### Flips: 410, rs: 35, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5387112e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.07813065e-08
Norm of the params: 6.0928144
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028629506
Train loss (w/o reg) on all data: 0.023993418
Test loss (w/o reg) on all data: 0.01683711
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.544535e-07
Norm of the params: 9.629214
              Random: fixed  17 labels. Loss 0.01684. Accuracy 0.997.
### Flips: 410, rs: 35, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013787
Test loss (w/o reg) on all data: 0.0026560924
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.493058e-08
Norm of the params: 6.0927863
     Influence (LOO): fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8144583e-08
Norm of the params: 6.0928273
                Loss: fixed  56 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027473757
Train loss (w/o reg) on all data: 0.022750724
Test loss (w/o reg) on all data: 0.016122969
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2597695e-05
Norm of the params: 9.719087
              Random: fixed  19 labels. Loss 0.01612. Accuracy 0.998.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042893376
Train loss (w/o reg) on all data: 0.03812115
Test loss (w/o reg) on all data: 0.025615703
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.950999e-06
Norm of the params: 9.769572
Flipped loss: 0.02562. Accuracy: 0.993
### Flips: 410, rs: 36, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1938855e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0729495e-08
Norm of the params: 6.092823
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041610178
Train loss (w/o reg) on all data: 0.0367807
Test loss (w/o reg) on all data: 0.025085371
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.7567373e-06
Norm of the params: 9.827998
              Random: fixed   3 labels. Loss 0.02509. Accuracy 0.993.
### Flips: 410, rs: 36, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2037495e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1658635e-08
Norm of the params: 6.0928235
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040237937
Train loss (w/o reg) on all data: 0.035321068
Test loss (w/o reg) on all data: 0.024624385
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.801953e-06
Norm of the params: 9.916522
              Random: fixed   6 labels. Loss 0.02462. Accuracy 0.993.
### Flips: 410, rs: 36, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601338
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.968406e-08
Norm of the params: 6.092792
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5404256e-08
Norm of the params: 6.0928197
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03926381
Train loss (w/o reg) on all data: 0.034387175
Test loss (w/o reg) on all data: 0.023617528
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7120781e-06
Norm of the params: 9.875866
              Random: fixed   9 labels. Loss 0.02362. Accuracy 0.996.
### Flips: 410, rs: 36, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.904112e-09
Norm of the params: 6.0928135
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.610751e-09
Norm of the params: 6.0928235
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038539767
Train loss (w/o reg) on all data: 0.033676896
Test loss (w/o reg) on all data: 0.022721214
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.1885497e-06
Norm of the params: 9.861916
              Random: fixed  12 labels. Loss 0.02272. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600895
Test loss (w/o reg) on all data: 0.0026560181
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.671528e-08
Norm of the params: 6.092865
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162387
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1125802e-08
Norm of the params: 6.092818
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037193842
Train loss (w/o reg) on all data: 0.032405905
Test loss (w/o reg) on all data: 0.019210814
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6264513e-06
Norm of the params: 9.785642
              Random: fixed  16 labels. Loss 0.01921. Accuracy 0.998.
### Flips: 410, rs: 36, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3520642e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.01413695e-08
Norm of the params: 6.092811
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03611321
Train loss (w/o reg) on all data: 0.03136636
Test loss (w/o reg) on all data: 0.018583404
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7202459e-06
Norm of the params: 9.743563
              Random: fixed  18 labels. Loss 0.01858. Accuracy 0.998.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047103737
Train loss (w/o reg) on all data: 0.042700425
Test loss (w/o reg) on all data: 0.022849476
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.947193e-06
Norm of the params: 9.38436
Flipped loss: 0.02285. Accuracy: 0.999
### Flips: 410, rs: 37, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.002656083
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1624215e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011214
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7540636e-08
Norm of the params: 6.092828
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0464155
Train loss (w/o reg) on all data: 0.04204048
Test loss (w/o reg) on all data: 0.022155264
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0550785e-05
Norm of the params: 9.354167
              Random: fixed   2 labels. Loss 0.02216. Accuracy 0.998.
### Flips: 410, rs: 37, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9490239e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5326712e-08
Norm of the params: 6.092824
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045981653
Train loss (w/o reg) on all data: 0.041561704
Test loss (w/o reg) on all data: 0.022026006
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.4343087e-06
Norm of the params: 9.402074
              Random: fixed   3 labels. Loss 0.02203. Accuracy 0.998.
### Flips: 410, rs: 37, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7978698e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4839304e-08
Norm of the params: 6.09282
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043134373
Train loss (w/o reg) on all data: 0.038752988
Test loss (w/o reg) on all data: 0.021117961
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0713734e-05
Norm of the params: 9.360965
              Random: fixed   7 labels. Loss 0.02112. Accuracy 0.998.
### Flips: 410, rs: 37, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012885
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8997488e-08
Norm of the params: 6.0928006
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.39223e-08
Norm of the params: 6.092823
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0424501
Train loss (w/o reg) on all data: 0.038108867
Test loss (w/o reg) on all data: 0.020991309
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.670825e-07
Norm of the params: 9.317977
              Random: fixed   8 labels. Loss 0.02099. Accuracy 0.998.
### Flips: 410, rs: 37, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601246
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0389896e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4414282e-08
Norm of the params: 6.092822
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040198065
Train loss (w/o reg) on all data: 0.03593299
Test loss (w/o reg) on all data: 0.018767867
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7545774e-06
Norm of the params: 9.235882
              Random: fixed  13 labels. Loss 0.01877. Accuracy 0.999.
### Flips: 410, rs: 37, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096014875
Test loss (w/o reg) on all data: 0.002656097
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.195486e-08
Norm of the params: 6.0927672
     Influence (LOO): fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601286
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.316756e-08
Norm of the params: 6.0928006
                Loss: fixed  62 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038519397
Train loss (w/o reg) on all data: 0.034112517
Test loss (w/o reg) on all data: 0.017653644
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7899188e-06
Norm of the params: 9.3881645
              Random: fixed  16 labels. Loss 0.01765. Accuracy 0.999.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037419744
Train loss (w/o reg) on all data: 0.033343904
Test loss (w/o reg) on all data: 0.017918797
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.695673e-06
Norm of the params: 9.028666
Flipped loss: 0.01792. Accuracy: 0.997
### Flips: 410, rs: 38, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.96871e-09
Norm of the params: 6.092821
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.782274e-09
Norm of the params: 6.0928144
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03650353
Train loss (w/o reg) on all data: 0.032362007
Test loss (w/o reg) on all data: 0.01663711
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5896328e-06
Norm of the params: 9.101127
              Random: fixed   2 labels. Loss 0.01664. Accuracy 0.997.
### Flips: 410, rs: 38, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.002656086
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4295812e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.002366e-08
Norm of the params: 6.0928154
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034958094
Train loss (w/o reg) on all data: 0.030947555
Test loss (w/o reg) on all data: 0.016038945
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.200342e-06
Norm of the params: 8.956046
              Random: fixed   5 labels. Loss 0.01604. Accuracy 0.997.
### Flips: 410, rs: 38, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4179258e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2468587e-08
Norm of the params: 6.092825
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03495809
Train loss (w/o reg) on all data: 0.030946825
Test loss (w/o reg) on all data: 0.016038314
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6350934e-06
Norm of the params: 8.956859
              Random: fixed   5 labels. Loss 0.01604. Accuracy 0.997.
### Flips: 410, rs: 38, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4113032e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2460036e-08
Norm of the params: 6.092825
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034402438
Train loss (w/o reg) on all data: 0.030472444
Test loss (w/o reg) on all data: 0.0150072435
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1735395e-06
Norm of the params: 8.865657
              Random: fixed   6 labels. Loss 0.01501. Accuracy 0.998.
### Flips: 410, rs: 38, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6449567e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2106684e-09
Norm of the params: 6.092819
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034402438
Train loss (w/o reg) on all data: 0.030473491
Test loss (w/o reg) on all data: 0.015007699
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6135159e-06
Norm of the params: 8.864474
              Random: fixed   6 labels. Loss 0.01501. Accuracy 0.998.
### Flips: 410, rs: 38, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6421607e-08
Norm of the params: 6.092803
     Influence (LOO): fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2426158e-09
Norm of the params: 6.0928197
                Loss: fixed  44 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03382872
Train loss (w/o reg) on all data: 0.030025916
Test loss (w/o reg) on all data: 0.014308863
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3047869e-06
Norm of the params: 8.721015
              Random: fixed   8 labels. Loss 0.01431. Accuracy 0.999.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044793766
Train loss (w/o reg) on all data: 0.040808834
Test loss (w/o reg) on all data: 0.023742815
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.927168e-06
Norm of the params: 8.927407
Flipped loss: 0.02374. Accuracy: 0.996
### Flips: 410, rs: 39, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4041323e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9059204e-09
Norm of the params: 6.092812
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044449907
Train loss (w/o reg) on all data: 0.04059905
Test loss (w/o reg) on all data: 0.023378141
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.5012513e-06
Norm of the params: 8.775943
              Random: fixed   1 labels. Loss 0.02338. Accuracy 0.996.
### Flips: 410, rs: 39, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0117295e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.29472495e-08
Norm of the params: 6.092825
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04354104
Train loss (w/o reg) on all data: 0.039653193
Test loss (w/o reg) on all data: 0.023038514
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.5004076e-06
Norm of the params: 8.817991
              Random: fixed   3 labels. Loss 0.02304. Accuracy 0.996.
### Flips: 410, rs: 39, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601394
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5640185e-08
Norm of the params: 6.092784
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.735258e-08
Norm of the params: 6.092826
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042968977
Train loss (w/o reg) on all data: 0.03906082
Test loss (w/o reg) on all data: 0.022847245
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.8871697e-06
Norm of the params: 8.840992
              Random: fixed   4 labels. Loss 0.02285. Accuracy 0.996.
### Flips: 410, rs: 39, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096006785
Test loss (w/o reg) on all data: 0.002655961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0674859e-07
Norm of the params: 6.0929
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3917354e-08
Norm of the params: 6.0928164
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043045335
Train loss (w/o reg) on all data: 0.039167304
Test loss (w/o reg) on all data: 0.0225122
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4905983e-06
Norm of the params: 8.806852
              Random: fixed   5 labels. Loss 0.02251. Accuracy 0.997.
### Flips: 410, rs: 39, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2797611e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.294755e-09
Norm of the params: 6.092819
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040770315
Train loss (w/o reg) on all data: 0.037015807
Test loss (w/o reg) on all data: 0.020426048
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.67305e-06
Norm of the params: 8.665457
              Random: fixed  10 labels. Loss 0.02043. Accuracy 0.998.
### Flips: 410, rs: 39, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013414
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6624797e-08
Norm of the params: 6.092791
     Influence (LOO): fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560463
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.776589e-08
Norm of the params: 6.0928245
                Loss: fixed  61 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037580326
Train loss (w/o reg) on all data: 0.03369568
Test loss (w/o reg) on all data: 0.01883645
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.310638e-06
Norm of the params: 8.814359
              Random: fixed  14 labels. Loss 0.01884. Accuracy 0.997.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05622698
Train loss (w/o reg) on all data: 0.05131164
Test loss (w/o reg) on all data: 0.039817467
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1743684e-05
Norm of the params: 9.914982
Flipped loss: 0.03982. Accuracy: 0.991
### Flips: 615, rs: 0, checks: 205
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038330257
Train loss (w/o reg) on all data: 0.0016576881
Test loss (w/o reg) on all data: 0.0032281124
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4748464e-08
Norm of the params: 6.595965
     Influence (LOO): fixed  90 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004031903
Train loss (w/o reg) on all data: 0.0016388076
Test loss (w/o reg) on all data: 0.0033775298
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.12415066e-07
Norm of the params: 6.9182305
                Loss: fixed  90 labels. Loss 0.00338. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052313384
Train loss (w/o reg) on all data: 0.047359623
Test loss (w/o reg) on all data: 0.03545481
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.682952e-06
Norm of the params: 9.953654
              Random: fixed   9 labels. Loss 0.03545. Accuracy 0.992.
### Flips: 615, rs: 0, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960096
Test loss (w/o reg) on all data: 0.0026560328
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3559456e-08
Norm of the params: 6.092854
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013385
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7161345e-08
Norm of the params: 6.0927925
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05128418
Train loss (w/o reg) on all data: 0.046297833
Test loss (w/o reg) on all data: 0.033527464
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1692194e-05
Norm of the params: 9.986338
              Random: fixed  12 labels. Loss 0.03353. Accuracy 0.992.
### Flips: 615, rs: 0, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.460675e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6104137e-08
Norm of the params: 6.092808
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049925387
Train loss (w/o reg) on all data: 0.045211226
Test loss (w/o reg) on all data: 0.030668167
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.0716474e-06
Norm of the params: 9.709956
              Random: fixed  18 labels. Loss 0.03067. Accuracy 0.994.
### Flips: 615, rs: 0, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2515582e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960119
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.938465e-09
Norm of the params: 6.0928164
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04980878
Train loss (w/o reg) on all data: 0.045098696
Test loss (w/o reg) on all data: 0.02919992
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.7162753e-06
Norm of the params: 9.705758
              Random: fixed  19 labels. Loss 0.02920. Accuracy 0.996.
### Flips: 615, rs: 0, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4073537e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011243
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.498935e-08
Norm of the params: 6.092827
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048999704
Train loss (w/o reg) on all data: 0.044240087
Test loss (w/o reg) on all data: 0.02859944
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.622645e-06
Norm of the params: 9.756658
              Random: fixed  21 labels. Loss 0.02860. Accuracy 0.994.
### Flips: 615, rs: 0, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.023345e-09
Norm of the params: 6.0928173
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6307337e-09
Norm of the params: 6.0928144
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045868646
Train loss (w/o reg) on all data: 0.041034814
Test loss (w/o reg) on all data: 0.026994672
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7527215e-06
Norm of the params: 9.832429
              Random: fixed  26 labels. Loss 0.02699. Accuracy 0.994.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055216644
Train loss (w/o reg) on all data: 0.05127708
Test loss (w/o reg) on all data: 0.03108027
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.1691118e-06
Norm of the params: 8.876448
Flipped loss: 0.03108. Accuracy: 0.993
### Flips: 615, rs: 1, checks: 205
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052523897
Train loss (w/o reg) on all data: 0.0026844665
Test loss (w/o reg) on all data: 0.0042869095
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9909666e-08
Norm of the params: 7.1664815
     Influence (LOO): fixed  81 labels. Loss 0.00429. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.691876e-09
Norm of the params: 6.092819
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05210777
Train loss (w/o reg) on all data: 0.04805498
Test loss (w/o reg) on all data: 0.02821265
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.709126e-06
Norm of the params: 9.003101
              Random: fixed   7 labels. Loss 0.02821. Accuracy 0.994.
### Flips: 615, rs: 1, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1316861e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1071305e-08
Norm of the params: 6.092813
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051325604
Train loss (w/o reg) on all data: 0.047284875
Test loss (w/o reg) on all data: 0.026740631
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.3762865e-06
Norm of the params: 8.989694
              Random: fixed   9 labels. Loss 0.02674. Accuracy 0.996.
### Flips: 615, rs: 1, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601352
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3729362e-08
Norm of the params: 6.09279
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6588363e-08
Norm of the params: 6.0928173
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050768055
Train loss (w/o reg) on all data: 0.046846077
Test loss (w/o reg) on all data: 0.026242223
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.1473e-06
Norm of the params: 8.856611
              Random: fixed  11 labels. Loss 0.02624. Accuracy 0.996.
### Flips: 615, rs: 1, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012943
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.165817e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601159
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9468226e-08
Norm of the params: 6.0928216
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04762356
Train loss (w/o reg) on all data: 0.04379654
Test loss (w/o reg) on all data: 0.023694657
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.2609184e-06
Norm of the params: 8.748738
              Random: fixed  18 labels. Loss 0.02369. Accuracy 0.996.
### Flips: 615, rs: 1, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.906621e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8366891e-08
Norm of the params: 6.0928235
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04667943
Train loss (w/o reg) on all data: 0.042938996
Test loss (w/o reg) on all data: 0.023267433
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.4721934e-07
Norm of the params: 8.649201
              Random: fixed  21 labels. Loss 0.02327. Accuracy 0.995.
### Flips: 615, rs: 1, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5215237e-08
Norm of the params: 6.092816
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.00265604
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.686029e-08
Norm of the params: 6.0928235
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04292268
Train loss (w/o reg) on all data: 0.039094202
Test loss (w/o reg) on all data: 0.021298315
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.7469737e-06
Norm of the params: 8.750404
              Random: fixed  28 labels. Loss 0.02130. Accuracy 0.996.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047830224
Train loss (w/o reg) on all data: 0.04340573
Test loss (w/o reg) on all data: 0.030375237
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.072765e-06
Norm of the params: 9.406904
Flipped loss: 0.03038. Accuracy: 0.995
### Flips: 615, rs: 2, checks: 205
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031607756
Train loss (w/o reg) on all data: 0.0011806579
Test loss (w/o reg) on all data: 0.004183868
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.949191e-08
Norm of the params: 6.29304
     Influence (LOO): fixed  74 labels. Loss 0.00418. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032423516
Train loss (w/o reg) on all data: 0.0011854164
Test loss (w/o reg) on all data: 0.0027705044
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.809704e-08
Norm of the params: 6.4139466
                Loss: fixed  75 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047254287
Train loss (w/o reg) on all data: 0.042921603
Test loss (w/o reg) on all data: 0.029922744
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.5465174e-06
Norm of the params: 9.308794
              Random: fixed   2 labels. Loss 0.02992. Accuracy 0.993.
### Flips: 615, rs: 2, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601278
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.404835e-08
Norm of the params: 6.092802
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3379304e-08
Norm of the params: 6.0928264
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045520738
Train loss (w/o reg) on all data: 0.04106225
Test loss (w/o reg) on all data: 0.029859731
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.6461298e-06
Norm of the params: 9.442974
              Random: fixed   7 labels. Loss 0.02986. Accuracy 0.991.
### Flips: 615, rs: 2, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8687238e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1389546e-08
Norm of the params: 6.0928154
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044576537
Train loss (w/o reg) on all data: 0.040201515
Test loss (w/o reg) on all data: 0.027142921
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4532658e-06
Norm of the params: 9.354168
              Random: fixed  12 labels. Loss 0.02714. Accuracy 0.993.
### Flips: 615, rs: 2, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7229283e-08
Norm of the params: 6.0928125
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601082
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.283065e-08
Norm of the params: 6.092834
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041443698
Train loss (w/o reg) on all data: 0.03714604
Test loss (w/o reg) on all data: 0.021844806
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.945302e-06
Norm of the params: 9.271092
              Random: fixed  21 labels. Loss 0.02184. Accuracy 0.995.
### Flips: 615, rs: 2, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601034
Test loss (w/o reg) on all data: 0.0026560363
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.831329e-08
Norm of the params: 6.092843
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5981865e-08
Norm of the params: 6.0928135
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402577
Train loss (w/o reg) on all data: 0.035750724
Test loss (w/o reg) on all data: 0.021445815
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6868464e-06
Norm of the params: 9.494181
              Random: fixed  23 labels. Loss 0.02145. Accuracy 0.996.
### Flips: 615, rs: 2, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011034
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.66992e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.333224e-08
Norm of the params: 6.092813
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0396463
Train loss (w/o reg) on all data: 0.03521666
Test loss (w/o reg) on all data: 0.020271825
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5293006e-06
Norm of the params: 9.412377
              Random: fixed  26 labels. Loss 0.02027. Accuracy 0.998.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05681175
Train loss (w/o reg) on all data: 0.052143734
Test loss (w/o reg) on all data: 0.039503668
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.934292e-06
Norm of the params: 9.662314
Flipped loss: 0.03950. Accuracy: 0.987
### Flips: 615, rs: 3, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032454098
Train loss (w/o reg) on all data: 0.0012382173
Test loss (w/o reg) on all data: 0.0028660262
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.088305e-08
Norm of the params: 6.335917
     Influence (LOO): fixed  90 labels. Loss 0.00287. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039714994
Train loss (w/o reg) on all data: 0.0016631981
Test loss (w/o reg) on all data: 0.003404724
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7455277e-08
Norm of the params: 6.7945585
                Loss: fixed  89 labels. Loss 0.00340. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055401895
Train loss (w/o reg) on all data: 0.05080089
Test loss (w/o reg) on all data: 0.036430303
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.884329e-06
Norm of the params: 9.592711
              Random: fixed   6 labels. Loss 0.03643. Accuracy 0.991.
### Flips: 615, rs: 3, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.311592e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.077576e-09
Norm of the params: 6.092822
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054811567
Train loss (w/o reg) on all data: 0.05025275
Test loss (w/o reg) on all data: 0.036285993
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.8208578e-06
Norm of the params: 9.548628
              Random: fixed   8 labels. Loss 0.03629. Accuracy 0.989.
### Flips: 615, rs: 3, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2396775e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.42917e-09
Norm of the params: 6.0928216
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052911595
Train loss (w/o reg) on all data: 0.0483341
Test loss (w/o reg) on all data: 0.034509476
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0645889e-05
Norm of the params: 9.568173
              Random: fixed  12 labels. Loss 0.03451. Accuracy 0.991.
### Flips: 615, rs: 3, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601323
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8944516e-08
Norm of the params: 6.0927944
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7507325e-08
Norm of the params: 6.092822
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05219858
Train loss (w/o reg) on all data: 0.0475898
Test loss (w/o reg) on all data: 0.03367069
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.892769e-06
Norm of the params: 9.600815
              Random: fixed  14 labels. Loss 0.03367. Accuracy 0.991.
### Flips: 615, rs: 3, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9052758e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7029516e-08
Norm of the params: 6.0928183
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05043869
Train loss (w/o reg) on all data: 0.045871235
Test loss (w/o reg) on all data: 0.031510197
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.9092766e-06
Norm of the params: 9.557672
              Random: fixed  18 labels. Loss 0.03151. Accuracy 0.992.
### Flips: 615, rs: 3, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960097
Test loss (w/o reg) on all data: 0.0026560295
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3612526e-08
Norm of the params: 6.0928535
     Influence (LOO): fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1035161e-08
Norm of the params: 6.092818
                Loss: fixed  91 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047262296
Train loss (w/o reg) on all data: 0.042610113
Test loss (w/o reg) on all data: 0.030560398
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3988729e-06
Norm of the params: 9.645915
              Random: fixed  24 labels. Loss 0.03056. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049138486
Train loss (w/o reg) on all data: 0.045461256
Test loss (w/o reg) on all data: 0.028519183
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6202347e-06
Norm of the params: 8.575816
Flipped loss: 0.02852. Accuracy: 0.992
### Flips: 615, rs: 4, checks: 205
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013263
Test loss (w/o reg) on all data: 0.0026561068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2093706e-08
Norm of the params: 6.092794
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9033138e-08
Norm of the params: 6.0928183
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047041737
Train loss (w/o reg) on all data: 0.043292657
Test loss (w/o reg) on all data: 0.027936067
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.3253036e-06
Norm of the params: 8.659191
              Random: fixed   4 labels. Loss 0.02794. Accuracy 0.993.
### Flips: 615, rs: 4, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096013874
Test loss (w/o reg) on all data: 0.0026561054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7657608e-08
Norm of the params: 6.0927854
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8384613e-08
Norm of the params: 6.092823
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045726143
Train loss (w/o reg) on all data: 0.041950073
Test loss (w/o reg) on all data: 0.02595138
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0867562e-06
Norm of the params: 8.690303
              Random: fixed   8 labels. Loss 0.02595. Accuracy 0.995.
### Flips: 615, rs: 4, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560826
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0979384e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2616372e-08
Norm of the params: 6.09282
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04443023
Train loss (w/o reg) on all data: 0.040472113
Test loss (w/o reg) on all data: 0.02597033
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0427457e-05
Norm of the params: 8.897322
              Random: fixed  10 labels. Loss 0.02597. Accuracy 0.994.
### Flips: 615, rs: 4, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7833477e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4290267e-08
Norm of the params: 6.0928144
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041959032
Train loss (w/o reg) on all data: 0.037918497
Test loss (w/o reg) on all data: 0.024759712
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.6150879e-06
Norm of the params: 8.989477
              Random: fixed  15 labels. Loss 0.02476. Accuracy 0.994.
### Flips: 615, rs: 4, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3121409e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0515923e-08
Norm of the params: 6.09282
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04073459
Train loss (w/o reg) on all data: 0.03670588
Test loss (w/o reg) on all data: 0.023234049
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.7239787e-06
Norm of the params: 8.976313
              Random: fixed  18 labels. Loss 0.02323. Accuracy 0.994.
### Flips: 615, rs: 4, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7828905e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1976286e-09
Norm of the params: 6.0928183
                Loss: fixed  72 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03963926
Train loss (w/o reg) on all data: 0.03553143
Test loss (w/o reg) on all data: 0.023002906
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9305453e-06
Norm of the params: 9.064026
              Random: fixed  20 labels. Loss 0.02300. Accuracy 0.995.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050950613
Train loss (w/o reg) on all data: 0.04660763
Test loss (w/o reg) on all data: 0.032681793
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.707112e-06
Norm of the params: 9.319854
Flipped loss: 0.03268. Accuracy: 0.991
### Flips: 615, rs: 5, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3548601e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.10998695e-08
Norm of the params: 6.0928116
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049897533
Train loss (w/o reg) on all data: 0.04552719
Test loss (w/o reg) on all data: 0.030541727
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.0388832e-06
Norm of the params: 9.349166
              Random: fixed   3 labels. Loss 0.03054. Accuracy 0.991.
### Flips: 615, rs: 5, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3778835e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.13763345e-08
Norm of the params: 6.09281
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049304657
Train loss (w/o reg) on all data: 0.045156788
Test loss (w/o reg) on all data: 0.027337406
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2489911e-05
Norm of the params: 9.108096
              Random: fixed   8 labels. Loss 0.02734. Accuracy 0.994.
### Flips: 615, rs: 5, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4500187e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4342748e-08
Norm of the params: 6.0928216
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047901
Train loss (w/o reg) on all data: 0.04373006
Test loss (w/o reg) on all data: 0.026034338
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.757006e-06
Norm of the params: 9.13339
              Random: fixed  12 labels. Loss 0.02603. Accuracy 0.995.
### Flips: 615, rs: 5, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012716
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5284352e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16825705e-08
Norm of the params: 6.092822
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0456235
Train loss (w/o reg) on all data: 0.04159294
Test loss (w/o reg) on all data: 0.024095517
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.2782735e-06
Norm of the params: 8.978373
              Random: fixed  18 labels. Loss 0.02410. Accuracy 0.995.
### Flips: 615, rs: 5, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013123
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6838333e-08
Norm of the params: 6.092796
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010946
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5356734e-08
Norm of the params: 6.092832
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04400983
Train loss (w/o reg) on all data: 0.039904848
Test loss (w/o reg) on all data: 0.022867607
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.7671588e-06
Norm of the params: 9.060884
              Random: fixed  21 labels. Loss 0.02287. Accuracy 0.995.
### Flips: 615, rs: 5, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.981133e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6386034e-08
Norm of the params: 6.0928197
                Loss: fixed  79 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038362
Train loss (w/o reg) on all data: 0.033979658
Test loss (w/o reg) on all data: 0.01833937
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.734456e-06
Norm of the params: 9.361991
              Random: fixed  29 labels. Loss 0.01834. Accuracy 0.998.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055417374
Train loss (w/o reg) on all data: 0.051295318
Test loss (w/o reg) on all data: 0.033343952
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.281737e-06
Norm of the params: 9.079711
Flipped loss: 0.03334. Accuracy: 0.993
### Flips: 615, rs: 6, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2521927e-08
Norm of the params: 6.092808
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.550122e-09
Norm of the params: 6.0928216
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052706912
Train loss (w/o reg) on all data: 0.048456926
Test loss (w/o reg) on all data: 0.03281554
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.5087593e-06
Norm of the params: 9.219528
              Random: fixed   6 labels. Loss 0.03282. Accuracy 0.993.
### Flips: 615, rs: 6, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4948456e-08
Norm of the params: 6.0928087
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960108
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9534237e-08
Norm of the params: 6.092834
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050082322
Train loss (w/o reg) on all data: 0.045664594
Test loss (w/o reg) on all data: 0.03279983
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.961571e-06
Norm of the params: 9.3997135
              Random: fixed  10 labels. Loss 0.03280. Accuracy 0.994.
### Flips: 615, rs: 6, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2398182e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.901938e-09
Norm of the params: 6.0928173
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047583356
Train loss (w/o reg) on all data: 0.043392837
Test loss (w/o reg) on all data: 0.030099886
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.307627e-06
Norm of the params: 9.1548
              Random: fixed  17 labels. Loss 0.03010. Accuracy 0.992.
### Flips: 615, rs: 6, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8228908e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3685045e-08
Norm of the params: 6.092816
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046380915
Train loss (w/o reg) on all data: 0.04221613
Test loss (w/o reg) on all data: 0.029534973
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8018799e-06
Norm of the params: 9.126649
              Random: fixed  19 labels. Loss 0.02953. Accuracy 0.993.
### Flips: 615, rs: 6, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162384
Train loss (w/o reg) on all data: 0.00096013525
Test loss (w/o reg) on all data: 0.002656115
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3845876e-08
Norm of the params: 6.092788
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010655
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2445786e-08
Norm of the params: 6.092837
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043699723
Train loss (w/o reg) on all data: 0.0396206
Test loss (w/o reg) on all data: 0.02596177
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.5991685e-06
Norm of the params: 9.032301
              Random: fixed  26 labels. Loss 0.02596. Accuracy 0.993.
### Flips: 615, rs: 6, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010877
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4754572e-08
Norm of the params: 6.092834
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8595882e-08
Norm of the params: 6.0928054
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04182738
Train loss (w/o reg) on all data: 0.03755642
Test loss (w/o reg) on all data: 0.025692152
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.614898e-06
Norm of the params: 9.242251
              Random: fixed  29 labels. Loss 0.02569. Accuracy 0.995.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049300395
Train loss (w/o reg) on all data: 0.0454583
Test loss (w/o reg) on all data: 0.028183205
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6026898e-06
Norm of the params: 8.765953
Flipped loss: 0.02818. Accuracy: 0.997
### Flips: 615, rs: 7, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7933297e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.898834e-08
Norm of the params: 6.09281
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045923363
Train loss (w/o reg) on all data: 0.04175114
Test loss (w/o reg) on all data: 0.026557334
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1909185e-06
Norm of the params: 9.134795
              Random: fixed   5 labels. Loss 0.02656. Accuracy 0.996.
### Flips: 615, rs: 7, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560898
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1159224e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.965545e-09
Norm of the params: 6.0928283
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04431746
Train loss (w/o reg) on all data: 0.04013376
Test loss (w/o reg) on all data: 0.02611894
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6703273e-06
Norm of the params: 9.147354
              Random: fixed   7 labels. Loss 0.02612. Accuracy 0.997.
### Flips: 615, rs: 7, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0055388e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.002656083
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7117367e-08
Norm of the params: 6.0928082
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042831164
Train loss (w/o reg) on all data: 0.03864681
Test loss (w/o reg) on all data: 0.024919959
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.203475e-06
Norm of the params: 9.148063
              Random: fixed  11 labels. Loss 0.02492. Accuracy 0.997.
### Flips: 615, rs: 7, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600855
Test loss (w/o reg) on all data: 0.0026560342
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5076314e-08
Norm of the params: 6.092872
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010877
Test loss (w/o reg) on all data: 0.0026560368
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7403736e-08
Norm of the params: 6.0928335
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041175388
Train loss (w/o reg) on all data: 0.0369565
Test loss (w/o reg) on all data: 0.023897827
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.552994e-06
Norm of the params: 9.185734
              Random: fixed  13 labels. Loss 0.02390. Accuracy 0.998.
### Flips: 615, rs: 7, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4578553e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3384588e-08
Norm of the params: 6.0928144
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038176455
Train loss (w/o reg) on all data: 0.033872265
Test loss (w/o reg) on all data: 0.023310127
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2918872e-06
Norm of the params: 9.278136
              Random: fixed  18 labels. Loss 0.02331. Accuracy 0.997.
### Flips: 615, rs: 7, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4989434e-08
Norm of the params: 6.0928125
     Influence (LOO): fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0667939e-08
Norm of the params: 6.0928164
                Loss: fixed  66 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03741894
Train loss (w/o reg) on all data: 0.033205584
Test loss (w/o reg) on all data: 0.02337648
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2987092e-06
Norm of the params: 9.17971
              Random: fixed  20 labels. Loss 0.02338. Accuracy 0.996.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048199356
Train loss (w/o reg) on all data: 0.04390439
Test loss (w/o reg) on all data: 0.024946388
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.7748284e-06
Norm of the params: 9.268189
Flipped loss: 0.02495. Accuracy: 0.996
### Flips: 615, rs: 8, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.018933e-08
Norm of the params: 6.092829
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.939602e-09
Norm of the params: 6.0928197
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04761792
Train loss (w/o reg) on all data: 0.0433388
Test loss (w/o reg) on all data: 0.0249474
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.7587188e-06
Norm of the params: 9.251073
              Random: fixed   1 labels. Loss 0.02495. Accuracy 0.995.
### Flips: 615, rs: 8, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5375633e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560903
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6871537e-08
Norm of the params: 6.0928197
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045408655
Train loss (w/o reg) on all data: 0.041102223
Test loss (w/o reg) on all data: 0.022265755
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.3243316e-06
Norm of the params: 9.280554
              Random: fixed   7 labels. Loss 0.02227. Accuracy 0.997.
### Flips: 615, rs: 8, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013356
Test loss (w/o reg) on all data: 0.0026560943
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.852405e-08
Norm of the params: 6.092793
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4016028e-08
Norm of the params: 6.092821
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04387055
Train loss (w/o reg) on all data: 0.039481193
Test loss (w/o reg) on all data: 0.020927373
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5688496e-06
Norm of the params: 9.369477
              Random: fixed  10 labels. Loss 0.02093. Accuracy 0.999.
### Flips: 615, rs: 8, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5336385e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.919557e-09
Norm of the params: 6.092826
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042726003
Train loss (w/o reg) on all data: 0.038412627
Test loss (w/o reg) on all data: 0.020205133
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5416327e-06
Norm of the params: 9.288032
              Random: fixed  13 labels. Loss 0.02021. Accuracy 0.999.
### Flips: 615, rs: 8, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601315
Test loss (w/o reg) on all data: 0.0026560915
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2092959e-08
Norm of the params: 6.092796
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.043993e-08
Norm of the params: 6.0928254
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04094325
Train loss (w/o reg) on all data: 0.036583554
Test loss (w/o reg) on all data: 0.0193025
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.273626e-06
Norm of the params: 9.337767
              Random: fixed  16 labels. Loss 0.01930. Accuracy 0.999.
### Flips: 615, rs: 8, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.563796e-09
Norm of the params: 6.092822
     Influence (LOO): fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0051636e-08
Norm of the params: 6.092818
                Loss: fixed  69 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036956448
Train loss (w/o reg) on all data: 0.032802176
Test loss (w/o reg) on all data: 0.015887339
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8786271e-06
Norm of the params: 9.115123
              Random: fixed  23 labels. Loss 0.01589. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053275358
Train loss (w/o reg) on all data: 0.049413748
Test loss (w/o reg) on all data: 0.031628262
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.527154e-06
Norm of the params: 8.788187
Flipped loss: 0.03163. Accuracy: 0.995
### Flips: 615, rs: 9, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7663178e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3997238e-08
Norm of the params: 6.092823
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051817622
Train loss (w/o reg) on all data: 0.04779853
Test loss (w/o reg) on all data: 0.03107944
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.673341e-06
Norm of the params: 8.965593
              Random: fixed   3 labels. Loss 0.03108. Accuracy 0.995.
### Flips: 615, rs: 9, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4017519e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.525846e-08
Norm of the params: 6.092813
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04743336
Train loss (w/o reg) on all data: 0.04338545
Test loss (w/o reg) on all data: 0.030434132
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.0736326e-06
Norm of the params: 8.997679
              Random: fixed  11 labels. Loss 0.03043. Accuracy 0.994.
### Flips: 615, rs: 9, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.40675676e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.550848e-09
Norm of the params: 6.0928144
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044401504
Train loss (w/o reg) on all data: 0.040377088
Test loss (w/o reg) on all data: 0.029994002
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5915297e-06
Norm of the params: 8.97153
              Random: fixed  17 labels. Loss 0.02999. Accuracy 0.995.
### Flips: 615, rs: 9, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2475834e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.749962e-09
Norm of the params: 6.092814
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041639663
Train loss (w/o reg) on all data: 0.037348006
Test loss (w/o reg) on all data: 0.031134577
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9103156e-06
Norm of the params: 9.26462
              Random: fixed  20 labels. Loss 0.03113. Accuracy 0.992.
### Flips: 615, rs: 9, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2049976e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.852611e-09
Norm of the params: 6.092813
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040515788
Train loss (w/o reg) on all data: 0.03630402
Test loss (w/o reg) on all data: 0.029553302
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.3215266e-06
Norm of the params: 9.177985
              Random: fixed  23 labels. Loss 0.02955. Accuracy 0.994.
### Flips: 615, rs: 9, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5169816e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0281723e-08
Norm of the params: 6.092812
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03935593
Train loss (w/o reg) on all data: 0.035058547
Test loss (w/o reg) on all data: 0.029443547
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.526509e-06
Norm of the params: 9.270798
              Random: fixed  25 labels. Loss 0.02944. Accuracy 0.993.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04517596
Train loss (w/o reg) on all data: 0.04076001
Test loss (w/o reg) on all data: 0.021157797
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.645402e-06
Norm of the params: 9.397819
Flipped loss: 0.02116. Accuracy: 0.996
### Flips: 615, rs: 10, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.500456e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2758245e-08
Norm of the params: 6.0928154
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04384402
Train loss (w/o reg) on all data: 0.039579395
Test loss (w/o reg) on all data: 0.01993352
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.6551135e-06
Norm of the params: 9.235397
              Random: fixed   2 labels. Loss 0.01993. Accuracy 0.997.
### Flips: 615, rs: 10, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012646
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1996929e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0241101e-08
Norm of the params: 6.0928197
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04132455
Train loss (w/o reg) on all data: 0.037056364
Test loss (w/o reg) on all data: 0.019344315
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.886704e-06
Norm of the params: 9.239246
              Random: fixed   6 labels. Loss 0.01934. Accuracy 0.997.
### Flips: 615, rs: 10, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600987
Test loss (w/o reg) on all data: 0.0026560344
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.086789e-08
Norm of the params: 6.09285
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560444
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8187446e-08
Norm of the params: 6.092823
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040134136
Train loss (w/o reg) on all data: 0.03578768
Test loss (w/o reg) on all data: 0.018838923
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6145693e-06
Norm of the params: 9.323579
              Random: fixed   9 labels. Loss 0.01884. Accuracy 0.997.
### Flips: 615, rs: 10, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012815
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.866934e-08
Norm of the params: 6.092801
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.39369725e-08
Norm of the params: 6.092816
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038425244
Train loss (w/o reg) on all data: 0.034244604
Test loss (w/o reg) on all data: 0.018606555
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.062861e-06
Norm of the params: 9.144002
              Random: fixed  13 labels. Loss 0.01861. Accuracy 0.996.
### Flips: 615, rs: 10, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0093887e-08
Norm of the params: 6.092817
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.092573e-08
Norm of the params: 6.0928164
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03743512
Train loss (w/o reg) on all data: 0.033246465
Test loss (w/o reg) on all data: 0.017213844
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1863461e-05
Norm of the params: 9.152765
              Random: fixed  15 labels. Loss 0.01721. Accuracy 0.998.
### Flips: 615, rs: 10, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4880254e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5712724e-08
Norm of the params: 6.09282
                Loss: fixed  67 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03574801
Train loss (w/o reg) on all data: 0.031540807
Test loss (w/o reg) on all data: 0.015533346
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7185222e-06
Norm of the params: 9.173006
              Random: fixed  20 labels. Loss 0.01553. Accuracy 0.999.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053621087
Train loss (w/o reg) on all data: 0.049463306
Test loss (w/o reg) on all data: 0.034417927
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.1188138e-06
Norm of the params: 9.118969
Flipped loss: 0.03442. Accuracy: 0.994
### Flips: 615, rs: 11, checks: 205
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7417226e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004410125
Train loss (w/o reg) on all data: 0.0018226737
Test loss (w/o reg) on all data: 0.004105964
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.4519758e-08
Norm of the params: 7.19368
                Loss: fixed  80 labels. Loss 0.00411. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052035727
Train loss (w/o reg) on all data: 0.048007682
Test loss (w/o reg) on all data: 0.03264281
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.307732e-06
Norm of the params: 8.9755745
              Random: fixed   4 labels. Loss 0.03264. Accuracy 0.994.
### Flips: 615, rs: 11, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.058662e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601119
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2328717e-08
Norm of the params: 6.092829
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050753407
Train loss (w/o reg) on all data: 0.04672854
Test loss (w/o reg) on all data: 0.030294374
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.4963633e-06
Norm of the params: 8.972031
              Random: fixed   9 labels. Loss 0.03029. Accuracy 0.995.
### Flips: 615, rs: 11, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4097378e-08
Norm of the params: 6.092827
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.314778e-09
Norm of the params: 6.092821
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048025988
Train loss (w/o reg) on all data: 0.04415849
Test loss (w/o reg) on all data: 0.0280449
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.61389e-06
Norm of the params: 8.794886
              Random: fixed  15 labels. Loss 0.02804. Accuracy 0.996.
### Flips: 615, rs: 11, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601034
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3886476e-08
Norm of the params: 6.092841
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3277922e-08
Norm of the params: 6.092817
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04802599
Train loss (w/o reg) on all data: 0.044159222
Test loss (w/o reg) on all data: 0.028045198
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.998243e-06
Norm of the params: 8.794055
              Random: fixed  15 labels. Loss 0.02805. Accuracy 0.996.
### Flips: 615, rs: 11, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601033
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.39617e-08
Norm of the params: 6.092841
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3326771e-08
Norm of the params: 6.092817
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046231307
Train loss (w/o reg) on all data: 0.04202026
Test loss (w/o reg) on all data: 0.027745971
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.0930466e-06
Norm of the params: 9.177197
              Random: fixed  18 labels. Loss 0.02775. Accuracy 0.995.
### Flips: 615, rs: 11, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601016
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2405e-08
Norm of the params: 6.092845
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7511373e-09
Norm of the params: 6.0928206
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043962583
Train loss (w/o reg) on all data: 0.03967886
Test loss (w/o reg) on all data: 0.026779708
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.191266e-06
Norm of the params: 9.256048
              Random: fixed  22 labels. Loss 0.02678. Accuracy 0.996.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055572383
Train loss (w/o reg) on all data: 0.051815346
Test loss (w/o reg) on all data: 0.028312692
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3373168e-06
Norm of the params: 8.668374
Flipped loss: 0.02831. Accuracy: 0.996
### Flips: 615, rs: 12, checks: 205
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035324683
Train loss (w/o reg) on all data: 0.0013948568
Test loss (w/o reg) on all data: 0.0029086487
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.795087e-08
Norm of the params: 6.5385194
     Influence (LOO): fixed  81 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035324686
Train loss (w/o reg) on all data: 0.0013948581
Test loss (w/o reg) on all data: 0.0029086762
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.396616e-08
Norm of the params: 6.538517
                Loss: fixed  81 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05400537
Train loss (w/o reg) on all data: 0.050310865
Test loss (w/o reg) on all data: 0.026872711
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.496932e-06
Norm of the params: 8.595937
              Random: fixed   3 labels. Loss 0.02687. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601024
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3615763e-08
Norm of the params: 6.0928435
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.638301e-09
Norm of the params: 6.092814
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050973963
Train loss (w/o reg) on all data: 0.047221705
Test loss (w/o reg) on all data: 0.025790973
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.480616e-05
Norm of the params: 8.66286
              Random: fixed  11 labels. Loss 0.02579. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4675803e-08
Norm of the params: 6.092834
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.724095e-08
Norm of the params: 6.092816
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050616365
Train loss (w/o reg) on all data: 0.046880115
Test loss (w/o reg) on all data: 0.025693944
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7983735e-05
Norm of the params: 8.644363
              Random: fixed  12 labels. Loss 0.02569. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601091
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2656335e-08
Norm of the params: 6.092833
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.661168e-08
Norm of the params: 6.0928154
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04713167
Train loss (w/o reg) on all data: 0.0430964
Test loss (w/o reg) on all data: 0.023924936
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.772231e-06
Norm of the params: 8.983617
              Random: fixed  18 labels. Loss 0.02392. Accuracy 0.994.
### Flips: 615, rs: 12, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.876062e-09
Norm of the params: 6.0928144
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0486667e-08
Norm of the params: 6.092819
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045836695
Train loss (w/o reg) on all data: 0.041831493
Test loss (w/o reg) on all data: 0.02288927
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.9894525e-06
Norm of the params: 8.950083
              Random: fixed  20 labels. Loss 0.02289. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5377876e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3060372e-08
Norm of the params: 6.0928154
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043748584
Train loss (w/o reg) on all data: 0.039747484
Test loss (w/o reg) on all data: 0.021199606
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8935607e-06
Norm of the params: 8.945502
              Random: fixed  25 labels. Loss 0.02120. Accuracy 0.997.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050495062
Train loss (w/o reg) on all data: 0.046128944
Test loss (w/o reg) on all data: 0.032072484
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5869724e-06
Norm of the params: 9.344644
Flipped loss: 0.03207. Accuracy: 0.991
### Flips: 615, rs: 13, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7146723e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035932385
Train loss (w/o reg) on all data: 0.0013895625
Test loss (w/o reg) on all data: 0.0031607538
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3105388e-08
Norm of the params: 6.6387887
                Loss: fixed  73 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04945214
Train loss (w/o reg) on all data: 0.045039624
Test loss (w/o reg) on all data: 0.030264676
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7482745e-06
Norm of the params: 9.394165
              Random: fixed   3 labels. Loss 0.03026. Accuracy 0.992.
### Flips: 615, rs: 13, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8944343e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7124803e-08
Norm of the params: 6.09281
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047708865
Train loss (w/o reg) on all data: 0.043100845
Test loss (w/o reg) on all data: 0.029156867
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.596598e-06
Norm of the params: 9.60002
              Random: fixed   6 labels. Loss 0.02916. Accuracy 0.994.
### Flips: 615, rs: 13, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8617163e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.274541e-09
Norm of the params: 6.092819
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046917327
Train loss (w/o reg) on all data: 0.04249591
Test loss (w/o reg) on all data: 0.026576042
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.555819e-06
Norm of the params: 9.403634
              Random: fixed   9 labels. Loss 0.02658. Accuracy 0.997.
### Flips: 615, rs: 13, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560442
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1775395e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3175728e-08
Norm of the params: 6.0928116
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046048798
Train loss (w/o reg) on all data: 0.041569605
Test loss (w/o reg) on all data: 0.025374738
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.6025006e-06
Norm of the params: 9.464875
              Random: fixed  11 labels. Loss 0.02537. Accuracy 0.997.
### Flips: 615, rs: 13, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.691664e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5337187e-08
Norm of the params: 6.092816
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04593729
Train loss (w/o reg) on all data: 0.041463446
Test loss (w/o reg) on all data: 0.025171837
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.497435e-06
Norm of the params: 9.459222
              Random: fixed  12 labels. Loss 0.02517. Accuracy 0.997.
### Flips: 615, rs: 13, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600764
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5909698e-07
Norm of the params: 6.092887
     Influence (LOO): fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9411005e-08
Norm of the params: 6.09283
                Loss: fixed  74 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042563513
Train loss (w/o reg) on all data: 0.03798005
Test loss (w/o reg) on all data: 0.024457373
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4780145e-06
Norm of the params: 9.574408
              Random: fixed  16 labels. Loss 0.02446. Accuracy 0.995.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05239712
Train loss (w/o reg) on all data: 0.048107762
Test loss (w/o reg) on all data: 0.025849497
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4479214e-06
Norm of the params: 9.2621355
Flipped loss: 0.02585. Accuracy: 0.999
### Flips: 615, rs: 14, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.37574885e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4294737e-08
Norm of the params: 6.09282
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050391387
Train loss (w/o reg) on all data: 0.046116043
Test loss (w/o reg) on all data: 0.023709934
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.7349674e-06
Norm of the params: 9.246993
              Random: fixed   5 labels. Loss 0.02371. Accuracy 0.999.
### Flips: 615, rs: 14, checks: 410
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0077551e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2345033e-08
Norm of the params: 6.092816
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046369255
Train loss (w/o reg) on all data: 0.04210459
Test loss (w/o reg) on all data: 0.022189673
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.148388e-06
Norm of the params: 9.235437
              Random: fixed  13 labels. Loss 0.02219. Accuracy 0.999.
### Flips: 615, rs: 14, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9968715e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560472
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6625249e-08
Norm of the params: 6.0928183
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04316865
Train loss (w/o reg) on all data: 0.038777623
Test loss (w/o reg) on all data: 0.021633394
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9194518e-06
Norm of the params: 9.371262
              Random: fixed  17 labels. Loss 0.02163. Accuracy 0.999.
### Flips: 615, rs: 14, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.454991e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601231
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8620747e-08
Norm of the params: 6.0928097
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03948005
Train loss (w/o reg) on all data: 0.034851417
Test loss (w/o reg) on all data: 0.018419728
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.4903953e-06
Norm of the params: 9.621469
              Random: fixed  24 labels. Loss 0.01842. Accuracy 0.998.
### Flips: 615, rs: 14, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.245237e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1134423e-09
Norm of the params: 6.0928264
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03615478
Train loss (w/o reg) on all data: 0.03153526
Test loss (w/o reg) on all data: 0.017738927
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5110407e-06
Norm of the params: 9.611992
              Random: fixed  29 labels. Loss 0.01774. Accuracy 0.998.
### Flips: 615, rs: 14, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.103395e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960131
Test loss (w/o reg) on all data: 0.002656087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3267074e-08
Norm of the params: 6.0927973
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03351383
Train loss (w/o reg) on all data: 0.028960854
Test loss (w/o reg) on all data: 0.016960667
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5310496e-06
Norm of the params: 9.542513
              Random: fixed  33 labels. Loss 0.01696. Accuracy 0.997.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055393655
Train loss (w/o reg) on all data: 0.051684134
Test loss (w/o reg) on all data: 0.035740037
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.39073e-06
Norm of the params: 8.613387
Flipped loss: 0.03574. Accuracy: 0.988
### Flips: 615, rs: 15, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.713639e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9546777e-08
Norm of the params: 6.092818
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05126434
Train loss (w/o reg) on all data: 0.047574963
Test loss (w/o reg) on all data: 0.031917732
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.434105e-06
Norm of the params: 8.589967
              Random: fixed   8 labels. Loss 0.03192. Accuracy 0.992.
### Flips: 615, rs: 15, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162408
Train loss (w/o reg) on all data: 0.00096012634
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.974729e-08
Norm of the params: 6.092806
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011255
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.451731e-08
Norm of the params: 6.092827
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049148526
Train loss (w/o reg) on all data: 0.045504108
Test loss (w/o reg) on all data: 0.030664925
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5929655e-06
Norm of the params: 8.537467
              Random: fixed  13 labels. Loss 0.03066. Accuracy 0.991.
### Flips: 615, rs: 15, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560442
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9449339e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601247
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0351553e-08
Norm of the params: 6.0928073
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045733523
Train loss (w/o reg) on all data: 0.041863516
Test loss (w/o reg) on all data: 0.02930167
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.364615e-06
Norm of the params: 8.797733
              Random: fixed  19 labels. Loss 0.02930. Accuracy 0.990.
### Flips: 615, rs: 15, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960105
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.954681e-08
Norm of the params: 6.09284
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0995356e-08
Norm of the params: 6.0928164
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045606777
Train loss (w/o reg) on all data: 0.041804507
Test loss (w/o reg) on all data: 0.02896195
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.8890523e-06
Norm of the params: 8.720401
              Random: fixed  20 labels. Loss 0.02896. Accuracy 0.992.
### Flips: 615, rs: 15, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010766
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4273708e-08
Norm of the params: 6.092836
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.427323e-09
Norm of the params: 6.0928164
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045148063
Train loss (w/o reg) on all data: 0.041298125
Test loss (w/o reg) on all data: 0.028231042
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.481204e-06
Norm of the params: 8.774893
              Random: fixed  21 labels. Loss 0.02823. Accuracy 0.990.
### Flips: 615, rs: 15, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601084
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1182617e-08
Norm of the params: 6.092834
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9906885e-09
Norm of the params: 6.0928173
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040790033
Train loss (w/o reg) on all data: 0.037146404
Test loss (w/o reg) on all data: 0.025497975
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.856208e-06
Norm of the params: 8.536543
              Random: fixed  29 labels. Loss 0.02550. Accuracy 0.993.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054129608
Train loss (w/o reg) on all data: 0.050209116
Test loss (w/o reg) on all data: 0.033861566
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.8249905e-06
Norm of the params: 8.854931
Flipped loss: 0.03386. Accuracy: 0.992
### Flips: 615, rs: 16, checks: 205
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034099012
Train loss (w/o reg) on all data: 0.0013046075
Test loss (w/o reg) on all data: 0.0049528778
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.194437e-08
Norm of the params: 6.4889035
     Influence (LOO): fixed  82 labels. Loss 0.00495. Accuracy 0.999.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1270358e-08
Norm of the params: 6.092809
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0533905
Train loss (w/o reg) on all data: 0.049434066
Test loss (w/o reg) on all data: 0.032419726
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.774034e-06
Norm of the params: 8.8954315
              Random: fixed   2 labels. Loss 0.03242. Accuracy 0.991.
### Flips: 615, rs: 16, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096014055
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5392916e-08
Norm of the params: 6.0927825
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6388864e-08
Norm of the params: 6.092814
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051606975
Train loss (w/o reg) on all data: 0.047632277
Test loss (w/o reg) on all data: 0.030208886
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.325135e-06
Norm of the params: 8.915937
              Random: fixed   5 labels. Loss 0.03021. Accuracy 0.993.
### Flips: 615, rs: 16, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601557
Test loss (w/o reg) on all data: 0.0026561143
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7044425e-08
Norm of the params: 6.0927577
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.295263e-08
Norm of the params: 6.0928173
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049086984
Train loss (w/o reg) on all data: 0.04500709
Test loss (w/o reg) on all data: 0.02891714
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9667857e-06
Norm of the params: 9.033154
              Random: fixed  10 labels. Loss 0.02892. Accuracy 0.991.
### Flips: 615, rs: 16, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2436302e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0182359e-08
Norm of the params: 6.0928245
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046449754
Train loss (w/o reg) on all data: 0.042203087
Test loss (w/o reg) on all data: 0.026309896
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.313247e-06
Norm of the params: 9.215927
              Random: fixed  15 labels. Loss 0.02631. Accuracy 0.992.
### Flips: 615, rs: 16, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.623487e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.754118e-09
Norm of the params: 6.092813
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045831967
Train loss (w/o reg) on all data: 0.041485675
Test loss (w/o reg) on all data: 0.02614328
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.01094e-06
Norm of the params: 9.323404
              Random: fixed  16 labels. Loss 0.02614. Accuracy 0.992.
### Flips: 615, rs: 16, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600863
Test loss (w/o reg) on all data: 0.002656022
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6221434e-08
Norm of the params: 6.0928707
     Influence (LOO): fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7639618e-08
Norm of the params: 6.092823
                Loss: fixed  83 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04360563
Train loss (w/o reg) on all data: 0.03918143
Test loss (w/o reg) on all data: 0.024536679
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.9049602e-06
Norm of the params: 9.406594
              Random: fixed  20 labels. Loss 0.02454. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05744344
Train loss (w/o reg) on all data: 0.05297945
Test loss (w/o reg) on all data: 0.032324776
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.1382706e-06
Norm of the params: 9.448798
Flipped loss: 0.03232. Accuracy: 0.994
### Flips: 615, rs: 17, checks: 205
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030695195
Train loss (w/o reg) on all data: 0.001141026
Test loss (w/o reg) on all data: 0.0028270297
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.213964e-08
Norm of the params: 6.2104645
     Influence (LOO): fixed  88 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003593239
Train loss (w/o reg) on all data: 0.0013895432
Test loss (w/o reg) on all data: 0.0031607407
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7033375e-08
Norm of the params: 6.638819
                Loss: fixed  88 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054235045
Train loss (w/o reg) on all data: 0.04953347
Test loss (w/o reg) on all data: 0.029580113
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1672536e-06
Norm of the params: 9.696981
              Random: fixed   7 labels. Loss 0.02958. Accuracy 0.996.
### Flips: 615, rs: 17, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.25409e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.958344e-09
Norm of the params: 6.0928206
                Loss: fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052968666
Train loss (w/o reg) on all data: 0.048316173
Test loss (w/o reg) on all data: 0.028468544
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.946785e-06
Norm of the params: 9.6462345
              Random: fixed  10 labels. Loss 0.02847. Accuracy 0.995.
### Flips: 615, rs: 17, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6061811e-08
Norm of the params: 6.092813
     Influence (LOO): fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0091754e-08
Norm of the params: 6.092823
                Loss: fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05097693
Train loss (w/o reg) on all data: 0.04643191
Test loss (w/o reg) on all data: 0.027230615
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6985758e-06
Norm of the params: 9.534168
              Random: fixed  15 labels. Loss 0.02723. Accuracy 0.995.
### Flips: 615, rs: 17, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011185
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7516328e-08
Norm of the params: 6.092828
     Influence (LOO): fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.4325652e-09
Norm of the params: 6.092813
                Loss: fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046715684
Train loss (w/o reg) on all data: 0.04186684
Test loss (w/o reg) on all data: 0.023576176
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.669069e-06
Norm of the params: 9.847687
              Random: fixed  22 labels. Loss 0.02358. Accuracy 0.997.
### Flips: 615, rs: 17, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009600759
Test loss (w/o reg) on all data: 0.002656005
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.165482e-08
Norm of the params: 6.092889
     Influence (LOO): fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9199077e-08
Norm of the params: 6.092813
                Loss: fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044797704
Train loss (w/o reg) on all data: 0.040062968
Test loss (w/o reg) on all data: 0.021781566
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2047005e-05
Norm of the params: 9.731121
              Random: fixed  27 labels. Loss 0.02178. Accuracy 0.997.
### Flips: 615, rs: 17, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011005
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6637935e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8326144e-08
Norm of the params: 6.0928082
                Loss: fixed  89 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042385638
Train loss (w/o reg) on all data: 0.037660822
Test loss (w/o reg) on all data: 0.020753011
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8521404e-06
Norm of the params: 9.720922
              Random: fixed  32 labels. Loss 0.02075. Accuracy 0.997.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05216793
Train loss (w/o reg) on all data: 0.047925588
Test loss (w/o reg) on all data: 0.02885411
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3686652e-05
Norm of the params: 9.211235
Flipped loss: 0.02885. Accuracy: 0.997
### Flips: 615, rs: 18, checks: 205
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035215395
Train loss (w/o reg) on all data: 0.0013973832
Test loss (w/o reg) on all data: 0.003233289
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.490171e-08
Norm of the params: 6.517908
     Influence (LOO): fixed  77 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601273
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6464627e-08
Norm of the params: 6.092803
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050826505
Train loss (w/o reg) on all data: 0.046597753
Test loss (w/o reg) on all data: 0.026483845
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.0148666e-06
Norm of the params: 9.196467
              Random: fixed   3 labels. Loss 0.02648. Accuracy 0.997.
### Flips: 615, rs: 18, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096010196
Test loss (w/o reg) on all data: 0.002656049
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0095376e-08
Norm of the params: 6.0928454
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.646252e-08
Norm of the params: 6.09282
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04874124
Train loss (w/o reg) on all data: 0.044527926
Test loss (w/o reg) on all data: 0.024334613
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0263746e-05
Norm of the params: 9.179668
              Random: fixed   9 labels. Loss 0.02433. Accuracy 0.998.
### Flips: 615, rs: 18, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601361
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.297535e-08
Norm of the params: 6.0927896
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.838598e-08
Norm of the params: 6.0928063
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04736112
Train loss (w/o reg) on all data: 0.043301534
Test loss (w/o reg) on all data: 0.023739362
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.34484835e-05
Norm of the params: 9.010646
              Random: fixed  13 labels. Loss 0.02374. Accuracy 0.999.
### Flips: 615, rs: 18, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012995
Test loss (w/o reg) on all data: 0.0026560987
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7950954e-08
Norm of the params: 6.092798
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601121
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5132197e-08
Norm of the params: 6.0928273
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04531905
Train loss (w/o reg) on all data: 0.041088525
Test loss (w/o reg) on all data: 0.023571173
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3073808e-06
Norm of the params: 9.198399
              Random: fixed  16 labels. Loss 0.02357. Accuracy 0.998.
### Flips: 615, rs: 18, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.002656095
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7106727e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5600893e-08
Norm of the params: 6.09282
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043541487
Train loss (w/o reg) on all data: 0.03923642
Test loss (w/o reg) on all data: 0.022013966
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.030832e-06
Norm of the params: 9.279083
              Random: fixed  19 labels. Loss 0.02201. Accuracy 0.998.
### Flips: 615, rs: 18, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4736877e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5490182e-08
Norm of the params: 6.092816
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0412111
Train loss (w/o reg) on all data: 0.036768723
Test loss (w/o reg) on all data: 0.020877348
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.833166e-07
Norm of the params: 9.425894
              Random: fixed  23 labels. Loss 0.02088. Accuracy 0.998.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04891937
Train loss (w/o reg) on all data: 0.044271823
Test loss (w/o reg) on all data: 0.029130813
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2736243e-06
Norm of the params: 9.641105
Flipped loss: 0.02913. Accuracy: 0.996
### Flips: 615, rs: 19, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.182503e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4155755e-08
Norm of the params: 6.0928144
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047724284
Train loss (w/o reg) on all data: 0.04309465
Test loss (w/o reg) on all data: 0.028984794
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.3287037e-06
Norm of the params: 9.622507
              Random: fixed   2 labels. Loss 0.02898. Accuracy 0.996.
### Flips: 615, rs: 19, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5581662e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012495
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.15954455e-08
Norm of the params: 6.0928073
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047393847
Train loss (w/o reg) on all data: 0.04280869
Test loss (w/o reg) on all data: 0.02863842
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.7499735e-06
Norm of the params: 9.576177
              Random: fixed   4 labels. Loss 0.02864. Accuracy 0.995.
### Flips: 615, rs: 19, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5309656e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1385101e-08
Norm of the params: 6.092808
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046808194
Train loss (w/o reg) on all data: 0.04217643
Test loss (w/o reg) on all data: 0.028680718
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.617234e-06
Norm of the params: 9.6247225
              Random: fixed   5 labels. Loss 0.02868. Accuracy 0.996.
### Flips: 615, rs: 19, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601128
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4842756e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1156057e-08
Norm of the params: 6.092808
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044084888
Train loss (w/o reg) on all data: 0.039346423
Test loss (w/o reg) on all data: 0.027922742
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2379922e-06
Norm of the params: 9.734953
              Random: fixed  11 labels. Loss 0.02792. Accuracy 0.996.
### Flips: 615, rs: 19, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011057
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1224416e-08
Norm of the params: 6.09283
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.771875e-09
Norm of the params: 6.0928144
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042741068
Train loss (w/o reg) on all data: 0.0378871
Test loss (w/o reg) on all data: 0.028067375
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4202915e-06
Norm of the params: 9.852884
              Random: fixed  14 labels. Loss 0.02807. Accuracy 0.993.
### Flips: 615, rs: 19, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600968
Test loss (w/o reg) on all data: 0.0026560372
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8491426e-08
Norm of the params: 6.092853
     Influence (LOO): fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0593265e-08
Norm of the params: 6.09282
                Loss: fixed  73 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04096803
Train loss (w/o reg) on all data: 0.036120053
Test loss (w/o reg) on all data: 0.027472964
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.043094e-06
Norm of the params: 9.846805
              Random: fixed  18 labels. Loss 0.02747. Accuracy 0.994.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05506355
Train loss (w/o reg) on all data: 0.05106023
Test loss (w/o reg) on all data: 0.028439812
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.263315e-06
Norm of the params: 8.947983
Flipped loss: 0.02844. Accuracy: 0.998
### Flips: 615, rs: 20, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.002656043
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7368994e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.391801e-09
Norm of the params: 6.0928144
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05308809
Train loss (w/o reg) on all data: 0.04915963
Test loss (w/o reg) on all data: 0.027530644
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.4079052e-06
Norm of the params: 8.863927
              Random: fixed   4 labels. Loss 0.02753. Accuracy 0.994.
### Flips: 615, rs: 20, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601328
Test loss (w/o reg) on all data: 0.0026560868
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5630304e-08
Norm of the params: 6.0927944
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7073774e-08
Norm of the params: 6.0928235
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051978543
Train loss (w/o reg) on all data: 0.048147198
Test loss (w/o reg) on all data: 0.026449472
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.00312955e-05
Norm of the params: 8.75368
              Random: fixed   8 labels. Loss 0.02645. Accuracy 0.995.
### Flips: 615, rs: 20, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096007064
Test loss (w/o reg) on all data: 0.0026559734
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1273461e-07
Norm of the params: 6.0928965
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2367178e-08
Norm of the params: 6.0928154
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05034894
Train loss (w/o reg) on all data: 0.04652099
Test loss (w/o reg) on all data: 0.025639597
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.8322754e-06
Norm of the params: 8.749803
              Random: fixed  12 labels. Loss 0.02564. Accuracy 0.996.
### Flips: 615, rs: 20, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600876
Test loss (w/o reg) on all data: 0.002656008
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.79893e-08
Norm of the params: 6.0928674
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.42619e-09
Norm of the params: 6.09282
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046411507
Train loss (w/o reg) on all data: 0.04254554
Test loss (w/o reg) on all data: 0.023155544
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.923364e-06
Norm of the params: 8.793141
              Random: fixed  18 labels. Loss 0.02316. Accuracy 0.996.
### Flips: 615, rs: 20, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2267845e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.877457e-09
Norm of the params: 6.092823
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044445276
Train loss (w/o reg) on all data: 0.040718004
Test loss (w/o reg) on all data: 0.02190743
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.8156907e-06
Norm of the params: 8.633969
              Random: fixed  23 labels. Loss 0.02191. Accuracy 0.997.
### Flips: 615, rs: 20, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012536
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2476295e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2023311e-08
Norm of the params: 6.092825
                Loss: fixed  82 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041223943
Train loss (w/o reg) on all data: 0.037654947
Test loss (w/o reg) on all data: 0.021178585
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0333594e-06
Norm of the params: 8.448664
              Random: fixed  29 labels. Loss 0.02118. Accuracy 0.995.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06147779
Train loss (w/o reg) on all data: 0.05773452
Test loss (w/o reg) on all data: 0.03197502
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.377218e-06
Norm of the params: 8.652483
Flipped loss: 0.03198. Accuracy: 0.996
### Flips: 615, rs: 21, checks: 205
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034181958
Train loss (w/o reg) on all data: 0.0012644359
Test loss (w/o reg) on all data: 0.0029283776
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8885864e-07
Norm of the params: 6.56317
     Influence (LOO): fixed  95 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047154156
Train loss (w/o reg) on all data: 0.001951242
Test loss (w/o reg) on all data: 0.003411653
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.479012e-07
Norm of the params: 7.4352856
                Loss: fixed  93 labels. Loss 0.00341. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05922688
Train loss (w/o reg) on all data: 0.055369508
Test loss (w/o reg) on all data: 0.030917266
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.521319e-06
Norm of the params: 8.783364
              Random: fixed   4 labels. Loss 0.03092. Accuracy 0.996.
### Flips: 615, rs: 21, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4847887e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034181953
Train loss (w/o reg) on all data: 0.0012644099
Test loss (w/o reg) on all data: 0.0029283892
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8768165e-08
Norm of the params: 6.563209
                Loss: fixed  95 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05512935
Train loss (w/o reg) on all data: 0.050795976
Test loss (w/o reg) on all data: 0.02962095
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.705928e-06
Norm of the params: 9.309538
              Random: fixed  12 labels. Loss 0.02962. Accuracy 0.998.
### Flips: 615, rs: 21, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.222017e-09
Norm of the params: 6.09282
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2106685e-08
Norm of the params: 6.092818
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053809963
Train loss (w/o reg) on all data: 0.049359065
Test loss (w/o reg) on all data: 0.028398804
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2662269e-05
Norm of the params: 9.434937
              Random: fixed  15 labels. Loss 0.02840. Accuracy 0.997.
### Flips: 615, rs: 21, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0568572e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6126157e-09
Norm of the params: 6.0928164
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051463157
Train loss (w/o reg) on all data: 0.04692944
Test loss (w/o reg) on all data: 0.02743349
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4179096e-06
Norm of the params: 9.522307
              Random: fixed  20 labels. Loss 0.02743. Accuracy 0.997.
### Flips: 615, rs: 21, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601282
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.896224e-08
Norm of the params: 6.092801
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560843
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0682347e-08
Norm of the params: 6.09281
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047185224
Train loss (w/o reg) on all data: 0.042717863
Test loss (w/o reg) on all data: 0.026628418
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.516457e-06
Norm of the params: 9.452364
              Random: fixed  29 labels. Loss 0.02663. Accuracy 0.996.
### Flips: 615, rs: 21, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4370655e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8729223e-08
Norm of the params: 6.0928264
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045811206
Train loss (w/o reg) on all data: 0.04133457
Test loss (w/o reg) on all data: 0.025485078
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1807762e-06
Norm of the params: 9.462174
              Random: fixed  34 labels. Loss 0.02549. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046631396
Train loss (w/o reg) on all data: 0.042263616
Test loss (w/o reg) on all data: 0.0219714
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9597333e-06
Norm of the params: 9.34642
Flipped loss: 0.02197. Accuracy: 0.997
### Flips: 615, rs: 22, checks: 205
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.578531e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16239365e-08
Norm of the params: 6.0928264
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04525225
Train loss (w/o reg) on all data: 0.041008066
Test loss (w/o reg) on all data: 0.02161644
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.043953e-06
Norm of the params: 9.213229
              Random: fixed   3 labels. Loss 0.02162. Accuracy 0.997.
### Flips: 615, rs: 22, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2518643e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2057236e-08
Norm of the params: 6.092821
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043763828
Train loss (w/o reg) on all data: 0.03964505
Test loss (w/o reg) on all data: 0.020157214
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1677873e-06
Norm of the params: 9.0761
              Random: fixed   8 labels. Loss 0.02016. Accuracy 0.998.
### Flips: 615, rs: 22, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1014517e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.27024755e-08
Norm of the params: 6.092814
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042252302
Train loss (w/o reg) on all data: 0.038163587
Test loss (w/o reg) on all data: 0.020223835
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8402623e-06
Norm of the params: 9.042915
              Random: fixed  10 labels. Loss 0.02022. Accuracy 0.998.
### Flips: 615, rs: 22, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7154678e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0291896e-08
Norm of the params: 6.0928173
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041959938
Train loss (w/o reg) on all data: 0.038055055
Test loss (w/o reg) on all data: 0.019979086
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.098258e-06
Norm of the params: 8.837286
              Random: fixed  12 labels. Loss 0.01998. Accuracy 0.998.
### Flips: 615, rs: 22, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5172015e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2870637e-08
Norm of the params: 6.092816
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038970504
Train loss (w/o reg) on all data: 0.035080243
Test loss (w/o reg) on all data: 0.018306473
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5020072e-06
Norm of the params: 8.820728
              Random: fixed  17 labels. Loss 0.01831. Accuracy 0.998.
### Flips: 615, rs: 22, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011994
Test loss (w/o reg) on all data: 0.002656075
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2254719e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0818e-08
Norm of the params: 6.0928216
                Loss: fixed  64 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037677404
Train loss (w/o reg) on all data: 0.03385399
Test loss (w/o reg) on all data: 0.017108634
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.637734e-06
Norm of the params: 8.744615
              Random: fixed  20 labels. Loss 0.01711. Accuracy 0.998.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05508625
Train loss (w/o reg) on all data: 0.050852347
Test loss (w/o reg) on all data: 0.02938765
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.0567348e-06
Norm of the params: 9.202068
Flipped loss: 0.02939. Accuracy: 0.994
### Flips: 615, rs: 23, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601095
Test loss (w/o reg) on all data: 0.0026560454
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2685566e-08
Norm of the params: 6.092832
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9088995e-08
Norm of the params: 6.092821
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05276051
Train loss (w/o reg) on all data: 0.048573066
Test loss (w/o reg) on all data: 0.028020795
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.2425445e-06
Norm of the params: 9.151446
              Random: fixed   7 labels. Loss 0.02802. Accuracy 0.995.
### Flips: 615, rs: 23, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.282016e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560877
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.587942e-08
Norm of the params: 6.092812
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051129885
Train loss (w/o reg) on all data: 0.046853535
Test loss (w/o reg) on all data: 0.026840288
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9429324e-06
Norm of the params: 9.248082
              Random: fixed  10 labels. Loss 0.02684. Accuracy 0.995.
### Flips: 615, rs: 23, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012076
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3633441e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.18367e-09
Norm of the params: 6.09282
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050419286
Train loss (w/o reg) on all data: 0.046340913
Test loss (w/o reg) on all data: 0.026103105
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.8182978e-06
Norm of the params: 9.031469
              Random: fixed  13 labels. Loss 0.02610. Accuracy 0.995.
### Flips: 615, rs: 23, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011447
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5766583e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1721897e-08
Norm of the params: 6.0928183
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04924842
Train loss (w/o reg) on all data: 0.045326725
Test loss (w/o reg) on all data: 0.023450246
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5612536e-06
Norm of the params: 8.856292
              Random: fixed  17 labels. Loss 0.02345. Accuracy 0.997.
### Flips: 615, rs: 23, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1045107e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8319513e-08
Norm of the params: 6.0928164
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04884955
Train loss (w/o reg) on all data: 0.044950064
Test loss (w/o reg) on all data: 0.023193594
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.8520022e-06
Norm of the params: 8.831176
              Random: fixed  18 labels. Loss 0.02319. Accuracy 0.997.
### Flips: 615, rs: 23, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5751683e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0937333e-08
Norm of the params: 6.0928135
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046699062
Train loss (w/o reg) on all data: 0.042808432
Test loss (w/o reg) on all data: 0.022091025
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.916851e-06
Norm of the params: 8.821144
              Random: fixed  22 labels. Loss 0.02209. Accuracy 0.999.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051354103
Train loss (w/o reg) on all data: 0.047223203
Test loss (w/o reg) on all data: 0.028970988
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.5082352e-06
Norm of the params: 9.089444
Flipped loss: 0.02897. Accuracy: 0.996
### Flips: 615, rs: 24, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012634
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2426006e-08
Norm of the params: 6.092805
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037569383
Train loss (w/o reg) on all data: 0.0014781053
Test loss (w/o reg) on all data: 0.0033888444
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.906967e-08
Norm of the params: 6.751049
                Loss: fixed  75 labels. Loss 0.00339. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051076777
Train loss (w/o reg) on all data: 0.04691373
Test loss (w/o reg) on all data: 0.028768841
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.552373e-06
Norm of the params: 9.124741
              Random: fixed   1 labels. Loss 0.02877. Accuracy 0.996.
### Flips: 615, rs: 24, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.07626e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.111263e-09
Norm of the params: 6.0928183
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049671017
Train loss (w/o reg) on all data: 0.045538723
Test loss (w/o reg) on all data: 0.02717442
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.9144227e-06
Norm of the params: 9.090976
              Random: fixed   6 labels. Loss 0.02717. Accuracy 0.995.
### Flips: 615, rs: 24, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7673013e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.34903075e-08
Norm of the params: 6.0928154
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04808348
Train loss (w/o reg) on all data: 0.04390188
Test loss (w/o reg) on all data: 0.02631833
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7334022e-06
Norm of the params: 9.145056
              Random: fixed  10 labels. Loss 0.02632. Accuracy 0.995.
### Flips: 615, rs: 24, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7115653e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.192968e-09
Norm of the params: 6.0928125
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045020986
Train loss (w/o reg) on all data: 0.040799674
Test loss (w/o reg) on all data: 0.02554323
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.0554055e-06
Norm of the params: 9.188375
              Random: fixed  15 labels. Loss 0.02554. Accuracy 0.993.
### Flips: 615, rs: 24, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5752906e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7241291e-08
Norm of the params: 6.0928116
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04321688
Train loss (w/o reg) on all data: 0.039037656
Test loss (w/o reg) on all data: 0.023893487
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5593818e-06
Norm of the params: 9.142457
              Random: fixed  18 labels. Loss 0.02389. Accuracy 0.995.
### Flips: 615, rs: 24, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1977476e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.14156595e-08
Norm of the params: 6.0928183
                Loss: fixed  76 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041541945
Train loss (w/o reg) on all data: 0.037488665
Test loss (w/o reg) on all data: 0.023606893
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2600145e-06
Norm of the params: 9.003642
              Random: fixed  22 labels. Loss 0.02361. Accuracy 0.995.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045371763
Train loss (w/o reg) on all data: 0.04107176
Test loss (w/o reg) on all data: 0.025001233
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5154772e-06
Norm of the params: 9.273621
Flipped loss: 0.02500. Accuracy: 0.994
### Flips: 615, rs: 25, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0992215e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.278835e-09
Norm of the params: 6.092815
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044122938
Train loss (w/o reg) on all data: 0.03986251
Test loss (w/o reg) on all data: 0.023522357
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.8312724e-06
Norm of the params: 9.230848
              Random: fixed   3 labels. Loss 0.02352. Accuracy 0.995.
### Flips: 615, rs: 25, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2590184e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.034877e-09
Norm of the params: 6.092819
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043688446
Train loss (w/o reg) on all data: 0.039305776
Test loss (w/o reg) on all data: 0.022942884
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.31313e-06
Norm of the params: 9.362338
              Random: fixed   4 labels. Loss 0.02294. Accuracy 0.995.
### Flips: 615, rs: 25, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601078
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8080211e-08
Norm of the params: 6.092835
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3891413e-09
Norm of the params: 6.0928164
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04125966
Train loss (w/o reg) on all data: 0.03697799
Test loss (w/o reg) on all data: 0.022343213
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5352898e-06
Norm of the params: 9.253829
              Random: fixed   8 labels. Loss 0.02234. Accuracy 0.995.
### Flips: 615, rs: 25, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601397
Test loss (w/o reg) on all data: 0.0026561215
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.914584e-08
Norm of the params: 6.092783
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5772655e-08
Norm of the params: 6.092821
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039332896
Train loss (w/o reg) on all data: 0.035262797
Test loss (w/o reg) on all data: 0.020119214
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4874635e-06
Norm of the params: 9.022305
              Random: fixed  12 labels. Loss 0.02012. Accuracy 0.994.
### Flips: 615, rs: 25, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4194002e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3621816e-08
Norm of the params: 6.092809
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393329
Train loss (w/o reg) on all data: 0.03526124
Test loss (w/o reg) on all data: 0.020119177
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.585035e-06
Norm of the params: 9.0240345
              Random: fixed  12 labels. Loss 0.02012. Accuracy 0.994.
### Flips: 615, rs: 25, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4196057e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3622983e-08
Norm of the params: 6.092809
                Loss: fixed  65 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03894399
Train loss (w/o reg) on all data: 0.034965415
Test loss (w/o reg) on all data: 0.020068228
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.69186e-06
Norm of the params: 8.920285
              Random: fixed  14 labels. Loss 0.02007. Accuracy 0.995.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051306017
Train loss (w/o reg) on all data: 0.0468228
Test loss (w/o reg) on all data: 0.032594807
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.2647955e-06
Norm of the params: 9.469124
Flipped loss: 0.03259. Accuracy: 0.991
### Flips: 615, rs: 26, checks: 205
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042025056
Train loss (w/o reg) on all data: 0.002058103
Test loss (w/o reg) on all data: 0.0034520924
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2106944e-08
Norm of the params: 6.5488977
     Influence (LOO): fixed  80 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048799645
Train loss (w/o reg) on all data: 0.0021595394
Test loss (w/o reg) on all data: 0.0053106416
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.1393656e-08
Norm of the params: 7.3762116
                Loss: fixed  77 labels. Loss 0.00531. Accuracy 0.999.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049615487
Train loss (w/o reg) on all data: 0.045038268
Test loss (w/o reg) on all data: 0.031748958
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.160903e-06
Norm of the params: 9.567884
              Random: fixed   4 labels. Loss 0.03175. Accuracy 0.991.
### Flips: 615, rs: 26, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.466026e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1813737e-08
Norm of the params: 6.092824
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047704663
Train loss (w/o reg) on all data: 0.043160584
Test loss (w/o reg) on all data: 0.02839919
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8220959e-06
Norm of the params: 9.533184
              Random: fixed   9 labels. Loss 0.02840. Accuracy 0.994.
### Flips: 615, rs: 26, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010225
Test loss (w/o reg) on all data: 0.00265605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9797219e-08
Norm of the params: 6.0928435
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012524
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1918975e-08
Norm of the params: 6.092806
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045412686
Train loss (w/o reg) on all data: 0.041053224
Test loss (w/o reg) on all data: 0.024781449
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4543087e-06
Norm of the params: 9.337516
              Random: fixed  18 labels. Loss 0.02478. Accuracy 0.997.
### Flips: 615, rs: 26, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1913856e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601113
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8208429e-08
Norm of the params: 6.0928297
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04102822
Train loss (w/o reg) on all data: 0.036367517
Test loss (w/o reg) on all data: 0.023275243
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.267537e-06
Norm of the params: 9.654743
              Random: fixed  25 labels. Loss 0.02328. Accuracy 0.996.
### Flips: 615, rs: 26, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1274826e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6665997e-08
Norm of the params: 6.092821
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039388947
Train loss (w/o reg) on all data: 0.034718536
Test loss (w/o reg) on all data: 0.022396455
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.436971e-06
Norm of the params: 9.664794
              Random: fixed  28 labels. Loss 0.02240. Accuracy 0.996.
### Flips: 615, rs: 26, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2690791e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4315688e-08
Norm of the params: 6.092814
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035548728
Train loss (w/o reg) on all data: 0.03096735
Test loss (w/o reg) on all data: 0.022139538
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.289695e-06
Norm of the params: 9.572229
              Random: fixed  33 labels. Loss 0.02214. Accuracy 0.996.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053669274
Train loss (w/o reg) on all data: 0.049253725
Test loss (w/o reg) on all data: 0.032635912
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0088661e-05
Norm of the params: 9.397393
Flipped loss: 0.03264. Accuracy: 0.995
### Flips: 615, rs: 27, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.499782e-09
Norm of the params: 6.0928187
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038183415
Train loss (w/o reg) on all data: 0.0015469695
Test loss (w/o reg) on all data: 0.003532439
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9647453e-08
Norm of the params: 6.739988
                Loss: fixed  83 labels. Loss 0.00353. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053014487
Train loss (w/o reg) on all data: 0.04866345
Test loss (w/o reg) on all data: 0.031970654
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3775758e-06
Norm of the params: 9.328494
              Random: fixed   2 labels. Loss 0.03197. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3527193e-08
Norm of the params: 6.092831
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.070631e-09
Norm of the params: 6.0928144
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051597077
Train loss (w/o reg) on all data: 0.047380626
Test loss (w/o reg) on all data: 0.02925306
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.0644257e-06
Norm of the params: 9.183083
              Random: fixed   9 labels. Loss 0.02925. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4538304e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2935826e-09
Norm of the params: 6.0928154
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051622942
Train loss (w/o reg) on all data: 0.047396977
Test loss (w/o reg) on all data: 0.028944932
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.400127e-06
Norm of the params: 9.1934395
              Random: fixed  10 labels. Loss 0.02894. Accuracy 0.997.
### Flips: 615, rs: 27, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.057175e-09
Norm of the params: 6.0928144
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.555769e-09
Norm of the params: 6.092819
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049758613
Train loss (w/o reg) on all data: 0.04526744
Test loss (w/o reg) on all data: 0.027590018
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9642848e-06
Norm of the params: 9.477525
              Random: fixed  14 labels. Loss 0.02759. Accuracy 0.997.
### Flips: 615, rs: 27, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601056
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8499515e-08
Norm of the params: 6.092839
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.979851e-09
Norm of the params: 6.0928197
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047198866
Train loss (w/o reg) on all data: 0.042702008
Test loss (w/o reg) on all data: 0.0261992
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.8351913e-06
Norm of the params: 9.48352
              Random: fixed  20 labels. Loss 0.02620. Accuracy 0.997.
### Flips: 615, rs: 27, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0305933e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2036872e-08
Norm of the params: 6.092822
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045685563
Train loss (w/o reg) on all data: 0.041204035
Test loss (w/o reg) on all data: 0.02484906
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.210325e-06
Norm of the params: 9.46734
              Random: fixed  23 labels. Loss 0.02485. Accuracy 0.999.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051698852
Train loss (w/o reg) on all data: 0.04782724
Test loss (w/o reg) on all data: 0.026962014
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.1244182e-06
Norm of the params: 8.799561
Flipped loss: 0.02696. Accuracy: 0.995
### Flips: 615, rs: 28, checks: 205
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035215402
Train loss (w/o reg) on all data: 0.0013973558
Test loss (w/o reg) on all data: 0.003233242
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.177291e-08
Norm of the params: 6.5179505
     Influence (LOO): fixed  77 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003242352
Train loss (w/o reg) on all data: 0.0011854039
Test loss (w/o reg) on all data: 0.0027704763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3015524e-08
Norm of the params: 6.413966
                Loss: fixed  77 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050546747
Train loss (w/o reg) on all data: 0.046544675
Test loss (w/o reg) on all data: 0.026313681
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3092864e-06
Norm of the params: 8.946589
              Random: fixed   1 labels. Loss 0.02631. Accuracy 0.994.
### Flips: 615, rs: 28, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010964
Test loss (w/o reg) on all data: 0.0026560398
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7305742e-08
Norm of the params: 6.092831
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012745
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5491598e-08
Norm of the params: 6.092803
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047158718
Train loss (w/o reg) on all data: 0.042853326
Test loss (w/o reg) on all data: 0.02542837
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.590899e-06
Norm of the params: 9.279432
              Random: fixed   5 labels. Loss 0.02543. Accuracy 0.994.
### Flips: 615, rs: 28, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8360568e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2083904e-08
Norm of the params: 6.0928235
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044085555
Train loss (w/o reg) on all data: 0.039700758
Test loss (w/o reg) on all data: 0.024021322
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.739767e-06
Norm of the params: 9.36461
              Random: fixed  12 labels. Loss 0.02402. Accuracy 0.995.
### Flips: 615, rs: 28, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601266
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2359428e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0687297e-08
Norm of the params: 6.092828
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04334289
Train loss (w/o reg) on all data: 0.0389199
Test loss (w/o reg) on all data: 0.024097124
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.623006e-06
Norm of the params: 9.405306
              Random: fixed  13 labels. Loss 0.02410. Accuracy 0.995.
### Flips: 615, rs: 28, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9009692e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.843342e-08
Norm of the params: 6.092808
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041777696
Train loss (w/o reg) on all data: 0.037364464
Test loss (w/o reg) on all data: 0.023197245
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.093259e-06
Norm of the params: 9.394928
              Random: fixed  16 labels. Loss 0.02320. Accuracy 0.995.
### Flips: 615, rs: 28, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.495737e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6324935e-08
Norm of the params: 6.0928144
                Loss: fixed  78 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03989813
Train loss (w/o reg) on all data: 0.035350084
Test loss (w/o reg) on all data: 0.023525244
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.5224203e-06
Norm of the params: 9.537345
              Random: fixed  20 labels. Loss 0.02353. Accuracy 0.994.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057609588
Train loss (w/o reg) on all data: 0.053667802
Test loss (w/o reg) on all data: 0.031327445
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2186878e-06
Norm of the params: 8.878946
Flipped loss: 0.03133. Accuracy: 0.996
### Flips: 615, rs: 29, checks: 205
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039729555
Train loss (w/o reg) on all data: 0.001747731
Test loss (w/o reg) on all data: 0.0031462377
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4622361e-08
Norm of the params: 6.6711683
     Influence (LOO): fixed  86 labels. Loss 0.00315. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.019431e-09
Norm of the params: 6.092815
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05760958
Train loss (w/o reg) on all data: 0.05366693
Test loss (w/o reg) on all data: 0.031323172
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0026773e-05
Norm of the params: 8.879922
              Random: fixed   0 labels. Loss 0.03132. Accuracy 0.996.
### Flips: 615, rs: 29, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0821019e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3392967e-09
Norm of the params: 6.092816
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05386199
Train loss (w/o reg) on all data: 0.049715158
Test loss (w/o reg) on all data: 0.028404757
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2836514e-06
Norm of the params: 9.1069565
              Random: fixed   9 labels. Loss 0.02840. Accuracy 0.997.
### Flips: 615, rs: 29, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5611938e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0520523e-08
Norm of the params: 6.0928173
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05374269
Train loss (w/o reg) on all data: 0.049708333
Test loss (w/o reg) on all data: 0.02724693
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.2772238e-06
Norm of the params: 8.9826
              Random: fixed  11 labels. Loss 0.02725. Accuracy 0.997.
### Flips: 615, rs: 29, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096015766
Test loss (w/o reg) on all data: 0.002656115
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.958158e-08
Norm of the params: 6.0927525
     Influence (LOO): fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012786
Test loss (w/o reg) on all data: 0.0026561003
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1065646e-08
Norm of the params: 6.0928016
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051962774
Train loss (w/o reg) on all data: 0.047941975
Test loss (w/o reg) on all data: 0.026027743
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.5459685e-06
Norm of the params: 8.967494
              Random: fixed  15 labels. Loss 0.02603. Accuracy 0.998.
### Flips: 615, rs: 29, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601346
Test loss (w/o reg) on all data: 0.002656099
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0791657e-08
Norm of the params: 6.092791
     Influence (LOO): fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.997045e-08
Norm of the params: 6.09282
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050051242
Train loss (w/o reg) on all data: 0.045872465
Test loss (w/o reg) on all data: 0.025351882
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.293224e-06
Norm of the params: 9.141967
              Random: fixed  18 labels. Loss 0.02535. Accuracy 0.998.
### Flips: 615, rs: 29, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.79494e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.23896875e-08
Norm of the params: 6.092814
                Loss: fixed  87 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048970003
Train loss (w/o reg) on all data: 0.044740725
Test loss (w/o reg) on all data: 0.024215044
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.279593e-06
Norm of the params: 9.197041
              Random: fixed  21 labels. Loss 0.02422. Accuracy 0.999.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053141374
Train loss (w/o reg) on all data: 0.04863906
Test loss (w/o reg) on all data: 0.033247426
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2573121e-05
Norm of the params: 9.489274
Flipped loss: 0.03325. Accuracy: 0.991
### Flips: 615, rs: 30, checks: 205
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053202603
Train loss (w/o reg) on all data: 0.0028358833
Test loss (w/o reg) on all data: 0.0035239495
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3787393e-07
Norm of the params: 7.048939
     Influence (LOO): fixed  82 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035324683
Train loss (w/o reg) on all data: 0.001394855
Test loss (w/o reg) on all data: 0.002908644
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8706207e-08
Norm of the params: 6.538522
                Loss: fixed  84 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05043586
Train loss (w/o reg) on all data: 0.04593926
Test loss (w/o reg) on all data: 0.03169215
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9488066e-06
Norm of the params: 9.483249
              Random: fixed   6 labels. Loss 0.03169. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1646317e-08
Norm of the params: 6.092817
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19460095e-08
Norm of the params: 6.0928183
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049308658
Train loss (w/o reg) on all data: 0.045065362
Test loss (w/o reg) on all data: 0.03026434
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.9469606e-06
Norm of the params: 9.212268
              Random: fixed  11 labels. Loss 0.03026. Accuracy 0.992.
### Flips: 615, rs: 30, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096002634
Test loss (w/o reg) on all data: 0.002655923
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7649953e-07
Norm of the params: 6.092969
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5686485e-08
Norm of the params: 6.092819
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04609591
Train loss (w/o reg) on all data: 0.041797955
Test loss (w/o reg) on all data: 0.027395112
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.5722503e-06
Norm of the params: 9.271417
              Random: fixed  17 labels. Loss 0.02740. Accuracy 0.992.
### Flips: 615, rs: 30, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7966316e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4294296e-08
Norm of the params: 6.092813
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045179818
Train loss (w/o reg) on all data: 0.040943936
Test loss (w/o reg) on all data: 0.026808327
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5656595e-06
Norm of the params: 9.204219
              Random: fixed  19 labels. Loss 0.02681. Accuracy 0.990.
### Flips: 615, rs: 30, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2215984e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560463
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8960375e-08
Norm of the params: 6.0928197
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04403346
Train loss (w/o reg) on all data: 0.03982198
Test loss (w/o reg) on all data: 0.025850873
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.690238e-05
Norm of the params: 9.177671
              Random: fixed  22 labels. Loss 0.02585. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5818136e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1543276e-08
Norm of the params: 6.092814
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041113887
Train loss (w/o reg) on all data: 0.037260547
Test loss (w/o reg) on all data: 0.0226846
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8762547e-06
Norm of the params: 8.7787695
              Random: fixed  29 labels. Loss 0.02268. Accuracy 0.994.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05755986
Train loss (w/o reg) on all data: 0.053943235
Test loss (w/o reg) on all data: 0.029339513
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4541965e-06
Norm of the params: 8.504849
Flipped loss: 0.02934. Accuracy: 0.996
### Flips: 615, rs: 31, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5195726e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2097603e-08
Norm of the params: 6.092808
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055387955
Train loss (w/o reg) on all data: 0.051728226
Test loss (w/o reg) on all data: 0.028461538
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4759817e-06
Norm of the params: 8.555384
              Random: fixed   5 labels. Loss 0.02846. Accuracy 0.997.
### Flips: 615, rs: 31, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601392
Test loss (w/o reg) on all data: 0.0026560922
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.154749e-08
Norm of the params: 6.0927844
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7066997e-08
Norm of the params: 6.092824
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05499594
Train loss (w/o reg) on all data: 0.051440183
Test loss (w/o reg) on all data: 0.027438598
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6670816e-06
Norm of the params: 8.432977
              Random: fixed   6 labels. Loss 0.02744. Accuracy 0.998.
### Flips: 615, rs: 31, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3305839e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1645718e-08
Norm of the params: 6.092818
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05357741
Train loss (w/o reg) on all data: 0.049925867
Test loss (w/o reg) on all data: 0.02587615
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6373782e-06
Norm of the params: 8.545806
              Random: fixed   9 labels. Loss 0.02588. Accuracy 0.998.
### Flips: 615, rs: 31, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601471
Test loss (w/o reg) on all data: 0.002656091
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.179917e-08
Norm of the params: 6.09277
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4427878e-08
Norm of the params: 6.0928216
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05187647
Train loss (w/o reg) on all data: 0.048195083
Test loss (w/o reg) on all data: 0.02475735
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0701386e-05
Norm of the params: 8.580663
              Random: fixed  12 labels. Loss 0.02476. Accuracy 0.998.
### Flips: 615, rs: 31, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601327
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8113935e-08
Norm of the params: 6.0927944
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2364757e-08
Norm of the params: 6.092829
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050150875
Train loss (w/o reg) on all data: 0.046524435
Test loss (w/o reg) on all data: 0.024273112
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.0801833e-06
Norm of the params: 8.516383
              Random: fixed  15 labels. Loss 0.02427. Accuracy 0.998.
### Flips: 615, rs: 31, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960119
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8105432e-08
Norm of the params: 6.092817
     Influence (LOO): fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.0298905e-09
Norm of the params: 6.0928216
                Loss: fixed  85 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047447942
Train loss (w/o reg) on all data: 0.043862585
Test loss (w/o reg) on all data: 0.022595719
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6047921e-06
Norm of the params: 8.468006
              Random: fixed  21 labels. Loss 0.02260. Accuracy 0.998.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052749664
Train loss (w/o reg) on all data: 0.04887831
Test loss (w/o reg) on all data: 0.03744199
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5362749e-06
Norm of the params: 8.799268
Flipped loss: 0.03744. Accuracy: 0.986
### Flips: 615, rs: 32, checks: 205
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004742028
Train loss (w/o reg) on all data: 0.0022246188
Test loss (w/o reg) on all data: 0.0056076
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3887374e-07
Norm of the params: 7.0956454
     Influence (LOO): fixed  81 labels. Loss 0.00561. Accuracy 0.998.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.133336e-09
Norm of the params: 6.092817
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05080978
Train loss (w/o reg) on all data: 0.046854634
Test loss (w/o reg) on all data: 0.037423477
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.683538e-06
Norm of the params: 8.893979
              Random: fixed   5 labels. Loss 0.03742. Accuracy 0.986.
### Flips: 615, rs: 32, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0588395e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601092
Test loss (w/o reg) on all data: 0.0026560389
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2647999e-08
Norm of the params: 6.092832
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048270423
Train loss (w/o reg) on all data: 0.044325393
Test loss (w/o reg) on all data: 0.03733691
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.7784365e-06
Norm of the params: 8.882601
              Random: fixed   9 labels. Loss 0.03734. Accuracy 0.988.
### Flips: 615, rs: 32, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601115
Test loss (w/o reg) on all data: 0.0026560419
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4161318e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601275
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4813307e-08
Norm of the params: 6.092803
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04586349
Train loss (w/o reg) on all data: 0.04207841
Test loss (w/o reg) on all data: 0.036495537
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.017513e-06
Norm of the params: 8.700669
              Random: fixed  14 labels. Loss 0.03650. Accuracy 0.987.
### Flips: 615, rs: 32, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2915756e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17636505e-08
Norm of the params: 6.09281
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04586349
Train loss (w/o reg) on all data: 0.04207754
Test loss (w/o reg) on all data: 0.03649411
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1319315e-06
Norm of the params: 8.701666
              Random: fixed  14 labels. Loss 0.03649. Accuracy 0.987.
### Flips: 615, rs: 32, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2917918e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.178662e-08
Norm of the params: 6.0928106
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043730855
Train loss (w/o reg) on all data: 0.040000826
Test loss (w/o reg) on all data: 0.03499292
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.5081016e-06
Norm of the params: 8.637161
              Random: fixed  18 labels. Loss 0.03499. Accuracy 0.987.
### Flips: 615, rs: 32, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162408
Train loss (w/o reg) on all data: 0.00096013857
Test loss (w/o reg) on all data: 0.002656094
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1977035e-08
Norm of the params: 6.0927863
     Influence (LOO): fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601157
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2902899e-08
Norm of the params: 6.092822
                Loss: fixed  84 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040283192
Train loss (w/o reg) on all data: 0.03625628
Test loss (w/o reg) on all data: 0.03408374
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4206556e-06
Norm of the params: 8.974311
              Random: fixed  23 labels. Loss 0.03408. Accuracy 0.987.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051809438
Train loss (w/o reg) on all data: 0.0476495
Test loss (w/o reg) on all data: 0.037469927
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.9126257e-06
Norm of the params: 9.121336
Flipped loss: 0.03747. Accuracy: 0.991
### Flips: 615, rs: 33, checks: 205
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003162044
Train loss (w/o reg) on all data: 0.0011657956
Test loss (w/o reg) on all data: 0.0042076535
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.756611e-08
Norm of the params: 6.3186207
     Influence (LOO): fixed  79 labels. Loss 0.00421. Accuracy 0.999.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601092
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8681803e-08
Norm of the params: 6.0928316
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050253004
Train loss (w/o reg) on all data: 0.046020735
Test loss (w/o reg) on all data: 0.038929082
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.450272e-06
Norm of the params: 9.200293
              Random: fixed   4 labels. Loss 0.03893. Accuracy 0.988.
### Flips: 615, rs: 33, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011127
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.321712e-08
Norm of the params: 6.092829
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.891002e-09
Norm of the params: 6.0928183
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04754021
Train loss (w/o reg) on all data: 0.04318071
Test loss (w/o reg) on all data: 0.034250017
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.057906e-06
Norm of the params: 9.337559
              Random: fixed  11 labels. Loss 0.03425. Accuracy 0.992.
### Flips: 615, rs: 33, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.112326e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3979466e-09
Norm of the params: 6.0928187
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04479965
Train loss (w/o reg) on all data: 0.040434014
Test loss (w/o reg) on all data: 0.034603212
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3250829e-05
Norm of the params: 9.344128
              Random: fixed  16 labels. Loss 0.03460. Accuracy 0.990.
### Flips: 615, rs: 33, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560777
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.668461e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.101974e-09
Norm of the params: 6.092819
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043401763
Train loss (w/o reg) on all data: 0.039180223
Test loss (w/o reg) on all data: 0.031135878
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.903977e-06
Norm of the params: 9.188622
              Random: fixed  20 labels. Loss 0.03114. Accuracy 0.993.
### Flips: 615, rs: 33, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601052
Test loss (w/o reg) on all data: 0.0026560496
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0653965e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.744277e-09
Norm of the params: 6.092816
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03816628
Train loss (w/o reg) on all data: 0.0339151
Test loss (w/o reg) on all data: 0.026975218
Train acc on all data:  0.9885728178944809
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.459299e-06
Norm of the params: 9.220826
              Random: fixed  29 labels. Loss 0.02698. Accuracy 0.995.
### Flips: 615, rs: 33, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7023472e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.968848e-08
Norm of the params: 6.09282
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0355412
Train loss (w/o reg) on all data: 0.031213295
Test loss (w/o reg) on all data: 0.025171582
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.792346e-06
Norm of the params: 9.303661
              Random: fixed  34 labels. Loss 0.02517. Accuracy 0.994.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052192234
Train loss (w/o reg) on all data: 0.047666244
Test loss (w/o reg) on all data: 0.032308765
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.8680569e-06
Norm of the params: 9.51419
Flipped loss: 0.03231. Accuracy: 0.991
### Flips: 615, rs: 34, checks: 205
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012687
Test loss (w/o reg) on all data: 0.0026560845
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6654756e-08
Norm of the params: 6.092803
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1221342e-08
Norm of the params: 6.0928216
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05100726
Train loss (w/o reg) on all data: 0.046310257
Test loss (w/o reg) on all data: 0.03235504
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.0932255e-06
Norm of the params: 9.692267
              Random: fixed   2 labels. Loss 0.03236. Accuracy 0.991.
### Flips: 615, rs: 34, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1799035e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.902303e-09
Norm of the params: 6.0928154
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04885299
Train loss (w/o reg) on all data: 0.0442726
Test loss (w/o reg) on all data: 0.03096382
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.1634855e-06
Norm of the params: 9.571196
              Random: fixed   7 labels. Loss 0.03096. Accuracy 0.991.
### Flips: 615, rs: 34, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009375
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2206768e-08
Norm of the params: 6.092859
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2360396e-08
Norm of the params: 6.0928135
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047962245
Train loss (w/o reg) on all data: 0.04339998
Test loss (w/o reg) on all data: 0.030294573
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.823282e-06
Norm of the params: 9.552241
              Random: fixed   9 labels. Loss 0.03029. Accuracy 0.991.
### Flips: 615, rs: 34, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8784795e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2005964e-08
Norm of the params: 6.0928154
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04600714
Train loss (w/o reg) on all data: 0.041456897
Test loss (w/o reg) on all data: 0.027958872
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.3151679e-06
Norm of the params: 9.53965
              Random: fixed  13 labels. Loss 0.02796. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.259819e-08
Norm of the params: 6.092821
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4189308e-08
Norm of the params: 6.0928082
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044913106
Train loss (w/o reg) on all data: 0.040323034
Test loss (w/o reg) on all data: 0.027720442
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.4154501e-06
Norm of the params: 9.581306
              Random: fixed  15 labels. Loss 0.02772. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6687144e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3475416e-08
Norm of the params: 6.0928164
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04166327
Train loss (w/o reg) on all data: 0.0369779
Test loss (w/o reg) on all data: 0.026884478
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2764548e-06
Norm of the params: 9.680261
              Random: fixed  21 labels. Loss 0.02688. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052471586
Train loss (w/o reg) on all data: 0.048646
Test loss (w/o reg) on all data: 0.025345962
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.150902e-06
Norm of the params: 8.747099
Flipped loss: 0.02535. Accuracy: 0.997
### Flips: 615, rs: 35, checks: 205
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.334249e-08
Norm of the params: 6.092822
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0250387e-08
Norm of the params: 6.09281
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05082967
Train loss (w/o reg) on all data: 0.047030594
Test loss (w/o reg) on all data: 0.023980208
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.295969e-06
Norm of the params: 8.716738
              Random: fixed   4 labels. Loss 0.02398. Accuracy 0.997.
### Flips: 615, rs: 35, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1347633e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3766305e-08
Norm of the params: 6.092819
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048544485
Train loss (w/o reg) on all data: 0.0447599
Test loss (w/o reg) on all data: 0.023571938
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.294093e-06
Norm of the params: 8.700096
              Random: fixed   8 labels. Loss 0.02357. Accuracy 0.997.
### Flips: 615, rs: 35, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.838274e-09
Norm of the params: 6.092813
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1891847e-08
Norm of the params: 6.09282
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04474954
Train loss (w/o reg) on all data: 0.0408272
Test loss (w/o reg) on all data: 0.021742757
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.461751e-06
Norm of the params: 8.8570175
              Random: fixed  14 labels. Loss 0.02174. Accuracy 0.996.
### Flips: 615, rs: 35, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601256
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4355444e-08
Norm of the params: 6.092806
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02441895e-08
Norm of the params: 6.092821
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04227167
Train loss (w/o reg) on all data: 0.038429394
Test loss (w/o reg) on all data: 0.021207638
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0153468e-06
Norm of the params: 8.766157
              Random: fixed  19 labels. Loss 0.02121. Accuracy 0.996.
### Flips: 615, rs: 35, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010056
Test loss (w/o reg) on all data: 0.0026560298
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2212363e-08
Norm of the params: 6.0928473
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0114856e-08
Norm of the params: 6.0928173
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040489964
Train loss (w/o reg) on all data: 0.03661488
Test loss (w/o reg) on all data: 0.020700337
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8377192e-06
Norm of the params: 8.803503
              Random: fixed  22 labels. Loss 0.02070. Accuracy 0.996.
### Flips: 615, rs: 35, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1827764e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0314031e-08
Norm of the params: 6.0928197
                Loss: fixed  75 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039999753
Train loss (w/o reg) on all data: 0.03617174
Test loss (w/o reg) on all data: 0.020763054
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.3066046e-06
Norm of the params: 8.749872
              Random: fixed  23 labels. Loss 0.02076. Accuracy 0.995.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056980316
Train loss (w/o reg) on all data: 0.05315661
Test loss (w/o reg) on all data: 0.028991811
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.800997e-06
Norm of the params: 8.744947
Flipped loss: 0.02899. Accuracy: 0.997
### Flips: 615, rs: 36, checks: 205
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601278
Test loss (w/o reg) on all data: 0.0026560826
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7757326e-08
Norm of the params: 6.092802
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.335974e-09
Norm of the params: 6.0928187
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055182107
Train loss (w/o reg) on all data: 0.051396493
Test loss (w/o reg) on all data: 0.028362129
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0401266e-05
Norm of the params: 8.70128
              Random: fixed   4 labels. Loss 0.02836. Accuracy 0.996.
### Flips: 615, rs: 36, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.97826e-08
Norm of the params: 6.092812
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601239
Test loss (w/o reg) on all data: 0.0026560829
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6973665e-08
Norm of the params: 6.0928082
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055089917
Train loss (w/o reg) on all data: 0.05132583
Test loss (w/o reg) on all data: 0.02828915
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3986358e-06
Norm of the params: 8.676501
              Random: fixed   5 labels. Loss 0.02829. Accuracy 0.996.
### Flips: 615, rs: 36, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.115232e-09
Norm of the params: 6.092819
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.950952e-09
Norm of the params: 6.0928154
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052257713
Train loss (w/o reg) on all data: 0.048534084
Test loss (w/o reg) on all data: 0.026046628
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.071143e-06
Norm of the params: 8.629751
              Random: fixed  12 labels. Loss 0.02605. Accuracy 0.998.
### Flips: 615, rs: 36, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6559351e-08
Norm of the params: 6.092826
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.112826e-09
Norm of the params: 6.0928154
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052148946
Train loss (w/o reg) on all data: 0.048385065
Test loss (w/o reg) on all data: 0.025719238
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6019403e-06
Norm of the params: 8.676268
              Random: fixed  13 labels. Loss 0.02572. Accuracy 0.998.
### Flips: 615, rs: 36, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5652718e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.059738e-09
Norm of the params: 6.0928144
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049267355
Train loss (w/o reg) on all data: 0.045346513
Test loss (w/o reg) on all data: 0.024887275
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.367981e-06
Norm of the params: 8.85533
              Random: fixed  19 labels. Loss 0.02489. Accuracy 0.998.
### Flips: 615, rs: 36, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4178865e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.2356335e-09
Norm of the params: 6.092819
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045878857
Train loss (w/o reg) on all data: 0.04188229
Test loss (w/o reg) on all data: 0.023667112
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.2485832e-06
Norm of the params: 8.940431
              Random: fixed  25 labels. Loss 0.02367. Accuracy 0.995.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057718076
Train loss (w/o reg) on all data: 0.05362045
Test loss (w/o reg) on all data: 0.03189007
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3776038e-06
Norm of the params: 9.052764
Flipped loss: 0.03189. Accuracy: 0.996
### Flips: 615, rs: 37, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051090494
Train loss (w/o reg) on all data: 0.0028375988
Test loss (w/o reg) on all data: 0.0036118387
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0146562e-07
Norm of the params: 6.740105
     Influence (LOO): fixed  86 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601321
Test loss (w/o reg) on all data: 0.0026560996
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2866314e-08
Norm of the params: 6.092795
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057157103
Train loss (w/o reg) on all data: 0.053049687
Test loss (w/o reg) on all data: 0.031894363
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.666201e-06
Norm of the params: 9.063571
              Random: fixed   1 labels. Loss 0.03189. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4942428e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.787746e-09
Norm of the params: 6.092818
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054950297
Train loss (w/o reg) on all data: 0.050928432
Test loss (w/o reg) on all data: 0.029150752
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.266936e-06
Norm of the params: 8.968683
              Random: fixed   6 labels. Loss 0.02915. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560936
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2378805e-08
Norm of the params: 6.092808
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7004874e-08
Norm of the params: 6.092829
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054107253
Train loss (w/o reg) on all data: 0.05014486
Test loss (w/o reg) on all data: 0.027544536
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.151693e-06
Norm of the params: 8.902127
              Random: fixed  10 labels. Loss 0.02754. Accuracy 0.997.
### Flips: 615, rs: 37, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5679618e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4476577e-08
Norm of the params: 6.092823
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053186644
Train loss (w/o reg) on all data: 0.049205177
Test loss (w/o reg) on all data: 0.026809102
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.092505e-06
Norm of the params: 8.92353
              Random: fixed  12 labels. Loss 0.02681. Accuracy 0.997.
### Flips: 615, rs: 37, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601317
Test loss (w/o reg) on all data: 0.0026560954
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3710077e-08
Norm of the params: 6.092797
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0696348e-08
Norm of the params: 6.0928245
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048875883
Train loss (w/o reg) on all data: 0.044877477
Test loss (w/o reg) on all data: 0.02307577
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8844464e-06
Norm of the params: 8.942491
              Random: fixed  22 labels. Loss 0.02308. Accuracy 0.998.
### Flips: 615, rs: 37, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601078
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8837478e-08
Norm of the params: 6.092835
     Influence (LOO): fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0206268e-08
Norm of the params: 6.0928164
                Loss: fixed  88 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04661113
Train loss (w/o reg) on all data: 0.04278212
Test loss (w/o reg) on all data: 0.021776997
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0933522e-06
Norm of the params: 8.751013
              Random: fixed  26 labels. Loss 0.02178. Accuracy 0.998.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05222145
Train loss (w/o reg) on all data: 0.04810789
Test loss (w/o reg) on all data: 0.03740298
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.5474472e-06
Norm of the params: 9.070349
Flipped loss: 0.03740. Accuracy: 0.988
### Flips: 615, rs: 38, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053958762
Train loss (w/o reg) on all data: 0.0028174815
Test loss (w/o reg) on all data: 0.0056588827
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.044846e-07
Norm of the params: 7.1810794
     Influence (LOO): fixed  78 labels. Loss 0.00566. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031620436
Train loss (w/o reg) on all data: 0.0011657994
Test loss (w/o reg) on all data: 0.004208031
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6434557e-07
Norm of the params: 6.318614
                Loss: fixed  80 labels. Loss 0.00421. Accuracy 0.999.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050642375
Train loss (w/o reg) on all data: 0.046379168
Test loss (w/o reg) on all data: 0.036515582
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4171264e-05
Norm of the params: 9.233858
              Random: fixed   4 labels. Loss 0.03652. Accuracy 0.987.
### Flips: 615, rs: 38, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009474
Test loss (w/o reg) on all data: 0.0026560205
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9399287e-08
Norm of the params: 6.0928564
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.22403705e-08
Norm of the params: 6.0928183
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04887991
Train loss (w/o reg) on all data: 0.044550728
Test loss (w/o reg) on all data: 0.035695963
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0173884e-05
Norm of the params: 9.305036
              Random: fixed   8 labels. Loss 0.03570. Accuracy 0.988.
### Flips: 615, rs: 38, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601108
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2821267e-08
Norm of the params: 6.09283
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5599298e-08
Norm of the params: 6.092809
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047167458
Train loss (w/o reg) on all data: 0.042781085
Test loss (w/o reg) on all data: 0.034713205
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.3162177e-06
Norm of the params: 9.366296
              Random: fixed  11 labels. Loss 0.03471. Accuracy 0.989.
### Flips: 615, rs: 38, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009125
Test loss (w/o reg) on all data: 0.0026560198
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3716518e-08
Norm of the params: 6.092862
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3363638e-08
Norm of the params: 6.0928206
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04593625
Train loss (w/o reg) on all data: 0.041639723
Test loss (w/o reg) on all data: 0.031508725
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.2487724e-06
Norm of the params: 9.269871
              Random: fixed  14 labels. Loss 0.03151. Accuracy 0.989.
### Flips: 615, rs: 38, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012483
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7842229e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0583431e-08
Norm of the params: 6.092819
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044849854
Train loss (w/o reg) on all data: 0.040325526
Test loss (w/o reg) on all data: 0.029813038
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.491283e-06
Norm of the params: 9.512441
              Random: fixed  17 labels. Loss 0.02981. Accuracy 0.991.
### Flips: 615, rs: 38, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601288
Test loss (w/o reg) on all data: 0.0026560924
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.536955e-08
Norm of the params: 6.0928006
     Influence (LOO): fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3575459e-08
Norm of the params: 6.0928235
                Loss: fixed  81 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042374883
Train loss (w/o reg) on all data: 0.037917435
Test loss (w/o reg) on all data: 0.02792572
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.34096e-06
Norm of the params: 9.441872
              Random: fixed  22 labels. Loss 0.02793. Accuracy 0.991.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050858334
Train loss (w/o reg) on all data: 0.046693794
Test loss (w/o reg) on all data: 0.02900503
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.410836e-06
Norm of the params: 9.126377
Flipped loss: 0.02901. Accuracy: 0.992
### Flips: 615, rs: 39, checks: 205
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035215395
Train loss (w/o reg) on all data: 0.001397351
Test loss (w/o reg) on all data: 0.0032331806
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6866575e-08
Norm of the params: 6.517957
     Influence (LOO): fixed  79 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601273
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6345234e-08
Norm of the params: 6.092803
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04987764
Train loss (w/o reg) on all data: 0.045714702
Test loss (w/o reg) on all data: 0.028607324
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5014318e-05
Norm of the params: 9.124623
              Random: fixed   3 labels. Loss 0.02861. Accuracy 0.992.
### Flips: 615, rs: 39, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.208468e-09
Norm of the params: 6.092821
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.648284e-09
Norm of the params: 6.092813
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048599668
Train loss (w/o reg) on all data: 0.044320624
Test loss (w/o reg) on all data: 0.027916763
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.972892e-06
Norm of the params: 9.250992
              Random: fixed   6 labels. Loss 0.02792. Accuracy 0.992.
### Flips: 615, rs: 39, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.291787e-08
Norm of the params: 6.092809
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0164127e-08
Norm of the params: 6.092821
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046662346
Train loss (w/o reg) on all data: 0.042514294
Test loss (w/o reg) on all data: 0.024663847
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.2269196e-06
Norm of the params: 9.108296
              Random: fixed  12 labels. Loss 0.02466. Accuracy 0.993.
### Flips: 615, rs: 39, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009194
Test loss (w/o reg) on all data: 0.002656019
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.080137e-08
Norm of the params: 6.0928617
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8798282e-08
Norm of the params: 6.0928173
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044244114
Train loss (w/o reg) on all data: 0.040172655
Test loss (w/o reg) on all data: 0.023312574
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3412165e-06
Norm of the params: 9.023813
              Random: fixed  17 labels. Loss 0.02331. Accuracy 0.996.
### Flips: 615, rs: 39, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600935
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4317048e-08
Norm of the params: 6.092858
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.559033e-09
Norm of the params: 6.092823
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04323348
Train loss (w/o reg) on all data: 0.03909932
Test loss (w/o reg) on all data: 0.022914471
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.29789e-06
Norm of the params: 9.09303
              Random: fixed  20 labels. Loss 0.02291. Accuracy 0.996.
### Flips: 615, rs: 39, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600864
Test loss (w/o reg) on all data: 0.0026560277
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.369706e-08
Norm of the params: 6.0928693
     Influence (LOO): fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3825674e-08
Norm of the params: 6.0928164
                Loss: fixed  80 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042285122
Train loss (w/o reg) on all data: 0.038074706
Test loss (w/o reg) on all data: 0.021737995
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1741242e-06
Norm of the params: 9.176509
              Random: fixed  23 labels. Loss 0.02174. Accuracy 0.997.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06343083
Train loss (w/o reg) on all data: 0.05948753
Test loss (w/o reg) on all data: 0.04368261
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.0921517e-06
Norm of the params: 8.88066
Flipped loss: 0.04368. Accuracy: 0.988
### Flips: 820, rs: 0, checks: 205
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009400807
Train loss (w/o reg) on all data: 0.006099999
Test loss (w/o reg) on all data: 0.007810604
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.079793e-07
Norm of the params: 8.125033
     Influence (LOO): fixed  97 labels. Loss 0.00781. Accuracy 0.998.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0029973504
Train loss (w/o reg) on all data: 0.001064276
Test loss (w/o reg) on all data: 0.0036883184
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7192557e-08
Norm of the params: 6.2178373
                Loss: fixed 105 labels. Loss 0.00369. Accuracy 0.999.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06305892
Train loss (w/o reg) on all data: 0.059077993
Test loss (w/o reg) on all data: 0.04146824
Train acc on all data:  0.975443715049842
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2248438e-05
Norm of the params: 8.922921
              Random: fixed   1 labels. Loss 0.04147. Accuracy 0.989.
### Flips: 820, rs: 0, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011045
Test loss (w/o reg) on all data: 0.0026560465
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5057032e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0029973513
Train loss (w/o reg) on all data: 0.0010642759
Test loss (w/o reg) on all data: 0.003688331
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6249627e-08
Norm of the params: 6.2178383
                Loss: fixed 105 labels. Loss 0.00369. Accuracy 0.999.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06120705
Train loss (w/o reg) on all data: 0.05726579
Test loss (w/o reg) on all data: 0.038430866
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.412127e-06
Norm of the params: 8.878357
              Random: fixed   8 labels. Loss 0.03843. Accuracy 0.988.
### Flips: 820, rs: 0, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1969878e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011255
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7637744e-08
Norm of the params: 6.0928264
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058928557
Train loss (w/o reg) on all data: 0.055083502
Test loss (w/o reg) on all data: 0.03654596
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.142219e-06
Norm of the params: 8.769328
              Random: fixed  15 labels. Loss 0.03655. Accuracy 0.989.
### Flips: 820, rs: 0, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.565316e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5712011e-08
Norm of the params: 6.0928116
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056922186
Train loss (w/o reg) on all data: 0.053124476
Test loss (w/o reg) on all data: 0.033618566
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.122757e-06
Norm of the params: 8.715173
              Random: fixed  20 labels. Loss 0.03362. Accuracy 0.991.
### Flips: 820, rs: 0, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601118
Test loss (w/o reg) on all data: 0.0026560486
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5047992e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601222
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5047311e-08
Norm of the params: 6.0928116
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054735065
Train loss (w/o reg) on all data: 0.050871707
Test loss (w/o reg) on all data: 0.030563276
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8557661e-06
Norm of the params: 8.790172
              Random: fixed  28 labels. Loss 0.03056. Accuracy 0.994.
### Flips: 820, rs: 0, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960148
Test loss (w/o reg) on all data: 0.0026561006
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8699765e-08
Norm of the params: 6.09277
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011243
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9764306e-08
Norm of the params: 6.0928264
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053248946
Train loss (w/o reg) on all data: 0.049553998
Test loss (w/o reg) on all data: 0.028054273
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.983626e-06
Norm of the params: 8.596449
              Random: fixed  33 labels. Loss 0.02805. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06515009
Train loss (w/o reg) on all data: 0.06121925
Test loss (w/o reg) on all data: 0.04508586
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.7352216e-06
Norm of the params: 8.866614
Flipped loss: 0.04509. Accuracy: 0.985
### Flips: 820, rs: 1, checks: 205
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012801044
Train loss (w/o reg) on all data: 0.008783731
Test loss (w/o reg) on all data: 0.008854349
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4749021e-07
Norm of the params: 8.963608
     Influence (LOO): fixed 103 labels. Loss 0.00885. Accuracy 0.997.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00591264
Train loss (w/o reg) on all data: 0.0029982885
Test loss (w/o reg) on all data: 0.004455175
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 9.040367e-08
Norm of the params: 7.634595
                Loss: fixed 115 labels. Loss 0.00446. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064678654
Train loss (w/o reg) on all data: 0.0606872
Test loss (w/o reg) on all data: 0.043014936
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.572227e-06
Norm of the params: 8.934717
              Random: fixed   4 labels. Loss 0.04301. Accuracy 0.987.
### Flips: 820, rs: 1, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601107
Test loss (w/o reg) on all data: 0.0026560451
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.471402e-08
Norm of the params: 6.09283
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601319
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4198824e-08
Norm of the params: 6.092796
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06259854
Train loss (w/o reg) on all data: 0.05863374
Test loss (w/o reg) on all data: 0.040702976
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.047719e-06
Norm of the params: 8.904827
              Random: fixed  10 labels. Loss 0.04070. Accuracy 0.990.
### Flips: 820, rs: 1, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0306732e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0769113e-08
Norm of the params: 6.0928082
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06170327
Train loss (w/o reg) on all data: 0.057687216
Test loss (w/o reg) on all data: 0.040026393
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6801536e-06
Norm of the params: 8.962203
              Random: fixed  12 labels. Loss 0.04003. Accuracy 0.986.
### Flips: 820, rs: 1, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013217
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5315384e-08
Norm of the params: 6.0927954
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4648665e-08
Norm of the params: 6.092829
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05904126
Train loss (w/o reg) on all data: 0.05505103
Test loss (w/o reg) on all data: 0.038019814
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.306754e-06
Norm of the params: 8.933349
              Random: fixed  19 labels. Loss 0.03802. Accuracy 0.991.
### Flips: 820, rs: 1, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8350823e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.620157e-08
Norm of the params: 6.0928216
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056871038
Train loss (w/o reg) on all data: 0.052738667
Test loss (w/o reg) on all data: 0.03710937
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.470946e-06
Norm of the params: 9.091064
              Random: fixed  24 labels. Loss 0.03711. Accuracy 0.991.
### Flips: 820, rs: 1, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3740204e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1069187e-08
Norm of the params: 6.0928144
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055880565
Train loss (w/o reg) on all data: 0.05177586
Test loss (w/o reg) on all data: 0.03597459
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.816997e-06
Norm of the params: 9.060577
              Random: fixed  28 labels. Loss 0.03597. Accuracy 0.990.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061380222
Train loss (w/o reg) on all data: 0.057036974
Test loss (w/o reg) on all data: 0.041698497
Train acc on all data:  0.974957451981522
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.734943e-06
Norm of the params: 9.320139
Flipped loss: 0.04170. Accuracy: 0.989
### Flips: 820, rs: 2, checks: 205
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006857331
Train loss (w/o reg) on all data: 0.0040044943
Test loss (w/o reg) on all data: 0.00404043
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.556777e-07
Norm of the params: 7.5535913
     Influence (LOO): fixed  97 labels. Loss 0.00404. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004026239
Train loss (w/o reg) on all data: 0.0015972938
Test loss (w/o reg) on all data: 0.004849037
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8490893e-07
Norm of the params: 6.9698563
                Loss: fixed 101 labels. Loss 0.00485. Accuracy 0.998.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059821233
Train loss (w/o reg) on all data: 0.055371817
Test loss (w/o reg) on all data: 0.038754836
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.1743985e-06
Norm of the params: 9.433364
              Random: fixed   6 labels. Loss 0.03875. Accuracy 0.991.
### Flips: 820, rs: 2, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960149
Test loss (w/o reg) on all data: 0.0026561103
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5546575e-08
Norm of the params: 6.0927677
     Influence (LOO): fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7073295e-08
Norm of the params: 6.092825
                Loss: fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058876317
Train loss (w/o reg) on all data: 0.05437757
Test loss (w/o reg) on all data: 0.035992395
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.4600984e-06
Norm of the params: 9.485513
              Random: fixed  10 labels. Loss 0.03599. Accuracy 0.994.
### Flips: 820, rs: 2, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011074
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0878327e-08
Norm of the params: 6.09283
     Influence (LOO): fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.3380255e-09
Norm of the params: 6.092816
                Loss: fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055305347
Train loss (w/o reg) on all data: 0.050946444
Test loss (w/o reg) on all data: 0.030506287
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.022411e-06
Norm of the params: 9.336921
              Random: fixed  21 labels. Loss 0.03051. Accuracy 0.997.
### Flips: 820, rs: 2, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960165
Test loss (w/o reg) on all data: 0.0026561206
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.725402e-08
Norm of the params: 6.0927415
     Influence (LOO): fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010783
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6777994e-08
Norm of the params: 6.0928354
                Loss: fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05274186
Train loss (w/o reg) on all data: 0.048587773
Test loss (w/o reg) on all data: 0.028377002
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4188715e-06
Norm of the params: 9.114918
              Random: fixed  28 labels. Loss 0.02838. Accuracy 0.996.
### Flips: 820, rs: 2, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.0026560803
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8759753e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2328732e-08
Norm of the params: 6.092823
                Loss: fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051830806
Train loss (w/o reg) on all data: 0.04764614
Test loss (w/o reg) on all data: 0.026991796
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.4821971e-06
Norm of the params: 9.148407
              Random: fixed  31 labels. Loss 0.02699. Accuracy 0.998.
### Flips: 820, rs: 2, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5551397e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02006625e-08
Norm of the params: 6.0928183
                Loss: fixed 104 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050296172
Train loss (w/o reg) on all data: 0.046017352
Test loss (w/o reg) on all data: 0.025554603
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.544284e-06
Norm of the params: 9.250751
              Random: fixed  35 labels. Loss 0.02555. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06239492
Train loss (w/o reg) on all data: 0.057884425
Test loss (w/o reg) on all data: 0.047284707
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.834941e-06
Norm of the params: 9.4978895
Flipped loss: 0.04728. Accuracy: 0.983
### Flips: 820, rs: 3, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007588823
Train loss (w/o reg) on all data: 0.0045990874
Test loss (w/o reg) on all data: 0.009636986
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.72596e-07
Norm of the params: 7.732704
     Influence (LOO): fixed 108 labels. Loss 0.00964. Accuracy 0.995.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005848226
Train loss (w/o reg) on all data: 0.002556965
Test loss (w/o reg) on all data: 0.012212455
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.904401e-08
Norm of the params: 8.113275
                Loss: fixed 109 labels. Loss 0.01221. Accuracy 0.997.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06067937
Train loss (w/o reg) on all data: 0.056127448
Test loss (w/o reg) on all data: 0.046098825
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.8333824e-06
Norm of the params: 9.541406
              Random: fixed   5 labels. Loss 0.04610. Accuracy 0.983.
### Flips: 820, rs: 3, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600988
Test loss (w/o reg) on all data: 0.0026560293
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2136097e-08
Norm of the params: 6.0928497
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0029973513
Train loss (w/o reg) on all data: 0.0010642759
Test loss (w/o reg) on all data: 0.003688331
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6252319e-08
Norm of the params: 6.2178383
                Loss: fixed 114 labels. Loss 0.00369. Accuracy 0.999.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06020584
Train loss (w/o reg) on all data: 0.055673797
Test loss (w/o reg) on all data: 0.04597962
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.3797897e-06
Norm of the params: 9.520549
              Random: fixed   7 labels. Loss 0.04598. Accuracy 0.982.
### Flips: 820, rs: 3, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009986
Test loss (w/o reg) on all data: 0.0026560307
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4145827e-08
Norm of the params: 6.092849
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7021076e-08
Norm of the params: 6.092814
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059961993
Train loss (w/o reg) on all data: 0.055580616
Test loss (w/o reg) on all data: 0.042459726
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2297047e-05
Norm of the params: 9.360959
              Random: fixed  13 labels. Loss 0.04246. Accuracy 0.987.
### Flips: 820, rs: 3, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601302
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.786851e-08
Norm of the params: 6.092798
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7319694e-08
Norm of the params: 6.0928235
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05613221
Train loss (w/o reg) on all data: 0.05145121
Test loss (w/o reg) on all data: 0.0359732
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.5385556e-06
Norm of the params: 9.675741
              Random: fixed  24 labels. Loss 0.03597. Accuracy 0.991.
### Flips: 820, rs: 3, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2115659e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4248452e-08
Norm of the params: 6.0928245
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05443369
Train loss (w/o reg) on all data: 0.049639788
Test loss (w/o reg) on all data: 0.03286502
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.284062e-06
Norm of the params: 9.791734
              Random: fixed  32 labels. Loss 0.03287. Accuracy 0.994.
### Flips: 820, rs: 3, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601467
Test loss (w/o reg) on all data: 0.0026561301
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.066032e-07
Norm of the params: 6.0927715
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6214571e-08
Norm of the params: 6.0928183
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050272003
Train loss (w/o reg) on all data: 0.045519486
Test loss (w/o reg) on all data: 0.029233392
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.0037065e-06
Norm of the params: 9.749374
              Random: fixed  43 labels. Loss 0.02923. Accuracy 0.995.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06573478
Train loss (w/o reg) on all data: 0.06185887
Test loss (w/o reg) on all data: 0.038416486
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.3552017e-06
Norm of the params: 8.804444
Flipped loss: 0.03842. Accuracy: 0.989
### Flips: 820, rs: 4, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009902097
Train loss (w/o reg) on all data: 0.0067905635
Test loss (w/o reg) on all data: 0.0066053933
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3470508e-07
Norm of the params: 7.888642
     Influence (LOO): fixed 105 labels. Loss 0.00661. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037958277
Train loss (w/o reg) on all data: 0.0014488011
Test loss (w/o reg) on all data: 0.0032007934
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5997421e-08
Norm of the params: 6.8513165
                Loss: fixed 113 labels. Loss 0.00320. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06352187
Train loss (w/o reg) on all data: 0.05959267
Test loss (w/o reg) on all data: 0.03654995
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.2652933e-06
Norm of the params: 8.864756
              Random: fixed   5 labels. Loss 0.03655. Accuracy 0.990.
### Flips: 820, rs: 4, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560407
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6324945e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9606506e-08
Norm of the params: 6.0928082
                Loss: fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062085282
Train loss (w/o reg) on all data: 0.05812434
Test loss (w/o reg) on all data: 0.035912868
Train acc on all data:  0.975929978118162
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.5334115e-06
Norm of the params: 8.9004965
              Random: fixed   9 labels. Loss 0.03591. Accuracy 0.989.
### Flips: 820, rs: 4, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.0026560382
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.965332e-08
Norm of the params: 6.092828
     Influence (LOO): fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601323
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.00037e-08
Norm of the params: 6.092794
                Loss: fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059803642
Train loss (w/o reg) on all data: 0.05577518
Test loss (w/o reg) on all data: 0.034672767
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.623658e-06
Norm of the params: 8.976036
              Random: fixed  14 labels. Loss 0.03467. Accuracy 0.992.
### Flips: 820, rs: 4, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8885344e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011383
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6011452e-08
Norm of the params: 6.092825
                Loss: fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058192126
Train loss (w/o reg) on all data: 0.05415217
Test loss (w/o reg) on all data: 0.03394353
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.072632e-06
Norm of the params: 8.988837
              Random: fixed  18 labels. Loss 0.03394. Accuracy 0.991.
### Flips: 820, rs: 4, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6981659e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6554123e-08
Norm of the params: 6.0928187
                Loss: fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055583168
Train loss (w/o reg) on all data: 0.051466037
Test loss (w/o reg) on all data: 0.03226597
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.8168687e-06
Norm of the params: 9.074285
              Random: fixed  25 labels. Loss 0.03227. Accuracy 0.991.
### Flips: 820, rs: 4, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6008716e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.845462e-08
Norm of the params: 6.0928082
                Loss: fixed 114 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054354373
Train loss (w/o reg) on all data: 0.05033723
Test loss (w/o reg) on all data: 0.031550657
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.2413934e-06
Norm of the params: 8.963421
              Random: fixed  30 labels. Loss 0.03155. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05828647
Train loss (w/o reg) on all data: 0.053960748
Test loss (w/o reg) on all data: 0.043140516
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.365405e-06
Norm of the params: 9.301314
Flipped loss: 0.04314. Accuracy: 0.982
### Flips: 820, rs: 5, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069091674
Train loss (w/o reg) on all data: 0.0040770494
Test loss (w/o reg) on all data: 0.0066767875
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1406336e-07
Norm of the params: 7.5261116
     Influence (LOO): fixed  95 labels. Loss 0.00668. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004721017
Train loss (w/o reg) on all data: 0.0019981088
Test loss (w/o reg) on all data: 0.004012918
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.732439e-08
Norm of the params: 7.3795776
                Loss: fixed  98 labels. Loss 0.00401. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05651792
Train loss (w/o reg) on all data: 0.052225713
Test loss (w/o reg) on all data: 0.042041663
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5333444e-05
Norm of the params: 9.265215
              Random: fixed   5 labels. Loss 0.04204. Accuracy 0.981.
### Flips: 820, rs: 5, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1217868e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9229273e-08
Norm of the params: 6.0928273
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05493155
Train loss (w/o reg) on all data: 0.05067834
Test loss (w/o reg) on all data: 0.038232803
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.3354487e-06
Norm of the params: 9.223028
              Random: fixed  10 labels. Loss 0.03823. Accuracy 0.983.
### Flips: 820, rs: 5, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.448509e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4332157e-08
Norm of the params: 6.0928197
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054285083
Train loss (w/o reg) on all data: 0.049951106
Test loss (w/o reg) on all data: 0.036958348
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.0211754e-06
Norm of the params: 9.310186
              Random: fixed  13 labels. Loss 0.03696. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0751147e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5456654e-08
Norm of the params: 6.0928273
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05189167
Train loss (w/o reg) on all data: 0.04734068
Test loss (w/o reg) on all data: 0.03415198
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.40801e-06
Norm of the params: 9.540431
              Random: fixed  19 labels. Loss 0.03415. Accuracy 0.990.
### Flips: 820, rs: 5, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011173
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1733418e-08
Norm of the params: 6.0928288
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0831416e-08
Norm of the params: 6.092813
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050700594
Train loss (w/o reg) on all data: 0.046085455
Test loss (w/o reg) on all data: 0.032260038
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.0416615e-05
Norm of the params: 9.607434
              Random: fixed  23 labels. Loss 0.03226. Accuracy 0.990.
### Flips: 820, rs: 5, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.798911e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4040856e-08
Norm of the params: 6.0928173
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04938828
Train loss (w/o reg) on all data: 0.044871595
Test loss (w/o reg) on all data: 0.029445698
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.909003e-06
Norm of the params: 9.504402
              Random: fixed  27 labels. Loss 0.02945. Accuracy 0.993.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06664702
Train loss (w/o reg) on all data: 0.06208528
Test loss (w/o reg) on all data: 0.04012371
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.6074314e-06
Norm of the params: 9.551698
Flipped loss: 0.04012. Accuracy: 0.991
### Flips: 820, rs: 6, checks: 205
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012348252
Train loss (w/o reg) on all data: 0.008399338
Test loss (w/o reg) on all data: 0.0067775967
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4157141e-06
Norm of the params: 8.886972
     Influence (LOO): fixed 105 labels. Loss 0.00678. Accuracy 0.999.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068669096
Train loss (w/o reg) on all data: 0.0032364419
Test loss (w/o reg) on all data: 0.005644558
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.7831694e-07
Norm of the params: 8.521112
                Loss: fixed 115 labels. Loss 0.00564. Accuracy 0.998.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06439611
Train loss (w/o reg) on all data: 0.059755374
Test loss (w/o reg) on all data: 0.038140602
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4575183e-05
Norm of the params: 9.634041
              Random: fixed   8 labels. Loss 0.03814. Accuracy 0.993.
### Flips: 820, rs: 6, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601762
Test loss (w/o reg) on all data: 0.0026561478
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.852976e-08
Norm of the params: 6.0927234
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601157
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1422954e-08
Norm of the params: 6.0928216
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061718985
Train loss (w/o reg) on all data: 0.05720078
Test loss (w/o reg) on all data: 0.036579728
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.784977e-06
Norm of the params: 9.506004
              Random: fixed  16 labels. Loss 0.03658. Accuracy 0.993.
### Flips: 820, rs: 6, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.052703e-08
Norm of the params: 6.0928087
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5793912e-08
Norm of the params: 6.092834
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060926504
Train loss (w/o reg) on all data: 0.056403592
Test loss (w/o reg) on all data: 0.034868192
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8519342e-05
Norm of the params: 9.510953
              Random: fixed  18 labels. Loss 0.03487. Accuracy 0.993.
### Flips: 820, rs: 6, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601258
Test loss (w/o reg) on all data: 0.0026560856
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.45695e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601019
Test loss (w/o reg) on all data: 0.0026560414
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5980832e-08
Norm of the params: 6.0928445
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059637718
Train loss (w/o reg) on all data: 0.05524573
Test loss (w/o reg) on all data: 0.032960013
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3704761e-05
Norm of the params: 9.372284
              Random: fixed  24 labels. Loss 0.03296. Accuracy 0.993.
### Flips: 820, rs: 6, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0067253e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.345319e-08
Norm of the params: 6.0928226
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057112552
Train loss (w/o reg) on all data: 0.052742388
Test loss (w/o reg) on all data: 0.030203104
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0246161e-05
Norm of the params: 9.348972
              Random: fixed  30 labels. Loss 0.03020. Accuracy 0.995.
### Flips: 820, rs: 6, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601401
Test loss (w/o reg) on all data: 0.0026561073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1105844e-08
Norm of the params: 6.092782
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560905
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.100332e-08
Norm of the params: 6.092818
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05436606
Train loss (w/o reg) on all data: 0.0499902
Test loss (w/o reg) on all data: 0.029801562
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.0278738e-06
Norm of the params: 9.355063
              Random: fixed  36 labels. Loss 0.02980. Accuracy 0.994.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06676259
Train loss (w/o reg) on all data: 0.062163305
Test loss (w/o reg) on all data: 0.040935665
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.081995e-06
Norm of the params: 9.590914
Flipped loss: 0.04094. Accuracy: 0.993
### Flips: 820, rs: 7, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0110589005
Train loss (w/o reg) on all data: 0.007186965
Test loss (w/o reg) on all data: 0.006661164
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0645466e-07
Norm of the params: 8.799927
     Influence (LOO): fixed 105 labels. Loss 0.00666. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096014264
Test loss (w/o reg) on all data: 0.0026561038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3330945e-08
Norm of the params: 6.092778
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066143006
Train loss (w/o reg) on all data: 0.061587095
Test loss (w/o reg) on all data: 0.040017337
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.4894206e-06
Norm of the params: 9.545588
              Random: fixed   3 labels. Loss 0.04002. Accuracy 0.993.
### Flips: 820, rs: 7, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013065
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.940616e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.184124e-09
Norm of the params: 6.092825
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062292177
Train loss (w/o reg) on all data: 0.0579702
Test loss (w/o reg) on all data: 0.037201546
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.1491254e-06
Norm of the params: 9.297287
              Random: fixed  14 labels. Loss 0.03720. Accuracy 0.993.
### Flips: 820, rs: 7, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560863
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1437178e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012716
Test loss (w/o reg) on all data: 0.0026560882
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5513694e-08
Norm of the params: 6.0928035
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059016753
Train loss (w/o reg) on all data: 0.054572135
Test loss (w/o reg) on all data: 0.03523902
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.962488e-06
Norm of the params: 9.428273
              Random: fixed  22 labels. Loss 0.03524. Accuracy 0.993.
### Flips: 820, rs: 7, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010655
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5818062e-08
Norm of the params: 6.092837
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7164709e-08
Norm of the params: 6.0928206
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05626449
Train loss (w/o reg) on all data: 0.0518412
Test loss (w/o reg) on all data: 0.03443569
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.224665e-06
Norm of the params: 9.405628
              Random: fixed  28 labels. Loss 0.03444. Accuracy 0.992.
### Flips: 820, rs: 7, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.383731e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.07686216e-08
Norm of the params: 6.0928154
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05521243
Train loss (w/o reg) on all data: 0.050727166
Test loss (w/o reg) on all data: 0.03354575
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.8571375e-06
Norm of the params: 9.471285
              Random: fixed  31 labels. Loss 0.03355. Accuracy 0.992.
### Flips: 820, rs: 7, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013804
Test loss (w/o reg) on all data: 0.0026560856
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4946835e-08
Norm of the params: 6.0927854
     Influence (LOO): fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012565
Test loss (w/o reg) on all data: 0.0026560866
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.378316e-08
Norm of the params: 6.0928054
                Loss: fixed 115 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05286202
Train loss (w/o reg) on all data: 0.048392598
Test loss (w/o reg) on all data: 0.030347835
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.55292e-06
Norm of the params: 9.454546
              Random: fixed  37 labels. Loss 0.03035. Accuracy 0.993.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065570906
Train loss (w/o reg) on all data: 0.061601084
Test loss (w/o reg) on all data: 0.043163925
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.20634595e-05
Norm of the params: 8.910466
Flipped loss: 0.04316. Accuracy: 0.987
### Flips: 820, rs: 8, checks: 205
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009629466
Train loss (w/o reg) on all data: 0.006062859
Test loss (w/o reg) on all data: 0.0052601467
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7262837e-07
Norm of the params: 8.445835
     Influence (LOO): fixed  99 labels. Loss 0.00526. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046240855
Train loss (w/o reg) on all data: 0.0019622627
Test loss (w/o reg) on all data: 0.0044076326
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.0095596e-07
Norm of the params: 7.296332
                Loss: fixed 106 labels. Loss 0.00441. Accuracy 0.999.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06417428
Train loss (w/o reg) on all data: 0.060188413
Test loss (w/o reg) on all data: 0.042390414
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.9667793e-06
Norm of the params: 8.92846
              Random: fixed   4 labels. Loss 0.04239. Accuracy 0.987.
### Flips: 820, rs: 8, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1407114e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4169228e-08
Norm of the params: 6.09281
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061411176
Train loss (w/o reg) on all data: 0.05759034
Test loss (w/o reg) on all data: 0.0384075
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.7739839e-06
Norm of the params: 8.741667
              Random: fixed  13 labels. Loss 0.03841. Accuracy 0.989.
### Flips: 820, rs: 8, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4017177e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.961125e-08
Norm of the params: 6.092819
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058338556
Train loss (w/o reg) on all data: 0.054501615
Test loss (w/o reg) on all data: 0.036363583
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.9406814e-05
Norm of the params: 8.760071
              Random: fixed  20 labels. Loss 0.03636. Accuracy 0.990.
### Flips: 820, rs: 8, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601104
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8683943e-08
Norm of the params: 6.0928307
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.637146e-09
Norm of the params: 6.0928173
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055180244
Train loss (w/o reg) on all data: 0.05124493
Test loss (w/o reg) on all data: 0.033691533
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1629887e-06
Norm of the params: 8.871657
              Random: fixed  29 labels. Loss 0.03369. Accuracy 0.992.
### Flips: 820, rs: 8, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960132
Test loss (w/o reg) on all data: 0.0026560926
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4570344e-08
Norm of the params: 6.0927954
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7309036e-08
Norm of the params: 6.092823
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053750567
Train loss (w/o reg) on all data: 0.049727537
Test loss (w/o reg) on all data: 0.033030458
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.979981e-06
Norm of the params: 8.969985
              Random: fixed  32 labels. Loss 0.03303. Accuracy 0.991.
### Flips: 820, rs: 8, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3313432e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.461955e-09
Norm of the params: 6.0928164
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051115625
Train loss (w/o reg) on all data: 0.04691607
Test loss (w/o reg) on all data: 0.029412543
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.684684e-06
Norm of the params: 9.164665
              Random: fixed  39 labels. Loss 0.02941. Accuracy 0.996.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06483604
Train loss (w/o reg) on all data: 0.06033213
Test loss (w/o reg) on all data: 0.043199968
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0611791e-05
Norm of the params: 9.49095
Flipped loss: 0.04320. Accuracy: 0.988
### Flips: 820, rs: 9, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013979109
Train loss (w/o reg) on all data: 0.010012751
Test loss (w/o reg) on all data: 0.0076168203
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6567466e-06
Norm of the params: 8.906578
     Influence (LOO): fixed  98 labels. Loss 0.00762. Accuracy 1.000.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059729284
Train loss (w/o reg) on all data: 0.0029260155
Test loss (w/o reg) on all data: 0.004319768
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2550983e-07
Norm of the params: 7.8062963
                Loss: fixed 109 labels. Loss 0.00432. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636999
Train loss (w/o reg) on all data: 0.059220888
Test loss (w/o reg) on all data: 0.041090712
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.9070345e-06
Norm of the params: 9.464683
              Random: fixed   5 labels. Loss 0.04109. Accuracy 0.990.
### Flips: 820, rs: 9, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049583586
Train loss (w/o reg) on all data: 0.0024776377
Test loss (w/o reg) on all data: 0.004178575
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2845427e-07
Norm of the params: 7.0437503
     Influence (LOO): fixed 111 labels. Loss 0.00418. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034181953
Train loss (w/o reg) on all data: 0.0012643808
Test loss (w/o reg) on all data: 0.002928271
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.4635054e-08
Norm of the params: 6.5632534
                Loss: fixed 112 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06194253
Train loss (w/o reg) on all data: 0.057551425
Test loss (w/o reg) on all data: 0.03907465
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.900035e-06
Norm of the params: 9.3713455
              Random: fixed  12 labels. Loss 0.03907. Accuracy 0.991.
### Flips: 820, rs: 9, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8174688e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.856766e-08
Norm of the params: 6.092825
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06019436
Train loss (w/o reg) on all data: 0.055832967
Test loss (w/o reg) on all data: 0.037273597
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.1732e-06
Norm of the params: 9.339583
              Random: fixed  18 labels. Loss 0.03727. Accuracy 0.991.
### Flips: 820, rs: 9, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.416056e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7501595e-09
Norm of the params: 6.0928173
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057973962
Train loss (w/o reg) on all data: 0.05353214
Test loss (w/o reg) on all data: 0.035482816
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.558205e-06
Norm of the params: 9.42531
              Random: fixed  23 labels. Loss 0.03548. Accuracy 0.992.
### Flips: 820, rs: 9, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8501131e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.763947e-08
Norm of the params: 6.09282
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05592838
Train loss (w/o reg) on all data: 0.051529814
Test loss (w/o reg) on all data: 0.03286239
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.253514e-06
Norm of the params: 9.3793
              Random: fixed  30 labels. Loss 0.03286. Accuracy 0.996.
### Flips: 820, rs: 9, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011185
Test loss (w/o reg) on all data: 0.0026560838
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0683976e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9348114e-08
Norm of the params: 6.092817
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055140506
Train loss (w/o reg) on all data: 0.050789557
Test loss (w/o reg) on all data: 0.032143585
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.0269503e-06
Norm of the params: 9.328398
              Random: fixed  32 labels. Loss 0.03214. Accuracy 0.995.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066595875
Train loss (w/o reg) on all data: 0.062696345
Test loss (w/o reg) on all data: 0.046570875
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.0069482e-06
Norm of the params: 8.831228
Flipped loss: 0.04657. Accuracy: 0.984
### Flips: 820, rs: 10, checks: 205
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011464659
Train loss (w/o reg) on all data: 0.008256181
Test loss (w/o reg) on all data: 0.0071712723
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2064513e-07
Norm of the params: 8.0105915
     Influence (LOO): fixed 109 labels. Loss 0.00717. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038453704
Train loss (w/o reg) on all data: 0.0015368832
Test loss (w/o reg) on all data: 0.0046509677
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.183428e-07
Norm of the params: 6.7948318
                Loss: fixed 119 labels. Loss 0.00465. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06392811
Train loss (w/o reg) on all data: 0.060225915
Test loss (w/o reg) on all data: 0.04370099
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.7718519e-06
Norm of the params: 8.604881
              Random: fixed   9 labels. Loss 0.04370. Accuracy 0.988.
### Flips: 820, rs: 10, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2099439e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6359557e-08
Norm of the params: 6.0928164
                Loss: fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062984526
Train loss (w/o reg) on all data: 0.059224248
Test loss (w/o reg) on all data: 0.042286612
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.455154e-06
Norm of the params: 8.672111
              Random: fixed  13 labels. Loss 0.04229. Accuracy 0.991.
### Flips: 820, rs: 10, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5002433e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8264561e-08
Norm of the params: 6.092819
                Loss: fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062357154
Train loss (w/o reg) on all data: 0.058542736
Test loss (w/o reg) on all data: 0.041611213
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.9447149e-05
Norm of the params: 8.734322
              Random: fixed  15 labels. Loss 0.04161. Accuracy 0.990.
### Flips: 820, rs: 10, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7441469e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4872627e-08
Norm of the params: 6.0928187
                Loss: fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059629917
Train loss (w/o reg) on all data: 0.055861067
Test loss (w/o reg) on all data: 0.038943738
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.3429958e-06
Norm of the params: 8.681995
              Random: fixed  22 labels. Loss 0.03894. Accuracy 0.990.
### Flips: 820, rs: 10, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7183614e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012483
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.768223e-08
Norm of the params: 6.0928073
                Loss: fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058628794
Train loss (w/o reg) on all data: 0.054840624
Test loss (w/o reg) on all data: 0.03789063
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.0061444e-06
Norm of the params: 8.704217
              Random: fixed  25 labels. Loss 0.03789. Accuracy 0.991.
### Flips: 820, rs: 10, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601274
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8924168e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2888734e-08
Norm of the params: 6.0928183
                Loss: fixed 121 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05595126
Train loss (w/o reg) on all data: 0.051869176
Test loss (w/o reg) on all data: 0.036501728
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1576186e-05
Norm of the params: 9.03558
              Random: fixed  32 labels. Loss 0.03650. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06536861
Train loss (w/o reg) on all data: 0.060836285
Test loss (w/o reg) on all data: 0.049301792
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.302056e-06
Norm of the params: 9.520842
Flipped loss: 0.04930. Accuracy: 0.988
### Flips: 820, rs: 11, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010311794
Train loss (w/o reg) on all data: 0.006479813
Test loss (w/o reg) on all data: 0.0071723736
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.763207e-07
Norm of the params: 8.754405
     Influence (LOO): fixed 108 labels. Loss 0.00717. Accuracy 0.998.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047029075
Train loss (w/o reg) on all data: 0.0020494675
Test loss (w/o reg) on all data: 0.0049565244
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4148264e-08
Norm of the params: 7.284834
                Loss: fixed 113 labels. Loss 0.00496. Accuracy 0.999.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06374558
Train loss (w/o reg) on all data: 0.059279162
Test loss (w/o reg) on all data: 0.04706508
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1580106e-06
Norm of the params: 9.451372
              Random: fixed   6 labels. Loss 0.04707. Accuracy 0.987.
### Flips: 820, rs: 11, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3585678e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.126542e-09
Norm of the params: 6.092813
                Loss: fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061176468
Train loss (w/o reg) on all data: 0.056671824
Test loss (w/o reg) on all data: 0.04372533
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.1179145e-06
Norm of the params: 9.491727
              Random: fixed  14 labels. Loss 0.04373. Accuracy 0.988.
### Flips: 820, rs: 11, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7490808e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.762485e-09
Norm of the params: 6.0928116
                Loss: fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059804738
Train loss (w/o reg) on all data: 0.05541109
Test loss (w/o reg) on all data: 0.039592434
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.1133544e-06
Norm of the params: 9.374057
              Random: fixed  19 labels. Loss 0.03959. Accuracy 0.989.
### Flips: 820, rs: 11, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601137
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.60025e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3797795e-08
Norm of the params: 6.092807
                Loss: fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058797758
Train loss (w/o reg) on all data: 0.054534495
Test loss (w/o reg) on all data: 0.038249314
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.7122837e-06
Norm of the params: 9.233919
              Random: fixed  23 labels. Loss 0.03825. Accuracy 0.991.
### Flips: 820, rs: 11, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601116
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.080099e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3148006e-08
Norm of the params: 6.0928073
                Loss: fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055656582
Train loss (w/o reg) on all data: 0.051429383
Test loss (w/o reg) on all data: 0.035988078
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.2178453e-06
Norm of the params: 9.194778
              Random: fixed  32 labels. Loss 0.03599. Accuracy 0.990.
### Flips: 820, rs: 11, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4627624e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0193061e-08
Norm of the params: 6.0928173
                Loss: fixed 117 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053851984
Train loss (w/o reg) on all data: 0.0495255
Test loss (w/o reg) on all data: 0.032408103
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.1369003e-06
Norm of the params: 9.302135
              Random: fixed  38 labels. Loss 0.03241. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06590101
Train loss (w/o reg) on all data: 0.061653677
Test loss (w/o reg) on all data: 0.043899275
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.2290153e-05
Norm of the params: 9.216648
Flipped loss: 0.04390. Accuracy: 0.988
### Flips: 820, rs: 12, checks: 205
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077618686
Train loss (w/o reg) on all data: 0.004317836
Test loss (w/o reg) on all data: 0.005801928
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.77239e-07
Norm of the params: 8.299437
     Influence (LOO): fixed 113 labels. Loss 0.00580. Accuracy 0.998.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039788825
Train loss (w/o reg) on all data: 0.0015596373
Test loss (w/o reg) on all data: 0.0030001227
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.661999e-07
Norm of the params: 6.955925
                Loss: fixed 118 labels. Loss 0.00300. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063878976
Train loss (w/o reg) on all data: 0.059654754
Test loss (w/o reg) on all data: 0.042177252
Train acc on all data:  0.973984925844882
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.3652667e-06
Norm of the params: 9.191544
              Random: fixed   6 labels. Loss 0.04218. Accuracy 0.989.
### Flips: 820, rs: 12, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.43929935e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.65182e-09
Norm of the params: 6.0928197
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06130229
Train loss (w/o reg) on all data: 0.057136096
Test loss (w/o reg) on all data: 0.039260894
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8223873e-05
Norm of the params: 9.128188
              Random: fixed  14 labels. Loss 0.03926. Accuracy 0.992.
### Flips: 820, rs: 12, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6272631e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.65043e-08
Norm of the params: 6.092812
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060506836
Train loss (w/o reg) on all data: 0.056328177
Test loss (w/o reg) on all data: 0.037176542
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.0700294e-06
Norm of the params: 9.141838
              Random: fixed  18 labels. Loss 0.03718. Accuracy 0.995.
### Flips: 820, rs: 12, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3166174e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7262288e-08
Norm of the params: 6.092824
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059906617
Train loss (w/o reg) on all data: 0.055758905
Test loss (w/o reg) on all data: 0.036805853
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.514314e-06
Norm of the params: 9.1079235
              Random: fixed  22 labels. Loss 0.03681. Accuracy 0.995.
### Flips: 820, rs: 12, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6915076e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960119
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.546241e-08
Norm of the params: 6.0928173
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058586426
Train loss (w/o reg) on all data: 0.05463799
Test loss (w/o reg) on all data: 0.035001002
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.4911824e-06
Norm of the params: 8.886434
              Random: fixed  28 labels. Loss 0.03500. Accuracy 0.995.
### Flips: 820, rs: 12, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4041153e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011994
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4125714e-09
Norm of the params: 6.092815
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058414184
Train loss (w/o reg) on all data: 0.054523256
Test loss (w/o reg) on all data: 0.03437609
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8403335e-06
Norm of the params: 8.821484
              Random: fixed  29 labels. Loss 0.03438. Accuracy 0.996.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065986395
Train loss (w/o reg) on all data: 0.0620133
Test loss (w/o reg) on all data: 0.043703787
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.15514e-06
Norm of the params: 8.914146
Flipped loss: 0.04370. Accuracy: 0.987
### Flips: 820, rs: 13, checks: 205
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009898276
Train loss (w/o reg) on all data: 0.0069913752
Test loss (w/o reg) on all data: 0.0052146744
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.319998e-07
Norm of the params: 7.624829
     Influence (LOO): fixed 113 labels. Loss 0.00521. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005486441
Train loss (w/o reg) on all data: 0.0025913015
Test loss (w/o reg) on all data: 0.0044029653
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.783577e-07
Norm of the params: 7.609388
                Loss: fixed 118 labels. Loss 0.00440. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06491142
Train loss (w/o reg) on all data: 0.061029617
Test loss (w/o reg) on all data: 0.042047214
Train acc on all data:  0.975200583515682
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3212361e-05
Norm of the params: 8.811127
              Random: fixed   5 labels. Loss 0.04205. Accuracy 0.989.
### Flips: 820, rs: 13, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17032775e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3442416e-08
Norm of the params: 6.0928173
                Loss: fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06240482
Train loss (w/o reg) on all data: 0.058225457
Test loss (w/o reg) on all data: 0.040786557
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.09069715e-05
Norm of the params: 9.142605
              Random: fixed  11 labels. Loss 0.04079. Accuracy 0.991.
### Flips: 820, rs: 13, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1739976e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.531522e-08
Norm of the params: 6.0928173
                Loss: fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06095028
Train loss (w/o reg) on all data: 0.056868747
Test loss (w/o reg) on all data: 0.037929606
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.3161893e-06
Norm of the params: 9.034968
              Random: fixed  19 labels. Loss 0.03793. Accuracy 0.992.
### Flips: 820, rs: 13, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1842007e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3460622e-08
Norm of the params: 6.092826
                Loss: fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0593304
Train loss (w/o reg) on all data: 0.055204388
Test loss (w/o reg) on all data: 0.03637564
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.919884e-06
Norm of the params: 9.084064
              Random: fixed  25 labels. Loss 0.03638. Accuracy 0.993.
### Flips: 820, rs: 13, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011197
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7061475e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.235604e-09
Norm of the params: 6.092813
                Loss: fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057169482
Train loss (w/o reg) on all data: 0.053101983
Test loss (w/o reg) on all data: 0.03573061
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.7182812e-06
Norm of the params: 9.0194235
              Random: fixed  31 labels. Loss 0.03573. Accuracy 0.993.
### Flips: 820, rs: 13, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096013205
Test loss (w/o reg) on all data: 0.0026560863
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8384532e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.28247954e-08
Norm of the params: 6.09282
                Loss: fixed 123 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055902384
Train loss (w/o reg) on all data: 0.051918678
Test loss (w/o reg) on all data: 0.034154367
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.5975385e-06
Norm of the params: 8.926038
              Random: fixed  35 labels. Loss 0.03415. Accuracy 0.993.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06951985
Train loss (w/o reg) on all data: 0.0645502
Test loss (w/o reg) on all data: 0.053262524
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.3716508e-05
Norm of the params: 9.969606
Flipped loss: 0.05326. Accuracy: 0.984
### Flips: 820, rs: 14, checks: 205
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01633709
Train loss (w/o reg) on all data: 0.012107079
Test loss (w/o reg) on all data: 0.014746759
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.2546603e-07
Norm of the params: 9.197837
     Influence (LOO): fixed 114 labels. Loss 0.01475. Accuracy 0.995.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0089666825
Train loss (w/o reg) on all data: 0.0049007465
Test loss (w/o reg) on all data: 0.008750845
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.726564e-07
Norm of the params: 9.01769
                Loss: fixed 122 labels. Loss 0.00875. Accuracy 0.996.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068127334
Train loss (w/o reg) on all data: 0.063225046
Test loss (w/o reg) on all data: 0.049883496
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.448575e-06
Norm of the params: 9.901804
              Random: fixed   6 labels. Loss 0.04988. Accuracy 0.984.
### Flips: 820, rs: 14, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042025056
Train loss (w/o reg) on all data: 0.0020581158
Test loss (w/o reg) on all data: 0.0034520617
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.066183e-07
Norm of the params: 6.5488777
     Influence (LOO): fixed 134 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003891849
Train loss (w/o reg) on all data: 0.0015965139
Test loss (w/o reg) on all data: 0.004274499
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.450194e-08
Norm of the params: 6.775449
                Loss: fixed 133 labels. Loss 0.00427. Accuracy 0.999.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066127956
Train loss (w/o reg) on all data: 0.061276503
Test loss (w/o reg) on all data: 0.04539649
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.013933e-06
Norm of the params: 9.850331
              Random: fixed  15 labels. Loss 0.04540. Accuracy 0.986.
### Flips: 820, rs: 14, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601076
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.122741e-08
Norm of the params: 6.092835
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.560448e-08
Norm of the params: 6.09281
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063515715
Train loss (w/o reg) on all data: 0.058740072
Test loss (w/o reg) on all data: 0.04386853
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.2004365e-06
Norm of the params: 9.773067
              Random: fixed  22 labels. Loss 0.04387. Accuracy 0.987.
### Flips: 820, rs: 14, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1213477e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7531812e-08
Norm of the params: 6.092811
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05862613
Train loss (w/o reg) on all data: 0.053763337
Test loss (w/o reg) on all data: 0.03856462
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.388637e-06
Norm of the params: 9.861841
              Random: fixed  37 labels. Loss 0.03856. Accuracy 0.992.
### Flips: 820, rs: 14, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960106
Test loss (w/o reg) on all data: 0.002656042
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3010422e-08
Norm of the params: 6.0928383
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011994
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9478736e-09
Norm of the params: 6.092815
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05518527
Train loss (w/o reg) on all data: 0.05024076
Test loss (w/o reg) on all data: 0.03607476
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.6902366e-06
Norm of the params: 9.944355
              Random: fixed  44 labels. Loss 0.03607. Accuracy 0.992.
### Flips: 820, rs: 14, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7932494e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011214
Test loss (w/o reg) on all data: 0.00265605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7904598e-08
Norm of the params: 6.0928273
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053270303
Train loss (w/o reg) on all data: 0.048459988
Test loss (w/o reg) on all data: 0.0350923
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.3189925e-06
Norm of the params: 9.808482
              Random: fixed  49 labels. Loss 0.03509. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05655946
Train loss (w/o reg) on all data: 0.052886304
Test loss (w/o reg) on all data: 0.031600773
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.2112144e-06
Norm of the params: 8.57106
Flipped loss: 0.03160. Accuracy: 0.995
### Flips: 820, rs: 15, checks: 205
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00547182
Train loss (w/o reg) on all data: 0.003044796
Test loss (w/o reg) on all data: 0.00385465
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.050412e-08
Norm of the params: 6.9670997
     Influence (LOO): fixed  87 labels. Loss 0.00385. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601163
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7961684e-08
Norm of the params: 6.092821
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053859927
Train loss (w/o reg) on all data: 0.050089207
Test loss (w/o reg) on all data: 0.029074222
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.8889447e-06
Norm of the params: 8.684145
              Random: fixed   6 labels. Loss 0.02907. Accuracy 0.996.
### Flips: 820, rs: 15, checks: 410
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.575748e-08
Norm of the params: 6.092811
     Influence (LOO): fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.963537e-09
Norm of the params: 6.092827
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052404568
Train loss (w/o reg) on all data: 0.048618607
Test loss (w/o reg) on all data: 0.027273633
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6830843e-06
Norm of the params: 8.70168
              Random: fixed  10 labels. Loss 0.02727. Accuracy 0.994.
### Flips: 820, rs: 15, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5007405e-08
Norm of the params: 6.092825
     Influence (LOO): fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8999112e-09
Norm of the params: 6.0928154
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04967675
Train loss (w/o reg) on all data: 0.045927443
Test loss (w/o reg) on all data: 0.02574315
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.960831e-06
Norm of the params: 8.659452
              Random: fixed  16 labels. Loss 0.02574. Accuracy 0.995.
### Flips: 820, rs: 15, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2032909e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011994
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.146423e-09
Norm of the params: 6.0928154
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04839232
Train loss (w/o reg) on all data: 0.044703584
Test loss (w/o reg) on all data: 0.025296306
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.677716e-06
Norm of the params: 8.589218
              Random: fixed  20 labels. Loss 0.02530. Accuracy 0.996.
### Flips: 820, rs: 15, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9681027e-08
Norm of the params: 6.092824
     Influence (LOO): fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5126735e-08
Norm of the params: 6.092813
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045980506
Train loss (w/o reg) on all data: 0.042448472
Test loss (w/o reg) on all data: 0.02184576
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5702357e-06
Norm of the params: 8.4047985
              Random: fixed  27 labels. Loss 0.02185. Accuracy 0.998.
### Flips: 820, rs: 15, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.985272e-09
Norm of the params: 6.092813
     Influence (LOO): fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1399071e-08
Norm of the params: 6.092821
                Loss: fixed  90 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045158274
Train loss (w/o reg) on all data: 0.041590847
Test loss (w/o reg) on all data: 0.021217877
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4968283e-06
Norm of the params: 8.446805
              Random: fixed  30 labels. Loss 0.02122. Accuracy 0.998.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060998593
Train loss (w/o reg) on all data: 0.0565906
Test loss (w/o reg) on all data: 0.033316933
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.937378e-06
Norm of the params: 9.389345
Flipped loss: 0.03332. Accuracy: 0.998
### Flips: 820, rs: 16, checks: 205
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005327002
Train loss (w/o reg) on all data: 0.00277484
Test loss (w/o reg) on all data: 0.0038954797
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8035077e-08
Norm of the params: 7.144455
     Influence (LOO): fixed  97 labels. Loss 0.00390. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003654967
Train loss (w/o reg) on all data: 0.0013723032
Test loss (w/o reg) on all data: 0.0038877595
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.591762e-07
Norm of the params: 6.7567205
                Loss: fixed  98 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05838616
Train loss (w/o reg) on all data: 0.05400887
Test loss (w/o reg) on all data: 0.03155939
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7826673e-06
Norm of the params: 9.356587
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.999.
### Flips: 820, rs: 16, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601046
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8932232e-08
Norm of the params: 6.09284
     Influence (LOO): fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5742877e-08
Norm of the params: 6.0928116
                Loss: fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0554816
Train loss (w/o reg) on all data: 0.05107126
Test loss (w/o reg) on all data: 0.029492408
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.2589974e-06
Norm of the params: 9.3918495
              Random: fixed  15 labels. Loss 0.02949. Accuracy 0.999.
### Flips: 820, rs: 16, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096013333
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6477804e-08
Norm of the params: 6.092792
     Influence (LOO): fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601231
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.509146e-08
Norm of the params: 6.09281
                Loss: fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053067155
Train loss (w/o reg) on all data: 0.048620395
Test loss (w/o reg) on all data: 0.028013606
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.949023e-06
Norm of the params: 9.430547
              Random: fixed  21 labels. Loss 0.02801. Accuracy 0.999.
### Flips: 820, rs: 16, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8120852e-08
Norm of the params: 6.092814
     Influence (LOO): fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7528048e-08
Norm of the params: 6.092818
                Loss: fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04900869
Train loss (w/o reg) on all data: 0.04445838
Test loss (w/o reg) on all data: 0.024955835
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.3686836e-06
Norm of the params: 9.539714
              Random: fixed  29 labels. Loss 0.02496. Accuracy 0.998.
### Flips: 820, rs: 16, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960129
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7444627e-08
Norm of the params: 6.0928
     Influence (LOO): fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3193937e-08
Norm of the params: 6.092814
                Loss: fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047919497
Train loss (w/o reg) on all data: 0.04338503
Test loss (w/o reg) on all data: 0.023987444
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8365562e-06
Norm of the params: 9.523098
              Random: fixed  33 labels. Loss 0.02399. Accuracy 0.999.
### Flips: 820, rs: 16, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010935
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9633044e-08
Norm of the params: 6.092832
     Influence (LOO): fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.200354e-09
Norm of the params: 6.0928183
                Loss: fixed  99 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04579652
Train loss (w/o reg) on all data: 0.041195203
Test loss (w/o reg) on all data: 0.023624184
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3676864e-06
Norm of the params: 9.593039
              Random: fixed  36 labels. Loss 0.02362. Accuracy 0.998.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06722176
Train loss (w/o reg) on all data: 0.063142985
Test loss (w/o reg) on all data: 0.044088293
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.5549937e-06
Norm of the params: 9.031917
Flipped loss: 0.04409. Accuracy: 0.992
### Flips: 820, rs: 17, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013301086
Train loss (w/o reg) on all data: 0.009467372
Test loss (w/o reg) on all data: 0.009446701
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7773917e-06
Norm of the params: 8.756386
     Influence (LOO): fixed 102 labels. Loss 0.00945. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033921339
Train loss (w/o reg) on all data: 0.0012257977
Test loss (w/o reg) on all data: 0.0036710794
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3942444e-08
Norm of the params: 6.582304
                Loss: fixed 115 labels. Loss 0.00367. Accuracy 0.999.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065015204
Train loss (w/o reg) on all data: 0.061031654
Test loss (w/o reg) on all data: 0.042147804
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.32626865e-05
Norm of the params: 8.925864
              Random: fixed   8 labels. Loss 0.04215. Accuracy 0.991.
### Flips: 820, rs: 17, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9616608e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.585767e-08
Norm of the params: 6.092825
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06196545
Train loss (w/o reg) on all data: 0.05786964
Test loss (w/o reg) on all data: 0.040259764
Train acc on all data:  0.975686846584002
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.7971659e-05
Norm of the params: 9.050759
              Random: fixed  16 labels. Loss 0.04026. Accuracy 0.989.
### Flips: 820, rs: 17, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011255
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8849343e-08
Norm of the params: 6.092827
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2960019e-08
Norm of the params: 6.092811
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059835427
Train loss (w/o reg) on all data: 0.055636328
Test loss (w/o reg) on all data: 0.038303036
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.095797e-06
Norm of the params: 9.164167
              Random: fixed  22 labels. Loss 0.03830. Accuracy 0.993.
### Flips: 820, rs: 17, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7516648e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1972413e-08
Norm of the params: 6.0928144
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05691483
Train loss (w/o reg) on all data: 0.052707996
Test loss (w/o reg) on all data: 0.035297494
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.079307e-06
Norm of the params: 9.172604
              Random: fixed  28 labels. Loss 0.03530. Accuracy 0.993.
### Flips: 820, rs: 17, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601137
Test loss (w/o reg) on all data: 0.0026560444
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1608395e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1465315e-08
Norm of the params: 6.092809
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054572314
Train loss (w/o reg) on all data: 0.0502827
Test loss (w/o reg) on all data: 0.033667535
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.017934e-06
Norm of the params: 9.262411
              Random: fixed  34 labels. Loss 0.03367. Accuracy 0.995.
### Flips: 820, rs: 17, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1905015e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0856815e-08
Norm of the params: 6.0928154
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053214125
Train loss (w/o reg) on all data: 0.048827376
Test loss (w/o reg) on all data: 0.031524587
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6658722e-06
Norm of the params: 9.366694
              Random: fixed  39 labels. Loss 0.03152. Accuracy 0.997.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06366266
Train loss (w/o reg) on all data: 0.059448406
Test loss (w/o reg) on all data: 0.04955202
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.3864463e-06
Norm of the params: 9.18069
Flipped loss: 0.04955. Accuracy: 0.983
### Flips: 820, rs: 18, checks: 205
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060720295
Train loss (w/o reg) on all data: 0.0033395924
Test loss (w/o reg) on all data: 0.00501196
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2540589e-07
Norm of the params: 7.392479
     Influence (LOO): fixed 109 labels. Loss 0.00501. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006469336
Train loss (w/o reg) on all data: 0.003043526
Test loss (w/o reg) on all data: 0.004068403
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7283783e-07
Norm of the params: 8.2774515
                Loss: fixed 106 labels. Loss 0.00407. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061384335
Train loss (w/o reg) on all data: 0.057249356
Test loss (w/o reg) on all data: 0.044799265
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1233342e-05
Norm of the params: 9.093931
              Random: fixed   9 labels. Loss 0.04480. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601462
Test loss (w/o reg) on all data: 0.0026561066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8531237e-08
Norm of the params: 6.092772
     Influence (LOO): fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960115
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8718197e-08
Norm of the params: 6.092823
                Loss: fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060617022
Train loss (w/o reg) on all data: 0.056323282
Test loss (w/o reg) on all data: 0.04484673
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.8023718e-06
Norm of the params: 9.266866
              Random: fixed  12 labels. Loss 0.04485. Accuracy 0.987.
### Flips: 820, rs: 18, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7670248e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010935
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8261615e-08
Norm of the params: 6.092833
                Loss: fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058378857
Train loss (w/o reg) on all data: 0.054062992
Test loss (w/o reg) on all data: 0.04355514
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.9707724e-06
Norm of the params: 9.290709
              Random: fixed  18 labels. Loss 0.04356. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010836
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6554988e-08
Norm of the params: 6.092835
     Influence (LOO): fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.713503e-09
Norm of the params: 6.092814
                Loss: fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054865994
Train loss (w/o reg) on all data: 0.050331604
Test loss (w/o reg) on all data: 0.041977435
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.2848204e-06
Norm of the params: 9.523016
              Random: fixed  25 labels. Loss 0.04198. Accuracy 0.984.
### Flips: 820, rs: 18, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5605949e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.522904e-08
Norm of the params: 6.0928125
                Loss: fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05336205
Train loss (w/o reg) on all data: 0.048808962
Test loss (w/o reg) on all data: 0.040431064
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.7411366e-06
Norm of the params: 9.542627
              Random: fixed  28 labels. Loss 0.04043. Accuracy 0.986.
### Flips: 820, rs: 18, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4041343e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2116389e-08
Norm of the params: 6.092812
                Loss: fixed 112 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053273726
Train loss (w/o reg) on all data: 0.048678312
Test loss (w/o reg) on all data: 0.039670873
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.074845e-06
Norm of the params: 9.586881
              Random: fixed  29 labels. Loss 0.03967. Accuracy 0.984.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06737045
Train loss (w/o reg) on all data: 0.06332989
Test loss (w/o reg) on all data: 0.045695193
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.5080144e-06
Norm of the params: 8.989508
Flipped loss: 0.04570. Accuracy: 0.988
### Flips: 820, rs: 19, checks: 205
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010129565
Train loss (w/o reg) on all data: 0.006787445
Test loss (w/o reg) on all data: 0.006418751
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.789238e-07
Norm of the params: 8.175719
     Influence (LOO): fixed 110 labels. Loss 0.00642. Accuracy 0.999.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059313076
Train loss (w/o reg) on all data: 0.0027887544
Test loss (w/o reg) on all data: 0.004661041
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7378612e-07
Norm of the params: 7.927867
                Loss: fixed 114 labels. Loss 0.00466. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065601945
Train loss (w/o reg) on all data: 0.061579794
Test loss (w/o reg) on all data: 0.04406128
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.6699827e-06
Norm of the params: 8.969006
              Random: fixed   6 labels. Loss 0.04406. Accuracy 0.988.
### Flips: 820, rs: 19, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3720088e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0311171e-08
Norm of the params: 6.0928264
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06327622
Train loss (w/o reg) on all data: 0.059066005
Test loss (w/o reg) on all data: 0.04236318
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.2576168e-06
Norm of the params: 9.17629
              Random: fixed  12 labels. Loss 0.04236. Accuracy 0.989.
### Flips: 820, rs: 19, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9498258e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5608393e-08
Norm of the params: 6.092822
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061216824
Train loss (w/o reg) on all data: 0.056631684
Test loss (w/o reg) on all data: 0.039304286
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.037672e-06
Norm of the params: 9.576156
              Random: fixed  18 labels. Loss 0.03930. Accuracy 0.991.
### Flips: 820, rs: 19, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6022527e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5592382e-08
Norm of the params: 6.092819
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057682335
Train loss (w/o reg) on all data: 0.05312364
Test loss (w/o reg) on all data: 0.03648005
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.3213756e-06
Norm of the params: 9.548502
              Random: fixed  27 labels. Loss 0.03648. Accuracy 0.993.
### Flips: 820, rs: 19, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9874953e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2076521e-08
Norm of the params: 6.092816
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05500885
Train loss (w/o reg) on all data: 0.050545
Test loss (w/o reg) on all data: 0.033989504
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.713374e-06
Norm of the params: 9.448651
              Random: fixed  33 labels. Loss 0.03399. Accuracy 0.993.
### Flips: 820, rs: 19, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601111
Test loss (w/o reg) on all data: 0.0026560372
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.50509e-08
Norm of the params: 6.09283
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6820638e-08
Norm of the params: 6.092834
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05375926
Train loss (w/o reg) on all data: 0.049496185
Test loss (w/o reg) on all data: 0.033209883
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.0587556e-06
Norm of the params: 9.233713
              Random: fixed  36 labels. Loss 0.03321. Accuracy 0.993.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06479839
Train loss (w/o reg) on all data: 0.06070393
Test loss (w/o reg) on all data: 0.043216486
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.603498e-06
Norm of the params: 9.049269
Flipped loss: 0.04322. Accuracy: 0.986
### Flips: 820, rs: 20, checks: 205
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0064133485
Train loss (w/o reg) on all data: 0.0034138677
Test loss (w/o reg) on all data: 0.0046940567
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 7.410482e-08
Norm of the params: 7.745296
     Influence (LOO): fixed 108 labels. Loss 0.00469. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035324683
Train loss (w/o reg) on all data: 0.0013948468
Test loss (w/o reg) on all data: 0.0029086368
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.692774e-08
Norm of the params: 6.538534
                Loss: fixed 112 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06370506
Train loss (w/o reg) on all data: 0.059603762
Test loss (w/o reg) on all data: 0.041455094
Train acc on all data:  0.975929978118162
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.721012e-06
Norm of the params: 9.056812
              Random: fixed   5 labels. Loss 0.04146. Accuracy 0.989.
### Flips: 820, rs: 20, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009600914
Test loss (w/o reg) on all data: 0.0026560177
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.544164e-08
Norm of the params: 6.0928607
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8128476e-08
Norm of the params: 6.0928144
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062429525
Train loss (w/o reg) on all data: 0.058532126
Test loss (w/o reg) on all data: 0.039684348
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.2643236e-06
Norm of the params: 8.8288145
              Random: fixed  10 labels. Loss 0.03968. Accuracy 0.990.
### Flips: 820, rs: 20, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8474004e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.265736e-08
Norm of the params: 6.0928264
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060789242
Train loss (w/o reg) on all data: 0.056837644
Test loss (w/o reg) on all data: 0.038352586
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.316446e-06
Norm of the params: 8.889992
              Random: fixed  14 labels. Loss 0.03835. Accuracy 0.992.
### Flips: 820, rs: 20, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600909
Test loss (w/o reg) on all data: 0.0026560172
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.601378e-08
Norm of the params: 6.092862
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.673465e-08
Norm of the params: 6.0928154
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05852913
Train loss (w/o reg) on all data: 0.054558873
Test loss (w/o reg) on all data: 0.0364079
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.2927095e-06
Norm of the params: 8.910954
              Random: fixed  21 labels. Loss 0.03641. Accuracy 0.991.
### Flips: 820, rs: 20, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3972354e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.180405e-09
Norm of the params: 6.092814
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054967467
Train loss (w/o reg) on all data: 0.05078323
Test loss (w/o reg) on all data: 0.034841273
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.4728495e-06
Norm of the params: 9.147933
              Random: fixed  29 labels. Loss 0.03484. Accuracy 0.990.
### Flips: 820, rs: 20, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601001
Test loss (w/o reg) on all data: 0.0026560398
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.168272e-08
Norm of the params: 6.0928473
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0490727e-08
Norm of the params: 6.0928154
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054698806
Train loss (w/o reg) on all data: 0.05061395
Test loss (w/o reg) on all data: 0.034549356
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.2443858e-06
Norm of the params: 9.038645
              Random: fixed  30 labels. Loss 0.03455. Accuracy 0.990.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060241558
Train loss (w/o reg) on all data: 0.05625035
Test loss (w/o reg) on all data: 0.038955882
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.6978415e-06
Norm of the params: 8.934439
Flipped loss: 0.03896. Accuracy: 0.991
### Flips: 820, rs: 21, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006346944
Train loss (w/o reg) on all data: 0.0038194545
Test loss (w/o reg) on all data: 0.0048913565
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8935253e-07
Norm of the params: 7.109837
     Influence (LOO): fixed  97 labels. Loss 0.00489. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003340338
Train loss (w/o reg) on all data: 0.0012140848
Test loss (w/o reg) on all data: 0.0034257479
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9931984e-08
Norm of the params: 6.5211244
                Loss: fixed 100 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056705076
Train loss (w/o reg) on all data: 0.052735664
Test loss (w/o reg) on all data: 0.035991102
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1057591e-05
Norm of the params: 8.910008
              Random: fixed  10 labels. Loss 0.03599. Accuracy 0.990.
### Flips: 820, rs: 21, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096008094
Test loss (w/o reg) on all data: 0.0026560025
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0715981e-07
Norm of the params: 6.09288
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6874573e-08
Norm of the params: 6.092811
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055233367
Train loss (w/o reg) on all data: 0.05128849
Test loss (w/o reg) on all data: 0.035133865
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.7689167e-06
Norm of the params: 8.882428
              Random: fixed  14 labels. Loss 0.03513. Accuracy 0.990.
### Flips: 820, rs: 21, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601008
Test loss (w/o reg) on all data: 0.002656031
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.972444e-08
Norm of the params: 6.092846
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4703317e-08
Norm of the params: 6.092812
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05461235
Train loss (w/o reg) on all data: 0.050677087
Test loss (w/o reg) on all data: 0.034863174
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.281284e-06
Norm of the params: 8.871599
              Random: fixed  16 labels. Loss 0.03486. Accuracy 0.991.
### Flips: 820, rs: 21, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.217903e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0098126e-08
Norm of the params: 6.092822
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051793728
Train loss (w/o reg) on all data: 0.047847703
Test loss (w/o reg) on all data: 0.034620907
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3009292e-06
Norm of the params: 8.883722
              Random: fixed  25 labels. Loss 0.03462. Accuracy 0.992.
### Flips: 820, rs: 21, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9700021e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601223
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6875775e-08
Norm of the params: 6.092812
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050212175
Train loss (w/o reg) on all data: 0.046237383
Test loss (w/o reg) on all data: 0.032861155
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.799978e-06
Norm of the params: 8.916043
              Random: fixed  29 labels. Loss 0.03286. Accuracy 0.993.
### Flips: 820, rs: 21, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9143675e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.856774e-09
Norm of the params: 6.0928144
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04815582
Train loss (w/o reg) on all data: 0.04417695
Test loss (w/o reg) on all data: 0.031805024
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.770872e-06
Norm of the params: 8.920616
              Random: fixed  33 labels. Loss 0.03181. Accuracy 0.993.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062964246
Train loss (w/o reg) on all data: 0.05851282
Test loss (w/o reg) on all data: 0.037985615
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.0307436e-06
Norm of the params: 9.435494
Flipped loss: 0.03799. Accuracy: 0.988
### Flips: 820, rs: 22, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058335443
Train loss (w/o reg) on all data: 0.0030782945
Test loss (w/o reg) on all data: 0.006558066
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6098724e-07
Norm of the params: 7.4232745
     Influence (LOO): fixed 105 labels. Loss 0.00656. Accuracy 0.998.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046002027
Train loss (w/o reg) on all data: 0.001988141
Test loss (w/o reg) on all data: 0.006363289
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0917664e-07
Norm of the params: 7.2278094
                Loss: fixed 104 labels. Loss 0.00636. Accuracy 0.999.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06287466
Train loss (w/o reg) on all data: 0.05845541
Test loss (w/o reg) on all data: 0.037659496
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0162636e-05
Norm of the params: 9.401325
              Random: fixed   1 labels. Loss 0.03766. Accuracy 0.987.
### Flips: 820, rs: 22, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3513529e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.316262e-09
Norm of the params: 6.0928187
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061732125
Train loss (w/o reg) on all data: 0.057284236
Test loss (w/o reg) on all data: 0.03757481
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.107613e-06
Norm of the params: 9.431744
              Random: fixed   5 labels. Loss 0.03757. Accuracy 0.985.
### Flips: 820, rs: 22, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.565241e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601177
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0842695e-09
Norm of the params: 6.0928187
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059347957
Train loss (w/o reg) on all data: 0.054866787
Test loss (w/o reg) on all data: 0.036681443
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.8281313e-06
Norm of the params: 9.466965
              Random: fixed  10 labels. Loss 0.03668. Accuracy 0.987.
### Flips: 820, rs: 22, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9924292e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1347284e-09
Norm of the params: 6.0928135
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05756784
Train loss (w/o reg) on all data: 0.05309019
Test loss (w/o reg) on all data: 0.035089493
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.1444144e-06
Norm of the params: 9.4632435
              Random: fixed  15 labels. Loss 0.03509. Accuracy 0.990.
### Flips: 820, rs: 22, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8573345e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.835819e-09
Norm of the params: 6.092812
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055583693
Train loss (w/o reg) on all data: 0.050908186
Test loss (w/o reg) on all data: 0.035334755
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8749527e-06
Norm of the params: 9.670064
              Random: fixed  20 labels. Loss 0.03533. Accuracy 0.990.
### Flips: 820, rs: 22, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012943
Test loss (w/o reg) on all data: 0.0026560852
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1193971e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2329268e-08
Norm of the params: 6.0928183
                Loss: fixed 108 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05253937
Train loss (w/o reg) on all data: 0.04782043
Test loss (w/o reg) on all data: 0.032983318
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6649122e-06
Norm of the params: 9.714875
              Random: fixed  28 labels. Loss 0.03298. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05834212
Train loss (w/o reg) on all data: 0.05375671
Test loss (w/o reg) on all data: 0.043699823
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.9839385e-06
Norm of the params: 9.576442
Flipped loss: 0.04370. Accuracy: 0.991
### Flips: 820, rs: 23, checks: 205
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009483117
Train loss (w/o reg) on all data: 0.0060464744
Test loss (w/o reg) on all data: 0.008169843
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.716986e-07
Norm of the params: 8.290527
     Influence (LOO): fixed  98 labels. Loss 0.00817. Accuracy 0.998.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004944386
Train loss (w/o reg) on all data: 0.002053201
Test loss (w/o reg) on all data: 0.007968746
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3715386e-08
Norm of the params: 7.60419
                Loss: fixed 103 labels. Loss 0.00797. Accuracy 0.999.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05673053
Train loss (w/o reg) on all data: 0.051887978
Test loss (w/o reg) on all data: 0.044167485
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.406324e-06
Norm of the params: 9.841295
              Random: fixed   3 labels. Loss 0.04417. Accuracy 0.991.
### Flips: 820, rs: 23, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028651748
Train loss (w/o reg) on all data: 0.0009913456
Test loss (w/o reg) on all data: 0.003305643
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3607502e-08
Norm of the params: 6.121812
     Influence (LOO): fixed 106 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044541997
Train loss (w/o reg) on all data: 0.0018262406
Test loss (w/o reg) on all data: 0.0060848924
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.427e-08
Norm of the params: 7.249771
                Loss: fixed 104 labels. Loss 0.00608. Accuracy 0.999.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05481956
Train loss (w/o reg) on all data: 0.04986265
Test loss (w/o reg) on all data: 0.04305575
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3017568e-05
Norm of the params: 9.956818
              Random: fixed   7 labels. Loss 0.04306. Accuracy 0.990.
### Flips: 820, rs: 23, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601231
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7211818e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.593523e-09
Norm of the params: 6.092819
                Loss: fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053790133
Train loss (w/o reg) on all data: 0.048723996
Test loss (w/o reg) on all data: 0.041885808
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.5432615e-05
Norm of the params: 10.065922
              Random: fixed  10 labels. Loss 0.04189. Accuracy 0.989.
### Flips: 820, rs: 23, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601057
Test loss (w/o reg) on all data: 0.0026560342
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0850464e-08
Norm of the params: 6.092839
     Influence (LOO): fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601232
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1464192e-08
Norm of the params: 6.092809
                Loss: fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05354149
Train loss (w/o reg) on all data: 0.048481382
Test loss (w/o reg) on all data: 0.04117986
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4203464e-05
Norm of the params: 10.059925
              Random: fixed  12 labels. Loss 0.04118. Accuracy 0.990.
### Flips: 820, rs: 23, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.178375e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3262107e-09
Norm of the params: 6.092818
                Loss: fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05091103
Train loss (w/o reg) on all data: 0.045866575
Test loss (w/o reg) on all data: 0.0354775
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.2426556e-06
Norm of the params: 10.044356
              Random: fixed  22 labels. Loss 0.03548. Accuracy 0.989.
### Flips: 820, rs: 23, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601081
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3488617e-08
Norm of the params: 6.092835
     Influence (LOO): fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.809044e-08
Norm of the params: 6.0928054
                Loss: fixed 107 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049994092
Train loss (w/o reg) on all data: 0.04500048
Test loss (w/o reg) on all data: 0.0339246
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3894324e-06
Norm of the params: 9.993613
              Random: fixed  24 labels. Loss 0.03392. Accuracy 0.990.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058620255
Train loss (w/o reg) on all data: 0.054116305
Test loss (w/o reg) on all data: 0.034085847
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.5145723e-06
Norm of the params: 9.490997
Flipped loss: 0.03409. Accuracy: 0.995
### Flips: 820, rs: 24, checks: 205
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069393963
Train loss (w/o reg) on all data: 0.0037330089
Test loss (w/o reg) on all data: 0.006406772
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.549659e-08
Norm of the params: 8.007979
     Influence (LOO): fixed  95 labels. Loss 0.00641. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031620432
Train loss (w/o reg) on all data: 0.0011657863
Test loss (w/o reg) on all data: 0.004207651
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9661162e-08
Norm of the params: 6.3186345
                Loss: fixed 100 labels. Loss 0.00421. Accuracy 0.999.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055487823
Train loss (w/o reg) on all data: 0.05109379
Test loss (w/o reg) on all data: 0.03241548
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.4067726e-06
Norm of the params: 9.374468
              Random: fixed  10 labels. Loss 0.03242. Accuracy 0.993.
### Flips: 820, rs: 24, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1016583e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010935
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3772476e-08
Norm of the params: 6.092833
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054938365
Train loss (w/o reg) on all data: 0.050545957
Test loss (w/o reg) on all data: 0.031811714
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8896695e-06
Norm of the params: 9.372737
              Random: fixed  12 labels. Loss 0.03181. Accuracy 0.994.
### Flips: 820, rs: 24, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600929
Test loss (w/o reg) on all data: 0.0026560097
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5080093e-08
Norm of the params: 6.0928583
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4657343e-08
Norm of the params: 6.0928135
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051692512
Train loss (w/o reg) on all data: 0.04724394
Test loss (w/o reg) on all data: 0.028579326
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6113398e-06
Norm of the params: 9.432466
              Random: fixed  20 labels. Loss 0.02858. Accuracy 0.993.
### Flips: 820, rs: 24, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012553
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8249073e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2655253e-08
Norm of the params: 6.092825
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05163877
Train loss (w/o reg) on all data: 0.04728084
Test loss (w/o reg) on all data: 0.028067978
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.9215198e-06
Norm of the params: 9.335877
              Random: fixed  22 labels. Loss 0.02807. Accuracy 0.995.
### Flips: 820, rs: 24, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011267
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1092461e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1188037e-08
Norm of the params: 6.0928116
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047631897
Train loss (w/o reg) on all data: 0.04344332
Test loss (w/o reg) on all data: 0.024745459
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.988495e-06
Norm of the params: 9.152683
              Random: fixed  32 labels. Loss 0.02475. Accuracy 0.997.
### Flips: 820, rs: 24, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.01278586e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.843721e-09
Norm of the params: 6.0928197
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044875316
Train loss (w/o reg) on all data: 0.040699523
Test loss (w/o reg) on all data: 0.023160828
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3441678e-06
Norm of the params: 9.138701
              Random: fixed  37 labels. Loss 0.02316. Accuracy 0.996.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06648832
Train loss (w/o reg) on all data: 0.062039334
Test loss (w/o reg) on all data: 0.041019443
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.955599e-06
Norm of the params: 9.432906
Flipped loss: 0.04102. Accuracy: 0.992
### Flips: 820, rs: 25, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012179736
Train loss (w/o reg) on all data: 0.008417044
Test loss (w/o reg) on all data: 0.0071028783
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7811284e-07
Norm of the params: 8.674897
     Influence (LOO): fixed 106 labels. Loss 0.00710. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096009223
Test loss (w/o reg) on all data: 0.0026560319
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1204824e-08
Norm of the params: 6.092861
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636782
Train loss (w/o reg) on all data: 0.059203986
Test loss (w/o reg) on all data: 0.038232945
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3531077e-05
Norm of the params: 9.459613
              Random: fixed   6 labels. Loss 0.03823. Accuracy 0.993.
### Flips: 820, rs: 25, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960124
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6589132e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19436265e-08
Norm of the params: 6.0928254
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06208574
Train loss (w/o reg) on all data: 0.057512306
Test loss (w/o reg) on all data: 0.03690142
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.5262327e-06
Norm of the params: 9.563927
              Random: fixed  11 labels. Loss 0.03690. Accuracy 0.992.
### Flips: 820, rs: 25, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7428066e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601231
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4875535e-08
Norm of the params: 6.09281
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05988779
Train loss (w/o reg) on all data: 0.055167496
Test loss (w/o reg) on all data: 0.03533074
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2698984e-05
Norm of the params: 9.716267
              Random: fixed  16 labels. Loss 0.03533. Accuracy 0.992.
### Flips: 820, rs: 25, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4538181e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.209781e-08
Norm of the params: 6.0928135
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056757506
Train loss (w/o reg) on all data: 0.05196352
Test loss (w/o reg) on all data: 0.03385371
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.7063115e-06
Norm of the params: 9.791821
              Random: fixed  25 labels. Loss 0.03385. Accuracy 0.992.
### Flips: 820, rs: 25, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.530089e-09
Norm of the params: 6.092814
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.01931e-08
Norm of the params: 6.0928164
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054443177
Train loss (w/o reg) on all data: 0.04950972
Test loss (w/o reg) on all data: 0.033207424
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.953092e-06
Norm of the params: 9.933233
              Random: fixed  31 labels. Loss 0.03321. Accuracy 0.994.
### Flips: 820, rs: 25, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1901985e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1534902e-09
Norm of the params: 6.0928154
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052560378
Train loss (w/o reg) on all data: 0.047471236
Test loss (w/o reg) on all data: 0.032687843
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.338737e-06
Norm of the params: 10.088746
              Random: fixed  34 labels. Loss 0.03269. Accuracy 0.994.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059742097
Train loss (w/o reg) on all data: 0.056087445
Test loss (w/o reg) on all data: 0.041600216
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.067122e-06
Norm of the params: 8.549447
Flipped loss: 0.04160. Accuracy: 0.988
### Flips: 820, rs: 26, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006510605
Train loss (w/o reg) on all data: 0.0038763608
Test loss (w/o reg) on all data: 0.0042288
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.7456904e-08
Norm of the params: 7.258436
     Influence (LOO): fixed  98 labels. Loss 0.00423. Accuracy 0.999.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004602718
Train loss (w/o reg) on all data: 0.0019784495
Test loss (w/o reg) on all data: 0.004494385
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.12449136e-07
Norm of the params: 7.244679
                Loss: fixed  99 labels. Loss 0.00449. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057904594
Train loss (w/o reg) on all data: 0.05413376
Test loss (w/o reg) on all data: 0.03804317
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2828492e-05
Norm of the params: 8.684274
              Random: fixed   6 labels. Loss 0.03804. Accuracy 0.989.
### Flips: 820, rs: 26, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013333
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4692263e-08
Norm of the params: 6.092793
     Influence (LOO): fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011074
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.600799e-08
Norm of the params: 6.0928297
                Loss: fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05717788
Train loss (w/o reg) on all data: 0.053380053
Test loss (w/o reg) on all data: 0.037714966
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.6165055e-06
Norm of the params: 8.715301
              Random: fixed   9 labels. Loss 0.03771. Accuracy 0.989.
### Flips: 820, rs: 26, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096009724
Test loss (w/o reg) on all data: 0.0026560293
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.443133e-08
Norm of the params: 6.0928526
     Influence (LOO): fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3304359e-08
Norm of the params: 6.0928154
                Loss: fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05337867
Train loss (w/o reg) on all data: 0.04933861
Test loss (w/o reg) on all data: 0.036178228
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.242128e-06
Norm of the params: 8.988953
              Random: fixed  16 labels. Loss 0.03618. Accuracy 0.991.
### Flips: 820, rs: 26, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012745
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.763859e-08
Norm of the params: 6.092803
     Influence (LOO): fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0807388e-08
Norm of the params: 6.092821
                Loss: fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0524936
Train loss (w/o reg) on all data: 0.048467115
Test loss (w/o reg) on all data: 0.0346138
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.87844e-06
Norm of the params: 8.973833
              Random: fixed  20 labels. Loss 0.03461. Accuracy 0.990.
### Flips: 820, rs: 26, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.340207e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9589438e-08
Norm of the params: 6.0928097
                Loss: fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049972758
Train loss (w/o reg) on all data: 0.04567625
Test loss (w/o reg) on all data: 0.03364509
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5554803e-06
Norm of the params: 9.269849
              Random: fixed  25 labels. Loss 0.03365. Accuracy 0.990.
### Flips: 820, rs: 26, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7909398e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601177
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.37540646e-08
Norm of the params: 6.092819
                Loss: fixed 102 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04929418
Train loss (w/o reg) on all data: 0.045194272
Test loss (w/o reg) on all data: 0.031527862
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.506901e-06
Norm of the params: 9.055285
              Random: fixed  29 labels. Loss 0.03153. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062147293
Train loss (w/o reg) on all data: 0.058269113
Test loss (w/o reg) on all data: 0.03749024
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.5501018e-05
Norm of the params: 8.807021
Flipped loss: 0.03749. Accuracy: 0.991
### Flips: 820, rs: 27, checks: 205
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004566919
Train loss (w/o reg) on all data: 0.00207683
Test loss (w/o reg) on all data: 0.0043576327
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.4745442e-08
Norm of the params: 7.0570383
     Influence (LOO): fixed 102 labels. Loss 0.00436. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9937938e-08
Norm of the params: 6.092811
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062009767
Train loss (w/o reg) on all data: 0.058084946
Test loss (w/o reg) on all data: 0.037035063
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.7908163e-06
Norm of the params: 8.859822
              Random: fixed   1 labels. Loss 0.03704. Accuracy 0.992.
### Flips: 820, rs: 27, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9887558e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.652006e-08
Norm of the params: 6.092825
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05956957
Train loss (w/o reg) on all data: 0.055704843
Test loss (w/o reg) on all data: 0.034425035
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.828427e-06
Norm of the params: 8.791733
              Random: fixed  10 labels. Loss 0.03443. Accuracy 0.992.
### Flips: 820, rs: 27, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600965
Test loss (w/o reg) on all data: 0.0026560212
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8367856e-08
Norm of the params: 6.092853
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7549754e-08
Norm of the params: 6.092814
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05614542
Train loss (w/o reg) on all data: 0.052268133
Test loss (w/o reg) on all data: 0.030787509
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6012027e-06
Norm of the params: 8.806007
              Random: fixed  18 labels. Loss 0.03079. Accuracy 0.996.
### Flips: 820, rs: 27, checks: 820
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0399175e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0544955e-08
Norm of the params: 6.092819
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052188423
Train loss (w/o reg) on all data: 0.048173185
Test loss (w/o reg) on all data: 0.027874706
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.73219e-06
Norm of the params: 8.961292
              Random: fixed  25 labels. Loss 0.02787. Accuracy 0.997.
### Flips: 820, rs: 27, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0282806e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0716308e-09
Norm of the params: 6.0928226
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048983835
Train loss (w/o reg) on all data: 0.044836596
Test loss (w/o reg) on all data: 0.02744416
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7155874e-06
Norm of the params: 9.107402
              Random: fixed  32 labels. Loss 0.02744. Accuracy 0.997.
### Flips: 820, rs: 27, checks: 1230
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.091059e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5315258e-08
Norm of the params: 6.092813
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046471417
Train loss (w/o reg) on all data: 0.042452753
Test loss (w/o reg) on all data: 0.025222156
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.8161342e-06
Norm of the params: 8.965115
              Random: fixed  38 labels. Loss 0.02522. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06124956
Train loss (w/o reg) on all data: 0.057115536
Test loss (w/o reg) on all data: 0.03624035
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.302924e-06
Norm of the params: 9.092884
Flipped loss: 0.03624. Accuracy: 0.994
### Flips: 820, rs: 28, checks: 205
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0050964714
Train loss (w/o reg) on all data: 0.002787381
Test loss (w/o reg) on all data: 0.0036486648
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9305076e-08
Norm of the params: 6.79572
     Influence (LOO): fixed  92 labels. Loss 0.00365. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036529936
Train loss (w/o reg) on all data: 0.0013744212
Test loss (w/o reg) on all data: 0.0030182458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6633183e-08
Norm of the params: 6.7506623
                Loss: fixed  93 labels. Loss 0.00302. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05778645
Train loss (w/o reg) on all data: 0.053362954
Test loss (w/o reg) on all data: 0.034677133
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.534534e-06
Norm of the params: 9.405844
              Random: fixed   7 labels. Loss 0.03468. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.276527e-09
Norm of the params: 6.0928173
     Influence (LOO): fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.031902e-09
Norm of the params: 6.0928173
                Loss: fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056930427
Train loss (w/o reg) on all data: 0.052676015
Test loss (w/o reg) on all data: 0.03319582
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.6596276e-06
Norm of the params: 9.224328
              Random: fixed  11 labels. Loss 0.03320. Accuracy 0.992.
### Flips: 820, rs: 28, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601128
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.624942e-08
Norm of the params: 6.092827
     Influence (LOO): fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601225
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1505864e-08
Norm of the params: 6.0928106
                Loss: fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054619938
Train loss (w/o reg) on all data: 0.050299354
Test loss (w/o reg) on all data: 0.031440195
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3378605e-05
Norm of the params: 9.295788
              Random: fixed  15 labels. Loss 0.03144. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.038618e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.04873905e-08
Norm of the params: 6.092826
                Loss: fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051117647
Train loss (w/o reg) on all data: 0.04703746
Test loss (w/o reg) on all data: 0.026879184
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.705137e-06
Norm of the params: 9.033481
              Random: fixed  23 labels. Loss 0.02688. Accuracy 0.996.
### Flips: 820, rs: 28, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6746604e-08
Norm of the params: 6.09282
     Influence (LOO): fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5948698e-08
Norm of the params: 6.092812
                Loss: fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048453875
Train loss (w/o reg) on all data: 0.044219546
Test loss (w/o reg) on all data: 0.025324333
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.960456e-06
Norm of the params: 9.202529
              Random: fixed  29 labels. Loss 0.02532. Accuracy 0.997.
### Flips: 820, rs: 28, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011825
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3504235e-09
Norm of the params: 6.092818
     Influence (LOO): fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0646819e-08
Norm of the params: 6.092815
                Loss: fixed  94 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045173872
Train loss (w/o reg) on all data: 0.041010354
Test loss (w/o reg) on all data: 0.024166705
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.0238963e-06
Norm of the params: 9.125258
              Random: fixed  35 labels. Loss 0.02417. Accuracy 0.997.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06760638
Train loss (w/o reg) on all data: 0.06365257
Test loss (w/o reg) on all data: 0.0437583
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.5426815e-05
Norm of the params: 8.892482
Flipped loss: 0.04376. Accuracy: 0.988
### Flips: 820, rs: 29, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010433465
Train loss (w/o reg) on all data: 0.007366427
Test loss (w/o reg) on all data: 0.0060804863
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7950575e-07
Norm of the params: 7.832035
     Influence (LOO): fixed 112 labels. Loss 0.00608. Accuracy 0.999.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005039706
Train loss (w/o reg) on all data: 0.0022480246
Test loss (w/o reg) on all data: 0.003179967
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.068732e-08
Norm of the params: 7.4721904
                Loss: fixed 116 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06605009
Train loss (w/o reg) on all data: 0.061953206
Test loss (w/o reg) on all data: 0.041996084
Train acc on all data:  0.974957451981522
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0669373e-05
Norm of the params: 9.051947
              Random: fixed   4 labels. Loss 0.04200. Accuracy 0.989.
### Flips: 820, rs: 29, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8836378e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2451423e-08
Norm of the params: 6.092811
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06446551
Train loss (w/o reg) on all data: 0.06049155
Test loss (w/o reg) on all data: 0.038004138
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.8393844e-06
Norm of the params: 8.915108
              Random: fixed  12 labels. Loss 0.03800. Accuracy 0.991.
### Flips: 820, rs: 29, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960119
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.233674e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11731735e-08
Norm of the params: 6.092816
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06293024
Train loss (w/o reg) on all data: 0.05892296
Test loss (w/o reg) on all data: 0.035774454
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.384696e-06
Norm of the params: 8.952408
              Random: fixed  17 labels. Loss 0.03577. Accuracy 0.992.
### Flips: 820, rs: 29, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1534332e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3988497e-08
Norm of the params: 6.0928216
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060684312
Train loss (w/o reg) on all data: 0.056862596
Test loss (w/o reg) on all data: 0.032671064
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.5589472e-06
Norm of the params: 8.742671
              Random: fixed  24 labels. Loss 0.03267. Accuracy 0.994.
### Flips: 820, rs: 29, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010923
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.287845e-08
Norm of the params: 6.092833
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601285
Test loss (w/o reg) on all data: 0.0026560773
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0270587e-08
Norm of the params: 6.0928006
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056436103
Train loss (w/o reg) on all data: 0.05238265
Test loss (w/o reg) on all data: 0.030218864
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.4988574e-06
Norm of the params: 9.003835
              Random: fixed  33 labels. Loss 0.03022. Accuracy 0.995.
### Flips: 820, rs: 29, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010306
Test loss (w/o reg) on all data: 0.002656032
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.86285e-08
Norm of the params: 6.0928426
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0114817e-08
Norm of the params: 6.092813
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05239422
Train loss (w/o reg) on all data: 0.04823597
Test loss (w/o reg) on all data: 0.025750792
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.9479885e-06
Norm of the params: 9.119481
              Random: fixed  42 labels. Loss 0.02575. Accuracy 0.994.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062349863
Train loss (w/o reg) on all data: 0.058556132
Test loss (w/o reg) on all data: 0.04142447
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5696161e-06
Norm of the params: 8.710606
Flipped loss: 0.04142. Accuracy: 0.992
### Flips: 820, rs: 30, checks: 205
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059825475
Train loss (w/o reg) on all data: 0.003285259
Test loss (w/o reg) on all data: 0.0046906867
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6923544e-07
Norm of the params: 7.3447785
     Influence (LOO): fixed 106 labels. Loss 0.00469. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005124082
Train loss (w/o reg) on all data: 0.0023740497
Test loss (w/o reg) on all data: 0.00405127
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3377259e-07
Norm of the params: 7.4162416
                Loss: fixed 105 labels. Loss 0.00405. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059843663
Train loss (w/o reg) on all data: 0.055975463
Test loss (w/o reg) on all data: 0.03829014
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.1125044e-06
Norm of the params: 8.79568
              Random: fixed   9 labels. Loss 0.03829. Accuracy 0.993.
### Flips: 820, rs: 30, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8187766e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8484661e-08
Norm of the params: 6.092824
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058348253
Train loss (w/o reg) on all data: 0.054524012
Test loss (w/o reg) on all data: 0.037222434
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.455596e-06
Norm of the params: 8.74556
              Random: fixed  13 labels. Loss 0.03722. Accuracy 0.992.
### Flips: 820, rs: 30, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601407
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8770292e-08
Norm of the params: 6.092782
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4193445e-08
Norm of the params: 6.09282
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05584251
Train loss (w/o reg) on all data: 0.051903803
Test loss (w/o reg) on all data: 0.033607647
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.202465e-06
Norm of the params: 8.8754835
              Random: fixed  21 labels. Loss 0.03361. Accuracy 0.993.
### Flips: 820, rs: 30, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4416556e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.46662e-09
Norm of the params: 6.092813
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05430979
Train loss (w/o reg) on all data: 0.050303876
Test loss (w/o reg) on all data: 0.032881994
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.6519934e-06
Norm of the params: 8.950878
              Random: fixed  25 labels. Loss 0.03288. Accuracy 0.993.
### Flips: 820, rs: 30, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0633871e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.793453e-09
Norm of the params: 6.092815
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050877456
Train loss (w/o reg) on all data: 0.046899103
Test loss (w/o reg) on all data: 0.027363487
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.262759e-06
Norm of the params: 8.920038
              Random: fixed  35 labels. Loss 0.02736. Accuracy 0.994.
### Flips: 820, rs: 30, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2420914e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.35852645e-08
Norm of the params: 6.0928164
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04855375
Train loss (w/o reg) on all data: 0.044669192
Test loss (w/o reg) on all data: 0.025393743
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.38965e-06
Norm of the params: 8.814258
              Random: fixed  40 labels. Loss 0.02539. Accuracy 0.995.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05900407
Train loss (w/o reg) on all data: 0.054460865
Test loss (w/o reg) on all data: 0.036805805
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.5259516e-06
Norm of the params: 9.532268
Flipped loss: 0.03681. Accuracy: 0.990
### Flips: 820, rs: 31, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005325866
Train loss (w/o reg) on all data: 0.0026376038
Test loss (w/o reg) on all data: 0.004578932
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1883183e-07
Norm of the params: 7.3324785
     Influence (LOO): fixed  93 labels. Loss 0.00458. Accuracy 0.999.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033921348
Train loss (w/o reg) on all data: 0.0012258018
Test loss (w/o reg) on all data: 0.003671037
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0579223e-08
Norm of the params: 6.582299
                Loss: fixed  95 labels. Loss 0.00367. Accuracy 0.999.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057373762
Train loss (w/o reg) on all data: 0.052835327
Test loss (w/o reg) on all data: 0.034545664
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.23967375e-05
Norm of the params: 9.527262
              Random: fixed   5 labels. Loss 0.03455. Accuracy 0.992.
### Flips: 820, rs: 31, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.387106e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3202826e-08
Norm of the params: 6.0928116
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054364473
Train loss (w/o reg) on all data: 0.049815055
Test loss (w/o reg) on all data: 0.03403986
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.5151648e-05
Norm of the params: 9.538782
              Random: fixed  12 labels. Loss 0.03404. Accuracy 0.993.
### Flips: 820, rs: 31, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012745
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9128505e-08
Norm of the params: 6.092802
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010993
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4058834e-08
Norm of the params: 6.092831
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052201115
Train loss (w/o reg) on all data: 0.047564648
Test loss (w/o reg) on all data: 0.033724762
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.1357713e-06
Norm of the params: 9.629606
              Random: fixed  17 labels. Loss 0.03372. Accuracy 0.990.
### Flips: 820, rs: 31, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4282582e-08
Norm of the params: 6.09281
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8141604e-08
Norm of the params: 6.0928216
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050114375
Train loss (w/o reg) on all data: 0.045291383
Test loss (w/o reg) on all data: 0.033696663
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.537247e-06
Norm of the params: 9.821395
              Random: fixed  20 labels. Loss 0.03370. Accuracy 0.988.
### Flips: 820, rs: 31, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9322842e-08
Norm of the params: 6.092823
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2367866e-08
Norm of the params: 6.092823
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049675174
Train loss (w/o reg) on all data: 0.044717487
Test loss (w/o reg) on all data: 0.032811053
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.2251933e-06
Norm of the params: 9.957597
              Random: fixed  22 labels. Loss 0.03281. Accuracy 0.991.
### Flips: 820, rs: 31, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9648619e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0791019e-08
Norm of the params: 6.092822
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047646817
Train loss (w/o reg) on all data: 0.042618927
Test loss (w/o reg) on all data: 0.03158213
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.455747e-06
Norm of the params: 10.027852
              Random: fixed  26 labels. Loss 0.03158. Accuracy 0.991.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05938316
Train loss (w/o reg) on all data: 0.055196006
Test loss (w/o reg) on all data: 0.04358993
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.0145598e-06
Norm of the params: 9.151128
Flipped loss: 0.04359. Accuracy: 0.984
### Flips: 820, rs: 32, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0108239595
Train loss (w/o reg) on all data: 0.007139027
Test loss (w/o reg) on all data: 0.007892334
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7324166e-06
Norm of the params: 8.584792
     Influence (LOO): fixed 103 labels. Loss 0.00789. Accuracy 0.998.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006594332
Train loss (w/o reg) on all data: 0.0032927063
Test loss (w/o reg) on all data: 0.008011155
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.2431954e-07
Norm of the params: 8.1260395
                Loss: fixed 106 labels. Loss 0.00801. Accuracy 0.997.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058360726
Train loss (w/o reg) on all data: 0.05418375
Test loss (w/o reg) on all data: 0.043602653
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.366087e-06
Norm of the params: 9.139993
              Random: fixed   3 labels. Loss 0.04360. Accuracy 0.983.
### Flips: 820, rs: 32, checks: 410
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034880126
Train loss (w/o reg) on all data: 0.0013270465
Test loss (w/o reg) on all data: 0.0033187578
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5739084e-07
Norm of the params: 6.5741405
     Influence (LOO): fixed 115 labels. Loss 0.00332. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003631209
Train loss (w/o reg) on all data: 0.0013501212
Test loss (w/o reg) on all data: 0.003313979
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.0812675e-08
Norm of the params: 6.754389
                Loss: fixed 114 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057445686
Train loss (w/o reg) on all data: 0.053345054
Test loss (w/o reg) on all data: 0.042393737
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.4083878e-06
Norm of the params: 9.056085
              Random: fixed   9 labels. Loss 0.04239. Accuracy 0.984.
### Flips: 820, rs: 32, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560465
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9518911e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8847832e-08
Norm of the params: 6.0928173
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055703178
Train loss (w/o reg) on all data: 0.05162
Test loss (w/o reg) on all data: 0.04171527
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.8639407e-06
Norm of the params: 9.03679
              Random: fixed  17 labels. Loss 0.04172. Accuracy 0.987.
### Flips: 820, rs: 32, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5888878e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.651463e-09
Norm of the params: 6.09282
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054907322
Train loss (w/o reg) on all data: 0.05086228
Test loss (w/o reg) on all data: 0.039799668
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.520114e-06
Norm of the params: 8.99449
              Random: fixed  20 labels. Loss 0.03980. Accuracy 0.988.
### Flips: 820, rs: 32, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096010877
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3384254e-08
Norm of the params: 6.092834
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.758643e-09
Norm of the params: 6.092814
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052409798
Train loss (w/o reg) on all data: 0.048389174
Test loss (w/o reg) on all data: 0.038135912
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.806394e-06
Norm of the params: 8.967299
              Random: fixed  27 labels. Loss 0.03814. Accuracy 0.989.
### Flips: 820, rs: 32, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0448912e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601177
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8464398e-09
Norm of the params: 6.092819
                Loss: fixed 116 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0518736
Train loss (w/o reg) on all data: 0.04780043
Test loss (w/o reg) on all data: 0.03722756
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.4090697e-05
Norm of the params: 9.025705
              Random: fixed  28 labels. Loss 0.03723. Accuracy 0.988.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061924916
Train loss (w/o reg) on all data: 0.057675634
Test loss (w/o reg) on all data: 0.040073935
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3557905e-06
Norm of the params: 9.218765
Flipped loss: 0.04007. Accuracy: 0.994
### Flips: 820, rs: 33, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007188472
Train loss (w/o reg) on all data: 0.004409065
Test loss (w/o reg) on all data: 0.007869523
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.752411e-07
Norm of the params: 7.4557457
     Influence (LOO): fixed  98 labels. Loss 0.00787. Accuracy 0.998.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.22390675e-08
Norm of the params: 6.092811
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060540684
Train loss (w/o reg) on all data: 0.056213897
Test loss (w/o reg) on all data: 0.038089957
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.4051506e-06
Norm of the params: 9.302459
              Random: fixed   5 labels. Loss 0.03809. Accuracy 0.992.
### Flips: 820, rs: 33, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010667
Test loss (w/o reg) on all data: 0.0026560351
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4466667e-08
Norm of the params: 6.0928364
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601307
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5077059e-08
Norm of the params: 6.0927973
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05848821
Train loss (w/o reg) on all data: 0.05401725
Test loss (w/o reg) on all data: 0.036821242
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3416559e-06
Norm of the params: 9.456172
              Random: fixed  10 labels. Loss 0.03682. Accuracy 0.992.
### Flips: 820, rs: 33, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011686
Test loss (w/o reg) on all data: 0.0026560472
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.526297e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6047567e-08
Norm of the params: 6.092819
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057587553
Train loss (w/o reg) on all data: 0.05309892
Test loss (w/o reg) on all data: 0.036321275
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.15616e-06
Norm of the params: 9.474844
              Random: fixed  12 labels. Loss 0.03632. Accuracy 0.991.
### Flips: 820, rs: 33, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.367002e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.624206e-08
Norm of the params: 6.092818
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055059467
Train loss (w/o reg) on all data: 0.050724167
Test loss (w/o reg) on all data: 0.0348364
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.3719915e-06
Norm of the params: 9.311605
              Random: fixed  18 labels. Loss 0.03484. Accuracy 0.990.
### Flips: 820, rs: 33, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009600265
Test loss (w/o reg) on all data: 0.0026558852
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.359938e-07
Norm of the params: 6.0929694
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.09362e-08
Norm of the params: 6.0928154
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05122415
Train loss (w/o reg) on all data: 0.04718823
Test loss (w/o reg) on all data: 0.031125456
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.034397e-06
Norm of the params: 8.984343
              Random: fixed  28 labels. Loss 0.03113. Accuracy 0.993.
### Flips: 820, rs: 33, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601297
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8436223e-08
Norm of the params: 6.092799
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1258483e-08
Norm of the params: 6.092822
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050235484
Train loss (w/o reg) on all data: 0.046200376
Test loss (w/o reg) on all data: 0.030529186
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.7439721e-06
Norm of the params: 8.983437
              Random: fixed  30 labels. Loss 0.03053. Accuracy 0.993.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0621848
Train loss (w/o reg) on all data: 0.058698136
Test loss (w/o reg) on all data: 0.038565185
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.4711087e-06
Norm of the params: 8.350646
Flipped loss: 0.03857. Accuracy: 0.987
### Flips: 820, rs: 34, checks: 205
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070768455
Train loss (w/o reg) on all data: 0.0040486054
Test loss (w/o reg) on all data: 0.0044295304
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0843608e-07
Norm of the params: 7.782339
     Influence (LOO): fixed  99 labels. Loss 0.00443. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0029973504
Train loss (w/o reg) on all data: 0.001064274
Test loss (w/o reg) on all data: 0.0036883156
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.506033e-09
Norm of the params: 6.2178397
                Loss: fixed 102 labels. Loss 0.00369. Accuracy 0.999.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060109783
Train loss (w/o reg) on all data: 0.05657868
Test loss (w/o reg) on all data: 0.038319085
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.1465967e-06
Norm of the params: 8.403694
              Random: fixed   6 labels. Loss 0.03832. Accuracy 0.989.
### Flips: 820, rs: 34, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5518232e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3439533e-08
Norm of the params: 6.092809
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058259323
Train loss (w/o reg) on all data: 0.05466978
Test loss (w/o reg) on all data: 0.037420582
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.7841695e-06
Norm of the params: 8.472952
              Random: fixed   9 labels. Loss 0.03742. Accuracy 0.987.
### Flips: 820, rs: 34, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601269
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2309426e-08
Norm of the params: 6.0928035
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.217191e-08
Norm of the params: 6.092824
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055560466
Train loss (w/o reg) on all data: 0.051958684
Test loss (w/o reg) on all data: 0.03645606
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.331541e-06
Norm of the params: 8.487382
              Random: fixed  15 labels. Loss 0.03646. Accuracy 0.987.
### Flips: 820, rs: 34, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7342524e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6386705e-08
Norm of the params: 6.092812
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053901594
Train loss (w/o reg) on all data: 0.05038809
Test loss (w/o reg) on all data: 0.03478773
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3965251e-06
Norm of the params: 8.382726
              Random: fixed  20 labels. Loss 0.03479. Accuracy 0.987.
### Flips: 820, rs: 34, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4936807e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1715957e-08
Norm of the params: 6.092822
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051220458
Train loss (w/o reg) on all data: 0.047652654
Test loss (w/o reg) on all data: 0.03320842
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.2392943e-06
Norm of the params: 8.447251
              Random: fixed  27 labels. Loss 0.03321. Accuracy 0.990.
### Flips: 820, rs: 34, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.45303085e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1407579e-08
Norm of the params: 6.092826
                Loss: fixed 103 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047676325
Train loss (w/o reg) on all data: 0.04418537
Test loss (w/o reg) on all data: 0.030515902
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.210911e-06
Norm of the params: 8.35578
              Random: fixed  35 labels. Loss 0.03052. Accuracy 0.991.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05950945
Train loss (w/o reg) on all data: 0.05565625
Test loss (w/o reg) on all data: 0.03491058
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.6610353e-06
Norm of the params: 8.778608
Flipped loss: 0.03491. Accuracy: 0.993
### Flips: 820, rs: 35, checks: 205
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042025065
Train loss (w/o reg) on all data: 0.0020581384
Test loss (w/o reg) on all data: 0.0034521057
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6155378e-08
Norm of the params: 6.548845
     Influence (LOO): fixed  95 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00355716
Train loss (w/o reg) on all data: 0.0013549911
Test loss (w/o reg) on all data: 0.002838925
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0797659e-07
Norm of the params: 6.636518
                Loss: fixed  95 labels. Loss 0.00284. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05784945
Train loss (w/o reg) on all data: 0.05412225
Test loss (w/o reg) on all data: 0.033313308
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.299466e-06
Norm of the params: 8.6338825
              Random: fixed   4 labels. Loss 0.03331. Accuracy 0.994.
### Flips: 820, rs: 35, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11605765e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.347444e-09
Norm of the params: 6.092819
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05532059
Train loss (w/o reg) on all data: 0.051451497
Test loss (w/o reg) on all data: 0.03141307
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5765517e-06
Norm of the params: 8.796697
              Random: fixed  10 labels. Loss 0.03141. Accuracy 0.995.
### Flips: 820, rs: 35, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7363929e-08
Norm of the params: 6.092819
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9058454e-08
Norm of the params: 6.0928097
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054061685
Train loss (w/o reg) on all data: 0.05002201
Test loss (w/o reg) on all data: 0.028450353
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.1695556e-06
Norm of the params: 8.988519
              Random: fixed  12 labels. Loss 0.02845. Accuracy 0.997.
### Flips: 820, rs: 35, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2493652e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9323126e-08
Norm of the params: 6.092816
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051956918
Train loss (w/o reg) on all data: 0.04783135
Test loss (w/o reg) on all data: 0.026404465
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6450625e-05
Norm of the params: 9.083576
              Random: fixed  18 labels. Loss 0.02640. Accuracy 0.998.
### Flips: 820, rs: 35, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.218823e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.002656045
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6522061e-08
Norm of the params: 6.0928197
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04932286
Train loss (w/o reg) on all data: 0.045197576
Test loss (w/o reg) on all data: 0.023721335
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.0867322e-06
Norm of the params: 9.083261
              Random: fixed  24 labels. Loss 0.02372. Accuracy 0.997.
### Flips: 820, rs: 35, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.929871e-08
Norm of the params: 6.092818
     Influence (LOO): fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.558816e-08
Norm of the params: 6.0928173
                Loss: fixed  96 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047340628
Train loss (w/o reg) on all data: 0.043156352
Test loss (w/o reg) on all data: 0.022778189
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0374069e-05
Norm of the params: 9.147979
              Random: fixed  27 labels. Loss 0.02278. Accuracy 0.998.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059147086
Train loss (w/o reg) on all data: 0.054795083
Test loss (w/o reg) on all data: 0.044574875
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6415057e-06
Norm of the params: 9.329529
Flipped loss: 0.04457. Accuracy: 0.986
### Flips: 820, rs: 36, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059448415
Train loss (w/o reg) on all data: 0.0033145775
Test loss (w/o reg) on all data: 0.0054416526
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5285458e-07
Norm of the params: 7.25295
     Influence (LOO): fixed  99 labels. Loss 0.00544. Accuracy 0.998.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.202999e-08
Norm of the params: 6.092804
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057325426
Train loss (w/o reg) on all data: 0.052992556
Test loss (w/o reg) on all data: 0.044006146
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.990468e-06
Norm of the params: 9.308995
              Random: fixed   4 labels. Loss 0.04401. Accuracy 0.989.
### Flips: 820, rs: 36, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.172165e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3238115e-08
Norm of the params: 6.092823
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05455526
Train loss (w/o reg) on all data: 0.050015956
Test loss (w/o reg) on all data: 0.042539887
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.560894e-06
Norm of the params: 9.528174
              Random: fixed  10 labels. Loss 0.04254. Accuracy 0.989.
### Flips: 820, rs: 36, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3117999e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.203464e-09
Norm of the params: 6.0928164
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05230345
Train loss (w/o reg) on all data: 0.04780139
Test loss (w/o reg) on all data: 0.038674425
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.8999355e-06
Norm of the params: 9.489001
              Random: fixed  17 labels. Loss 0.03867. Accuracy 0.988.
### Flips: 820, rs: 36, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601246
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.674857e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3119407e-08
Norm of the params: 6.0928235
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05007988
Train loss (w/o reg) on all data: 0.045482796
Test loss (w/o reg) on all data: 0.036547314
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.5448143e-06
Norm of the params: 9.58862
              Random: fixed  24 labels. Loss 0.03655. Accuracy 0.988.
### Flips: 820, rs: 36, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011447
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8906003e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.733347e-08
Norm of the params: 6.092809
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049926378
Train loss (w/o reg) on all data: 0.04549763
Test loss (w/o reg) on all data: 0.034907132
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.6665965e-06
Norm of the params: 9.411426
              Random: fixed  27 labels. Loss 0.03491. Accuracy 0.990.
### Flips: 820, rs: 36, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162408
Train loss (w/o reg) on all data: 0.0009601463
Test loss (w/o reg) on all data: 0.0026560987
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.804427e-08
Norm of the params: 6.0927734
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9023545e-08
Norm of the params: 6.0928206
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047914326
Train loss (w/o reg) on all data: 0.04361387
Test loss (w/o reg) on all data: 0.03425036
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2040858e-06
Norm of the params: 9.274114
              Random: fixed  33 labels. Loss 0.03425. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06122307
Train loss (w/o reg) on all data: 0.056306954
Test loss (w/o reg) on all data: 0.04223336
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2188306e-05
Norm of the params: 9.915763
Flipped loss: 0.04223. Accuracy: 0.986
### Flips: 820, rs: 37, checks: 205
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071671894
Train loss (w/o reg) on all data: 0.0038950308
Test loss (w/o reg) on all data: 0.004933328
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1040355e-07
Norm of the params: 8.089695
     Influence (LOO): fixed 104 labels. Loss 0.00493. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005084266
Train loss (w/o reg) on all data: 0.0022658098
Test loss (w/o reg) on all data: 0.005708414
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1630768e-07
Norm of the params: 7.5079374
                Loss: fixed 107 labels. Loss 0.00571. Accuracy 0.998.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057887666
Train loss (w/o reg) on all data: 0.052940186
Test loss (w/o reg) on all data: 0.03805115
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.5886027e-06
Norm of the params: 9.947343
              Random: fixed   8 labels. Loss 0.03805. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6504106e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601111
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3577684e-08
Norm of the params: 6.092829
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056561217
Train loss (w/o reg) on all data: 0.051675376
Test loss (w/o reg) on all data: 0.038418792
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.352347e-05
Norm of the params: 9.885181
              Random: fixed  11 labels. Loss 0.03842. Accuracy 0.988.
### Flips: 820, rs: 37, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7390222e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011185
Test loss (w/o reg) on all data: 0.0026560384
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5848713e-08
Norm of the params: 6.092829
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054258414
Train loss (w/o reg) on all data: 0.049269106
Test loss (w/o reg) on all data: 0.036838185
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.590402e-06
Norm of the params: 9.989302
              Random: fixed  16 labels. Loss 0.03684. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601258
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9381466e-08
Norm of the params: 6.092806
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601079
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1136423e-08
Norm of the params: 6.092835
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052975822
Train loss (w/o reg) on all data: 0.048090186
Test loss (w/o reg) on all data: 0.03414034
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.66918e-06
Norm of the params: 9.8849745
              Random: fixed  23 labels. Loss 0.03414. Accuracy 0.990.
### Flips: 820, rs: 37, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601329
Test loss (w/o reg) on all data: 0.0026560877
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.455973e-08
Norm of the params: 6.0927944
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0142634e-08
Norm of the params: 6.0928254
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051168483
Train loss (w/o reg) on all data: 0.046237323
Test loss (w/o reg) on all data: 0.031302657
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.845537e-06
Norm of the params: 9.93092
              Random: fixed  29 labels. Loss 0.03130. Accuracy 0.990.
### Flips: 820, rs: 37, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.04245474e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17806565e-08
Norm of the params: 6.0928206
                Loss: fixed 111 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04866873
Train loss (w/o reg) on all data: 0.043883137
Test loss (w/o reg) on all data: 0.030294117
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.258088e-06
Norm of the params: 9.783245
              Random: fixed  36 labels. Loss 0.03029. Accuracy 0.991.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06232179
Train loss (w/o reg) on all data: 0.05744169
Test loss (w/o reg) on all data: 0.04251027
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.8097977e-06
Norm of the params: 9.8793745
Flipped loss: 0.04251. Accuracy: 0.988
### Flips: 820, rs: 38, checks: 205
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005785918
Train loss (w/o reg) on all data: 0.0032863992
Test loss (w/o reg) on all data: 0.0037968536
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9404096e-07
Norm of the params: 7.070387
     Influence (LOO): fixed  96 labels. Loss 0.00380. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005495299
Train loss (w/o reg) on all data: 0.002622849
Test loss (w/o reg) on all data: 0.0040920386
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4416696e-07
Norm of the params: 7.5795116
                Loss: fixed  97 labels. Loss 0.00409. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059219465
Train loss (w/o reg) on all data: 0.054457273
Test loss (w/o reg) on all data: 0.039919775
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.6370055e-06
Norm of the params: 9.759295
              Random: fixed   8 labels. Loss 0.03992. Accuracy 0.991.
### Flips: 820, rs: 38, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7138449e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.97465e-09
Norm of the params: 6.092809
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056467965
Train loss (w/o reg) on all data: 0.051867705
Test loss (w/o reg) on all data: 0.038962666
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6752447e-06
Norm of the params: 9.591936
              Random: fixed  14 labels. Loss 0.03896. Accuracy 0.989.
### Flips: 820, rs: 38, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.967361e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4624895e-08
Norm of the params: 6.092811
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055018365
Train loss (w/o reg) on all data: 0.05048674
Test loss (w/o reg) on all data: 0.03806248
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.605668e-06
Norm of the params: 9.520112
              Random: fixed  19 labels. Loss 0.03806. Accuracy 0.988.
### Flips: 820, rs: 38, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.48619215e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.65345e-08
Norm of the params: 6.092808
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051507324
Train loss (w/o reg) on all data: 0.047100812
Test loss (w/o reg) on all data: 0.03376842
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.000603e-06
Norm of the params: 9.387769
              Random: fixed  27 labels. Loss 0.03377. Accuracy 0.989.
### Flips: 820, rs: 38, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.34154075e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.351699e-09
Norm of the params: 6.0928135
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049702026
Train loss (w/o reg) on all data: 0.045362912
Test loss (w/o reg) on all data: 0.03157239
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8601238e-06
Norm of the params: 9.315701
              Random: fixed  31 labels. Loss 0.03157. Accuracy 0.990.
### Flips: 820, rs: 38, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.622595e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3552005e-08
Norm of the params: 6.0928235
                Loss: fixed 101 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0483074
Train loss (w/o reg) on all data: 0.043750454
Test loss (w/o reg) on all data: 0.030069744
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.205089e-06
Norm of the params: 9.546673
              Random: fixed  36 labels. Loss 0.03007. Accuracy 0.993.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062113676
Train loss (w/o reg) on all data: 0.05814671
Test loss (w/o reg) on all data: 0.038002364
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.857742e-06
Norm of the params: 8.907262
Flipped loss: 0.03800. Accuracy: 0.990
### Flips: 820, rs: 39, checks: 205
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00795972
Train loss (w/o reg) on all data: 0.0047729546
Test loss (w/o reg) on all data: 0.0057686856
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2832133e-07
Norm of the params: 7.98344
     Influence (LOO): fixed 100 labels. Loss 0.00577. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033921348
Train loss (w/o reg) on all data: 0.0012257945
Test loss (w/o reg) on all data: 0.003671085
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5026498e-08
Norm of the params: 6.58231
                Loss: fixed 105 labels. Loss 0.00367. Accuracy 0.999.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06063143
Train loss (w/o reg) on all data: 0.0566878
Test loss (w/o reg) on all data: 0.037432257
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.1583642e-06
Norm of the params: 8.881026
              Random: fixed   4 labels. Loss 0.03743. Accuracy 0.991.
### Flips: 820, rs: 39, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.426246e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601177
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0184864e-08
Norm of the params: 6.0928183
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057130024
Train loss (w/o reg) on all data: 0.052924946
Test loss (w/o reg) on all data: 0.033769127
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.1066602e-06
Norm of the params: 9.170692
              Random: fixed  15 labels. Loss 0.03377. Accuracy 0.990.
### Flips: 820, rs: 39, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.332037e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0319696e-08
Norm of the params: 6.0928154
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055190682
Train loss (w/o reg) on all data: 0.051022302
Test loss (w/o reg) on all data: 0.031071294
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.3496015e-06
Norm of the params: 9.130586
              Random: fixed  21 labels. Loss 0.03107. Accuracy 0.990.
### Flips: 820, rs: 39, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3790229e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2071341e-08
Norm of the params: 6.092822
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055190682
Train loss (w/o reg) on all data: 0.051022038
Test loss (w/o reg) on all data: 0.031069223
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7531544e-06
Norm of the params: 9.1308775
              Random: fixed  21 labels. Loss 0.03107. Accuracy 0.990.
### Flips: 820, rs: 39, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3679515e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1838049e-08
Norm of the params: 6.092822
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053206615
Train loss (w/o reg) on all data: 0.048949547
Test loss (w/o reg) on all data: 0.030641068
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1079857e-05
Norm of the params: 9.227208
              Random: fixed  24 labels. Loss 0.03064. Accuracy 0.991.
### Flips: 820, rs: 39, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4786774e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601115
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9174067e-08
Norm of the params: 6.092829
                Loss: fixed 106 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05007061
Train loss (w/o reg) on all data: 0.04579722
Test loss (w/o reg) on all data: 0.028059512
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8905303e-06
Norm of the params: 9.24488
              Random: fixed  32 labels. Loss 0.02806. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071189776
Train loss (w/o reg) on all data: 0.067707315
Test loss (w/o reg) on all data: 0.047931075
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.922699e-06
Norm of the params: 8.34561
Flipped loss: 0.04793. Accuracy: 0.990
### Flips: 1025, rs: 0, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013502464
Train loss (w/o reg) on all data: 0.009811017
Test loss (w/o reg) on all data: 0.008626925
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.941784e-07
Norm of the params: 8.592377
     Influence (LOO): fixed 112 labels. Loss 0.00863. Accuracy 0.999.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056594433
Train loss (w/o reg) on all data: 0.0025778702
Test loss (w/o reg) on all data: 0.008136332
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.229806e-07
Norm of the params: 7.850571
                Loss: fixed 125 labels. Loss 0.00814. Accuracy 0.997.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069555625
Train loss (w/o reg) on all data: 0.06625987
Test loss (w/o reg) on all data: 0.045387942
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0341065e-05
Norm of the params: 8.118815
              Random: fixed   9 labels. Loss 0.04539. Accuracy 0.993.
### Flips: 1025, rs: 0, checks: 410
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031620434
Train loss (w/o reg) on all data: 0.0011657865
Test loss (w/o reg) on all data: 0.004207649
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.05468e-08
Norm of the params: 6.3186345
     Influence (LOO): fixed 130 labels. Loss 0.00421. Accuracy 0.999.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010906
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.874631e-08
Norm of the params: 6.0928316
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06651712
Train loss (w/o reg) on all data: 0.06316094
Test loss (w/o reg) on all data: 0.04231216
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.610432e-06
Norm of the params: 8.192899
              Random: fixed  17 labels. Loss 0.04231. Accuracy 0.993.
### Flips: 1025, rs: 0, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.623551e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.432176e-08
Norm of the params: 6.092812
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06538927
Train loss (w/o reg) on all data: 0.062009413
Test loss (w/o reg) on all data: 0.037428007
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.847681e-06
Norm of the params: 8.221747
              Random: fixed  25 labels. Loss 0.03743. Accuracy 0.995.
### Flips: 1025, rs: 0, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601344
Test loss (w/o reg) on all data: 0.0026560854
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5151093e-08
Norm of the params: 6.0927925
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6902645e-08
Norm of the params: 6.0928235
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06251885
Train loss (w/o reg) on all data: 0.05919526
Test loss (w/o reg) on all data: 0.0362397
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.686389e-06
Norm of the params: 8.153027
              Random: fixed  32 labels. Loss 0.03624. Accuracy 0.994.
### Flips: 1025, rs: 0, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4555233e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601259
Test loss (w/o reg) on all data: 0.0026560798
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8252559e-08
Norm of the params: 6.0928054
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0603664
Train loss (w/o reg) on all data: 0.056966074
Test loss (w/o reg) on all data: 0.033680186
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9277206e-06
Norm of the params: 8.246606
              Random: fixed  38 labels. Loss 0.03368. Accuracy 0.995.
### Flips: 1025, rs: 0, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601594
Test loss (w/o reg) on all data: 0.002656117
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9193355e-08
Norm of the params: 6.092751
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9022104e-08
Norm of the params: 6.092818
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0566687
Train loss (w/o reg) on all data: 0.05318139
Test loss (w/o reg) on all data: 0.03302958
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.750254e-06
Norm of the params: 8.351416
              Random: fixed  46 labels. Loss 0.03303. Accuracy 0.996.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06942299
Train loss (w/o reg) on all data: 0.06534038
Test loss (w/o reg) on all data: 0.045748584
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.1922746e-06
Norm of the params: 9.036163
Flipped loss: 0.04575. Accuracy: 0.986
### Flips: 1025, rs: 1, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011576994
Train loss (w/o reg) on all data: 0.008181228
Test loss (w/o reg) on all data: 0.008930406
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5365812e-07
Norm of the params: 8.2410755
     Influence (LOO): fixed 113 labels. Loss 0.00893. Accuracy 0.999.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004414445
Train loss (w/o reg) on all data: 0.0019268346
Test loss (w/o reg) on all data: 0.0035603915
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3926415e-07
Norm of the params: 7.0535245
                Loss: fixed 124 labels. Loss 0.00356. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06830137
Train loss (w/o reg) on all data: 0.06417725
Test loss (w/o reg) on all data: 0.044344295
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1869896e-05
Norm of the params: 9.081985
              Random: fixed   4 labels. Loss 0.04434. Accuracy 0.986.
### Flips: 1025, rs: 1, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004196245
Train loss (w/o reg) on all data: 0.002025619
Test loss (w/o reg) on all data: 0.00399217
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.473236e-08
Norm of the params: 6.588818
     Influence (LOO): fixed 125 labels. Loss 0.00399. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034181955
Train loss (w/o reg) on all data: 0.001264381
Test loss (w/o reg) on all data: 0.002928329
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.901443e-08
Norm of the params: 6.563253
                Loss: fixed 126 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06712856
Train loss (w/o reg) on all data: 0.06305298
Test loss (w/o reg) on all data: 0.041605283
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7929696e-06
Norm of the params: 9.028376
              Random: fixed  10 labels. Loss 0.04161. Accuracy 0.990.
### Flips: 1025, rs: 1, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2993179e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9423046e-08
Norm of the params: 6.0928063
                Loss: fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064991355
Train loss (w/o reg) on all data: 0.060852244
Test loss (w/o reg) on all data: 0.038942073
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0391325e-05
Norm of the params: 9.098473
              Random: fixed  17 labels. Loss 0.03894. Accuracy 0.993.
### Flips: 1025, rs: 1, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013077
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1820555e-08
Norm of the params: 6.0927978
     Influence (LOO): fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0610994e-08
Norm of the params: 6.092825
                Loss: fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06275004
Train loss (w/o reg) on all data: 0.0585183
Test loss (w/o reg) on all data: 0.03785395
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.0556818e-06
Norm of the params: 9.199716
              Random: fixed  24 labels. Loss 0.03785. Accuracy 0.992.
### Flips: 1025, rs: 1, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012827
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3998348e-08
Norm of the params: 6.092801
     Influence (LOO): fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7257362e-08
Norm of the params: 6.0928254
                Loss: fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602988
Train loss (w/o reg) on all data: 0.056112338
Test loss (w/o reg) on all data: 0.035106022
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.385035e-06
Norm of the params: 9.150371
              Random: fixed  32 labels. Loss 0.03511. Accuracy 0.993.
### Flips: 1025, rs: 1, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8478244e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5902677e-08
Norm of the params: 6.0928288
                Loss: fixed 127 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055472806
Train loss (w/o reg) on all data: 0.051260415
Test loss (w/o reg) on all data: 0.032077946
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.8922013e-06
Norm of the params: 9.1786585
              Random: fixed  43 labels. Loss 0.03208. Accuracy 0.993.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07073122
Train loss (w/o reg) on all data: 0.066586256
Test loss (w/o reg) on all data: 0.057705365
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.1540516e-06
Norm of the params: 9.10491
Flipped loss: 0.05771. Accuracy: 0.976
### Flips: 1025, rs: 2, checks: 205
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016594896
Train loss (w/o reg) on all data: 0.012479803
Test loss (w/o reg) on all data: 0.013492236
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.3752765e-07
Norm of the params: 9.072038
     Influence (LOO): fixed 118 labels. Loss 0.01349. Accuracy 0.997.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010066232
Train loss (w/o reg) on all data: 0.005793889
Test loss (w/o reg) on all data: 0.010425813
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.854187e-07
Norm of the params: 9.243748
                Loss: fixed 127 labels. Loss 0.01043. Accuracy 0.998.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067517355
Train loss (w/o reg) on all data: 0.06325437
Test loss (w/o reg) on all data: 0.053365286
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1193053e-06
Norm of the params: 9.23362
              Random: fixed  13 labels. Loss 0.05337. Accuracy 0.979.
### Flips: 1025, rs: 2, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004841199
Train loss (w/o reg) on all data: 0.002447853
Test loss (w/o reg) on all data: 0.005156286
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5025199e-07
Norm of the params: 6.918592
     Influence (LOO): fixed 137 labels. Loss 0.00516. Accuracy 0.998.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038886121
Train loss (w/o reg) on all data: 0.0015265885
Test loss (w/o reg) on all data: 0.0074313534
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.502421e-08
Norm of the params: 6.87317
                Loss: fixed 137 labels. Loss 0.00743. Accuracy 0.999.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0671864
Train loss (w/o reg) on all data: 0.06281539
Test loss (w/o reg) on all data: 0.052762806
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.6606835e-06
Norm of the params: 9.349876
              Random: fixed  15 labels. Loss 0.05276. Accuracy 0.980.
### Flips: 1025, rs: 2, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4991816e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4673472e-08
Norm of the params: 6.092808
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0665108
Train loss (w/o reg) on all data: 0.062169634
Test loss (w/o reg) on all data: 0.0514276
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.1818514e-05
Norm of the params: 9.317897
              Random: fixed  18 labels. Loss 0.05143. Accuracy 0.983.
### Flips: 1025, rs: 2, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.006396e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601046
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5313165e-08
Norm of the params: 6.09284
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0644978
Train loss (w/o reg) on all data: 0.060135055
Test loss (w/o reg) on all data: 0.050948035
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.1344928e-06
Norm of the params: 9.34103
              Random: fixed  24 labels. Loss 0.05095. Accuracy 0.982.
### Flips: 1025, rs: 2, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601128
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8007987e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601272
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6067449e-08
Norm of the params: 6.0928035
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063398086
Train loss (w/o reg) on all data: 0.05914527
Test loss (w/o reg) on all data: 0.047822207
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.8966676e-06
Norm of the params: 9.222594
              Random: fixed  30 labels. Loss 0.04782. Accuracy 0.985.
### Flips: 1025, rs: 2, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601239
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3977389e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.801425e-08
Norm of the params: 6.092825
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060808774
Train loss (w/o reg) on all data: 0.05655282
Test loss (w/o reg) on all data: 0.045675907
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.2025567e-06
Norm of the params: 9.226002
              Random: fixed  39 labels. Loss 0.04568. Accuracy 0.985.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067684576
Train loss (w/o reg) on all data: 0.06358633
Test loss (w/o reg) on all data: 0.04640169
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.2137153e-06
Norm of the params: 9.05345
Flipped loss: 0.04640. Accuracy: 0.984
### Flips: 1025, rs: 3, checks: 205
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013776911
Train loss (w/o reg) on all data: 0.010647378
Test loss (w/o reg) on all data: 0.010110215
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.5342267e-07
Norm of the params: 7.911427
     Influence (LOO): fixed 111 labels. Loss 0.01011. Accuracy 0.997.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0045558857
Train loss (w/o reg) on all data: 0.0018901285
Test loss (w/o reg) on all data: 0.0049189897
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.452773e-08
Norm of the params: 7.3017225
                Loss: fixed 122 labels. Loss 0.00492. Accuracy 0.998.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06663765
Train loss (w/o reg) on all data: 0.06259395
Test loss (w/o reg) on all data: 0.046192255
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.2019393e-05
Norm of the params: 8.992993
              Random: fixed   5 labels. Loss 0.04619. Accuracy 0.985.
### Flips: 1025, rs: 3, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2667277e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5582782e-08
Norm of the params: 6.0928087
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065051004
Train loss (w/o reg) on all data: 0.061228387
Test loss (w/o reg) on all data: 0.04409363
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.064562e-06
Norm of the params: 8.7437
              Random: fixed  12 labels. Loss 0.04409. Accuracy 0.985.
### Flips: 1025, rs: 3, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9826963e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5328078e-08
Norm of the params: 6.092819
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063270785
Train loss (w/o reg) on all data: 0.059595313
Test loss (w/o reg) on all data: 0.040359143
Train acc on all data:  0.976173109652322
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.060542e-06
Norm of the params: 8.573766
              Random: fixed  19 labels. Loss 0.04036. Accuracy 0.989.
### Flips: 1025, rs: 3, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3334301e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.30678535e-08
Norm of the params: 6.092809
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061405487
Train loss (w/o reg) on all data: 0.057658155
Test loss (w/o reg) on all data: 0.04045384
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.3202073e-06
Norm of the params: 8.657175
              Random: fixed  22 labels. Loss 0.04045. Accuracy 0.989.
### Flips: 1025, rs: 3, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3644297e-08
Norm of the params: 6.0928288
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013036
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.844975e-08
Norm of the params: 6.0927973
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059368
Train loss (w/o reg) on all data: 0.05562742
Test loss (w/o reg) on all data: 0.040178243
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.9255186e-06
Norm of the params: 8.649367
              Random: fixed  26 labels. Loss 0.04018. Accuracy 0.987.
### Flips: 1025, rs: 3, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.002656051
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1594913e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012536
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8691075e-08
Norm of the params: 6.092806
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056386802
Train loss (w/o reg) on all data: 0.052651633
Test loss (w/o reg) on all data: 0.038483147
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4613439e-05
Norm of the params: 8.643111
              Random: fixed  33 labels. Loss 0.03848. Accuracy 0.990.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07502421
Train loss (w/o reg) on all data: 0.07089089
Test loss (w/o reg) on all data: 0.060638543
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.093942e-06
Norm of the params: 9.092106
Flipped loss: 0.06064. Accuracy: 0.982
### Flips: 1025, rs: 4, checks: 205
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021359857
Train loss (w/o reg) on all data: 0.017049229
Test loss (w/o reg) on all data: 0.0115067
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.9793695e-07
Norm of the params: 9.285072
     Influence (LOO): fixed 120 labels. Loss 0.01151. Accuracy 0.998.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011858914
Train loss (w/o reg) on all data: 0.007113569
Test loss (w/o reg) on all data: 0.010049883
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.430445e-07
Norm of the params: 9.742017
                Loss: fixed 130 labels. Loss 0.01005. Accuracy 0.997.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07305473
Train loss (w/o reg) on all data: 0.06869204
Test loss (w/o reg) on all data: 0.060090266
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 5.6572253e-06
Norm of the params: 9.340969
              Random: fixed   7 labels. Loss 0.06009. Accuracy 0.980.
### Flips: 1025, rs: 4, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0064591337
Train loss (w/o reg) on all data: 0.0036803973
Test loss (w/o reg) on all data: 0.0043657636
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0095953e-06
Norm of the params: 7.454846
     Influence (LOO): fixed 143 labels. Loss 0.00437. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033921343
Train loss (w/o reg) on all data: 0.0012257807
Test loss (w/o reg) on all data: 0.0036710475
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5096055e-08
Norm of the params: 6.58233
                Loss: fixed 146 labels. Loss 0.00367. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07006303
Train loss (w/o reg) on all data: 0.0656653
Test loss (w/o reg) on all data: 0.058352865
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.684477e-06
Norm of the params: 9.378413
              Random: fixed  14 labels. Loss 0.05835. Accuracy 0.980.
### Flips: 1025, rs: 4, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2706404e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8890686e-08
Norm of the params: 6.0928164
                Loss: fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06782305
Train loss (w/o reg) on all data: 0.06340292
Test loss (w/o reg) on all data: 0.055095434
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.2742983e-06
Norm of the params: 9.40227
              Random: fixed  20 labels. Loss 0.05510. Accuracy 0.981.
### Flips: 1025, rs: 4, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162387
Train loss (w/o reg) on all data: 0.0009600855
Test loss (w/o reg) on all data: 0.0026560156
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2107357e-08
Norm of the params: 6.09287
     Influence (LOO): fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9118032e-08
Norm of the params: 6.092817
                Loss: fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06684471
Train loss (w/o reg) on all data: 0.062484708
Test loss (w/o reg) on all data: 0.051382653
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.636336e-06
Norm of the params: 9.338099
              Random: fixed  25 labels. Loss 0.05138. Accuracy 0.985.
### Flips: 1025, rs: 4, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3710414e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0509192e-08
Norm of the params: 6.092814
                Loss: fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06612282
Train loss (w/o reg) on all data: 0.061912317
Test loss (w/o reg) on all data: 0.04837532
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.85742e-06
Norm of the params: 9.176604
              Random: fixed  30 labels. Loss 0.04838. Accuracy 0.986.
### Flips: 1025, rs: 4, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7544196e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3329428e-08
Norm of the params: 6.0928054
                Loss: fixed 147 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062336262
Train loss (w/o reg) on all data: 0.057915322
Test loss (w/o reg) on all data: 0.043287225
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0608498e-05
Norm of the params: 9.403127
              Random: fixed  42 labels. Loss 0.04329. Accuracy 0.990.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06539923
Train loss (w/o reg) on all data: 0.061644766
Test loss (w/o reg) on all data: 0.03820097
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0777238e-05
Norm of the params: 8.665405
Flipped loss: 0.03820. Accuracy: 0.993
### Flips: 1025, rs: 5, checks: 205
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005304494
Train loss (w/o reg) on all data: 0.002835948
Test loss (w/o reg) on all data: 0.0043905117
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.8401464e-08
Norm of the params: 7.0264444
     Influence (LOO): fixed 111 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040726573
Train loss (w/o reg) on all data: 0.0016647164
Test loss (w/o reg) on all data: 0.0037560551
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.4353754e-08
Norm of the params: 6.939656
                Loss: fixed 111 labels. Loss 0.00376. Accuracy 0.999.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063067675
Train loss (w/o reg) on all data: 0.059213653
Test loss (w/o reg) on all data: 0.03702977
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0964922e-05
Norm of the params: 8.779548
              Random: fixed   6 labels. Loss 0.03703. Accuracy 0.992.
### Flips: 1025, rs: 5, checks: 410
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5660758e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5305407e-08
Norm of the params: 6.092807
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061055135
Train loss (w/o reg) on all data: 0.057161305
Test loss (w/o reg) on all data: 0.035733584
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2517487e-05
Norm of the params: 8.824773
              Random: fixed  10 labels. Loss 0.03573. Accuracy 0.995.
### Flips: 1025, rs: 5, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4602588e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.08832e-08
Norm of the params: 6.0928164
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05879256
Train loss (w/o reg) on all data: 0.05476881
Test loss (w/o reg) on all data: 0.03380889
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.0363877e-06
Norm of the params: 8.970792
              Random: fixed  17 labels. Loss 0.03381. Accuracy 0.992.
### Flips: 1025, rs: 5, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.732078e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7369823e-08
Norm of the params: 6.092821
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05802635
Train loss (w/o reg) on all data: 0.05394111
Test loss (w/o reg) on all data: 0.03305767
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3248149e-05
Norm of the params: 9.039076
              Random: fixed  18 labels. Loss 0.03306. Accuracy 0.992.
### Flips: 1025, rs: 5, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1382302e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601118
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8731665e-08
Norm of the params: 6.0928283
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05614148
Train loss (w/o reg) on all data: 0.052031435
Test loss (w/o reg) on all data: 0.03035576
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.183411e-06
Norm of the params: 9.06647
              Random: fixed  23 labels. Loss 0.03036. Accuracy 0.996.
### Flips: 1025, rs: 5, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6529354e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6523337e-08
Norm of the params: 6.0928183
                Loss: fixed 113 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05340037
Train loss (w/o reg) on all data: 0.049094796
Test loss (w/o reg) on all data: 0.027125677
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.7098777e-06
Norm of the params: 9.279628
              Random: fixed  31 labels. Loss 0.02713. Accuracy 0.996.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0708796
Train loss (w/o reg) on all data: 0.06651686
Test loss (w/o reg) on all data: 0.04238596
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3023541e-05
Norm of the params: 9.34103
Flipped loss: 0.04239. Accuracy: 0.991
### Flips: 1025, rs: 6, checks: 205
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013384044
Train loss (w/o reg) on all data: 0.009645601
Test loss (w/o reg) on all data: 0.010111394
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1940531e-06
Norm of the params: 8.646899
     Influence (LOO): fixed 109 labels. Loss 0.01011. Accuracy 0.997.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005221355
Train loss (w/o reg) on all data: 0.002404074
Test loss (w/o reg) on all data: 0.004646167
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7308286e-07
Norm of the params: 7.506372
                Loss: fixed 120 labels. Loss 0.00465. Accuracy 0.999.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06787356
Train loss (w/o reg) on all data: 0.06358849
Test loss (w/o reg) on all data: 0.040308
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.2627655e-06
Norm of the params: 9.2575
              Random: fixed   8 labels. Loss 0.04031. Accuracy 0.992.
### Flips: 1025, rs: 6, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030695184
Train loss (w/o reg) on all data: 0.0011410358
Test loss (w/o reg) on all data: 0.0028270544
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8499334e-08
Norm of the params: 6.210447
     Influence (LOO): fixed 124 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.23264545e-08
Norm of the params: 6.09281
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067186534
Train loss (w/o reg) on all data: 0.06294567
Test loss (w/o reg) on all data: 0.03864464
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.367087e-06
Norm of the params: 9.20963
              Random: fixed  11 labels. Loss 0.03864. Accuracy 0.993.
### Flips: 1025, rs: 6, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1125457e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011197
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0347986e-08
Norm of the params: 6.092828
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0643825
Train loss (w/o reg) on all data: 0.060116384
Test loss (w/o reg) on all data: 0.036438182
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.943893e-06
Norm of the params: 9.237012
              Random: fixed  20 labels. Loss 0.03644. Accuracy 0.993.
### Flips: 1025, rs: 6, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.009106e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601102
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3272849e-08
Norm of the params: 6.092831
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0632714
Train loss (w/o reg) on all data: 0.058970448
Test loss (w/o reg) on all data: 0.035715863
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8764216e-06
Norm of the params: 9.274653
              Random: fixed  23 labels. Loss 0.03572. Accuracy 0.993.
### Flips: 1025, rs: 6, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560405
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.278988e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601323
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4906077e-08
Norm of the params: 6.0927944
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061238658
Train loss (w/o reg) on all data: 0.056996875
Test loss (w/o reg) on all data: 0.034486923
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.249282e-06
Norm of the params: 9.210629
              Random: fixed  30 labels. Loss 0.03449. Accuracy 0.994.
### Flips: 1025, rs: 6, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.865625e-08
Norm of the params: 6.092815
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.21051675e-08
Norm of the params: 6.092822
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059593014
Train loss (w/o reg) on all data: 0.055382207
Test loss (w/o reg) on all data: 0.031194221
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.8248128e-06
Norm of the params: 9.176937
              Random: fixed  35 labels. Loss 0.03119. Accuracy 0.995.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0740489
Train loss (w/o reg) on all data: 0.070416614
Test loss (w/o reg) on all data: 0.050209377
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.1304456e-06
Norm of the params: 8.523249
Flipped loss: 0.05021. Accuracy: 0.988
### Flips: 1025, rs: 7, checks: 205
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016692124
Train loss (w/o reg) on all data: 0.013017761
Test loss (w/o reg) on all data: 0.01212718
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2405584e-06
Norm of the params: 8.572474
     Influence (LOO): fixed 117 labels. Loss 0.01213. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007973418
Train loss (w/o reg) on all data: 0.0041354154
Test loss (w/o reg) on all data: 0.0084311245
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.407439e-07
Norm of the params: 8.761281
                Loss: fixed 133 labels. Loss 0.00843. Accuracy 0.997.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06937895
Train loss (w/o reg) on all data: 0.065688476
Test loss (w/o reg) on all data: 0.04570214
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.1277252e-06
Norm of the params: 8.591244
              Random: fixed  13 labels. Loss 0.04570. Accuracy 0.990.
### Flips: 1025, rs: 7, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004787836
Train loss (w/o reg) on all data: 0.0023320285
Test loss (w/o reg) on all data: 0.0043394174
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.933219e-08
Norm of the params: 7.0082917
     Influence (LOO): fixed 139 labels. Loss 0.00434. Accuracy 0.999.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038792342
Train loss (w/o reg) on all data: 0.0014944371
Test loss (w/o reg) on all data: 0.0036702256
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.156277e-08
Norm of the params: 6.9062243
                Loss: fixed 140 labels. Loss 0.00367. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06776608
Train loss (w/o reg) on all data: 0.064001076
Test loss (w/o reg) on all data: 0.042089377
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.335147e-06
Norm of the params: 8.677561
              Random: fixed  22 labels. Loss 0.04209. Accuracy 0.993.
### Flips: 1025, rs: 7, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6474065e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3938391e-08
Norm of the params: 6.0928082
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06571674
Train loss (w/o reg) on all data: 0.061852433
Test loss (w/o reg) on all data: 0.0397752
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.3705632e-06
Norm of the params: 8.791262
              Random: fixed  26 labels. Loss 0.03978. Accuracy 0.993.
### Flips: 1025, rs: 7, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.169794e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8659906e-08
Norm of the params: 6.092822
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064774945
Train loss (w/o reg) on all data: 0.060950655
Test loss (w/o reg) on all data: 0.038686443
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.70303e-06
Norm of the params: 8.745613
              Random: fixed  28 labels. Loss 0.03869. Accuracy 0.993.
### Flips: 1025, rs: 7, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.101457e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.972957e-08
Norm of the params: 6.0928264
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06254626
Train loss (w/o reg) on all data: 0.0586878
Test loss (w/o reg) on all data: 0.037349608
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.2506e-06
Norm of the params: 8.784607
              Random: fixed  36 labels. Loss 0.03735. Accuracy 0.995.
### Flips: 1025, rs: 7, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010487
Test loss (w/o reg) on all data: 0.0026560277
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4394184e-08
Norm of the params: 6.09284
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601333
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8880953e-08
Norm of the params: 6.092793
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061061278
Train loss (w/o reg) on all data: 0.057302278
Test loss (w/o reg) on all data: 0.034308556
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.6529293e-06
Norm of the params: 8.670642
              Random: fixed  44 labels. Loss 0.03431. Accuracy 0.993.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070721954
Train loss (w/o reg) on all data: 0.06649594
Test loss (w/o reg) on all data: 0.05116338
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4039193e-06
Norm of the params: 9.193494
Flipped loss: 0.05116. Accuracy: 0.982
### Flips: 1025, rs: 8, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015829466
Train loss (w/o reg) on all data: 0.0120014865
Test loss (w/o reg) on all data: 0.010986073
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.907738e-06
Norm of the params: 8.749833
     Influence (LOO): fixed 120 labels. Loss 0.01099. Accuracy 0.997.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008814588
Train loss (w/o reg) on all data: 0.0046733813
Test loss (w/o reg) on all data: 0.008627481
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.628548e-07
Norm of the params: 9.100778
                Loss: fixed 130 labels. Loss 0.00863. Accuracy 0.998.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06922892
Train loss (w/o reg) on all data: 0.065133795
Test loss (w/o reg) on all data: 0.046263024
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2909354e-05
Norm of the params: 9.049996
              Random: fixed  10 labels. Loss 0.04626. Accuracy 0.986.
### Flips: 1025, rs: 8, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601318
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9767953e-08
Norm of the params: 6.0927954
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.002656049
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3078492e-08
Norm of the params: 6.09283
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06706481
Train loss (w/o reg) on all data: 0.063077174
Test loss (w/o reg) on all data: 0.04460799
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.74356e-06
Norm of the params: 8.930435
              Random: fixed  16 labels. Loss 0.04461. Accuracy 0.986.
### Flips: 1025, rs: 8, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3507633e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9841774e-08
Norm of the params: 6.0928254
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06452559
Train loss (w/o reg) on all data: 0.060345866
Test loss (w/o reg) on all data: 0.041760266
Train acc on all data:  0.975686846584002
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.3580166e-06
Norm of the params: 9.142998
              Random: fixed  26 labels. Loss 0.04176. Accuracy 0.989.
### Flips: 1025, rs: 8, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5593963e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5834924e-08
Norm of the params: 6.0928144
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06284704
Train loss (w/o reg) on all data: 0.058904275
Test loss (w/o reg) on all data: 0.03785525
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8839265e-06
Norm of the params: 8.880051
              Random: fixed  36 labels. Loss 0.03786. Accuracy 0.994.
### Flips: 1025, rs: 8, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.370477e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3713438e-08
Norm of the params: 6.092818
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05941189
Train loss (w/o reg) on all data: 0.05537086
Test loss (w/o reg) on all data: 0.036709428
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.5058596e-06
Norm of the params: 8.99003
              Random: fixed  43 labels. Loss 0.03671. Accuracy 0.993.
### Flips: 1025, rs: 8, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0770177e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4775924e-08
Norm of the params: 6.092816
                Loss: fixed 141 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053846154
Train loss (w/o reg) on all data: 0.04965268
Test loss (w/o reg) on all data: 0.034300596
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.638419e-06
Norm of the params: 9.158028
              Random: fixed  54 labels. Loss 0.03430. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07161014
Train loss (w/o reg) on all data: 0.06733199
Test loss (w/o reg) on all data: 0.050041992
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.6187411e-06
Norm of the params: 9.250027
Flipped loss: 0.05004. Accuracy: 0.985
### Flips: 1025, rs: 9, checks: 205
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016542392
Train loss (w/o reg) on all data: 0.012669285
Test loss (w/o reg) on all data: 0.009278887
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.9626506e-07
Norm of the params: 8.801259
     Influence (LOO): fixed 115 labels. Loss 0.00928. Accuracy 0.998.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076357527
Train loss (w/o reg) on all data: 0.0038285146
Test loss (w/o reg) on all data: 0.006101141
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8225376e-07
Norm of the params: 8.726097
                Loss: fixed 131 labels. Loss 0.00610. Accuracy 0.999.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069708385
Train loss (w/o reg) on all data: 0.065712705
Test loss (w/o reg) on all data: 0.045865953
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.7739034e-06
Norm of the params: 8.939443
              Random: fixed   8 labels. Loss 0.04587. Accuracy 0.987.
### Flips: 1025, rs: 9, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601832
Test loss (w/o reg) on all data: 0.0026561418
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1661565e-07
Norm of the params: 6.092711
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003433903
Train loss (w/o reg) on all data: 0.00125997
Test loss (w/o reg) on all data: 0.0029031872
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3851197e-08
Norm of the params: 6.593835
                Loss: fixed 137 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06858606
Train loss (w/o reg) on all data: 0.06447139
Test loss (w/o reg) on all data: 0.045199543
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.964914e-06
Norm of the params: 9.071575
              Random: fixed  13 labels. Loss 0.04520. Accuracy 0.989.
### Flips: 1025, rs: 9, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.295687e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010766
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8021682e-08
Norm of the params: 6.0928354
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06764393
Train loss (w/o reg) on all data: 0.06346777
Test loss (w/o reg) on all data: 0.0437809
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3239482e-05
Norm of the params: 9.139106
              Random: fixed  16 labels. Loss 0.04378. Accuracy 0.989.
### Flips: 1025, rs: 9, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.18472165e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011546
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4607259e-08
Norm of the params: 6.092822
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066194266
Train loss (w/o reg) on all data: 0.062065482
Test loss (w/o reg) on all data: 0.04199543
Train acc on all data:  0.973741794310722
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.857262e-05
Norm of the params: 9.087113
              Random: fixed  22 labels. Loss 0.04200. Accuracy 0.989.
### Flips: 1025, rs: 9, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6005924e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960115
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.526682e-09
Norm of the params: 6.092824
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06326737
Train loss (w/o reg) on all data: 0.058999084
Test loss (w/o reg) on all data: 0.040420942
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.8297234e-06
Norm of the params: 9.23936
              Random: fixed  30 labels. Loss 0.04042. Accuracy 0.990.
### Flips: 1025, rs: 9, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.483116e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0573554e-08
Norm of the params: 6.0928154
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059669282
Train loss (w/o reg) on all data: 0.055449113
Test loss (w/o reg) on all data: 0.036987748
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.290402e-06
Norm of the params: 9.187133
              Random: fixed  39 labels. Loss 0.03699. Accuracy 0.994.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06840642
Train loss (w/o reg) on all data: 0.06407609
Test loss (w/o reg) on all data: 0.048695263
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.1518924e-06
Norm of the params: 9.306268
Flipped loss: 0.04870. Accuracy: 0.981
### Flips: 1025, rs: 10, checks: 205
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019825399
Train loss (w/o reg) on all data: 0.015352734
Test loss (w/o reg) on all data: 0.014069896
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.1364278e-06
Norm of the params: 9.457974
     Influence (LOO): fixed 106 labels. Loss 0.01407. Accuracy 0.995.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008587165
Train loss (w/o reg) on all data: 0.004448395
Test loss (w/o reg) on all data: 0.00851712
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9586567e-07
Norm of the params: 9.098099
                Loss: fixed 121 labels. Loss 0.00852. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067682914
Train loss (w/o reg) on all data: 0.063452184
Test loss (w/o reg) on all data: 0.047729142
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.2109143e-06
Norm of the params: 9.198619
              Random: fixed   6 labels. Loss 0.04773. Accuracy 0.983.
### Flips: 1025, rs: 10, checks: 410
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004754982
Train loss (w/o reg) on all data: 0.002451124
Test loss (w/o reg) on all data: 0.0031152642
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0658262e-07
Norm of the params: 6.788016
     Influence (LOO): fixed 133 labels. Loss 0.00312. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0947583e-08
Norm of the params: 6.092813
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06589112
Train loss (w/o reg) on all data: 0.061772883
Test loss (w/o reg) on all data: 0.046213713
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.0601057e-05
Norm of the params: 9.075499
              Random: fixed  14 labels. Loss 0.04621. Accuracy 0.986.
### Flips: 1025, rs: 10, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8853687e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601113
Test loss (w/o reg) on all data: 0.0026560456
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.603691e-08
Norm of the params: 6.0928288
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06321191
Train loss (w/o reg) on all data: 0.05929906
Test loss (w/o reg) on all data: 0.043986246
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.341034e-06
Norm of the params: 8.8463
              Random: fixed  21 labels. Loss 0.04399. Accuracy 0.986.
### Flips: 1025, rs: 10, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.94787e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.584122e-08
Norm of the params: 6.092809
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061781533
Train loss (w/o reg) on all data: 0.05795489
Test loss (w/o reg) on all data: 0.04122045
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1864011e-05
Norm of the params: 8.748309
              Random: fixed  27 labels. Loss 0.04122. Accuracy 0.984.
### Flips: 1025, rs: 10, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010946
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.573665e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8595282e-08
Norm of the params: 6.09281
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0601935
Train loss (w/o reg) on all data: 0.05631556
Test loss (w/o reg) on all data: 0.039700586
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.4505163e-06
Norm of the params: 8.806751
              Random: fixed  33 labels. Loss 0.03970. Accuracy 0.988.
### Flips: 1025, rs: 10, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012524
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.003274e-08
Norm of the params: 6.092806
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601104
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.526903e-08
Norm of the params: 6.09283
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060021732
Train loss (w/o reg) on all data: 0.056077544
Test loss (w/o reg) on all data: 0.03987683
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.7976304e-06
Norm of the params: 8.881655
              Random: fixed  34 labels. Loss 0.03988. Accuracy 0.987.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07063795
Train loss (w/o reg) on all data: 0.066628434
Test loss (w/o reg) on all data: 0.044893254
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.9324427e-05
Norm of the params: 8.954904
Flipped loss: 0.04489. Accuracy: 0.985
### Flips: 1025, rs: 11, checks: 205
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010855534
Train loss (w/o reg) on all data: 0.0074685956
Test loss (w/o reg) on all data: 0.006712807
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1437027e-06
Norm of the params: 8.230357
     Influence (LOO): fixed 114 labels. Loss 0.00671. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004033638
Train loss (w/o reg) on all data: 0.0017032464
Test loss (w/o reg) on all data: 0.0067676804
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.1894086e-08
Norm of the params: 6.8269935
                Loss: fixed 121 labels. Loss 0.00677. Accuracy 0.998.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068334825
Train loss (w/o reg) on all data: 0.06426098
Test loss (w/o reg) on all data: 0.043964487
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.5628343e-06
Norm of the params: 9.026457
              Random: fixed   6 labels. Loss 0.04396. Accuracy 0.988.
### Flips: 1025, rs: 11, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5438626e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2599844e-08
Norm of the params: 6.0928135
                Loss: fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064651415
Train loss (w/o reg) on all data: 0.060663883
Test loss (w/o reg) on all data: 0.040540397
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.9119147e-06
Norm of the params: 8.930318
              Random: fixed  16 labels. Loss 0.04054. Accuracy 0.989.
### Flips: 1025, rs: 11, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.48485e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.715645e-09
Norm of the params: 6.092814
                Loss: fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06073268
Train loss (w/o reg) on all data: 0.056275286
Test loss (w/o reg) on all data: 0.03597862
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.4180126e-06
Norm of the params: 9.441816
              Random: fixed  26 labels. Loss 0.03598. Accuracy 0.988.
### Flips: 1025, rs: 11, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7960876e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601142
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1347074e-08
Norm of the params: 6.092824
                Loss: fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059718926
Train loss (w/o reg) on all data: 0.055270933
Test loss (w/o reg) on all data: 0.03491765
Train acc on all data:  0.975443715049842
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.888406e-06
Norm of the params: 9.431855
              Random: fixed  30 labels. Loss 0.03492. Accuracy 0.989.
### Flips: 1025, rs: 11, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1468943e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.720569e-09
Norm of the params: 6.0928164
                Loss: fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05770255
Train loss (w/o reg) on all data: 0.05335499
Test loss (w/o reg) on all data: 0.03344146
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.952021e-06
Norm of the params: 9.324764
              Random: fixed  35 labels. Loss 0.03344. Accuracy 0.990.
### Flips: 1025, rs: 11, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4450964e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0843505e-08
Norm of the params: 6.0928135
                Loss: fixed 126 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054526523
Train loss (w/o reg) on all data: 0.050249115
Test loss (w/o reg) on all data: 0.031197919
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.2151695e-06
Norm of the params: 9.249225
              Random: fixed  41 labels. Loss 0.03120. Accuracy 0.990.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06772521
Train loss (w/o reg) on all data: 0.063423276
Test loss (w/o reg) on all data: 0.041455995
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.329037e-06
Norm of the params: 9.275705
Flipped loss: 0.04146. Accuracy: 0.995
### Flips: 1025, rs: 12, checks: 205
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008200353
Train loss (w/o reg) on all data: 0.004833463
Test loss (w/o reg) on all data: 0.0047768666
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9667597e-07
Norm of the params: 8.205961
     Influence (LOO): fixed 112 labels. Loss 0.00478. Accuracy 0.999.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005891827
Train loss (w/o reg) on all data: 0.002675842
Test loss (w/o reg) on all data: 0.004534917
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.787604e-07
Norm of the params: 8.019956
                Loss: fixed 114 labels. Loss 0.00453. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06691582
Train loss (w/o reg) on all data: 0.062872425
Test loss (w/o reg) on all data: 0.039810617
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3850631e-05
Norm of the params: 8.992658
              Random: fixed   6 labels. Loss 0.03981. Accuracy 0.994.
### Flips: 1025, rs: 12, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1442007e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041470416
Train loss (w/o reg) on all data: 0.0016604429
Test loss (w/o reg) on all data: 0.0031115557
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8511723e-08
Norm of the params: 7.05209
                Loss: fixed 118 labels. Loss 0.00311. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06339647
Train loss (w/o reg) on all data: 0.059499487
Test loss (w/o reg) on all data: 0.03794816
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.046843e-06
Norm of the params: 8.828343
              Random: fixed  14 labels. Loss 0.03795. Accuracy 0.996.
### Flips: 1025, rs: 12, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3261798e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3646735e-08
Norm of the params: 6.0928154
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06105021
Train loss (w/o reg) on all data: 0.057067424
Test loss (w/o reg) on all data: 0.036693737
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.5582433e-06
Norm of the params: 8.925005
              Random: fixed  20 labels. Loss 0.03669. Accuracy 0.995.
### Flips: 1025, rs: 12, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.263579e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0186226e-08
Norm of the params: 6.0928164
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058141377
Train loss (w/o reg) on all data: 0.054143015
Test loss (w/o reg) on all data: 0.034163665
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2211729e-05
Norm of the params: 8.942441
              Random: fixed  26 labels. Loss 0.03416. Accuracy 0.995.
### Flips: 1025, rs: 12, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601396
Test loss (w/o reg) on all data: 0.0026560912
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.547265e-08
Norm of the params: 6.092783
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.0026560486
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3992798e-08
Norm of the params: 6.092821
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05717207
Train loss (w/o reg) on all data: 0.0532733
Test loss (w/o reg) on all data: 0.033034325
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.300831e-06
Norm of the params: 8.83037
              Random: fixed  31 labels. Loss 0.03303. Accuracy 0.995.
### Flips: 1025, rs: 12, checks: 1230
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4424368e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3140114e-08
Norm of the params: 6.0928173
                Loss: fixed 120 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055040665
Train loss (w/o reg) on all data: 0.051165164
Test loss (w/o reg) on all data: 0.031109482
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.2457507e-06
Norm of the params: 8.803977
              Random: fixed  36 labels. Loss 0.03111. Accuracy 0.995.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074892156
Train loss (w/o reg) on all data: 0.07138876
Test loss (w/o reg) on all data: 0.05206361
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.622418e-06
Norm of the params: 8.370665
Flipped loss: 0.05206. Accuracy: 0.988
### Flips: 1025, rs: 13, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013881318
Train loss (w/o reg) on all data: 0.010831519
Test loss (w/o reg) on all data: 0.007712623
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1002637e-07
Norm of the params: 7.8099923
     Influence (LOO): fixed 120 labels. Loss 0.00771. Accuracy 0.998.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038500098
Train loss (w/o reg) on all data: 0.001569349
Test loss (w/o reg) on all data: 0.0033296084
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9353263e-08
Norm of the params: 6.753756
                Loss: fixed 133 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07319311
Train loss (w/o reg) on all data: 0.0694948
Test loss (w/o reg) on all data: 0.051996656
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.0904402e-06
Norm of the params: 8.600367
              Random: fixed   4 labels. Loss 0.05200. Accuracy 0.988.
### Flips: 1025, rs: 13, checks: 410
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4628104e-08
Norm of the params: 6.0928235
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11833165e-08
Norm of the params: 6.0928226
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07126969
Train loss (w/o reg) on all data: 0.06749157
Test loss (w/o reg) on all data: 0.047143884
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.00019e-06
Norm of the params: 8.692671
              Random: fixed  13 labels. Loss 0.04714. Accuracy 0.989.
### Flips: 1025, rs: 13, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.860013e-09
Norm of the params: 6.092816
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1802779e-08
Norm of the params: 6.092819
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06712162
Train loss (w/o reg) on all data: 0.06334381
Test loss (w/o reg) on all data: 0.0427948
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.374347e-06
Norm of the params: 8.6923065
              Random: fixed  27 labels. Loss 0.04279. Accuracy 0.991.
### Flips: 1025, rs: 13, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8493818e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3817217e-08
Norm of the params: 6.092821
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067121625
Train loss (w/o reg) on all data: 0.063346095
Test loss (w/o reg) on all data: 0.042789955
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7328679e-05
Norm of the params: 8.689687
              Random: fixed  27 labels. Loss 0.04279. Accuracy 0.991.
### Flips: 1025, rs: 13, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7396019e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011546
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2611209e-08
Norm of the params: 6.092822
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065169945
Train loss (w/o reg) on all data: 0.06133006
Test loss (w/o reg) on all data: 0.041180786
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.4422194e-06
Norm of the params: 8.76343
              Random: fixed  32 labels. Loss 0.04118. Accuracy 0.990.
### Flips: 1025, rs: 13, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.156403e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.074423e-08
Norm of the params: 6.0928235
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061371893
Train loss (w/o reg) on all data: 0.057351436
Test loss (w/o reg) on all data: 0.03721221
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.03407665e-05
Norm of the params: 8.967114
              Random: fixed  41 labels. Loss 0.03721. Accuracy 0.991.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06883011
Train loss (w/o reg) on all data: 0.064191975
Test loss (w/o reg) on all data: 0.04492943
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.654249e-06
Norm of the params: 9.631342
Flipped loss: 0.04493. Accuracy: 0.988
### Flips: 1025, rs: 14, checks: 205
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019896528
Train loss (w/o reg) on all data: 0.015166017
Test loss (w/o reg) on all data: 0.0104246205
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.096922e-07
Norm of the params: 9.726777
     Influence (LOO): fixed 113 labels. Loss 0.01042. Accuracy 0.998.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009571428
Train loss (w/o reg) on all data: 0.00505308
Test loss (w/o reg) on all data: 0.0063690664
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8112962e-06
Norm of the params: 9.506154
                Loss: fixed 125 labels. Loss 0.00637. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06785389
Train loss (w/o reg) on all data: 0.063185744
Test loss (w/o reg) on all data: 0.04395432
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.0096268e-06
Norm of the params: 9.66245
              Random: fixed   4 labels. Loss 0.04395. Accuracy 0.990.
### Flips: 1025, rs: 14, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005172149
Train loss (w/o reg) on all data: 0.0026730371
Test loss (w/o reg) on all data: 0.0041827285
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.570806e-08
Norm of the params: 7.0698123
     Influence (LOO): fixed 140 labels. Loss 0.00418. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004222356
Train loss (w/o reg) on all data: 0.0017138054
Test loss (w/o reg) on all data: 0.0046242517
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.565223e-07
Norm of the params: 7.0831494
                Loss: fixed 140 labels. Loss 0.00462. Accuracy 0.999.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066680096
Train loss (w/o reg) on all data: 0.062088393
Test loss (w/o reg) on all data: 0.041551113
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.237186e-06
Norm of the params: 9.58301
              Random: fixed  12 labels. Loss 0.04155. Accuracy 0.993.
### Flips: 1025, rs: 14, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2985908e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7801609e-08
Norm of the params: 6.0928254
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06647282
Train loss (w/o reg) on all data: 0.061934587
Test loss (w/o reg) on all data: 0.03980453
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.6477012e-05
Norm of the params: 9.527052
              Random: fixed  17 labels. Loss 0.03980. Accuracy 0.994.
### Flips: 1025, rs: 14, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6472764e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8132022e-08
Norm of the params: 6.092818
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065771446
Train loss (w/o reg) on all data: 0.061381646
Test loss (w/o reg) on all data: 0.038274344
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.2980855e-06
Norm of the params: 9.369956
              Random: fixed  22 labels. Loss 0.03827. Accuracy 0.995.
### Flips: 1025, rs: 14, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960121
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6235298e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8258102e-08
Norm of the params: 6.0928216
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06458141
Train loss (w/o reg) on all data: 0.06016845
Test loss (w/o reg) on all data: 0.036849268
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0226497e-05
Norm of the params: 9.394635
              Random: fixed  26 labels. Loss 0.03685. Accuracy 0.993.
### Flips: 1025, rs: 14, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.04441e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012634
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.041416e-08
Norm of the params: 6.0928044
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06263609
Train loss (w/o reg) on all data: 0.05827398
Test loss (w/o reg) on all data: 0.034524508
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.095622e-05
Norm of the params: 9.340356
              Random: fixed  33 labels. Loss 0.03452. Accuracy 0.993.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0711774
Train loss (w/o reg) on all data: 0.06712025
Test loss (w/o reg) on all data: 0.04860259
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.3846287e-06
Norm of the params: 9.007947
Flipped loss: 0.04860. Accuracy: 0.984
### Flips: 1025, rs: 15, checks: 205
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016545208
Train loss (w/o reg) on all data: 0.012595341
Test loss (w/o reg) on all data: 0.008203559
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6746866e-07
Norm of the params: 8.888046
     Influence (LOO): fixed 115 labels. Loss 0.00820. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00509115
Train loss (w/o reg) on all data: 0.002276799
Test loss (w/o reg) on all data: 0.004321268
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9471791e-07
Norm of the params: 7.5024676
                Loss: fixed 128 labels. Loss 0.00432. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069801524
Train loss (w/o reg) on all data: 0.06563958
Test loss (w/o reg) on all data: 0.04773206
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1502258e-06
Norm of the params: 9.123541
              Random: fixed   4 labels. Loss 0.04773. Accuracy 0.987.
### Flips: 1025, rs: 15, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0393959e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5203652e-08
Norm of the params: 6.0928154
                Loss: fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06797502
Train loss (w/o reg) on all data: 0.06397047
Test loss (w/o reg) on all data: 0.045855064
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.0470035e-06
Norm of the params: 8.949358
              Random: fixed  11 labels. Loss 0.04586. Accuracy 0.984.
### Flips: 1025, rs: 15, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.58435e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011074
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7754731e-08
Norm of the params: 6.092829
                Loss: fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06589993
Train loss (w/o reg) on all data: 0.06165475
Test loss (w/o reg) on all data: 0.044519007
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.65513e-06
Norm of the params: 9.214317
              Random: fixed  16 labels. Loss 0.04452. Accuracy 0.986.
### Flips: 1025, rs: 15, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.50152e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2768057e-08
Norm of the params: 6.092821
                Loss: fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06259717
Train loss (w/o reg) on all data: 0.058230672
Test loss (w/o reg) on all data: 0.039067354
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.306951e-06
Norm of the params: 9.345049
              Random: fixed  26 labels. Loss 0.03907. Accuracy 0.994.
### Flips: 1025, rs: 15, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8509226e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0896636e-08
Norm of the params: 6.0928183
                Loss: fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060900882
Train loss (w/o reg) on all data: 0.056699544
Test loss (w/o reg) on all data: 0.03642282
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4317723e-06
Norm of the params: 9.166613
              Random: fixed  31 labels. Loss 0.03642. Accuracy 0.996.
### Flips: 1025, rs: 15, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9458663e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601269
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.646055e-08
Norm of the params: 6.092803
                Loss: fixed 132 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05915913
Train loss (w/o reg) on all data: 0.054951984
Test loss (w/o reg) on all data: 0.034147583
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.2087669e-06
Norm of the params: 9.172943
              Random: fixed  36 labels. Loss 0.03415. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073473215
Train loss (w/o reg) on all data: 0.06992307
Test loss (w/o reg) on all data: 0.056560785
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.275176e-06
Norm of the params: 8.426318
Flipped loss: 0.05656. Accuracy: 0.982
### Flips: 1025, rs: 16, checks: 205
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018228153
Train loss (w/o reg) on all data: 0.014496959
Test loss (w/o reg) on all data: 0.018555226
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.107907e-07
Norm of the params: 8.638513
     Influence (LOO): fixed 114 labels. Loss 0.01856. Accuracy 0.993.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005535905
Train loss (w/o reg) on all data: 0.0026255415
Test loss (w/o reg) on all data: 0.007844773
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3093285e-07
Norm of the params: 7.6293693
                Loss: fixed 132 labels. Loss 0.00784. Accuracy 0.998.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07116146
Train loss (w/o reg) on all data: 0.06765559
Test loss (w/o reg) on all data: 0.054407243
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.032311e-06
Norm of the params: 8.373609
              Random: fixed   8 labels. Loss 0.05441. Accuracy 0.980.
### Flips: 1025, rs: 16, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041646995
Train loss (w/o reg) on all data: 0.0019412518
Test loss (w/o reg) on all data: 0.003911601
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7330836e-08
Norm of the params: 6.668504
     Influence (LOO): fixed 137 labels. Loss 0.00391. Accuracy 0.999.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037943898
Train loss (w/o reg) on all data: 0.0015304756
Test loss (w/o reg) on all data: 0.0035050388
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6254839e-07
Norm of the params: 6.7289147
                Loss: fixed 138 labels. Loss 0.00351. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07013137
Train loss (w/o reg) on all data: 0.06640595
Test loss (w/o reg) on all data: 0.053457133
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.5562124e-06
Norm of the params: 8.631818
              Random: fixed  11 labels. Loss 0.05346. Accuracy 0.982.
### Flips: 1025, rs: 16, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1997145e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2129331e-08
Norm of the params: 6.092825
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06886884
Train loss (w/o reg) on all data: 0.0650887
Test loss (w/o reg) on all data: 0.051579624
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.3448274e-05
Norm of the params: 8.694988
              Random: fixed  16 labels. Loss 0.05158. Accuracy 0.984.
### Flips: 1025, rs: 16, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011447
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9578795e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013024
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6960492e-08
Norm of the params: 6.092798
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06777433
Train loss (w/o reg) on all data: 0.06397417
Test loss (w/o reg) on all data: 0.04906183
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.8746751e-06
Norm of the params: 8.717984
              Random: fixed  20 labels. Loss 0.04906. Accuracy 0.986.
### Flips: 1025, rs: 16, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.016634e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012827
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.612209e-08
Norm of the params: 6.0928016
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06357095
Train loss (w/o reg) on all data: 0.05971378
Test loss (w/o reg) on all data: 0.045246106
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.6509274e-06
Norm of the params: 8.783127
              Random: fixed  31 labels. Loss 0.04525. Accuracy 0.988.
### Flips: 1025, rs: 16, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3937753e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1764608e-08
Norm of the params: 6.0928087
                Loss: fixed 139 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06253429
Train loss (w/o reg) on all data: 0.058623835
Test loss (w/o reg) on all data: 0.044111937
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.76774e-06
Norm of the params: 8.84359
              Random: fixed  33 labels. Loss 0.04411. Accuracy 0.988.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074209966
Train loss (w/o reg) on all data: 0.06966334
Test loss (w/o reg) on all data: 0.047175378
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1649717e-05
Norm of the params: 9.535858
Flipped loss: 0.04718. Accuracy: 0.992
### Flips: 1025, rs: 17, checks: 205
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020497087
Train loss (w/o reg) on all data: 0.01636516
Test loss (w/o reg) on all data: 0.015114679
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.3844844e-07
Norm of the params: 9.090576
     Influence (LOO): fixed 113 labels. Loss 0.01511. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069026994
Train loss (w/o reg) on all data: 0.003295798
Test loss (w/o reg) on all data: 0.004978876
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9770594e-07
Norm of the params: 8.493411
                Loss: fixed 136 labels. Loss 0.00498. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07303963
Train loss (w/o reg) on all data: 0.068521984
Test loss (w/o reg) on all data: 0.04549529
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.9715556e-06
Norm of the params: 9.50541
              Random: fixed   6 labels. Loss 0.04550. Accuracy 0.994.
### Flips: 1025, rs: 17, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004041302
Train loss (w/o reg) on all data: 0.0019269268
Test loss (w/o reg) on all data: 0.0029808192
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.0962244e-08
Norm of the params: 6.5028834
     Influence (LOO): fixed 142 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096008775
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.266189e-08
Norm of the params: 6.0928674
                Loss: fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07057967
Train loss (w/o reg) on all data: 0.06616637
Test loss (w/o reg) on all data: 0.04346448
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.906347e-06
Norm of the params: 9.394999
              Random: fixed  15 labels. Loss 0.04346. Accuracy 0.993.
### Flips: 1025, rs: 17, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.00265605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7583796e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7020383e-08
Norm of the params: 6.0928216
                Loss: fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066214316
Train loss (w/o reg) on all data: 0.061794687
Test loss (w/o reg) on all data: 0.041792363
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.1716534e-06
Norm of the params: 9.401733
              Random: fixed  25 labels. Loss 0.04179. Accuracy 0.992.
### Flips: 1025, rs: 17, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096009584
Test loss (w/o reg) on all data: 0.0026560216
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1830208e-08
Norm of the params: 6.092855
     Influence (LOO): fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3765008e-08
Norm of the params: 6.0928164
                Loss: fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06352869
Train loss (w/o reg) on all data: 0.05912633
Test loss (w/o reg) on all data: 0.039000515
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8640106e-06
Norm of the params: 9.383346
              Random: fixed  35 labels. Loss 0.03900. Accuracy 0.994.
### Flips: 1025, rs: 17, checks: 1025
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3849707e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5991803e-08
Norm of the params: 6.092818
                Loss: fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06172715
Train loss (w/o reg) on all data: 0.057341494
Test loss (w/o reg) on all data: 0.03672367
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0823147e-06
Norm of the params: 9.36553
              Random: fixed  41 labels. Loss 0.03672. Accuracy 0.995.
### Flips: 1025, rs: 17, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601015
Test loss (w/o reg) on all data: 0.0026560384
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.371516e-08
Norm of the params: 6.0928454
     Influence (LOO): fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0851201e-08
Norm of the params: 6.0928097
                Loss: fixed 143 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059084516
Train loss (w/o reg) on all data: 0.054820806
Test loss (w/o reg) on all data: 0.034103867
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.334436e-06
Norm of the params: 9.234404
              Random: fixed  50 labels. Loss 0.03410. Accuracy 0.994.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06856688
Train loss (w/o reg) on all data: 0.06426447
Test loss (w/o reg) on all data: 0.04546089
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.664496e-06
Norm of the params: 9.276223
Flipped loss: 0.04546. Accuracy: 0.989
### Flips: 1025, rs: 18, checks: 205
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011757793
Train loss (w/o reg) on all data: 0.008200999
Test loss (w/o reg) on all data: 0.0081720445
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.906772e-07
Norm of the params: 8.434209
     Influence (LOO): fixed 110 labels. Loss 0.00817. Accuracy 0.998.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004486574
Train loss (w/o reg) on all data: 0.0019013067
Test loss (w/o reg) on all data: 0.0039615924
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3805158e-07
Norm of the params: 7.190643
                Loss: fixed 119 labels. Loss 0.00396. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066715844
Train loss (w/o reg) on all data: 0.0623514
Test loss (w/o reg) on all data: 0.04415872
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1489116e-05
Norm of the params: 9.34285
              Random: fixed   4 labels. Loss 0.04416. Accuracy 0.988.
### Flips: 1025, rs: 18, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5778242e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035701008
Train loss (w/o reg) on all data: 0.0013474802
Test loss (w/o reg) on all data: 0.0028978754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3055778e-08
Norm of the params: 6.667264
                Loss: fixed 121 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061531484
Train loss (w/o reg) on all data: 0.057408318
Test loss (w/o reg) on all data: 0.03779587
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9719818e-06
Norm of the params: 9.080934
              Random: fixed  20 labels. Loss 0.03780. Accuracy 0.991.
### Flips: 1025, rs: 18, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2660265e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.42417464e-08
Norm of the params: 6.092814
                Loss: fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05995687
Train loss (w/o reg) on all data: 0.055834208
Test loss (w/o reg) on all data: 0.036102217
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.7781882e-06
Norm of the params: 9.080379
              Random: fixed  24 labels. Loss 0.03610. Accuracy 0.989.
### Flips: 1025, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.863844e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3386196e-08
Norm of the params: 6.0928116
                Loss: fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058969986
Train loss (w/o reg) on all data: 0.054784674
Test loss (w/o reg) on all data: 0.034557715
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.8947496e-06
Norm of the params: 9.149109
              Random: fixed  27 labels. Loss 0.03456. Accuracy 0.992.
### Flips: 1025, rs: 18, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5385998e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0042975e-08
Norm of the params: 6.092813
                Loss: fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05702617
Train loss (w/o reg) on all data: 0.052780405
Test loss (w/o reg) on all data: 0.033125304
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.1983053e-06
Norm of the params: 9.2149515
              Random: fixed  33 labels. Loss 0.03313. Accuracy 0.992.
### Flips: 1025, rs: 18, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9102977e-09
Norm of the params: 6.0928173
     Influence (LOO): fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7404517e-09
Norm of the params: 6.0928164
                Loss: fixed 122 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052843086
Train loss (w/o reg) on all data: 0.048603065
Test loss (w/o reg) on all data: 0.030582447
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.2867314e-06
Norm of the params: 9.208715
              Random: fixed  42 labels. Loss 0.03058. Accuracy 0.991.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071598016
Train loss (w/o reg) on all data: 0.06701573
Test loss (w/o reg) on all data: 0.049679283
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6183534e-05
Norm of the params: 9.573179
Flipped loss: 0.04968. Accuracy: 0.988
### Flips: 1025, rs: 19, checks: 205
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023711998
Train loss (w/o reg) on all data: 0.01880313
Test loss (w/o reg) on all data: 0.012784925
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.274065e-07
Norm of the params: 9.908449
     Influence (LOO): fixed 109 labels. Loss 0.01278. Accuracy 0.998.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011900609
Train loss (w/o reg) on all data: 0.007435005
Test loss (w/o reg) on all data: 0.009278965
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2388306e-07
Norm of the params: 9.450506
                Loss: fixed 131 labels. Loss 0.00928. Accuracy 0.998.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071058646
Train loss (w/o reg) on all data: 0.06661275
Test loss (w/o reg) on all data: 0.04978399
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.131551e-06
Norm of the params: 9.429634
              Random: fixed   3 labels. Loss 0.04978. Accuracy 0.986.
### Flips: 1025, rs: 19, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004202506
Train loss (w/o reg) on all data: 0.0020581048
Test loss (w/o reg) on all data: 0.0034520712
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7721557e-08
Norm of the params: 6.548895
     Influence (LOO): fixed 147 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036519817
Train loss (w/o reg) on all data: 0.001406026
Test loss (w/o reg) on all data: 0.0033979365
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.118082e-08
Norm of the params: 6.702173
                Loss: fixed 147 labels. Loss 0.00340. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069498196
Train loss (w/o reg) on all data: 0.06502914
Test loss (w/o reg) on all data: 0.04513687
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.2823938e-06
Norm of the params: 9.454165
              Random: fixed  10 labels. Loss 0.04514. Accuracy 0.990.
### Flips: 1025, rs: 19, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009600601
Test loss (w/o reg) on all data: 0.002655981
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9234995e-08
Norm of the params: 6.0929127
     Influence (LOO): fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0759524e-08
Norm of the params: 6.09282
                Loss: fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065495096
Train loss (w/o reg) on all data: 0.06108699
Test loss (w/o reg) on all data: 0.040034715
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3550128e-05
Norm of the params: 9.38947
              Random: fixed  21 labels. Loss 0.04003. Accuracy 0.989.
### Flips: 1025, rs: 19, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601054
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6727412e-08
Norm of the params: 6.0928383
     Influence (LOO): fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.041382e-09
Norm of the params: 6.0928187
                Loss: fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0628035
Train loss (w/o reg) on all data: 0.05858684
Test loss (w/o reg) on all data: 0.034679785
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.1424823e-06
Norm of the params: 9.1833105
              Random: fixed  32 labels. Loss 0.03468. Accuracy 0.994.
### Flips: 1025, rs: 19, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960128
Test loss (w/o reg) on all data: 0.0026560954
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2937102e-08
Norm of the params: 6.0928016
     Influence (LOO): fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.881984e-08
Norm of the params: 6.09283
                Loss: fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060067035
Train loss (w/o reg) on all data: 0.055912014
Test loss (w/o reg) on all data: 0.031511404
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3669504e-05
Norm of the params: 9.115944
              Random: fixed  46 labels. Loss 0.03151. Accuracy 0.996.
### Flips: 1025, rs: 19, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656082
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.382729e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7803737e-08
Norm of the params: 6.092823
                Loss: fixed 148 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058715988
Train loss (w/o reg) on all data: 0.054686263
Test loss (w/o reg) on all data: 0.029597795
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.939221e-06
Norm of the params: 8.977444
              Random: fixed  50 labels. Loss 0.02960. Accuracy 0.997.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069320105
Train loss (w/o reg) on all data: 0.064405076
Test loss (w/o reg) on all data: 0.06248376
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.7666995e-06
Norm of the params: 9.914664
Flipped loss: 0.06248. Accuracy: 0.977
### Flips: 1025, rs: 20, checks: 205
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022166707
Train loss (w/o reg) on all data: 0.0177705
Test loss (w/o reg) on all data: 0.023107152
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1124199e-06
Norm of the params: 9.376787
     Influence (LOO): fixed 106 labels. Loss 0.02311. Accuracy 0.989.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009092098
Train loss (w/o reg) on all data: 0.0048733596
Test loss (w/o reg) on all data: 0.015694013
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8569524e-07
Norm of the params: 9.185575
                Loss: fixed 128 labels. Loss 0.01569. Accuracy 0.995.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0680378
Train loss (w/o reg) on all data: 0.06278486
Test loss (w/o reg) on all data: 0.062150862
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 8.559274e-06
Norm of the params: 10.249821
              Random: fixed   3 labels. Loss 0.06215. Accuracy 0.975.
### Flips: 1025, rs: 20, checks: 410
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004988256
Train loss (w/o reg) on all data: 0.0024096752
Test loss (w/o reg) on all data: 0.0068426235
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2336267e-07
Norm of the params: 7.181338
     Influence (LOO): fixed 139 labels. Loss 0.00684. Accuracy 0.997.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043454943
Train loss (w/o reg) on all data: 0.0017598651
Test loss (w/o reg) on all data: 0.008713977
Train acc on all data:  1.0
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.311237e-08
Norm of the params: 7.1911464
                Loss: fixed 138 labels. Loss 0.00871. Accuracy 0.995.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06715966
Train loss (w/o reg) on all data: 0.062063746
Test loss (w/o reg) on all data: 0.057167854
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.004764e-06
Norm of the params: 10.095459
              Random: fixed  10 labels. Loss 0.05717. Accuracy 0.978.
### Flips: 1025, rs: 20, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560773
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.505188e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011534
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2749171e-08
Norm of the params: 6.092822
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064696915
Train loss (w/o reg) on all data: 0.05982096
Test loss (w/o reg) on all data: 0.04911637
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.5603824e-06
Norm of the params: 9.875177
              Random: fixed  23 labels. Loss 0.04912. Accuracy 0.985.
### Flips: 1025, rs: 20, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960087
Test loss (w/o reg) on all data: 0.0026560333
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.222684e-08
Norm of the params: 6.0928693
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5352466e-08
Norm of the params: 6.0928164
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061393
Train loss (w/o reg) on all data: 0.05700768
Test loss (w/o reg) on all data: 0.043695353
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3565219e-06
Norm of the params: 9.365169
              Random: fixed  36 labels. Loss 0.04370. Accuracy 0.989.
### Flips: 1025, rs: 20, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.476916e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560375
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.31111e-08
Norm of the params: 6.092823
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060255043
Train loss (w/o reg) on all data: 0.05580286
Test loss (w/o reg) on all data: 0.04126509
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.0132375e-06
Norm of the params: 9.436297
              Random: fixed  39 labels. Loss 0.04127. Accuracy 0.990.
### Flips: 1025, rs: 20, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.757105e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601137
Test loss (w/o reg) on all data: 0.002656037
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.557425e-08
Norm of the params: 6.092825
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058886062
Train loss (w/o reg) on all data: 0.054528944
Test loss (w/o reg) on all data: 0.039815605
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9975262e-06
Norm of the params: 9.335009
              Random: fixed  45 labels. Loss 0.03982. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06885724
Train loss (w/o reg) on all data: 0.06499502
Test loss (w/o reg) on all data: 0.044702027
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.727798e-06
Norm of the params: 8.788873
Flipped loss: 0.04470. Accuracy: 0.993
### Flips: 1025, rs: 21, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00885667
Train loss (w/o reg) on all data: 0.006185224
Test loss (w/o reg) on all data: 0.0039065564
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 4.292694e-07
Norm of the params: 7.309509
     Influence (LOO): fixed 111 labels. Loss 0.00391. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034192405
Train loss (w/o reg) on all data: 0.0013469021
Test loss (w/o reg) on all data: 0.0034306056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4571836e-08
Norm of the params: 6.4379168
                Loss: fixed 115 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06801219
Train loss (w/o reg) on all data: 0.06417994
Test loss (w/o reg) on all data: 0.043788917
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.6908261e-06
Norm of the params: 8.754713
              Random: fixed   3 labels. Loss 0.04379. Accuracy 0.993.
### Flips: 1025, rs: 21, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2073064e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.392221e-09
Norm of the params: 6.092814
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06590629
Train loss (w/o reg) on all data: 0.062063754
Test loss (w/o reg) on all data: 0.041647613
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.0602869e-06
Norm of the params: 8.766461
              Random: fixed   8 labels. Loss 0.04165. Accuracy 0.992.
### Flips: 1025, rs: 21, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.84259e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601232
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.24959785e-08
Norm of the params: 6.09281
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06191519
Train loss (w/o reg) on all data: 0.05805191
Test loss (w/o reg) on all data: 0.037841696
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.43472e-06
Norm of the params: 8.790085
              Random: fixed  18 labels. Loss 0.03784. Accuracy 0.994.
### Flips: 1025, rs: 21, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1722445e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4085191e-08
Norm of the params: 6.09282
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05846673
Train loss (w/o reg) on all data: 0.05464895
Test loss (w/o reg) on all data: 0.033331383
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.194858e-06
Norm of the params: 8.738169
              Random: fixed  30 labels. Loss 0.03333. Accuracy 0.996.
### Flips: 1025, rs: 21, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601081
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.058104e-08
Norm of the params: 6.092835
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0605734e-08
Norm of the params: 6.0928087
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056371875
Train loss (w/o reg) on all data: 0.052455194
Test loss (w/o reg) on all data: 0.031815127
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3912487e-06
Norm of the params: 8.850628
              Random: fixed  35 labels. Loss 0.03182. Accuracy 0.996.
### Flips: 1025, rs: 21, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.36418e-09
Norm of the params: 6.0928144
     Influence (LOO): fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.400454e-09
Norm of the params: 6.0928164
                Loss: fixed 118 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05540028
Train loss (w/o reg) on all data: 0.05145791
Test loss (w/o reg) on all data: 0.031472024
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.789262e-06
Norm of the params: 8.879604
              Random: fixed  37 labels. Loss 0.03147. Accuracy 0.995.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072278
Train loss (w/o reg) on all data: 0.068818204
Test loss (w/o reg) on all data: 0.056429774
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.1304706e-06
Norm of the params: 8.318408
Flipped loss: 0.05643. Accuracy: 0.982
### Flips: 1025, rs: 22, checks: 205
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017315388
Train loss (w/o reg) on all data: 0.0131801255
Test loss (w/o reg) on all data: 0.013371459
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0071677e-06
Norm of the params: 9.094244
     Influence (LOO): fixed 113 labels. Loss 0.01337. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008911643
Train loss (w/o reg) on all data: 0.0047656368
Test loss (w/o reg) on all data: 0.0104948385
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.2464973e-07
Norm of the params: 9.10605
                Loss: fixed 127 labels. Loss 0.01049. Accuracy 0.995.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071682006
Train loss (w/o reg) on all data: 0.068271406
Test loss (w/o reg) on all data: 0.055754688
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.01083e-06
Norm of the params: 8.259055
              Random: fixed   2 labels. Loss 0.05575. Accuracy 0.983.
### Flips: 1025, rs: 22, checks: 410
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042870585
Train loss (w/o reg) on all data: 0.0019566913
Test loss (w/o reg) on all data: 0.00357074
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1784418e-08
Norm of the params: 6.8269577
     Influence (LOO): fixed 138 labels. Loss 0.00357. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035241684
Train loss (w/o reg) on all data: 0.0013204502
Test loss (w/o reg) on all data: 0.0031818582
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.689572e-08
Norm of the params: 6.638853
                Loss: fixed 139 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070185155
Train loss (w/o reg) on all data: 0.06676788
Test loss (w/o reg) on all data: 0.051891115
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.652747e-06
Norm of the params: 8.267133
              Random: fixed   7 labels. Loss 0.05189. Accuracy 0.984.
### Flips: 1025, rs: 22, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656051
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1878566e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601239
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.402212e-09
Norm of the params: 6.0928082
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06724853
Train loss (w/o reg) on all data: 0.06385044
Test loss (w/o reg) on all data: 0.047508426
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.1919603e-06
Norm of the params: 8.243897
              Random: fixed  17 labels. Loss 0.04751. Accuracy 0.985.
### Flips: 1025, rs: 22, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601051
Test loss (w/o reg) on all data: 0.0026560375
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1748794e-08
Norm of the params: 6.09284
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1308788e-08
Norm of the params: 6.092815
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06619086
Train loss (w/o reg) on all data: 0.06271339
Test loss (w/o reg) on all data: 0.046693664
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3650406e-06
Norm of the params: 8.339626
              Random: fixed  20 labels. Loss 0.04669. Accuracy 0.985.
### Flips: 1025, rs: 22, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6810686e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4844466e-08
Norm of the params: 6.0928197
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06243626
Train loss (w/o reg) on all data: 0.058661938
Test loss (w/o reg) on all data: 0.04243359
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.527424e-06
Norm of the params: 8.688293
              Random: fixed  34 labels. Loss 0.04243. Accuracy 0.987.
### Flips: 1025, rs: 22, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601309
Test loss (w/o reg) on all data: 0.002656097
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3763748e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3014154e-08
Norm of the params: 6.092828
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05925741
Train loss (w/o reg) on all data: 0.05530897
Test loss (w/o reg) on all data: 0.038475387
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.0360222e-06
Norm of the params: 8.886437
              Random: fixed  43 labels. Loss 0.03848. Accuracy 0.991.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072317235
Train loss (w/o reg) on all data: 0.068532676
Test loss (w/o reg) on all data: 0.05159524
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.572344e-06
Norm of the params: 8.700071
Flipped loss: 0.05160. Accuracy: 0.984
### Flips: 1025, rs: 23, checks: 205
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01486261
Train loss (w/o reg) on all data: 0.0107887285
Test loss (w/o reg) on all data: 0.012583141
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.001281e-07
Norm of the params: 9.026497
     Influence (LOO): fixed 117 labels. Loss 0.01258. Accuracy 0.998.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006007701
Train loss (w/o reg) on all data: 0.0027278983
Test loss (w/o reg) on all data: 0.006670827
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.699668e-08
Norm of the params: 8.099139
                Loss: fixed 129 labels. Loss 0.00667. Accuracy 0.998.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0705604
Train loss (w/o reg) on all data: 0.066649884
Test loss (w/o reg) on all data: 0.049247943
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.1207896e-06
Norm of the params: 8.843661
              Random: fixed   6 labels. Loss 0.04925. Accuracy 0.986.
### Flips: 1025, rs: 23, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8323036e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2797042e-08
Norm of the params: 6.092814
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06936526
Train loss (w/o reg) on all data: 0.065418005
Test loss (w/o reg) on all data: 0.04805396
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.9634896e-06
Norm of the params: 8.885106
              Random: fixed  10 labels. Loss 0.04805. Accuracy 0.988.
### Flips: 1025, rs: 23, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601127
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5618326e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2072127e-08
Norm of the params: 6.092809
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06671276
Train loss (w/o reg) on all data: 0.06280004
Test loss (w/o reg) on all data: 0.045870196
Train acc on all data:  0.973984925844882
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.694755e-06
Norm of the params: 8.846148
              Random: fixed  17 labels. Loss 0.04587. Accuracy 0.989.
### Flips: 1025, rs: 23, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1923551e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601232
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7209326e-08
Norm of the params: 6.09281
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064633794
Train loss (w/o reg) on all data: 0.060638886
Test loss (w/o reg) on all data: 0.044879332
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3261913e-06
Norm of the params: 8.938578
              Random: fixed  22 labels. Loss 0.04488. Accuracy 0.985.
### Flips: 1025, rs: 23, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4150122e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9579172e-08
Norm of the params: 6.0928106
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061333872
Train loss (w/o reg) on all data: 0.057394553
Test loss (w/o reg) on all data: 0.03968086
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.2511843e-06
Norm of the params: 8.876171
              Random: fixed  32 labels. Loss 0.03968. Accuracy 0.990.
### Flips: 1025, rs: 23, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.955036e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3458642e-08
Norm of the params: 6.092809
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058234595
Train loss (w/o reg) on all data: 0.05398493
Test loss (w/o reg) on all data: 0.03773229
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.779415e-06
Norm of the params: 9.219181
              Random: fixed  41 labels. Loss 0.03773. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06966115
Train loss (w/o reg) on all data: 0.06558251
Test loss (w/o reg) on all data: 0.053056914
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5515574e-05
Norm of the params: 9.031769
Flipped loss: 0.05306. Accuracy: 0.981
### Flips: 1025, rs: 24, checks: 205
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01557035
Train loss (w/o reg) on all data: 0.011089215
Test loss (w/o reg) on all data: 0.01126027
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.885835e-07
Norm of the params: 9.466926
     Influence (LOO): fixed 118 labels. Loss 0.01126. Accuracy 0.997.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0095428135
Train loss (w/o reg) on all data: 0.0056127594
Test loss (w/o reg) on all data: 0.0071563246
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5936295e-07
Norm of the params: 8.865725
                Loss: fixed 128 labels. Loss 0.00716. Accuracy 0.997.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068546996
Train loss (w/o reg) on all data: 0.0645319
Test loss (w/o reg) on all data: 0.05098201
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.2217662e-05
Norm of the params: 8.961138
              Random: fixed   5 labels. Loss 0.05098. Accuracy 0.981.
### Flips: 1025, rs: 24, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.06105125e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035241684
Train loss (w/o reg) on all data: 0.0013204738
Test loss (w/o reg) on all data: 0.0031818838
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.344993e-08
Norm of the params: 6.638817
                Loss: fixed 139 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06664193
Train loss (w/o reg) on all data: 0.062476583
Test loss (w/o reg) on all data: 0.0489666
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.4108507e-06
Norm of the params: 9.127261
              Random: fixed  10 labels. Loss 0.04897. Accuracy 0.983.
### Flips: 1025, rs: 24, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0044429e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.387113e-09
Norm of the params: 6.092822
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063604616
Train loss (w/o reg) on all data: 0.059415817
Test loss (w/o reg) on all data: 0.04552546
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.288327e-06
Norm of the params: 9.152918
              Random: fixed  18 labels. Loss 0.04553. Accuracy 0.983.
### Flips: 1025, rs: 24, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013525
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3714866e-08
Norm of the params: 6.09279
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601135
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8508148e-08
Norm of the params: 6.092825
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061791122
Train loss (w/o reg) on all data: 0.057791144
Test loss (w/o reg) on all data: 0.042030163
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.5158822e-06
Norm of the params: 8.944245
              Random: fixed  28 labels. Loss 0.04203. Accuracy 0.988.
### Flips: 1025, rs: 24, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8940687e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9778405e-09
Norm of the params: 6.0928135
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059380367
Train loss (w/o reg) on all data: 0.055272147
Test loss (w/o reg) on all data: 0.039752096
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5291386e-06
Norm of the params: 9.064455
              Random: fixed  35 labels. Loss 0.03975. Accuracy 0.991.
### Flips: 1025, rs: 24, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.075875e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601232
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8222192e-08
Norm of the params: 6.092809
                Loss: fixed 140 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05666634
Train loss (w/o reg) on all data: 0.05285231
Test loss (w/o reg) on all data: 0.03402817
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3865601e-06
Norm of the params: 8.733878
              Random: fixed  47 labels. Loss 0.03403. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07250448
Train loss (w/o reg) on all data: 0.06878096
Test loss (w/o reg) on all data: 0.050343808
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.953038e-06
Norm of the params: 8.629627
Flipped loss: 0.05034. Accuracy: 0.988
### Flips: 1025, rs: 25, checks: 205
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015965959
Train loss (w/o reg) on all data: 0.012002454
Test loss (w/o reg) on all data: 0.010230088
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.0937723e-07
Norm of the params: 8.903376
     Influence (LOO): fixed 118 labels. Loss 0.01023. Accuracy 0.999.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0066993935
Train loss (w/o reg) on all data: 0.003265883
Test loss (w/o reg) on all data: 0.011643984
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.0226273e-07
Norm of the params: 8.28675
                Loss: fixed 128 labels. Loss 0.01164. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070765726
Train loss (w/o reg) on all data: 0.06700222
Test loss (w/o reg) on all data: 0.04830101
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.5105094e-06
Norm of the params: 8.675831
              Random: fixed   7 labels. Loss 0.04830. Accuracy 0.986.
### Flips: 1025, rs: 25, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.120016e-09
Norm of the params: 6.092821
     Influence (LOO): fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0029973513
Train loss (w/o reg) on all data: 0.0010642759
Test loss (w/o reg) on all data: 0.003688331
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6248226e-08
Norm of the params: 6.2178383
                Loss: fixed 135 labels. Loss 0.00369. Accuracy 0.999.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069843516
Train loss (w/o reg) on all data: 0.06598322
Test loss (w/o reg) on all data: 0.047480766
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.1451613e-06
Norm of the params: 8.786691
              Random: fixed   9 labels. Loss 0.04748. Accuracy 0.986.
### Flips: 1025, rs: 25, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960126
Test loss (w/o reg) on all data: 0.0026560863
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3131316e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4829284e-08
Norm of the params: 6.0928235
                Loss: fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067343146
Train loss (w/o reg) on all data: 0.06339394
Test loss (w/o reg) on all data: 0.044055905
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4836453e-06
Norm of the params: 8.887293
              Random: fixed  16 labels. Loss 0.04406. Accuracy 0.990.
### Flips: 1025, rs: 25, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.856413e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8949468e-08
Norm of the params: 6.0928063
                Loss: fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06431336
Train loss (w/o reg) on all data: 0.06024006
Test loss (w/o reg) on all data: 0.041839916
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.014689e-06
Norm of the params: 9.025854
              Random: fixed  23 labels. Loss 0.04184. Accuracy 0.993.
### Flips: 1025, rs: 25, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010964
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8029198e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2121079e-08
Norm of the params: 6.092808
                Loss: fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0627356
Train loss (w/o reg) on all data: 0.058796894
Test loss (w/o reg) on all data: 0.037967302
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.450937e-06
Norm of the params: 8.875484
              Random: fixed  31 labels. Loss 0.03797. Accuracy 0.996.
### Flips: 1025, rs: 25, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7595225e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011086
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1017845e-08
Norm of the params: 6.09283
                Loss: fixed 136 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061019428
Train loss (w/o reg) on all data: 0.05704794
Test loss (w/o reg) on all data: 0.033856113
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.456128e-06
Norm of the params: 8.912336
              Random: fixed  37 labels. Loss 0.03386. Accuracy 0.994.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072044134
Train loss (w/o reg) on all data: 0.068178706
Test loss (w/o reg) on all data: 0.051559836
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4405363e-06
Norm of the params: 8.792534
Flipped loss: 0.05156. Accuracy: 0.981
### Flips: 1025, rs: 26, checks: 205
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01659406
Train loss (w/o reg) on all data: 0.012815299
Test loss (w/o reg) on all data: 0.013668626
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.4556905e-06
Norm of the params: 8.693398
     Influence (LOO): fixed 113 labels. Loss 0.01367. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006789604
Train loss (w/o reg) on all data: 0.0034998362
Test loss (w/o reg) on all data: 0.004754844
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8049889e-07
Norm of the params: 8.111434
                Loss: fixed 131 labels. Loss 0.00475. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06956269
Train loss (w/o reg) on all data: 0.06562025
Test loss (w/o reg) on all data: 0.050016183
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4648145e-06
Norm of the params: 8.8796835
              Random: fixed   6 labels. Loss 0.05002. Accuracy 0.982.
### Flips: 1025, rs: 26, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5897673e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033254568
Train loss (w/o reg) on all data: 0.0013136553
Test loss (w/o reg) on all data: 0.0029360377
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.093164e-08
Norm of the params: 6.343188
                Loss: fixed 136 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065803215
Train loss (w/o reg) on all data: 0.061941165
Test loss (w/o reg) on all data: 0.047052722
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 6.0666034e-06
Norm of the params: 8.788683
              Random: fixed  14 labels. Loss 0.04705. Accuracy 0.982.
### Flips: 1025, rs: 26, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7038932e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.04143e-08
Norm of the params: 6.09282
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06470036
Train loss (w/o reg) on all data: 0.060994674
Test loss (w/o reg) on all data: 0.04505622
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.4773054e-06
Norm of the params: 8.608932
              Random: fixed  18 labels. Loss 0.04506. Accuracy 0.983.
### Flips: 1025, rs: 26, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601289
Test loss (w/o reg) on all data: 0.0026560859
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1803992e-08
Norm of the params: 6.0928
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4119934e-08
Norm of the params: 6.092824
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636793
Train loss (w/o reg) on all data: 0.06000172
Test loss (w/o reg) on all data: 0.042713024
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.0128122e-06
Norm of the params: 8.576224
              Random: fixed  22 labels. Loss 0.04271. Accuracy 0.986.
### Flips: 1025, rs: 26, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3851655e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9074788e-08
Norm of the params: 6.0928082
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061342485
Train loss (w/o reg) on all data: 0.05782772
Test loss (w/o reg) on all data: 0.039507534
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1501372e-05
Norm of the params: 8.3842325
              Random: fixed  31 labels. Loss 0.03951. Accuracy 0.991.
### Flips: 1025, rs: 26, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560782
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.069204e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7796648e-08
Norm of the params: 6.0928254
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056390222
Train loss (w/o reg) on all data: 0.052618094
Test loss (w/o reg) on all data: 0.03524528
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.334586e-06
Norm of the params: 8.685766
              Random: fixed  41 labels. Loss 0.03525. Accuracy 0.991.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07269754
Train loss (w/o reg) on all data: 0.06857998
Test loss (w/o reg) on all data: 0.052055188
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.43439e-06
Norm of the params: 9.074757
Flipped loss: 0.05206. Accuracy: 0.983
### Flips: 1025, rs: 27, checks: 205
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02080683
Train loss (w/o reg) on all data: 0.01617683
Test loss (w/o reg) on all data: 0.015601719
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2506221e-06
Norm of the params: 9.62289
     Influence (LOO): fixed 112 labels. Loss 0.01560. Accuracy 0.994.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010107001
Train loss (w/o reg) on all data: 0.005905022
Test loss (w/o reg) on all data: 0.011095724
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2065574e-07
Norm of the params: 9.167311
                Loss: fixed 131 labels. Loss 0.01110. Accuracy 0.996.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07191208
Train loss (w/o reg) on all data: 0.06773553
Test loss (w/o reg) on all data: 0.04901046
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.3747845e-06
Norm of the params: 9.139529
              Random: fixed   6 labels. Loss 0.04901. Accuracy 0.984.
### Flips: 1025, rs: 27, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038330257
Train loss (w/o reg) on all data: 0.001657653
Test loss (w/o reg) on all data: 0.0032281035
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.688334e-08
Norm of the params: 6.596018
     Influence (LOO): fixed 141 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.684223e-09
Norm of the params: 6.0928173
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06998431
Train loss (w/o reg) on all data: 0.06578094
Test loss (w/o reg) on all data: 0.04690104
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.6202891e-06
Norm of the params: 9.168832
              Random: fixed  13 labels. Loss 0.04690. Accuracy 0.985.
### Flips: 1025, rs: 27, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.748214e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4306278e-08
Norm of the params: 6.092805
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06805033
Train loss (w/o reg) on all data: 0.0637318
Test loss (w/o reg) on all data: 0.044690516
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.0293788e-06
Norm of the params: 9.293585
              Random: fixed  18 labels. Loss 0.04469. Accuracy 0.987.
### Flips: 1025, rs: 27, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0490965e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7296756e-08
Norm of the params: 6.0928025
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06618361
Train loss (w/o reg) on all data: 0.0618191
Test loss (w/o reg) on all data: 0.04160052
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.2584845e-06
Norm of the params: 9.342925
              Random: fixed  26 labels. Loss 0.04160. Accuracy 0.989.
### Flips: 1025, rs: 27, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2979226e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.093128e-08
Norm of the params: 6.092812
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06513638
Train loss (w/o reg) on all data: 0.060952578
Test loss (w/o reg) on all data: 0.03841665
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.536966e-06
Norm of the params: 9.147465
              Random: fixed  33 labels. Loss 0.03842. Accuracy 0.994.
### Flips: 1025, rs: 27, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1599638e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.741723e-09
Norm of the params: 6.0928154
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06385078
Train loss (w/o reg) on all data: 0.05977145
Test loss (w/o reg) on all data: 0.03753603
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.77579e-06
Norm of the params: 9.032536
              Random: fixed  38 labels. Loss 0.03754. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074055895
Train loss (w/o reg) on all data: 0.07040758
Test loss (w/o reg) on all data: 0.05056211
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.9580344e-06
Norm of the params: 8.542031
Flipped loss: 0.05056. Accuracy: 0.986
### Flips: 1025, rs: 28, checks: 205
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02085612
Train loss (w/o reg) on all data: 0.016374677
Test loss (w/o reg) on all data: 0.011671878
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4152125e-06
Norm of the params: 9.467252
     Influence (LOO): fixed 109 labels. Loss 0.01167. Accuracy 0.998.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068913214
Train loss (w/o reg) on all data: 0.0035260732
Test loss (w/o reg) on all data: 0.007037948
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.278254e-07
Norm of the params: 8.20396
                Loss: fixed 128 labels. Loss 0.00704. Accuracy 0.998.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07291916
Train loss (w/o reg) on all data: 0.06936352
Test loss (w/o reg) on all data: 0.047456834
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1844967e-05
Norm of the params: 8.432844
              Random: fixed   7 labels. Loss 0.04746. Accuracy 0.986.
### Flips: 1025, rs: 28, checks: 410
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096009264
Test loss (w/o reg) on all data: 0.0026560286
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6672954e-08
Norm of the params: 6.09286
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096014066
Test loss (w/o reg) on all data: 0.0026560903
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.296545e-08
Norm of the params: 6.092781
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07104989
Train loss (w/o reg) on all data: 0.067426436
Test loss (w/o reg) on all data: 0.046689246
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.307196e-06
Norm of the params: 8.512877
              Random: fixed  12 labels. Loss 0.04669. Accuracy 0.984.
### Flips: 1025, rs: 28, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009600918
Test loss (w/o reg) on all data: 0.0026560244
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3338146e-08
Norm of the params: 6.09286
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6220324e-08
Norm of the params: 6.0928125
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0696031
Train loss (w/o reg) on all data: 0.06602027
Test loss (w/o reg) on all data: 0.04561415
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2900814e-05
Norm of the params: 8.465016
              Random: fixed  16 labels. Loss 0.04561. Accuracy 0.987.
### Flips: 1025, rs: 28, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9065931e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.491293e-09
Norm of the params: 6.0928144
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067622095
Train loss (w/o reg) on all data: 0.06379248
Test loss (w/o reg) on all data: 0.041330855
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.04827e-06
Norm of the params: 8.751701
              Random: fixed  23 labels. Loss 0.04133. Accuracy 0.989.
### Flips: 1025, rs: 28, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.281492e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2357232e-08
Norm of the params: 6.0928173
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06572917
Train loss (w/o reg) on all data: 0.06176895
Test loss (w/o reg) on all data: 0.038852718
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.64646e-06
Norm of the params: 8.899686
              Random: fixed  29 labels. Loss 0.03885. Accuracy 0.990.
### Flips: 1025, rs: 28, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012594
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.556032e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2703153e-08
Norm of the params: 6.0928135
                Loss: fixed 135 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06287534
Train loss (w/o reg) on all data: 0.05877068
Test loss (w/o reg) on all data: 0.0375241
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.3939173e-06
Norm of the params: 9.060533
              Random: fixed  36 labels. Loss 0.03752. Accuracy 0.991.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069292106
Train loss (w/o reg) on all data: 0.06477488
Test loss (w/o reg) on all data: 0.053553425
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.5266456e-06
Norm of the params: 9.504976
Flipped loss: 0.05355. Accuracy: 0.977
### Flips: 1025, rs: 29, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014507109
Train loss (w/o reg) on all data: 0.010508884
Test loss (w/o reg) on all data: 0.011130918
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8582708e-06
Norm of the params: 8.942287
     Influence (LOO): fixed 112 labels. Loss 0.01113. Accuracy 0.997.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00674514
Train loss (w/o reg) on all data: 0.0036023948
Test loss (w/o reg) on all data: 0.012667972
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.895387e-08
Norm of the params: 7.9281087
                Loss: fixed 121 labels. Loss 0.01267. Accuracy 0.996.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06668744
Train loss (w/o reg) on all data: 0.062076338
Test loss (w/o reg) on all data: 0.049192566
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.5771185e-06
Norm of the params: 9.603236
              Random: fixed  10 labels. Loss 0.04919. Accuracy 0.983.
### Flips: 1025, rs: 29, checks: 410
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042025056
Train loss (w/o reg) on all data: 0.0020580948
Test loss (w/o reg) on all data: 0.003451981
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.254855e-07
Norm of the params: 6.54891
     Influence (LOO): fixed 129 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601068
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8779292e-08
Norm of the params: 6.0928373
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065767184
Train loss (w/o reg) on all data: 0.06115071
Test loss (w/o reg) on all data: 0.044603664
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.906482e-06
Norm of the params: 9.608823
              Random: fixed  15 labels. Loss 0.04460. Accuracy 0.986.
### Flips: 1025, rs: 29, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601281
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5837623e-08
Norm of the params: 6.0928016
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5373228e-08
Norm of the params: 6.0928264
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061992407
Train loss (w/o reg) on all data: 0.05743106
Test loss (w/o reg) on all data: 0.040427014
Train acc on all data:  0.975686846584002
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2410033e-05
Norm of the params: 9.55128
              Random: fixed  25 labels. Loss 0.04043. Accuracy 0.989.
### Flips: 1025, rs: 29, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7416808e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3133551e-08
Norm of the params: 6.092809
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060579404
Train loss (w/o reg) on all data: 0.05588501
Test loss (w/o reg) on all data: 0.039562196
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.164024e-06
Norm of the params: 9.689579
              Random: fixed  29 labels. Loss 0.03956. Accuracy 0.990.
### Flips: 1025, rs: 29, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601309
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.742111e-08
Norm of the params: 6.0927963
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.536602e-09
Norm of the params: 6.092821
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057858586
Train loss (w/o reg) on all data: 0.053317532
Test loss (w/o reg) on all data: 0.03353757
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.881339e-06
Norm of the params: 9.530011
              Random: fixed  37 labels. Loss 0.03354. Accuracy 0.993.
### Flips: 1025, rs: 29, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4030237e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7556792e-08
Norm of the params: 6.092819
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057564706
Train loss (w/o reg) on all data: 0.053106267
Test loss (w/o reg) on all data: 0.033197887
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0446987e-06
Norm of the params: 9.4429245
              Random: fixed  39 labels. Loss 0.03320. Accuracy 0.993.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07298871
Train loss (w/o reg) on all data: 0.06853536
Test loss (w/o reg) on all data: 0.05042421
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.7823216e-06
Norm of the params: 9.437536
Flipped loss: 0.05042. Accuracy: 0.987
### Flips: 1025, rs: 30, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021714801
Train loss (w/o reg) on all data: 0.017367998
Test loss (w/o reg) on all data: 0.012739411
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.634468e-07
Norm of the params: 9.323951
     Influence (LOO): fixed 115 labels. Loss 0.01274. Accuracy 0.997.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056916084
Train loss (w/o reg) on all data: 0.0025574346
Test loss (w/o reg) on all data: 0.006601095
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3724215e-07
Norm of the params: 7.91729
                Loss: fixed 135 labels. Loss 0.00660. Accuracy 0.999.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07172601
Train loss (w/o reg) on all data: 0.06734626
Test loss (w/o reg) on all data: 0.0482182
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.0875157e-06
Norm of the params: 9.359216
              Random: fixed   7 labels. Loss 0.04822. Accuracy 0.989.
### Flips: 1025, rs: 30, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004041302
Train loss (w/o reg) on all data: 0.0019269403
Test loss (w/o reg) on all data: 0.0029808127
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7652758e-08
Norm of the params: 6.5028634
     Influence (LOO): fixed 141 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096008775
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.263907e-08
Norm of the params: 6.0928674
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070461474
Train loss (w/o reg) on all data: 0.066165626
Test loss (w/o reg) on all data: 0.047125928
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.7652156e-06
Norm of the params: 9.269141
              Random: fixed  12 labels. Loss 0.04713. Accuracy 0.989.
### Flips: 1025, rs: 30, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4122504e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2187974e-08
Norm of the params: 6.0928073
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069518134
Train loss (w/o reg) on all data: 0.06524277
Test loss (w/o reg) on all data: 0.04401981
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.013704e-06
Norm of the params: 9.247016
              Random: fixed  18 labels. Loss 0.04402. Accuracy 0.990.
### Flips: 1025, rs: 30, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601119
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2447827e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4100137e-08
Norm of the params: 6.09281
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06724835
Train loss (w/o reg) on all data: 0.06276255
Test loss (w/o reg) on all data: 0.041441154
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2354899e-05
Norm of the params: 9.471853
              Random: fixed  27 labels. Loss 0.04144. Accuracy 0.992.
### Flips: 1025, rs: 30, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0302161e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2734744e-08
Norm of the params: 6.092818
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06318209
Train loss (w/o reg) on all data: 0.05874262
Test loss (w/o reg) on all data: 0.03699446
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7055644e-06
Norm of the params: 9.422815
              Random: fixed  38 labels. Loss 0.03699. Accuracy 0.992.
### Flips: 1025, rs: 30, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010143
Test loss (w/o reg) on all data: 0.002656034
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0077468e-08
Norm of the params: 6.0928454
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012495
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3587857e-08
Norm of the params: 6.0928063
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060262933
Train loss (w/o reg) on all data: 0.055757686
Test loss (w/o reg) on all data: 0.034760706
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.8876817e-06
Norm of the params: 9.492364
              Random: fixed  47 labels. Loss 0.03476. Accuracy 0.993.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06976328
Train loss (w/o reg) on all data: 0.06598303
Test loss (w/o reg) on all data: 0.048889734
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.358071e-06
Norm of the params: 8.695119
Flipped loss: 0.04889. Accuracy: 0.986
### Flips: 1025, rs: 31, checks: 205
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017577536
Train loss (w/o reg) on all data: 0.013959558
Test loss (w/o reg) on all data: 0.016075835
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6140608e-06
Norm of the params: 8.506443
     Influence (LOO): fixed 104 labels. Loss 0.01608. Accuracy 0.997.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006181985
Train loss (w/o reg) on all data: 0.0032598572
Test loss (w/o reg) on all data: 0.008975738
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1649998e-07
Norm of the params: 7.644774
                Loss: fixed 123 labels. Loss 0.00898. Accuracy 0.998.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06684802
Train loss (w/o reg) on all data: 0.06286332
Test loss (w/o reg) on all data: 0.046108562
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.4702583e-06
Norm of the params: 8.927148
              Random: fixed  10 labels. Loss 0.04611. Accuracy 0.988.
### Flips: 1025, rs: 31, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601092
Test loss (w/o reg) on all data: 0.0026560354
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.069183e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028651734
Train loss (w/o reg) on all data: 0.0009913581
Test loss (w/o reg) on all data: 0.0033056594
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2573971e-08
Norm of the params: 6.12179
                Loss: fixed 129 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063387625
Train loss (w/o reg) on all data: 0.059350062
Test loss (w/o reg) on all data: 0.043975294
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.4266707e-06
Norm of the params: 8.986169
              Random: fixed  19 labels. Loss 0.04398. Accuracy 0.983.
### Flips: 1025, rs: 31, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3970789e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.145156e-09
Norm of the params: 6.0928164
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062412314
Train loss (w/o reg) on all data: 0.05836354
Test loss (w/o reg) on all data: 0.04292505
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2257836e-05
Norm of the params: 8.998638
              Random: fixed  22 labels. Loss 0.04293. Accuracy 0.986.
### Flips: 1025, rs: 31, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8858104e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8737774e-08
Norm of the params: 6.0928106
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061749455
Train loss (w/o reg) on all data: 0.057765808
Test loss (w/o reg) on all data: 0.041077066
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4677655e-06
Norm of the params: 8.925971
              Random: fixed  27 labels. Loss 0.04108. Accuracy 0.990.
### Flips: 1025, rs: 31, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2687805e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4244351e-08
Norm of the params: 6.0928106
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058265038
Train loss (w/o reg) on all data: 0.054362275
Test loss (w/o reg) on all data: 0.037470464
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.3200032e-06
Norm of the params: 8.8348875
              Random: fixed  37 labels. Loss 0.03747. Accuracy 0.989.
### Flips: 1025, rs: 31, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6584685e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7659045e-08
Norm of the params: 6.0928254
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057609707
Train loss (w/o reg) on all data: 0.05375946
Test loss (w/o reg) on all data: 0.035842445
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.784507e-06
Norm of the params: 8.775249
              Random: fixed  40 labels. Loss 0.03584. Accuracy 0.990.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06844594
Train loss (w/o reg) on all data: 0.06470142
Test loss (w/o reg) on all data: 0.04292034
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.365101e-06
Norm of the params: 8.653924
Flipped loss: 0.04292. Accuracy: 0.989
### Flips: 1025, rs: 32, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013298634
Train loss (w/o reg) on all data: 0.009429774
Test loss (w/o reg) on all data: 0.010416668
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.8804714e-07
Norm of the params: 8.796431
     Influence (LOO): fixed 112 labels. Loss 0.01042. Accuracy 0.997.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007305097
Train loss (w/o reg) on all data: 0.003764373
Test loss (w/o reg) on all data: 0.008854765
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.999294e-07
Norm of the params: 8.415134
                Loss: fixed 120 labels. Loss 0.00885. Accuracy 0.997.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06651337
Train loss (w/o reg) on all data: 0.06281449
Test loss (w/o reg) on all data: 0.040464714
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.2507933e-06
Norm of the params: 8.601024
              Random: fixed   8 labels. Loss 0.04046. Accuracy 0.990.
### Flips: 1025, rs: 32, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601033
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5564448e-08
Norm of the params: 6.092843
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038183415
Train loss (w/o reg) on all data: 0.0015469695
Test loss (w/o reg) on all data: 0.003532439
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9528965e-08
Norm of the params: 6.739988
                Loss: fixed 129 labels. Loss 0.00353. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06452982
Train loss (w/o reg) on all data: 0.060671154
Test loss (w/o reg) on all data: 0.038436104
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9742773e-05
Norm of the params: 8.784839
              Random: fixed  13 labels. Loss 0.03844. Accuracy 0.992.
### Flips: 1025, rs: 32, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601119
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4176639e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012413
Test loss (w/o reg) on all data: 0.0026560754
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2291833e-08
Norm of the params: 6.092809
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062570624
Train loss (w/o reg) on all data: 0.058646794
Test loss (w/o reg) on all data: 0.036198452
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.845159e-06
Norm of the params: 8.858698
              Random: fixed  19 labels. Loss 0.03620. Accuracy 0.992.
### Flips: 1025, rs: 32, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096005056
Test loss (w/o reg) on all data: 0.0026559876
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4396433e-07
Norm of the params: 6.09293
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0961433e-08
Norm of the params: 6.0928116
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05992245
Train loss (w/o reg) on all data: 0.05614576
Test loss (w/o reg) on all data: 0.03352613
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.2240557e-06
Norm of the params: 8.691017
              Random: fixed  28 labels. Loss 0.03353. Accuracy 0.992.
### Flips: 1025, rs: 32, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009600653
Test loss (w/o reg) on all data: 0.002655997
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1459435e-07
Norm of the params: 6.092904
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.672851e-08
Norm of the params: 6.092814
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057242613
Train loss (w/o reg) on all data: 0.053439725
Test loss (w/o reg) on all data: 0.03158217
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.4406228e-06
Norm of the params: 8.721112
              Random: fixed  35 labels. Loss 0.03158. Accuracy 0.994.
### Flips: 1025, rs: 32, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600748
Test loss (w/o reg) on all data: 0.0026560053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.935839e-08
Norm of the params: 6.092889
     Influence (LOO): fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5571167e-08
Norm of the params: 6.092814
                Loss: fixed 130 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056122497
Train loss (w/o reg) on all data: 0.05215616
Test loss (w/o reg) on all data: 0.031634975
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.340487e-06
Norm of the params: 8.906555
              Random: fixed  39 labels. Loss 0.03163. Accuracy 0.995.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06792783
Train loss (w/o reg) on all data: 0.06370344
Test loss (w/o reg) on all data: 0.043006282
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.756622e-06
Norm of the params: 9.191722
Flipped loss: 0.04301. Accuracy: 0.991
### Flips: 1025, rs: 33, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010889303
Train loss (w/o reg) on all data: 0.007552213
Test loss (w/o reg) on all data: 0.0076597547
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3681037e-07
Norm of the params: 8.169565
     Influence (LOO): fixed 111 labels. Loss 0.00766. Accuracy 0.998.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040145055
Train loss (w/o reg) on all data: 0.0016519999
Test loss (w/o reg) on all data: 0.0030373533
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4622378e-08
Norm of the params: 6.8738713
                Loss: fixed 121 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067575045
Train loss (w/o reg) on all data: 0.063322574
Test loss (w/o reg) on all data: 0.042179875
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.1369585e-06
Norm of the params: 9.222223
              Random: fixed   2 labels. Loss 0.04218. Accuracy 0.991.
### Flips: 1025, rs: 33, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.53684e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.528994e-08
Norm of the params: 6.0928283
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06528479
Train loss (w/o reg) on all data: 0.061085187
Test loss (w/o reg) on all data: 0.039883703
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7681077e-05
Norm of the params: 9.164718
              Random: fixed   9 labels. Loss 0.03988. Accuracy 0.991.
### Flips: 1025, rs: 33, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7355055e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7218698e-08
Norm of the params: 6.0928283
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06323544
Train loss (w/o reg) on all data: 0.0588849
Test loss (w/o reg) on all data: 0.039840925
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.0058306e-06
Norm of the params: 9.327954
              Random: fixed  13 labels. Loss 0.03984. Accuracy 0.991.
### Flips: 1025, rs: 33, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2427987e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5536573e-08
Norm of the params: 6.0928173
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060711537
Train loss (w/o reg) on all data: 0.056578293
Test loss (w/o reg) on all data: 0.0381922
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.494538e-06
Norm of the params: 9.092022
              Random: fixed  20 labels. Loss 0.03819. Accuracy 0.989.
### Flips: 1025, rs: 33, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4693271e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1975157e-08
Norm of the params: 6.092825
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056912668
Train loss (w/o reg) on all data: 0.052711356
Test loss (w/o reg) on all data: 0.03635715
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.794611e-06
Norm of the params: 9.166583
              Random: fixed  30 labels. Loss 0.03636. Accuracy 0.987.
### Flips: 1025, rs: 33, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5906432e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0777406e-08
Norm of the params: 6.09282
                Loss: fixed 125 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05373714
Train loss (w/o reg) on all data: 0.049445357
Test loss (w/o reg) on all data: 0.035231266
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.775832e-06
Norm of the params: 9.264753
              Random: fixed  37 labels. Loss 0.03523. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07176083
Train loss (w/o reg) on all data: 0.06762348
Test loss (w/o reg) on all data: 0.053874925
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.3478824e-06
Norm of the params: 9.096538
Flipped loss: 0.05387. Accuracy: 0.984
### Flips: 1025, rs: 34, checks: 205
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025757797
Train loss (w/o reg) on all data: 0.021024149
Test loss (w/o reg) on all data: 0.019988086
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5306447e-06
Norm of the params: 9.730003
     Influence (LOO): fixed 111 labels. Loss 0.01999. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012236404
Train loss (w/o reg) on all data: 0.007870599
Test loss (w/o reg) on all data: 0.0094098495
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4655486e-07
Norm of the params: 9.344309
                Loss: fixed 135 labels. Loss 0.00941. Accuracy 0.997.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07061802
Train loss (w/o reg) on all data: 0.06652806
Test loss (w/o reg) on all data: 0.051128805
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.3562776e-05
Norm of the params: 9.044289
              Random: fixed   7 labels. Loss 0.05113. Accuracy 0.987.
### Flips: 1025, rs: 34, checks: 410
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054578134
Train loss (w/o reg) on all data: 0.0029521475
Test loss (w/o reg) on all data: 0.004695301
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.081137e-08
Norm of the params: 7.0790763
     Influence (LOO): fixed 147 labels. Loss 0.00470. Accuracy 0.998.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034880126
Train loss (w/o reg) on all data: 0.0013270362
Test loss (w/o reg) on all data: 0.0033187736
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.02205675e-07
Norm of the params: 6.574156
                Loss: fixed 149 labels. Loss 0.00332. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0699549
Train loss (w/o reg) on all data: 0.06597856
Test loss (w/o reg) on all data: 0.047528125
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.022516e-06
Norm of the params: 8.917782
              Random: fixed  16 labels. Loss 0.04753. Accuracy 0.989.
### Flips: 1025, rs: 34, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096004066
Test loss (w/o reg) on all data: 0.0026559208
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.607188e-07
Norm of the params: 6.0929446
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9361199e-08
Norm of the params: 6.0928097
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069277294
Train loss (w/o reg) on all data: 0.06533985
Test loss (w/o reg) on all data: 0.04524618
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.589962e-05
Norm of the params: 8.874054
              Random: fixed  22 labels. Loss 0.04525. Accuracy 0.992.
### Flips: 1025, rs: 34, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013054
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1310364e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560405
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2404986e-08
Norm of the params: 6.0928273
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067029685
Train loss (w/o reg) on all data: 0.063068435
Test loss (w/o reg) on all data: 0.042483535
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.442276e-06
Norm of the params: 8.900844
              Random: fixed  29 labels. Loss 0.04248. Accuracy 0.993.
### Flips: 1025, rs: 34, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601321
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3748538e-08
Norm of the params: 6.092796
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9725858e-08
Norm of the params: 6.0928254
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06548904
Train loss (w/o reg) on all data: 0.061524212
Test loss (w/o reg) on all data: 0.041613773
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9739747e-06
Norm of the params: 8.90486
              Random: fixed  34 labels. Loss 0.04161. Accuracy 0.991.
### Flips: 1025, rs: 34, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560354
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1141682e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960132
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9799359e-08
Norm of the params: 6.092796
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06317726
Train loss (w/o reg) on all data: 0.05920882
Test loss (w/o reg) on all data: 0.041210692
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.5009215e-06
Norm of the params: 8.9089155
              Random: fixed  41 labels. Loss 0.04121. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07040042
Train loss (w/o reg) on all data: 0.06611317
Test loss (w/o reg) on all data: 0.047913827
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.40182e-06
Norm of the params: 9.25986
Flipped loss: 0.04791. Accuracy: 0.987
### Flips: 1025, rs: 35, checks: 205
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014569076
Train loss (w/o reg) on all data: 0.01044346
Test loss (w/o reg) on all data: 0.009396191
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8100532e-07
Norm of the params: 9.08363
     Influence (LOO): fixed 114 labels. Loss 0.00940. Accuracy 0.998.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005399732
Train loss (w/o reg) on all data: 0.0025581266
Test loss (w/o reg) on all data: 0.006568602
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.61899e-08
Norm of the params: 7.5387073
                Loss: fixed 131 labels. Loss 0.00657. Accuracy 0.998.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067827605
Train loss (w/o reg) on all data: 0.06339491
Test loss (w/o reg) on all data: 0.043491248
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.2041265e-06
Norm of the params: 9.415617
              Random: fixed  10 labels. Loss 0.04349. Accuracy 0.991.
### Flips: 1025, rs: 35, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8572038e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7231839e-08
Norm of the params: 6.0928097
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06694424
Train loss (w/o reg) on all data: 0.062510535
Test loss (w/o reg) on all data: 0.04284244
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.2852e-06
Norm of the params: 9.416692
              Random: fixed  14 labels. Loss 0.04284. Accuracy 0.992.
### Flips: 1025, rs: 35, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3386639e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.806304e-09
Norm of the params: 6.0928135
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06495989
Train loss (w/o reg) on all data: 0.060517352
Test loss (w/o reg) on all data: 0.04136459
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.8954282e-06
Norm of the params: 9.426067
              Random: fixed  21 labels. Loss 0.04136. Accuracy 0.992.
### Flips: 1025, rs: 35, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.108437e-08
Norm of the params: 6.0928297
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.744555e-09
Norm of the params: 6.092815
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06182807
Train loss (w/o reg) on all data: 0.05734494
Test loss (w/o reg) on all data: 0.037520483
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.2181327e-06
Norm of the params: 9.469036
              Random: fixed  31 labels. Loss 0.03752. Accuracy 0.991.
### Flips: 1025, rs: 35, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010667
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2975788e-08
Norm of the params: 6.0928364
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1898215e-08
Norm of the params: 6.092814
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060397808
Train loss (w/o reg) on all data: 0.055944607
Test loss (w/o reg) on all data: 0.035827737
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.202036e-06
Norm of the params: 9.437373
              Random: fixed  35 labels. Loss 0.03583. Accuracy 0.991.
### Flips: 1025, rs: 35, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601578
Test loss (w/o reg) on all data: 0.0026561131
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.403103e-08
Norm of the params: 6.0927534
     Influence (LOO): fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4087301e-08
Norm of the params: 6.092821
                Loss: fixed 134 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05651623
Train loss (w/o reg) on all data: 0.05226397
Test loss (w/o reg) on all data: 0.031640496
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.035889e-06
Norm of the params: 9.221995
              Random: fixed  45 labels. Loss 0.03164. Accuracy 0.993.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06841079
Train loss (w/o reg) on all data: 0.06455484
Test loss (w/o reg) on all data: 0.050787076
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.521928e-06
Norm of the params: 8.781743
Flipped loss: 0.05079. Accuracy: 0.983
### Flips: 1025, rs: 36, checks: 205
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011897682
Train loss (w/o reg) on all data: 0.007812456
Test loss (w/o reg) on all data: 0.009676055
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7044328e-07
Norm of the params: 9.039056
     Influence (LOO): fixed 108 labels. Loss 0.00968. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070680985
Train loss (w/o reg) on all data: 0.0037137908
Test loss (w/o reg) on all data: 0.0050477576
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4712526e-07
Norm of the params: 8.190613
                Loss: fixed 119 labels. Loss 0.00505. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06741712
Train loss (w/o reg) on all data: 0.0635856
Test loss (w/o reg) on all data: 0.04833346
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.4236467e-05
Norm of the params: 8.753879
              Random: fixed   5 labels. Loss 0.04833. Accuracy 0.986.
### Flips: 1025, rs: 36, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004051995
Train loss (w/o reg) on all data: 0.0017940673
Test loss (w/o reg) on all data: 0.003923343
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.038182e-08
Norm of the params: 6.7200117
     Influence (LOO): fixed 123 labels. Loss 0.00392. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.404033e-08
Norm of the params: 6.092814
                Loss: fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06591129
Train loss (w/o reg) on all data: 0.062088385
Test loss (w/o reg) on all data: 0.045379765
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.0387427e-06
Norm of the params: 8.744036
              Random: fixed  11 labels. Loss 0.04538. Accuracy 0.986.
### Flips: 1025, rs: 36, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6882922e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162387
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5418195e-08
Norm of the params: 6.0928197
                Loss: fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064832546
Train loss (w/o reg) on all data: 0.061153013
Test loss (w/o reg) on all data: 0.04344837
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.276167e-06
Norm of the params: 8.578502
              Random: fixed  16 labels. Loss 0.04345. Accuracy 0.987.
### Flips: 1025, rs: 36, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601222
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.025415e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4437421e-08
Norm of the params: 6.092831
                Loss: fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06387559
Train loss (w/o reg) on all data: 0.060164742
Test loss (w/o reg) on all data: 0.041767612
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.2226023e-06
Norm of the params: 8.614928
              Random: fixed  19 labels. Loss 0.04177. Accuracy 0.988.
### Flips: 1025, rs: 36, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7423817e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601121
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.258851e-08
Norm of the params: 6.0928273
                Loss: fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06070438
Train loss (w/o reg) on all data: 0.05657943
Test loss (w/o reg) on all data: 0.03727727
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.21268e-06
Norm of the params: 9.082896
              Random: fixed  27 labels. Loss 0.03728. Accuracy 0.990.
### Flips: 1025, rs: 36, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19868515e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2408317e-08
Norm of the params: 6.092819
                Loss: fixed 124 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057823412
Train loss (w/o reg) on all data: 0.05383222
Test loss (w/o reg) on all data: 0.03473149
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.2933098e-06
Norm of the params: 8.934416
              Random: fixed  33 labels. Loss 0.03473. Accuracy 0.991.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070018575
Train loss (w/o reg) on all data: 0.0650805
Test loss (w/o reg) on all data: 0.053327765
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.260608e-06
Norm of the params: 9.93788
Flipped loss: 0.05333. Accuracy: 0.980
### Flips: 1025, rs: 37, checks: 205
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019080322
Train loss (w/o reg) on all data: 0.014551264
Test loss (w/o reg) on all data: 0.011578748
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.908951e-07
Norm of the params: 9.517415
     Influence (LOO): fixed 107 labels. Loss 0.01158. Accuracy 0.998.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009113327
Train loss (w/o reg) on all data: 0.005027295
Test loss (w/o reg) on all data: 0.0058795805
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2816918e-07
Norm of the params: 9.039946
                Loss: fixed 127 labels. Loss 0.00588. Accuracy 0.999.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06840115
Train loss (w/o reg) on all data: 0.06353451
Test loss (w/o reg) on all data: 0.050176393
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.589554e-06
Norm of the params: 9.865732
              Random: fixed   6 labels. Loss 0.05018. Accuracy 0.982.
### Flips: 1025, rs: 37, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073907813
Train loss (w/o reg) on all data: 0.0038386087
Test loss (w/o reg) on all data: 0.006220542
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.498536e-07
Norm of the params: 8.428728
     Influence (LOO): fixed 130 labels. Loss 0.00622. Accuracy 0.998.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6939696e-08
Norm of the params: 6.09282
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06774074
Train loss (w/o reg) on all data: 0.06297324
Test loss (w/o reg) on all data: 0.048763126
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.935773e-06
Norm of the params: 9.764731
              Random: fixed   9 labels. Loss 0.04876. Accuracy 0.985.
### Flips: 1025, rs: 37, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.52853e-09
Norm of the params: 6.0928273
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.872288e-09
Norm of the params: 6.0928135
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06631578
Train loss (w/o reg) on all data: 0.061339278
Test loss (w/o reg) on all data: 0.04739348
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8611962e-06
Norm of the params: 9.976471
              Random: fixed  14 labels. Loss 0.04739. Accuracy 0.984.
### Flips: 1025, rs: 37, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601039
Test loss (w/o reg) on all data: 0.0026560454
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2301748e-08
Norm of the params: 6.0928416
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.085609e-09
Norm of the params: 6.0928164
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065442994
Train loss (w/o reg) on all data: 0.06049218
Test loss (w/o reg) on all data: 0.045778316
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.0877768e-06
Norm of the params: 9.950689
              Random: fixed  20 labels. Loss 0.04578. Accuracy 0.984.
### Flips: 1025, rs: 37, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601295
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6265402e-08
Norm of the params: 6.0927997
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2938514e-08
Norm of the params: 6.09281
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06281105
Train loss (w/o reg) on all data: 0.057901904
Test loss (w/o reg) on all data: 0.04230469
Train acc on all data:  0.975929978118162
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0579083e-05
Norm of the params: 9.908724
              Random: fixed  30 labels. Loss 0.04230. Accuracy 0.989.
### Flips: 1025, rs: 37, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601021
Test loss (w/o reg) on all data: 0.0026560354
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3765964e-08
Norm of the params: 6.0928435
     Influence (LOO): fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.724414e-09
Norm of the params: 6.0928164
                Loss: fixed 137 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059010807
Train loss (w/o reg) on all data: 0.054055106
Test loss (w/o reg) on all data: 0.039134465
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.63551e-06
Norm of the params: 9.955603
              Random: fixed  39 labels. Loss 0.03913. Accuracy 0.991.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06975737
Train loss (w/o reg) on all data: 0.06545075
Test loss (w/o reg) on all data: 0.055168882
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.097934e-06
Norm of the params: 9.28076
Flipped loss: 0.05517. Accuracy: 0.982
### Flips: 1025, rs: 38, checks: 205
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014590457
Train loss (w/o reg) on all data: 0.0103350915
Test loss (w/o reg) on all data: 0.015714461
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.7873464e-06
Norm of the params: 9.225362
     Influence (LOO): fixed 109 labels. Loss 0.01571. Accuracy 0.995.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008086001
Train loss (w/o reg) on all data: 0.0045239744
Test loss (w/o reg) on all data: 0.007693779
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5204108e-07
Norm of the params: 8.440411
                Loss: fixed 121 labels. Loss 0.00769. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06903688
Train loss (w/o reg) on all data: 0.064882286
Test loss (w/o reg) on all data: 0.05174399
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.4193648e-06
Norm of the params: 9.115477
              Random: fixed   5 labels. Loss 0.05174. Accuracy 0.983.
### Flips: 1025, rs: 38, checks: 410
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034013672
Train loss (w/o reg) on all data: 0.0013232387
Test loss (w/o reg) on all data: 0.0029672796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4990186e-08
Norm of the params: 6.4469037
     Influence (LOO): fixed 129 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039972235
Train loss (w/o reg) on all data: 0.0015984002
Test loss (w/o reg) on all data: 0.0038683275
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4968323e-08
Norm of the params: 6.9265046
                Loss: fixed 130 labels. Loss 0.00387. Accuracy 0.999.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068019256
Train loss (w/o reg) on all data: 0.06404889
Test loss (w/o reg) on all data: 0.050673757
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.467834e-06
Norm of the params: 8.911081
              Random: fixed   9 labels. Loss 0.05067. Accuracy 0.983.
### Flips: 1025, rs: 38, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2531783e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4926279e-08
Norm of the params: 6.092809
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06629638
Train loss (w/o reg) on all data: 0.06244866
Test loss (w/o reg) on all data: 0.048258856
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.9511165e-06
Norm of the params: 8.772363
              Random: fixed  15 labels. Loss 0.04826. Accuracy 0.984.
### Flips: 1025, rs: 38, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960107
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7450525e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601223
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.610641e-09
Norm of the params: 6.0928116
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06500693
Train loss (w/o reg) on all data: 0.06112829
Test loss (w/o reg) on all data: 0.045172114
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1914847e-05
Norm of the params: 8.80754
              Random: fixed  21 labels. Loss 0.04517. Accuracy 0.987.
### Flips: 1025, rs: 38, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3477332e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560496
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7032464e-08
Norm of the params: 6.0928254
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063341506
Train loss (w/o reg) on all data: 0.05968007
Test loss (w/o reg) on all data: 0.043013904
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.100082e-06
Norm of the params: 8.557375
              Random: fixed  27 labels. Loss 0.04301. Accuracy 0.987.
### Flips: 1025, rs: 38, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601248
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0635516e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.579416e-09
Norm of the params: 6.0928245
                Loss: fixed 131 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05804143
Train loss (w/o reg) on all data: 0.054443456
Test loss (w/o reg) on all data: 0.038370468
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.1814975e-06
Norm of the params: 8.482896
              Random: fixed  38 labels. Loss 0.03837. Accuracy 0.991.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06883493
Train loss (w/o reg) on all data: 0.064860016
Test loss (w/o reg) on all data: 0.053712692
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.1663585e-06
Norm of the params: 8.916178
Flipped loss: 0.05371. Accuracy: 0.986
### Flips: 1025, rs: 39, checks: 205
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01470165
Train loss (w/o reg) on all data: 0.0103217885
Test loss (w/o reg) on all data: 0.011564729
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4828664e-06
Norm of the params: 9.359339
     Influence (LOO): fixed 115 labels. Loss 0.01156. Accuracy 0.995.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010848694
Train loss (w/o reg) on all data: 0.0064625475
Test loss (w/o reg) on all data: 0.007779302
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.3370707e-07
Norm of the params: 9.366052
                Loss: fixed 120 labels. Loss 0.00778. Accuracy 0.999.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06640784
Train loss (w/o reg) on all data: 0.062479015
Test loss (w/o reg) on all data: 0.052343562
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.2342314e-06
Norm of the params: 8.864331
              Random: fixed   9 labels. Loss 0.05234. Accuracy 0.985.
### Flips: 1025, rs: 39, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004929009
Train loss (w/o reg) on all data: 0.0024416035
Test loss (w/o reg) on all data: 0.004192696
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4093058e-07
Norm of the params: 7.053234
     Influence (LOO): fixed 135 labels. Loss 0.00419. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003593239
Train loss (w/o reg) on all data: 0.0013895608
Test loss (w/o reg) on all data: 0.0031607454
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.587801e-08
Norm of the params: 6.6387925
                Loss: fixed 137 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06500801
Train loss (w/o reg) on all data: 0.061336845
Test loss (w/o reg) on all data: 0.04668127
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.743289e-06
Norm of the params: 8.568733
              Random: fixed  22 labels. Loss 0.04668. Accuracy 0.988.
### Flips: 1025, rs: 39, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601088
Test loss (w/o reg) on all data: 0.002656036
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.531369e-08
Norm of the params: 6.092833
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4359976e-08
Norm of the params: 6.0928106
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06327897
Train loss (w/o reg) on all data: 0.059483036
Test loss (w/o reg) on all data: 0.04550883
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.551083e-06
Norm of the params: 8.713139
              Random: fixed  27 labels. Loss 0.04551. Accuracy 0.988.
### Flips: 1025, rs: 39, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011435
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.535047e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.29967015e-08
Norm of the params: 6.092809
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06188267
Train loss (w/o reg) on all data: 0.05790076
Test loss (w/o reg) on all data: 0.042491347
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3406832e-05
Norm of the params: 8.924026
              Random: fixed  32 labels. Loss 0.04249. Accuracy 0.991.
### Flips: 1025, rs: 39, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3970197e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.686191e-09
Norm of the params: 6.0928216
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060036913
Train loss (w/o reg) on all data: 0.05611967
Test loss (w/o reg) on all data: 0.040916257
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7511786e-06
Norm of the params: 8.851262
              Random: fixed  38 labels. Loss 0.04092. Accuracy 0.991.
### Flips: 1025, rs: 39, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011476
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.224853e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.308591e-08
Norm of the params: 6.0928216
                Loss: fixed 138 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056901373
Train loss (w/o reg) on all data: 0.05300376
Test loss (w/o reg) on all data: 0.039051812
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.2927095e-06
Norm of the params: 8.829057
              Random: fixed  45 labels. Loss 0.03905. Accuracy 0.987.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077994704
Train loss (w/o reg) on all data: 0.07363813
Test loss (w/o reg) on all data: 0.05616202
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.0791556e-06
Norm of the params: 9.334418
Flipped loss: 0.05616. Accuracy: 0.980
### Flips: 1230, rs: 0, checks: 205
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025148908
Train loss (w/o reg) on all data: 0.0200238
Test loss (w/o reg) on all data: 0.018259147
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.2830617e-06
Norm of the params: 10.124334
     Influence (LOO): fixed 116 labels. Loss 0.01826. Accuracy 0.994.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010462775
Train loss (w/o reg) on all data: 0.006300542
Test loss (w/o reg) on all data: 0.006924718
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.395627e-07
Norm of the params: 9.123852
                Loss: fixed 145 labels. Loss 0.00692. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07687061
Train loss (w/o reg) on all data: 0.072604425
Test loss (w/o reg) on all data: 0.054728445
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.9668093e-06
Norm of the params: 9.237083
              Random: fixed   5 labels. Loss 0.05473. Accuracy 0.983.
### Flips: 1230, rs: 0, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006590208
Train loss (w/o reg) on all data: 0.0036737844
Test loss (w/o reg) on all data: 0.004710249
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9869653e-07
Norm of the params: 7.6373086
     Influence (LOO): fixed 152 labels. Loss 0.00471. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038306601
Train loss (w/o reg) on all data: 0.0016074256
Test loss (w/o reg) on all data: 0.0030214798
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.14655634e-07
Norm of the params: 6.6681848
                Loss: fixed 155 labels. Loss 0.00302. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07372511
Train loss (w/o reg) on all data: 0.069306076
Test loss (w/o reg) on all data: 0.051463816
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.8208723e-06
Norm of the params: 9.401106
              Random: fixed  17 labels. Loss 0.05146. Accuracy 0.984.
### Flips: 1230, rs: 0, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601126
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2296883e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1081943e-08
Norm of the params: 6.0928082
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071804814
Train loss (w/o reg) on all data: 0.06741757
Test loss (w/o reg) on all data: 0.04783265
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.503197e-06
Norm of the params: 9.367224
              Random: fixed  24 labels. Loss 0.04783. Accuracy 0.986.
### Flips: 1230, rs: 0, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010807
Test loss (w/o reg) on all data: 0.0026560405
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9564232e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3905904e-09
Norm of the params: 6.092816
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06862311
Train loss (w/o reg) on all data: 0.06430997
Test loss (w/o reg) on all data: 0.04496367
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.913476e-06
Norm of the params: 9.287775
              Random: fixed  35 labels. Loss 0.04496. Accuracy 0.987.
### Flips: 1230, rs: 0, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8234377e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4919465e-08
Norm of the params: 6.092807
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06530095
Train loss (w/o reg) on all data: 0.06091174
Test loss (w/o reg) on all data: 0.04068617
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.2172783e-06
Norm of the params: 9.369323
              Random: fixed  47 labels. Loss 0.04069. Accuracy 0.987.
### Flips: 1230, rs: 0, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1613497e-08
Norm of the params: 6.0928273
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601256
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0247232e-08
Norm of the params: 6.092806
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061285622
Train loss (w/o reg) on all data: 0.057010047
Test loss (w/o reg) on all data: 0.034662306
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.5415344e-06
Norm of the params: 9.247244
              Random: fixed  58 labels. Loss 0.03466. Accuracy 0.993.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07666468
Train loss (w/o reg) on all data: 0.07300006
Test loss (w/o reg) on all data: 0.053331323
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.165115e-06
Norm of the params: 8.561097
Flipped loss: 0.05333. Accuracy: 0.987
### Flips: 1230, rs: 1, checks: 205
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02376789
Train loss (w/o reg) on all data: 0.019676186
Test loss (w/o reg) on all data: 0.0124587985
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.32957e-06
Norm of the params: 9.04622
     Influence (LOO): fixed 118 labels. Loss 0.01246. Accuracy 0.997.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009392114
Train loss (w/o reg) on all data: 0.0054189353
Test loss (w/o reg) on all data: 0.01014798
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.8042285e-07
Norm of the params: 8.914234
                Loss: fixed 141 labels. Loss 0.01015. Accuracy 0.997.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07555914
Train loss (w/o reg) on all data: 0.07191547
Test loss (w/o reg) on all data: 0.052262105
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.1522194e-06
Norm of the params: 8.536587
              Random: fixed   8 labels. Loss 0.05226. Accuracy 0.987.
### Flips: 1230, rs: 1, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601234
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6359728e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003685669
Train loss (w/o reg) on all data: 0.0014657977
Test loss (w/o reg) on all data: 0.0037918712
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1231573e-08
Norm of the params: 6.66314
                Loss: fixed 153 labels. Loss 0.00379. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07463867
Train loss (w/o reg) on all data: 0.071010046
Test loss (w/o reg) on all data: 0.049941976
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.7885187e-06
Norm of the params: 8.518949
              Random: fixed  13 labels. Loss 0.04994. Accuracy 0.990.
### Flips: 1230, rs: 1, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601262
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9185487e-08
Norm of the params: 6.092805
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3843976e-08
Norm of the params: 6.0928235
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07254629
Train loss (w/o reg) on all data: 0.06872703
Test loss (w/o reg) on all data: 0.047488853
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2747871e-05
Norm of the params: 8.739863
              Random: fixed  22 labels. Loss 0.04749. Accuracy 0.989.
### Flips: 1230, rs: 1, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.0026560803
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6031501e-08
Norm of the params: 6.092804
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.472706e-09
Norm of the params: 6.092818
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07051312
Train loss (w/o reg) on all data: 0.06666761
Test loss (w/o reg) on all data: 0.04439368
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.2841883e-06
Norm of the params: 8.769846
              Random: fixed  32 labels. Loss 0.04439. Accuracy 0.989.
### Flips: 1230, rs: 1, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7211763e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0740047e-08
Norm of the params: 6.092822
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066149786
Train loss (w/o reg) on all data: 0.062395085
Test loss (w/o reg) on all data: 0.037056446
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1606737e-05
Norm of the params: 8.66568
              Random: fixed  48 labels. Loss 0.03706. Accuracy 0.994.
### Flips: 1230, rs: 1, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.28364155e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.560871e-09
Norm of the params: 6.0928245
                Loss: fixed 156 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06286322
Train loss (w/o reg) on all data: 0.058977816
Test loss (w/o reg) on all data: 0.034740664
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2353448e-05
Norm of the params: 8.815226
              Random: fixed  58 labels. Loss 0.03474. Accuracy 0.996.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079680525
Train loss (w/o reg) on all data: 0.07567552
Test loss (w/o reg) on all data: 0.06706878
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.5632595e-06
Norm of the params: 8.949867
Flipped loss: 0.06707. Accuracy: 0.971
### Flips: 1230, rs: 2, checks: 205
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03336711
Train loss (w/o reg) on all data: 0.027662113
Test loss (w/o reg) on all data: 0.025308102
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.563844e-06
Norm of the params: 10.681757
     Influence (LOO): fixed 118 labels. Loss 0.02531. Accuracy 0.990.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016753858
Train loss (w/o reg) on all data: 0.010788465
Test loss (w/o reg) on all data: 0.014259105
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.641719e-07
Norm of the params: 10.922813
                Loss: fixed 147 labels. Loss 0.01426. Accuracy 0.995.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07777277
Train loss (w/o reg) on all data: 0.07364106
Test loss (w/o reg) on all data: 0.06398251
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.7390917e-05
Norm of the params: 9.090333
              Random: fixed   9 labels. Loss 0.06398. Accuracy 0.972.
### Flips: 1230, rs: 2, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058998773
Train loss (w/o reg) on all data: 0.0033295285
Test loss (w/o reg) on all data: 0.005333177
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.266865e-07
Norm of the params: 7.1698656
     Influence (LOO): fixed 171 labels. Loss 0.00533. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00465594
Train loss (w/o reg) on all data: 0.002031274
Test loss (w/o reg) on all data: 0.0053685843
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4072282e-07
Norm of the params: 7.2452273
                Loss: fixed 173 labels. Loss 0.00537. Accuracy 0.999.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077226594
Train loss (w/o reg) on all data: 0.07319025
Test loss (w/o reg) on all data: 0.06256195
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 5.794481e-06
Norm of the params: 8.984817
              Random: fixed  14 labels. Loss 0.06256. Accuracy 0.974.
### Flips: 1230, rs: 2, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600876
Test loss (w/o reg) on all data: 0.0026560123
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.481434e-08
Norm of the params: 6.0928683
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013263
Test loss (w/o reg) on all data: 0.002656091
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6345758e-08
Norm of the params: 6.0927944
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07442389
Train loss (w/o reg) on all data: 0.07032416
Test loss (w/o reg) on all data: 0.060202677
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.902645e-06
Norm of the params: 9.055083
              Random: fixed  26 labels. Loss 0.06020. Accuracy 0.975.
### Flips: 1230, rs: 2, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162408
Train loss (w/o reg) on all data: 0.0009601115
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.176963e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010964
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.414753e-08
Norm of the params: 6.092832
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07285337
Train loss (w/o reg) on all data: 0.06864747
Test loss (w/o reg) on all data: 0.05786424
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 9.048576e-06
Norm of the params: 9.171595
              Random: fixed  30 labels. Loss 0.05786. Accuracy 0.978.
### Flips: 1230, rs: 2, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2711412e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9360822e-08
Norm of the params: 6.092811
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07082007
Train loss (w/o reg) on all data: 0.06669208
Test loss (w/o reg) on all data: 0.05507307
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.6001224e-05
Norm of the params: 9.086249
              Random: fixed  39 labels. Loss 0.05507. Accuracy 0.979.
### Flips: 1230, rs: 2, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010714
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.224968e-08
Norm of the params: 6.0928364
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010935
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4843818e-08
Norm of the params: 6.0928326
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06905904
Train loss (w/o reg) on all data: 0.06514112
Test loss (w/o reg) on all data: 0.048468407
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.7817329e-06
Norm of the params: 8.852029
              Random: fixed  50 labels. Loss 0.04847. Accuracy 0.984.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074456744
Train loss (w/o reg) on all data: 0.07063976
Test loss (w/o reg) on all data: 0.05358708
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 5.2693595e-06
Norm of the params: 8.73726
Flipped loss: 0.05359. Accuracy: 0.980
### Flips: 1230, rs: 3, checks: 205
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02521164
Train loss (w/o reg) on all data: 0.020905439
Test loss (w/o reg) on all data: 0.020130305
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8107237e-06
Norm of the params: 9.280303
     Influence (LOO): fixed 110 labels. Loss 0.02013. Accuracy 0.993.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0101236645
Train loss (w/o reg) on all data: 0.006187262
Test loss (w/o reg) on all data: 0.010816953
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.0304749e-07
Norm of the params: 8.872883
                Loss: fixed 132 labels. Loss 0.01082. Accuracy 0.997.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070736304
Train loss (w/o reg) on all data: 0.06692707
Test loss (w/o reg) on all data: 0.051230147
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 5.6636845e-06
Norm of the params: 8.728385
              Random: fixed  11 labels. Loss 0.05123. Accuracy 0.980.
### Flips: 1230, rs: 3, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006251145
Train loss (w/o reg) on all data: 0.0034846351
Test loss (w/o reg) on all data: 0.004826498
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4061155e-07
Norm of the params: 7.4384274
     Influence (LOO): fixed 146 labels. Loss 0.00483. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047445484
Train loss (w/o reg) on all data: 0.0019480325
Test loss (w/o reg) on all data: 0.0031681338
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2421246e-07
Norm of the params: 7.4786572
                Loss: fixed 144 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067949094
Train loss (w/o reg) on all data: 0.06406799
Test loss (w/o reg) on all data: 0.0491922
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.014756e-05
Norm of the params: 8.810343
              Random: fixed  19 labels. Loss 0.04919. Accuracy 0.982.
### Flips: 1230, rs: 3, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601199
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3553853e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.45660275e-08
Norm of the params: 6.092822
                Loss: fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066405945
Train loss (w/o reg) on all data: 0.062541895
Test loss (w/o reg) on all data: 0.048446815
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.9812535e-06
Norm of the params: 8.790962
              Random: fixed  25 labels. Loss 0.04845. Accuracy 0.982.
### Flips: 1230, rs: 3, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4577054e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.55145e-09
Norm of the params: 6.0928197
                Loss: fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06392795
Train loss (w/o reg) on all data: 0.059853185
Test loss (w/o reg) on all data: 0.046645205
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.2047205e-06
Norm of the params: 9.027473
              Random: fixed  30 labels. Loss 0.04665. Accuracy 0.984.
### Flips: 1230, rs: 3, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8483282e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2405862e-08
Norm of the params: 6.092819
                Loss: fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06318275
Train loss (w/o reg) on all data: 0.05911851
Test loss (w/o reg) on all data: 0.046850726
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.5679449e-06
Norm of the params: 9.015809
              Random: fixed  33 labels. Loss 0.04685. Accuracy 0.984.
### Flips: 1230, rs: 3, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601104
Test loss (w/o reg) on all data: 0.002656045
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.091064e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601321
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.867262e-08
Norm of the params: 6.0927954
                Loss: fixed 149 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061783776
Train loss (w/o reg) on all data: 0.057842392
Test loss (w/o reg) on all data: 0.04539521
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.5131853e-06
Norm of the params: 8.878497
              Random: fixed  39 labels. Loss 0.04540. Accuracy 0.984.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07588416
Train loss (w/o reg) on all data: 0.07175281
Test loss (w/o reg) on all data: 0.06604235
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 6.620641e-06
Norm of the params: 9.089941
Flipped loss: 0.06604. Accuracy: 0.976
### Flips: 1230, rs: 4, checks: 205
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028988134
Train loss (w/o reg) on all data: 0.023742504
Test loss (w/o reg) on all data: 0.024998108
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.540008e-06
Norm of the params: 10.242685
     Influence (LOO): fixed 112 labels. Loss 0.02500. Accuracy 0.991.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018035887
Train loss (w/o reg) on all data: 0.01217741
Test loss (w/o reg) on all data: 0.02331494
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.541538e-07
Norm of the params: 10.824488
                Loss: fixed 133 labels. Loss 0.02331. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07308037
Train loss (w/o reg) on all data: 0.06900009
Test loss (w/o reg) on all data: 0.06099811
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.7650971e-06
Norm of the params: 9.033586
              Random: fixed  11 labels. Loss 0.06100. Accuracy 0.978.
### Flips: 1230, rs: 4, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006391868
Train loss (w/o reg) on all data: 0.0033593262
Test loss (w/o reg) on all data: 0.006429722
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1991475e-07
Norm of the params: 7.7878647
     Influence (LOO): fixed 162 labels. Loss 0.00643. Accuracy 0.999.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006101804
Train loss (w/o reg) on all data: 0.0028491092
Test loss (w/o reg) on all data: 0.0061765397
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.380504e-08
Norm of the params: 8.065599
                Loss: fixed 163 labels. Loss 0.00618. Accuracy 0.999.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070473865
Train loss (w/o reg) on all data: 0.06621686
Test loss (w/o reg) on all data: 0.05819484
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 6.335782e-06
Norm of the params: 9.227134
              Random: fixed  22 labels. Loss 0.05819. Accuracy 0.978.
### Flips: 1230, rs: 4, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.055386e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0178232e-08
Norm of the params: 6.09282
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06815888
Train loss (w/o reg) on all data: 0.06386804
Test loss (w/o reg) on all data: 0.054967526
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.74129e-06
Norm of the params: 9.263741
              Random: fixed  33 labels. Loss 0.05497. Accuracy 0.977.
### Flips: 1230, rs: 4, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011325
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.961662e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012553
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4614176e-08
Norm of the params: 6.0928063
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06578124
Train loss (w/o reg) on all data: 0.061490364
Test loss (w/o reg) on all data: 0.05122994
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 5.5847036e-06
Norm of the params: 9.263781
              Random: fixed  45 labels. Loss 0.05123. Accuracy 0.980.
### Flips: 1230, rs: 4, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012524
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6834383e-08
Norm of the params: 6.0928063
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8859355e-09
Norm of the params: 6.092813
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064451426
Train loss (w/o reg) on all data: 0.06023252
Test loss (w/o reg) on all data: 0.04872432
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.0826228e-05
Norm of the params: 9.18576
              Random: fixed  50 labels. Loss 0.04872. Accuracy 0.983.
### Flips: 1230, rs: 4, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0769477e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6865572e-08
Norm of the params: 6.09281
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059819795
Train loss (w/o reg) on all data: 0.055363577
Test loss (w/o reg) on all data: 0.044575393
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.1483268e-06
Norm of the params: 9.440568
              Random: fixed  65 labels. Loss 0.04458. Accuracy 0.983.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0772668
Train loss (w/o reg) on all data: 0.07308815
Test loss (w/o reg) on all data: 0.058595207
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.1714466e-05
Norm of the params: 9.14183
Flipped loss: 0.05860. Accuracy: 0.978
### Flips: 1230, rs: 5, checks: 205
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030835735
Train loss (w/o reg) on all data: 0.025901187
Test loss (w/o reg) on all data: 0.017133744
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4904587e-06
Norm of the params: 9.934332
     Influence (LOO): fixed 114 labels. Loss 0.01713. Accuracy 0.998.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011511343
Train loss (w/o reg) on all data: 0.0070754034
Test loss (w/o reg) on all data: 0.014424965
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.175269e-07
Norm of the params: 9.419065
                Loss: fixed 141 labels. Loss 0.01442. Accuracy 0.995.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07714206
Train loss (w/o reg) on all data: 0.07312194
Test loss (w/o reg) on all data: 0.055950124
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.4206162e-06
Norm of the params: 8.966734
              Random: fixed   5 labels. Loss 0.05595. Accuracy 0.983.
### Flips: 1230, rs: 5, checks: 410
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0067715733
Train loss (w/o reg) on all data: 0.0037436385
Test loss (w/o reg) on all data: 0.004574483
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 6.976104e-07
Norm of the params: 7.7819467
     Influence (LOO): fixed 154 labels. Loss 0.00457. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033909427
Train loss (w/o reg) on all data: 0.0013267944
Test loss (w/o reg) on all data: 0.0030209476
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.438743e-08
Norm of the params: 6.425182
                Loss: fixed 157 labels. Loss 0.00302. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0735751
Train loss (w/o reg) on all data: 0.06959863
Test loss (w/o reg) on all data: 0.050508313
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.86338e-06
Norm of the params: 8.917929
              Random: fixed  19 labels. Loss 0.05051. Accuracy 0.984.
### Flips: 1230, rs: 5, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1410436e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5419149e-08
Norm of the params: 6.092819
                Loss: fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07311826
Train loss (w/o reg) on all data: 0.06908143
Test loss (w/o reg) on all data: 0.050187003
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.376461e-06
Norm of the params: 8.985356
              Random: fixed  22 labels. Loss 0.05019. Accuracy 0.984.
### Flips: 1230, rs: 5, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600938
Test loss (w/o reg) on all data: 0.0026560263
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4284334e-08
Norm of the params: 6.092858
     Influence (LOO): fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601253
Test loss (w/o reg) on all data: 0.0026560966
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.188246e-08
Norm of the params: 6.092806
                Loss: fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07204861
Train loss (w/o reg) on all data: 0.068062186
Test loss (w/o reg) on all data: 0.04643823
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0350123e-05
Norm of the params: 8.929081
              Random: fixed  27 labels. Loss 0.04644. Accuracy 0.990.
### Flips: 1230, rs: 5, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1987793e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2837178e-08
Norm of the params: 6.092814
                Loss: fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0707639
Train loss (w/o reg) on all data: 0.06686379
Test loss (w/o reg) on all data: 0.045236945
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 4.1587023e-06
Norm of the params: 8.831886
              Random: fixed  32 labels. Loss 0.04524. Accuracy 0.990.
### Flips: 1230, rs: 5, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.424831e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9719006e-09
Norm of the params: 6.0928144
                Loss: fixed 158 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06524763
Train loss (w/o reg) on all data: 0.061306488
Test loss (w/o reg) on all data: 0.04141999
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.144932e-06
Norm of the params: 8.878224
              Random: fixed  48 labels. Loss 0.04142. Accuracy 0.993.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08052293
Train loss (w/o reg) on all data: 0.07698609
Test loss (w/o reg) on all data: 0.06048754
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.504373e-06
Norm of the params: 8.410521
Flipped loss: 0.06049. Accuracy: 0.982
### Flips: 1230, rs: 6, checks: 205
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032611474
Train loss (w/o reg) on all data: 0.028448239
Test loss (w/o reg) on all data: 0.016711649
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9187195e-06
Norm of the params: 9.12495
     Influence (LOO): fixed 119 labels. Loss 0.01671. Accuracy 0.997.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010076046
Train loss (w/o reg) on all data: 0.005611619
Test loss (w/o reg) on all data: 0.019866439
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8057695e-07
Norm of the params: 9.449263
                Loss: fixed 150 labels. Loss 0.01987. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077623665
Train loss (w/o reg) on all data: 0.0740418
Test loss (w/o reg) on all data: 0.057892863
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.2474953e-06
Norm of the params: 8.463887
              Random: fixed  11 labels. Loss 0.05789. Accuracy 0.983.
### Flips: 1230, rs: 6, checks: 410
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006538714
Train loss (w/o reg) on all data: 0.0039623156
Test loss (w/o reg) on all data: 0.0051412564
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3234446e-08
Norm of the params: 7.178298
     Influence (LOO): fixed 161 labels. Loss 0.00514. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037594587
Train loss (w/o reg) on all data: 0.0014333875
Test loss (w/o reg) on all data: 0.0041353544
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1959986e-08
Norm of the params: 6.8206615
                Loss: fixed 164 labels. Loss 0.00414. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07468841
Train loss (w/o reg) on all data: 0.071054816
Test loss (w/o reg) on all data: 0.05560969
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.8651635e-06
Norm of the params: 8.524784
              Random: fixed  20 labels. Loss 0.05561. Accuracy 0.985.
### Flips: 1230, rs: 6, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.416398e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1090319e-08
Norm of the params: 6.092812
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07302321
Train loss (w/o reg) on all data: 0.06941032
Test loss (w/o reg) on all data: 0.054640997
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.467823e-05
Norm of the params: 8.500463
              Random: fixed  26 labels. Loss 0.05464. Accuracy 0.985.
### Flips: 1230, rs: 6, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4029898e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560454
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4997521e-08
Norm of the params: 6.0928173
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07055982
Train loss (w/o reg) on all data: 0.06665506
Test loss (w/o reg) on all data: 0.0520255
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.9818306e-05
Norm of the params: 8.837148
              Random: fixed  36 labels. Loss 0.05203. Accuracy 0.987.
### Flips: 1230, rs: 6, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601656
Test loss (w/o reg) on all data: 0.002656164
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.408747e-08
Norm of the params: 6.09274
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011703
Test loss (w/o reg) on all data: 0.002656093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.470617e-08
Norm of the params: 6.0928197
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067727186
Train loss (w/o reg) on all data: 0.06355751
Test loss (w/o reg) on all data: 0.04967378
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.7491972e-06
Norm of the params: 9.132006
              Random: fixed  45 labels. Loss 0.04967. Accuracy 0.989.
### Flips: 1230, rs: 6, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010586
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0585261e-08
Norm of the params: 6.092838
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7783629e-08
Norm of the params: 6.092809
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06642743
Train loss (w/o reg) on all data: 0.062219128
Test loss (w/o reg) on all data: 0.04922772
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.6182038e-06
Norm of the params: 9.174207
              Random: fixed  49 labels. Loss 0.04923. Accuracy 0.987.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0794323
Train loss (w/o reg) on all data: 0.07481227
Test loss (w/o reg) on all data: 0.057847317
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.4793055e-06
Norm of the params: 9.612524
Flipped loss: 0.05785. Accuracy: 0.980
### Flips: 1230, rs: 7, checks: 205
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030340932
Train loss (w/o reg) on all data: 0.02524604
Test loss (w/o reg) on all data: 0.018326435
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.5276743e-06
Norm of the params: 10.094445
     Influence (LOO): fixed 120 labels. Loss 0.01833. Accuracy 0.994.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012777476
Train loss (w/o reg) on all data: 0.0080488995
Test loss (w/o reg) on all data: 0.010589961
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.075171e-07
Norm of the params: 9.724789
                Loss: fixed 143 labels. Loss 0.01059. Accuracy 0.997.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07700013
Train loss (w/o reg) on all data: 0.072413415
Test loss (w/o reg) on all data: 0.05279476
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.3929546e-06
Norm of the params: 9.57781
              Random: fixed  10 labels. Loss 0.05279. Accuracy 0.984.
### Flips: 1230, rs: 7, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601102
Test loss (w/o reg) on all data: 0.0026560393
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7779988e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16003305e-08
Norm of the params: 6.0928125
                Loss: fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07326712
Train loss (w/o reg) on all data: 0.06849835
Test loss (w/o reg) on all data: 0.04756189
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2443175e-05
Norm of the params: 9.766033
              Random: fixed  22 labels. Loss 0.04756. Accuracy 0.989.
### Flips: 1230, rs: 7, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601136
Test loss (w/o reg) on all data: 0.0026560489
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1222391e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.44839385e-08
Norm of the params: 6.0928073
                Loss: fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07084317
Train loss (w/o reg) on all data: 0.06598297
Test loss (w/o reg) on all data: 0.045106173
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.2436094e-06
Norm of the params: 9.859208
              Random: fixed  30 labels. Loss 0.04511. Accuracy 0.988.
### Flips: 1230, rs: 7, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.884437e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0909245e-08
Norm of the params: 6.0928116
                Loss: fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06812758
Train loss (w/o reg) on all data: 0.06332431
Test loss (w/o reg) on all data: 0.042264175
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.6643067e-06
Norm of the params: 9.801293
              Random: fixed  37 labels. Loss 0.04226. Accuracy 0.988.
### Flips: 1230, rs: 7, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601024
Test loss (w/o reg) on all data: 0.0026560293
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5083319e-08
Norm of the params: 6.0928435
     Influence (LOO): fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601247
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3398959e-08
Norm of the params: 6.0928073
                Loss: fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067574635
Train loss (w/o reg) on all data: 0.06284439
Test loss (w/o reg) on all data: 0.040672153
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.352657e-06
Norm of the params: 9.726507
              Random: fixed  41 labels. Loss 0.04067. Accuracy 0.993.
### Flips: 1230, rs: 7, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096016185
Test loss (w/o reg) on all data: 0.0026561073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6229613e-08
Norm of the params: 6.092746
     Influence (LOO): fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2967566e-08
Norm of the params: 6.092824
                Loss: fixed 161 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06577412
Train loss (w/o reg) on all data: 0.06101412
Test loss (w/o reg) on all data: 0.03887681
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.9835708e-06
Norm of the params: 9.757047
              Random: fixed  48 labels. Loss 0.03888. Accuracy 0.993.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07934852
Train loss (w/o reg) on all data: 0.07476639
Test loss (w/o reg) on all data: 0.06540825
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 8.640977e-06
Norm of the params: 9.573011
Flipped loss: 0.06541. Accuracy: 0.980
### Flips: 1230, rs: 8, checks: 205
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034180485
Train loss (w/o reg) on all data: 0.028993683
Test loss (w/o reg) on all data: 0.023435017
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.0225204e-07
Norm of the params: 10.185091
     Influence (LOO): fixed 116 labels. Loss 0.02344. Accuracy 0.995.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01941211
Train loss (w/o reg) on all data: 0.013323803
Test loss (w/o reg) on all data: 0.016193045
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8349583e-06
Norm of the params: 11.034769
                Loss: fixed 137 labels. Loss 0.01619. Accuracy 0.994.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07798065
Train loss (w/o reg) on all data: 0.073337965
Test loss (w/o reg) on all data: 0.06119813
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.119304e-06
Norm of the params: 9.636063
              Random: fixed  10 labels. Loss 0.06120. Accuracy 0.979.
### Flips: 1230, rs: 8, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069743115
Train loss (w/o reg) on all data: 0.0042962334
Test loss (w/o reg) on all data: 0.004861295
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2942552e-07
Norm of the params: 7.318577
     Influence (LOO): fixed 172 labels. Loss 0.00486. Accuracy 0.999.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00752794
Train loss (w/o reg) on all data: 0.003892144
Test loss (w/o reg) on all data: 0.0057049906
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5018001e-07
Norm of the params: 8.527363
                Loss: fixed 169 labels. Loss 0.00570. Accuracy 0.999.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07692941
Train loss (w/o reg) on all data: 0.07247669
Test loss (w/o reg) on all data: 0.058713194
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.2678274e-06
Norm of the params: 9.436864
              Random: fixed  18 labels. Loss 0.05871. Accuracy 0.983.
### Flips: 1230, rs: 8, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5924668e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5471715e-08
Norm of the params: 6.0928035
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074899904
Train loss (w/o reg) on all data: 0.07068175
Test loss (w/o reg) on all data: 0.05285638
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.0058234e-05
Norm of the params: 9.18494
              Random: fixed  28 labels. Loss 0.05286. Accuracy 0.985.
### Flips: 1230, rs: 8, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.002656052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3026247e-08
Norm of the params: 6.0928216
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2530062e-08
Norm of the params: 6.0928087
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07304428
Train loss (w/o reg) on all data: 0.06899461
Test loss (w/o reg) on all data: 0.048276503
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3285609e-05
Norm of the params: 8.999626
              Random: fixed  36 labels. Loss 0.04828. Accuracy 0.991.
### Flips: 1230, rs: 8, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8600591e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011464
Test loss (w/o reg) on all data: 0.0026560524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4587767e-08
Norm of the params: 6.092824
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071106814
Train loss (w/o reg) on all data: 0.067022316
Test loss (w/o reg) on all data: 0.045970283
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.5614695e-06
Norm of the params: 9.038249
              Random: fixed  45 labels. Loss 0.04597. Accuracy 0.992.
### Flips: 1230, rs: 8, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4793086e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4243511e-08
Norm of the params: 6.092819
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06706668
Train loss (w/o reg) on all data: 0.0627754
Test loss (w/o reg) on all data: 0.04219536
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.253273e-06
Norm of the params: 9.264205
              Random: fixed  55 labels. Loss 0.04220. Accuracy 0.991.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07490831
Train loss (w/o reg) on all data: 0.07103566
Test loss (w/o reg) on all data: 0.057606004
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.070422e-06
Norm of the params: 8.800737
Flipped loss: 0.05761. Accuracy: 0.978
### Flips: 1230, rs: 9, checks: 205
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025792267
Train loss (w/o reg) on all data: 0.021066643
Test loss (w/o reg) on all data: 0.01472347
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.468853e-07
Norm of the params: 9.721754
     Influence (LOO): fixed 108 labels. Loss 0.01472. Accuracy 0.998.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008442852
Train loss (w/o reg) on all data: 0.005064183
Test loss (w/o reg) on all data: 0.005517353
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5102074e-07
Norm of the params: 8.220303
                Loss: fixed 140 labels. Loss 0.00552. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071797304
Train loss (w/o reg) on all data: 0.067981854
Test loss (w/o reg) on all data: 0.053269316
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.22544e-05
Norm of the params: 8.735501
              Random: fixed  13 labels. Loss 0.05327. Accuracy 0.984.
### Flips: 1230, rs: 9, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006878958
Train loss (w/o reg) on all data: 0.0034590843
Test loss (w/o reg) on all data: 0.004805787
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3944096e-07
Norm of the params: 8.270276
     Influence (LOO): fixed 143 labels. Loss 0.00481. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033046033
Train loss (w/o reg) on all data: 0.0012099703
Test loss (w/o reg) on all data: 0.0029275739
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6139174e-08
Norm of the params: 6.4724536
                Loss: fixed 149 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07083391
Train loss (w/o reg) on all data: 0.067072324
Test loss (w/o reg) on all data: 0.0504101
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.0020872e-06
Norm of the params: 8.673614
              Random: fixed  19 labels. Loss 0.05041. Accuracy 0.987.
### Flips: 1230, rs: 9, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601209
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.4111874e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.002656085
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5455249e-08
Norm of the params: 6.0928216
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06767959
Train loss (w/o reg) on all data: 0.06371156
Test loss (w/o reg) on all data: 0.04974642
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.844552e-06
Norm of the params: 8.908458
              Random: fixed  27 labels. Loss 0.04975. Accuracy 0.984.
### Flips: 1230, rs: 9, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0369506e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.29862965e-08
Norm of the params: 6.0928106
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06507624
Train loss (w/o reg) on all data: 0.06105124
Test loss (w/o reg) on all data: 0.04539455
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.878284e-06
Norm of the params: 8.97218
              Random: fixed  35 labels. Loss 0.04539. Accuracy 0.985.
### Flips: 1230, rs: 9, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560496
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2813098e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.565169e-08
Norm of the params: 6.092816
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06359732
Train loss (w/o reg) on all data: 0.05959259
Test loss (w/o reg) on all data: 0.042610183
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1407543e-05
Norm of the params: 8.949564
              Random: fixed  42 labels. Loss 0.04261. Accuracy 0.986.
### Flips: 1230, rs: 9, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3486111e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1430147e-08
Norm of the params: 6.09282
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062017545
Train loss (w/o reg) on all data: 0.05797359
Test loss (w/o reg) on all data: 0.041976202
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.3905715e-06
Norm of the params: 8.993282
              Random: fixed  47 labels. Loss 0.04198. Accuracy 0.987.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07552855
Train loss (w/o reg) on all data: 0.07140723
Test loss (w/o reg) on all data: 0.062618785
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.0298645e-05
Norm of the params: 9.0788965
Flipped loss: 0.06262. Accuracy: 0.972
### Flips: 1230, rs: 10, checks: 205
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028517015
Train loss (w/o reg) on all data: 0.023965575
Test loss (w/o reg) on all data: 0.018179083
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.8399494e-07
Norm of the params: 9.540902
     Influence (LOO): fixed 114 labels. Loss 0.01818. Accuracy 0.994.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013355886
Train loss (w/o reg) on all data: 0.008190812
Test loss (w/o reg) on all data: 0.015901862
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9947968e-06
Norm of the params: 10.1637335
                Loss: fixed 136 labels. Loss 0.01590. Accuracy 0.995.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07434782
Train loss (w/o reg) on all data: 0.07010533
Test loss (w/o reg) on all data: 0.061004136
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.0938984e-05
Norm of the params: 9.211393
              Random: fixed   5 labels. Loss 0.06100. Accuracy 0.972.
### Flips: 1230, rs: 10, checks: 410
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075534917
Train loss (w/o reg) on all data: 0.004632279
Test loss (w/o reg) on all data: 0.0048500043
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7448563e-07
Norm of the params: 7.643576
     Influence (LOO): fixed 155 labels. Loss 0.00485. Accuracy 0.999.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004607329
Train loss (w/o reg) on all data: 0.0020005258
Test loss (w/o reg) on all data: 0.0071063116
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.8873584e-08
Norm of the params: 7.220531
                Loss: fixed 157 labels. Loss 0.00711. Accuracy 0.998.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07243067
Train loss (w/o reg) on all data: 0.06819912
Test loss (w/o reg) on all data: 0.05763074
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.1938342e-05
Norm of the params: 9.199508
              Random: fixed  11 labels. Loss 0.05763. Accuracy 0.978.
### Flips: 1230, rs: 10, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601124
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3229322e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2059836e-08
Norm of the params: 6.092812
                Loss: fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071416944
Train loss (w/o reg) on all data: 0.0673403
Test loss (w/o reg) on all data: 0.05377289
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5070698e-05
Norm of the params: 9.0295515
              Random: fixed  21 labels. Loss 0.05377. Accuracy 0.981.
### Flips: 1230, rs: 10, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4176687e-08
Norm of the params: 6.0928097
     Influence (LOO): fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.830018e-08
Norm of the params: 6.0928125
                Loss: fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07016453
Train loss (w/o reg) on all data: 0.065896004
Test loss (w/o reg) on all data: 0.054420922
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4887954e-05
Norm of the params: 9.239622
              Random: fixed  24 labels. Loss 0.05442. Accuracy 0.982.
### Flips: 1230, rs: 10, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3667983e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4679115e-08
Norm of the params: 6.09282
                Loss: fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06713484
Train loss (w/o reg) on all data: 0.06285774
Test loss (w/o reg) on all data: 0.049529716
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.3337945e-06
Norm of the params: 9.248898
              Random: fixed  34 labels. Loss 0.04953. Accuracy 0.984.
### Flips: 1230, rs: 10, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5429146e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.144271e-08
Norm of the params: 6.0928206
                Loss: fixed 162 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06440933
Train loss (w/o reg) on all data: 0.060075413
Test loss (w/o reg) on all data: 0.047013633
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.7632348e-06
Norm of the params: 9.310118
              Random: fixed  41 labels. Loss 0.04701. Accuracy 0.986.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07602098
Train loss (w/o reg) on all data: 0.072436735
Test loss (w/o reg) on all data: 0.06136669
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.4947145e-06
Norm of the params: 8.466692
Flipped loss: 0.06137. Accuracy: 0.978
### Flips: 1230, rs: 11, checks: 205
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026597952
Train loss (w/o reg) on all data: 0.02228676
Test loss (w/o reg) on all data: 0.023812946
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.6908525e-06
Norm of the params: 9.28568
     Influence (LOO): fixed 111 labels. Loss 0.02381. Accuracy 0.989.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012898115
Train loss (w/o reg) on all data: 0.008193781
Test loss (w/o reg) on all data: 0.012292537
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.912167e-07
Norm of the params: 9.69983
                Loss: fixed 135 labels. Loss 0.01229. Accuracy 0.996.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07288108
Train loss (w/o reg) on all data: 0.069190785
Test loss (w/o reg) on all data: 0.059269007
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 5.6085596e-06
Norm of the params: 8.591037
              Random: fixed   9 labels. Loss 0.05927. Accuracy 0.975.
### Flips: 1230, rs: 11, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042583114
Train loss (w/o reg) on all data: 0.0019900377
Test loss (w/o reg) on all data: 0.0039910306
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4895413e-07
Norm of the params: 6.73539
     Influence (LOO): fixed 152 labels. Loss 0.00399. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028651743
Train loss (w/o reg) on all data: 0.000991363
Test loss (w/o reg) on all data: 0.0033057015
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2348685e-08
Norm of the params: 6.1217833
                Loss: fixed 154 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07197347
Train loss (w/o reg) on all data: 0.06846614
Test loss (w/o reg) on all data: 0.056201708
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.3018297e-06
Norm of the params: 8.375357
              Random: fixed  14 labels. Loss 0.05620. Accuracy 0.978.
### Flips: 1230, rs: 11, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.622657e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4820075e-08
Norm of the params: 6.0928187
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06953251
Train loss (w/o reg) on all data: 0.06597639
Test loss (w/o reg) on all data: 0.053174723
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.80187e-06
Norm of the params: 8.433419
              Random: fixed  21 labels. Loss 0.05317. Accuracy 0.978.
### Flips: 1230, rs: 11, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096007995
Test loss (w/o reg) on all data: 0.0026560049
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.688851e-08
Norm of the params: 6.092881
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560368
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3666094e-08
Norm of the params: 6.092819
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068898745
Train loss (w/o reg) on all data: 0.065458246
Test loss (w/o reg) on all data: 0.05004651
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 7.2891503e-06
Norm of the params: 8.295179
              Random: fixed  26 labels. Loss 0.05005. Accuracy 0.983.
### Flips: 1230, rs: 11, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096008065
Test loss (w/o reg) on all data: 0.002656007
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.717548e-08
Norm of the params: 6.0928793
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560358
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9693613e-08
Norm of the params: 6.0928216
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06777235
Train loss (w/o reg) on all data: 0.06421659
Test loss (w/o reg) on all data: 0.048613347
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 6.7057676e-06
Norm of the params: 8.432983
              Random: fixed  30 labels. Loss 0.04861. Accuracy 0.982.
### Flips: 1230, rs: 11, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3361667e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601224
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9588024e-09
Norm of the params: 6.092811
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066641286
Train loss (w/o reg) on all data: 0.06304223
Test loss (w/o reg) on all data: 0.047200438
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5727397e-05
Norm of the params: 8.484163
              Random: fixed  36 labels. Loss 0.04720. Accuracy 0.986.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080433674
Train loss (w/o reg) on all data: 0.07656255
Test loss (w/o reg) on all data: 0.062721886
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.9725936e-05
Norm of the params: 8.799001
Flipped loss: 0.06272. Accuracy: 0.979
### Flips: 1230, rs: 12, checks: 205
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033503536
Train loss (w/o reg) on all data: 0.028489688
Test loss (w/o reg) on all data: 0.025492664
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.0239547e-06
Norm of the params: 10.013839
     Influence (LOO): fixed 109 labels. Loss 0.02549. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011728274
Train loss (w/o reg) on all data: 0.006648531
Test loss (w/o reg) on all data: 0.021993103
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6466679e-06
Norm of the params: 10.079427
                Loss: fixed 145 labels. Loss 0.02199. Accuracy 0.993.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07982717
Train loss (w/o reg) on all data: 0.07601969
Test loss (w/o reg) on all data: 0.06117391
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.54782e-06
Norm of the params: 8.726373
              Random: fixed   4 labels. Loss 0.06117. Accuracy 0.980.
### Flips: 1230, rs: 12, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005774215
Train loss (w/o reg) on all data: 0.0032468534
Test loss (w/o reg) on all data: 0.0048276195
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.174902e-08
Norm of the params: 7.109658
     Influence (LOO): fixed 162 labels. Loss 0.00483. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005008771
Train loss (w/o reg) on all data: 0.0023621025
Test loss (w/o reg) on all data: 0.007455047
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3190015e-07
Norm of the params: 7.2755327
                Loss: fixed 163 labels. Loss 0.00746. Accuracy 0.998.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07863959
Train loss (w/o reg) on all data: 0.074836336
Test loss (w/o reg) on all data: 0.059157033
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 5.293303e-06
Norm of the params: 8.721534
              Random: fixed  10 labels. Loss 0.05916. Accuracy 0.980.
### Flips: 1230, rs: 12, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.251075e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9193664e-08
Norm of the params: 6.092813
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075852886
Train loss (w/o reg) on all data: 0.071796246
Test loss (w/o reg) on all data: 0.05679965
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.4407883e-06
Norm of the params: 9.007377
              Random: fixed  20 labels. Loss 0.05680. Accuracy 0.983.
### Flips: 1230, rs: 12, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011104
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2710037e-08
Norm of the params: 6.0928297
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601256
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3325626e-08
Norm of the params: 6.092806
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072694026
Train loss (w/o reg) on all data: 0.06854803
Test loss (w/o reg) on all data: 0.052382994
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.9408027e-06
Norm of the params: 9.106039
              Random: fixed  32 labels. Loss 0.05238. Accuracy 0.985.
### Flips: 1230, rs: 12, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010976
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3575504e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601314
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6481842e-08
Norm of the params: 6.0927963
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07051994
Train loss (w/o reg) on all data: 0.06631232
Test loss (w/o reg) on all data: 0.049243387
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.1679355e-06
Norm of the params: 9.173462
              Random: fixed  39 labels. Loss 0.04924. Accuracy 0.987.
### Flips: 1230, rs: 12, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011086
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1292703e-08
Norm of the params: 6.0928297
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.002656076
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.430707e-08
Norm of the params: 6.0928063
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06767394
Train loss (w/o reg) on all data: 0.063514374
Test loss (w/o reg) on all data: 0.04589341
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.6571962e-06
Norm of the params: 9.120924
              Random: fixed  46 labels. Loss 0.04589. Accuracy 0.988.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07376671
Train loss (w/o reg) on all data: 0.07006417
Test loss (w/o reg) on all data: 0.056372255
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 8.158515e-06
Norm of the params: 8.605272
Flipped loss: 0.05637. Accuracy: 0.981
### Flips: 1230, rs: 13, checks: 205
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02237326
Train loss (w/o reg) on all data: 0.018286072
Test loss (w/o reg) on all data: 0.016417308
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.399952e-06
Norm of the params: 9.041227
     Influence (LOO): fixed 114 labels. Loss 0.01642. Accuracy 0.995.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009145929
Train loss (w/o reg) on all data: 0.0049301707
Test loss (w/o reg) on all data: 0.012233049
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.5870016e-07
Norm of the params: 9.182329
                Loss: fixed 131 labels. Loss 0.01223. Accuracy 0.995.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07210393
Train loss (w/o reg) on all data: 0.06842583
Test loss (w/o reg) on all data: 0.05288662
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.10193e-06
Norm of the params: 8.576837
              Random: fixed   6 labels. Loss 0.05289. Accuracy 0.983.
### Flips: 1230, rs: 13, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7154864e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00464015
Train loss (w/o reg) on all data: 0.0019178498
Test loss (w/o reg) on all data: 0.005099849
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1235662e-07
Norm of the params: 7.378754
                Loss: fixed 143 labels. Loss 0.00510. Accuracy 0.999.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07018993
Train loss (w/o reg) on all data: 0.06634641
Test loss (w/o reg) on all data: 0.05083617
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.9249603e-05
Norm of the params: 8.767581
              Random: fixed  15 labels. Loss 0.05084. Accuracy 0.983.
### Flips: 1230, rs: 13, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.123772e-09
Norm of the params: 6.092818
     Influence (LOO): fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.4491524e-09
Norm of the params: 6.0928173
                Loss: fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068671025
Train loss (w/o reg) on all data: 0.064725004
Test loss (w/o reg) on all data: 0.049134098
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.109316e-06
Norm of the params: 8.883717
              Random: fixed  21 labels. Loss 0.04913. Accuracy 0.984.
### Flips: 1230, rs: 13, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4714004e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.823344e-08
Norm of the params: 6.0928164
                Loss: fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06799493
Train loss (w/o reg) on all data: 0.06409598
Test loss (w/o reg) on all data: 0.049145725
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.044948e-06
Norm of the params: 8.830574
              Random: fixed  23 labels. Loss 0.04915. Accuracy 0.985.
### Flips: 1230, rs: 13, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.927597e-09
Norm of the params: 6.092821
     Influence (LOO): fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5411724e-09
Norm of the params: 6.0928144
                Loss: fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0647278
Train loss (w/o reg) on all data: 0.060956303
Test loss (w/o reg) on all data: 0.04365405
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.5966317e-06
Norm of the params: 8.685038
              Random: fixed  33 labels. Loss 0.04365. Accuracy 0.985.
### Flips: 1230, rs: 13, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011086
Test loss (w/o reg) on all data: 0.002656043
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.15939445e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2754046e-08
Norm of the params: 6.0928116
                Loss: fixed 146 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06187823
Train loss (w/o reg) on all data: 0.05808871
Test loss (w/o reg) on all data: 0.040360827
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0691931e-05
Norm of the params: 8.705769
              Random: fixed  42 labels. Loss 0.04036. Accuracy 0.989.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07726137
Train loss (w/o reg) on all data: 0.07291822
Test loss (w/o reg) on all data: 0.058029886
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.0032087e-06
Norm of the params: 9.320036
Flipped loss: 0.05803. Accuracy: 0.984
### Flips: 1230, rs: 14, checks: 205
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024770794
Train loss (w/o reg) on all data: 0.01949545
Test loss (w/o reg) on all data: 0.013589059
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.8089236e-07
Norm of the params: 10.271654
     Influence (LOO): fixed 124 labels. Loss 0.01359. Accuracy 0.995.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012370236
Train loss (w/o reg) on all data: 0.0071306247
Test loss (w/o reg) on all data: 0.013314943
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4959929e-06
Norm of the params: 10.236808
                Loss: fixed 141 labels. Loss 0.01331. Accuracy 0.997.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07454446
Train loss (w/o reg) on all data: 0.07014661
Test loss (w/o reg) on all data: 0.05525361
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.3764123e-05
Norm of the params: 9.378536
              Random: fixed  10 labels. Loss 0.05525. Accuracy 0.981.
### Flips: 1230, rs: 14, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049331984
Train loss (w/o reg) on all data: 0.0026309916
Test loss (w/o reg) on all data: 0.0036984493
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2116482e-07
Norm of the params: 6.785583
     Influence (LOO): fixed 162 labels. Loss 0.00370. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060383407
Train loss (w/o reg) on all data: 0.0028644453
Test loss (w/o reg) on all data: 0.006063758
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.296043e-07
Norm of the params: 7.967303
                Loss: fixed 157 labels. Loss 0.00606. Accuracy 0.999.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0734905
Train loss (w/o reg) on all data: 0.06909141
Test loss (w/o reg) on all data: 0.051646013
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.7292654e-06
Norm of the params: 9.379866
              Random: fixed  18 labels. Loss 0.05165. Accuracy 0.985.
### Flips: 1230, rs: 14, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.576278e-08
Norm of the params: 6.092806
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960106
Test loss (w/o reg) on all data: 0.0026560284
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.503859e-08
Norm of the params: 6.0928392
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0727669
Train loss (w/o reg) on all data: 0.06853633
Test loss (w/o reg) on all data: 0.050839316
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.129232e-06
Norm of the params: 9.198447
              Random: fixed  23 labels. Loss 0.05084. Accuracy 0.985.
### Flips: 1230, rs: 14, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3995556e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012256
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1517617e-08
Norm of the params: 6.0928106
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07072619
Train loss (w/o reg) on all data: 0.06657244
Test loss (w/o reg) on all data: 0.04977861
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.5746587e-05
Norm of the params: 9.114547
              Random: fixed  31 labels. Loss 0.04978. Accuracy 0.983.
### Flips: 1230, rs: 14, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010946
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2697558e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2342445e-08
Norm of the params: 6.0928097
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068798006
Train loss (w/o reg) on all data: 0.064614564
Test loss (w/o reg) on all data: 0.049245324
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.9952916e-06
Norm of the params: 9.147069
              Random: fixed  37 labels. Loss 0.04925. Accuracy 0.983.
### Flips: 1230, rs: 14, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2351719e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.601628e-09
Norm of the params: 6.092814
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065724015
Train loss (w/o reg) on all data: 0.061398145
Test loss (w/o reg) on all data: 0.04572354
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.5473207e-06
Norm of the params: 9.30147
              Random: fixed  44 labels. Loss 0.04572. Accuracy 0.985.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077680685
Train loss (w/o reg) on all data: 0.073226795
Test loss (w/o reg) on all data: 0.057143282
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.3119625e-06
Norm of the params: 9.438103
Flipped loss: 0.05714. Accuracy: 0.982
### Flips: 1230, rs: 15, checks: 205
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028883606
Train loss (w/o reg) on all data: 0.02348214
Test loss (w/o reg) on all data: 0.02054597
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9920885e-06
Norm of the params: 10.393716
     Influence (LOO): fixed 117 labels. Loss 0.02055. Accuracy 0.995.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016691897
Train loss (w/o reg) on all data: 0.011350393
Test loss (w/o reg) on all data: 0.0134743955
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6915546e-07
Norm of the params: 10.335864
                Loss: fixed 138 labels. Loss 0.01347. Accuracy 0.996.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07679674
Train loss (w/o reg) on all data: 0.072313875
Test loss (w/o reg) on all data: 0.055553377
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3854873e-05
Norm of the params: 9.468757
              Random: fixed   5 labels. Loss 0.05555. Accuracy 0.983.
### Flips: 1230, rs: 15, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006010079
Train loss (w/o reg) on all data: 0.0032457574
Test loss (w/o reg) on all data: 0.0059421663
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3263654e-07
Norm of the params: 7.435485
     Influence (LOO): fixed 160 labels. Loss 0.00594. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0057759965
Train loss (w/o reg) on all data: 0.0026918703
Test loss (w/o reg) on all data: 0.005791538
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1893383e-07
Norm of the params: 7.853822
                Loss: fixed 161 labels. Loss 0.00579. Accuracy 0.999.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07564017
Train loss (w/o reg) on all data: 0.07132464
Test loss (w/o reg) on all data: 0.054379698
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.845284e-06
Norm of the params: 9.29035
              Random: fixed  13 labels. Loss 0.05438. Accuracy 0.983.
### Flips: 1230, rs: 15, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601134
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.23153985e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012204
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2723998e-08
Norm of the params: 6.092812
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07323031
Train loss (w/o reg) on all data: 0.06891933
Test loss (w/o reg) on all data: 0.050640583
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.214253e-06
Norm of the params: 9.285456
              Random: fixed  25 labels. Loss 0.05064. Accuracy 0.986.
### Flips: 1230, rs: 15, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601239
Test loss (w/o reg) on all data: 0.0026560705
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4653712e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.7468485e-09
Norm of the params: 6.092822
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07232474
Train loss (w/o reg) on all data: 0.06799972
Test loss (w/o reg) on all data: 0.049168654
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3156705e-05
Norm of the params: 9.300553
              Random: fixed  30 labels. Loss 0.04917. Accuracy 0.987.
### Flips: 1230, rs: 15, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096015306
Test loss (w/o reg) on all data: 0.0026561148
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2021735e-08
Norm of the params: 6.092761
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0165949e-08
Norm of the params: 6.0928197
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068332456
Train loss (w/o reg) on all data: 0.06405413
Test loss (w/o reg) on all data: 0.044804305
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.912981e-06
Norm of the params: 9.2502165
              Random: fixed  43 labels. Loss 0.04480. Accuracy 0.985.
### Flips: 1230, rs: 15, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.998529e-09
Norm of the params: 6.092816
     Influence (LOO): fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0610051e-08
Norm of the params: 6.09282
                Loss: fixed 164 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0662868
Train loss (w/o reg) on all data: 0.06213289
Test loss (w/o reg) on all data: 0.042320527
Train acc on all data:  0.975200583515682
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.372796e-06
Norm of the params: 9.114726
              Random: fixed  50 labels. Loss 0.04232. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074157014
Train loss (w/o reg) on all data: 0.07002714
Test loss (w/o reg) on all data: 0.058328234
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 9.481486e-06
Norm of the params: 9.088314
Flipped loss: 0.05833. Accuracy: 0.977
### Flips: 1230, rs: 16, checks: 205
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025232963
Train loss (w/o reg) on all data: 0.020114737
Test loss (w/o reg) on all data: 0.016007923
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.1559177e-06
Norm of the params: 10.117536
     Influence (LOO): fixed 115 labels. Loss 0.01601. Accuracy 0.993.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011371873
Train loss (w/o reg) on all data: 0.0064928266
Test loss (w/o reg) on all data: 0.011940529
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.3836507e-07
Norm of the params: 9.878306
                Loss: fixed 134 labels. Loss 0.01194. Accuracy 0.996.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07302657
Train loss (w/o reg) on all data: 0.06903796
Test loss (w/o reg) on all data: 0.05455905
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.1704952e-06
Norm of the params: 8.931526
              Random: fixed   8 labels. Loss 0.05456. Accuracy 0.985.
### Flips: 1230, rs: 16, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042025065
Train loss (w/o reg) on all data: 0.0020581186
Test loss (w/o reg) on all data: 0.0034520896
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9437355e-08
Norm of the params: 6.548875
     Influence (LOO): fixed 153 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042747627
Train loss (w/o reg) on all data: 0.0017456867
Test loss (w/o reg) on all data: 0.003736013
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.8231146e-08
Norm of the params: 7.1120687
                Loss: fixed 152 labels. Loss 0.00374. Accuracy 0.999.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07201432
Train loss (w/o reg) on all data: 0.06805391
Test loss (w/o reg) on all data: 0.051809102
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.3354066e-06
Norm of the params: 8.899895
              Random: fixed  13 labels. Loss 0.05181. Accuracy 0.984.
### Flips: 1230, rs: 16, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013106
Test loss (w/o reg) on all data: 0.002656075
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3697567e-08
Norm of the params: 6.092796
     Influence (LOO): fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3601924e-08
Norm of the params: 6.0928283
                Loss: fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06894939
Train loss (w/o reg) on all data: 0.06499775
Test loss (w/o reg) on all data: 0.047429025
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0044817e-05
Norm of the params: 8.8900385
              Random: fixed  25 labels. Loss 0.04743. Accuracy 0.987.
### Flips: 1230, rs: 16, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2574716e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.572608e-08
Norm of the params: 6.09282
                Loss: fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06660701
Train loss (w/o reg) on all data: 0.06270198
Test loss (w/o reg) on all data: 0.0451477
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.376946e-06
Norm of the params: 8.8374605
              Random: fixed  31 labels. Loss 0.04515. Accuracy 0.989.
### Flips: 1230, rs: 16, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012244
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.09595195e-08
Norm of the params: 6.0928106
     Influence (LOO): fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.829806e-09
Norm of the params: 6.0928154
                Loss: fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0659008
Train loss (w/o reg) on all data: 0.0621042
Test loss (w/o reg) on all data: 0.043986324
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.4472246e-06
Norm of the params: 8.713902
              Random: fixed  36 labels. Loss 0.04399. Accuracy 0.989.
### Flips: 1230, rs: 16, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5088677e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601113
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8092535e-08
Norm of the params: 6.0928283
                Loss: fixed 154 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064258024
Train loss (w/o reg) on all data: 0.06038621
Test loss (w/o reg) on all data: 0.042022966
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.264813e-06
Norm of the params: 8.799785
              Random: fixed  42 labels. Loss 0.04202. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071459204
Train loss (w/o reg) on all data: 0.0666909
Test loss (w/o reg) on all data: 0.053186346
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.169007e-06
Norm of the params: 9.76556
Flipped loss: 0.05319. Accuracy: 0.978
### Flips: 1230, rs: 17, checks: 205
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022672754
Train loss (w/o reg) on all data: 0.018153824
Test loss (w/o reg) on all data: 0.018965567
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.064879e-07
Norm of the params: 9.506764
     Influence (LOO): fixed 113 labels. Loss 0.01897. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012593331
Train loss (w/o reg) on all data: 0.0078011486
Test loss (w/o reg) on all data: 0.012558485
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.3685995e-07
Norm of the params: 9.789977
                Loss: fixed 131 labels. Loss 0.01256. Accuracy 0.996.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07096332
Train loss (w/o reg) on all data: 0.0662166
Test loss (w/o reg) on all data: 0.05115778
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.082135e-06
Norm of the params: 9.743428
              Random: fixed   3 labels. Loss 0.05116. Accuracy 0.979.
### Flips: 1230, rs: 17, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00658587
Train loss (w/o reg) on all data: 0.0040088515
Test loss (w/o reg) on all data: 0.0043347077
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.03397014e-07
Norm of the params: 7.179162
     Influence (LOO): fixed 148 labels. Loss 0.00433. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030117189
Train loss (w/o reg) on all data: 0.0010690917
Test loss (w/o reg) on all data: 0.0029146213
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.567032e-08
Norm of the params: 6.233181
                Loss: fixed 152 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068573356
Train loss (w/o reg) on all data: 0.06366792
Test loss (w/o reg) on all data: 0.04976038
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.974656e-06
Norm of the params: 9.904981
              Random: fixed  13 labels. Loss 0.04976. Accuracy 0.980.
### Flips: 1230, rs: 17, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1826476e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.0474055e-09
Norm of the params: 6.092814
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06747936
Train loss (w/o reg) on all data: 0.06261402
Test loss (w/o reg) on all data: 0.047893252
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.660749e-06
Norm of the params: 9.864415
              Random: fixed  18 labels. Loss 0.04789. Accuracy 0.982.
### Flips: 1230, rs: 17, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096013147
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.91047e-08
Norm of the params: 6.0927973
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011406
Test loss (w/o reg) on all data: 0.002656062
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7557655e-08
Norm of the params: 6.092824
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06689222
Train loss (w/o reg) on all data: 0.0621141
Test loss (w/o reg) on all data: 0.046723668
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.414564e-06
Norm of the params: 9.775601
              Random: fixed  21 labels. Loss 0.04672. Accuracy 0.983.
### Flips: 1230, rs: 17, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.149431e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601253
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.839737e-08
Norm of the params: 6.0928063
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06371992
Train loss (w/o reg) on all data: 0.059037227
Test loss (w/o reg) on all data: 0.04188383
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3999455e-05
Norm of the params: 9.677492
              Random: fixed  34 labels. Loss 0.04188. Accuracy 0.986.
### Flips: 1230, rs: 17, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096016773
Test loss (w/o reg) on all data: 0.002656172
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4501441e-07
Norm of the params: 6.092736
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011563
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6806228e-08
Norm of the params: 6.092822
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060503136
Train loss (w/o reg) on all data: 0.05599975
Test loss (w/o reg) on all data: 0.03806268
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.1653996e-06
Norm of the params: 9.490403
              Random: fixed  45 labels. Loss 0.03806. Accuracy 0.991.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07648539
Train loss (w/o reg) on all data: 0.072624
Test loss (w/o reg) on all data: 0.05469314
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.755491e-05
Norm of the params: 8.787939
Flipped loss: 0.05469. Accuracy: 0.983
### Flips: 1230, rs: 18, checks: 205
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027276875
Train loss (w/o reg) on all data: 0.022636829
Test loss (w/o reg) on all data: 0.017388644
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0835798e-06
Norm of the params: 9.633325
     Influence (LOO): fixed 109 labels. Loss 0.01739. Accuracy 0.994.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007278383
Train loss (w/o reg) on all data: 0.003561799
Test loss (w/o reg) on all data: 0.0068165716
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1976981e-06
Norm of the params: 8.621582
                Loss: fixed 140 labels. Loss 0.00682. Accuracy 0.999.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07500543
Train loss (w/o reg) on all data: 0.07116572
Test loss (w/o reg) on all data: 0.05213715
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.630926e-06
Norm of the params: 8.763226
              Random: fixed   8 labels. Loss 0.05214. Accuracy 0.985.
### Flips: 1230, rs: 18, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00539421
Train loss (w/o reg) on all data: 0.0026678573
Test loss (w/o reg) on all data: 0.0040995907
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7563788e-08
Norm of the params: 7.384244
     Influence (LOO): fixed 147 labels. Loss 0.00410. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.002656035
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4930882e-08
Norm of the params: 6.0928164
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07364241
Train loss (w/o reg) on all data: 0.06980124
Test loss (w/o reg) on all data: 0.051141217
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.3283894e-06
Norm of the params: 8.764897
              Random: fixed  13 labels. Loss 0.05114. Accuracy 0.983.
### Flips: 1230, rs: 18, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1403851e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4060867e-08
Norm of the params: 6.0928144
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07311155
Train loss (w/o reg) on all data: 0.06934622
Test loss (w/o reg) on all data: 0.048700716
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.1932086e-05
Norm of the params: 8.677939
              Random: fixed  18 labels. Loss 0.04870. Accuracy 0.983.
### Flips: 1230, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.415831e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0568526e-08
Norm of the params: 6.0928144
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07061632
Train loss (w/o reg) on all data: 0.06677141
Test loss (w/o reg) on all data: 0.04580412
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.728586e-06
Norm of the params: 8.769164
              Random: fixed  25 labels. Loss 0.04580. Accuracy 0.987.
### Flips: 1230, rs: 18, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4061872e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.314711e-08
Norm of the params: 6.0928197
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06627135
Train loss (w/o reg) on all data: 0.062384572
Test loss (w/o reg) on all data: 0.04233608
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.423858e-06
Norm of the params: 8.816782
              Random: fixed  36 labels. Loss 0.04234. Accuracy 0.991.
### Flips: 1230, rs: 18, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1833156e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4590546e-08
Norm of the params: 6.092817
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0647126
Train loss (w/o reg) on all data: 0.060899936
Test loss (w/o reg) on all data: 0.041803636
Train acc on all data:  0.976173109652322
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.7132618e-06
Norm of the params: 8.732315
              Random: fixed  42 labels. Loss 0.04180. Accuracy 0.989.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0740669
Train loss (w/o reg) on all data: 0.068965346
Test loss (w/o reg) on all data: 0.06616471
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 6.7070655e-06
Norm of the params: 10.101044
Flipped loss: 0.06616. Accuracy: 0.972
### Flips: 1230, rs: 19, checks: 205
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028787147
Train loss (w/o reg) on all data: 0.023504324
Test loss (w/o reg) on all data: 0.025711587
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4545645e-06
Norm of the params: 10.278933
     Influence (LOO): fixed 116 labels. Loss 0.02571. Accuracy 0.993.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015687965
Train loss (w/o reg) on all data: 0.009939972
Test loss (w/o reg) on all data: 0.018908191
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.0268411e-06
Norm of the params: 10.721934
                Loss: fixed 134 labels. Loss 0.01891. Accuracy 0.994.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07167334
Train loss (w/o reg) on all data: 0.06643105
Test loss (w/o reg) on all data: 0.05992626
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.493992e-06
Norm of the params: 10.239423
              Random: fixed  14 labels. Loss 0.05993. Accuracy 0.976.
### Flips: 1230, rs: 19, checks: 410
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007842386
Train loss (w/o reg) on all data: 0.004536763
Test loss (w/o reg) on all data: 0.0060838386
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6200703e-07
Norm of the params: 8.130959
     Influence (LOO): fixed 154 labels. Loss 0.00608. Accuracy 0.998.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061698128
Train loss (w/o reg) on all data: 0.0030084073
Test loss (w/o reg) on all data: 0.0050068884
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.2276693e-08
Norm of the params: 7.951611
                Loss: fixed 156 labels. Loss 0.00501. Accuracy 0.999.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069785744
Train loss (w/o reg) on all data: 0.06444715
Test loss (w/o reg) on all data: 0.057632405
Train acc on all data:  0.974957451981522
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.8816409e-05
Norm of the params: 10.333049
              Random: fixed  22 labels. Loss 0.05763. Accuracy 0.983.
### Flips: 1230, rs: 19, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601141
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6382335e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.22891155e-08
Norm of the params: 6.092812
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06819382
Train loss (w/o reg) on all data: 0.0630192
Test loss (w/o reg) on all data: 0.054562703
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.0482581e-05
Norm of the params: 10.173121
              Random: fixed  30 labels. Loss 0.05456. Accuracy 0.986.
### Flips: 1230, rs: 19, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.847898e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4842666e-08
Norm of the params: 6.092821
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06638403
Train loss (w/o reg) on all data: 0.061444037
Test loss (w/o reg) on all data: 0.049221322
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1223806e-05
Norm of the params: 9.939819
              Random: fixed  38 labels. Loss 0.04922. Accuracy 0.987.
### Flips: 1230, rs: 19, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096010504
Test loss (w/o reg) on all data: 0.0026560323
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7395337e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.254165e-09
Norm of the params: 6.092813
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06305374
Train loss (w/o reg) on all data: 0.058375284
Test loss (w/o reg) on all data: 0.04257143
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.4070814e-06
Norm of the params: 9.67312
              Random: fixed  49 labels. Loss 0.04257. Accuracy 0.990.
### Flips: 1230, rs: 19, checks: 1230
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012326
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9379044e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3328998e-08
Norm of the params: 6.09282
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06140832
Train loss (w/o reg) on all data: 0.0568459
Test loss (w/o reg) on all data: 0.040014956
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.253071e-06
Norm of the params: 9.552403
              Random: fixed  56 labels. Loss 0.04001. Accuracy 0.991.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075071804
Train loss (w/o reg) on all data: 0.071357496
Test loss (w/o reg) on all data: 0.062260702
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.147223e-06
Norm of the params: 8.618939
Flipped loss: 0.06226. Accuracy: 0.973
### Flips: 1230, rs: 20, checks: 205
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029119162
Train loss (w/o reg) on all data: 0.02450518
Test loss (w/o reg) on all data: 0.019719945
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.608569e-06
Norm of the params: 9.60623
     Influence (LOO): fixed 116 labels. Loss 0.01972. Accuracy 0.997.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013541842
Train loss (w/o reg) on all data: 0.008320191
Test loss (w/o reg) on all data: 0.018260794
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0189115e-06
Norm of the params: 10.219247
                Loss: fixed 135 labels. Loss 0.01826. Accuracy 0.994.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07413747
Train loss (w/o reg) on all data: 0.07037313
Test loss (w/o reg) on all data: 0.060037497
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.4760626e-06
Norm of the params: 8.676796
              Random: fixed   4 labels. Loss 0.06004. Accuracy 0.976.
### Flips: 1230, rs: 20, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007648578
Train loss (w/o reg) on all data: 0.004708303
Test loss (w/o reg) on all data: 0.0074388795
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0172566e-07
Norm of the params: 7.668474
     Influence (LOO): fixed 155 labels. Loss 0.00744. Accuracy 0.997.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041189995
Train loss (w/o reg) on all data: 0.0016953284
Test loss (w/o reg) on all data: 0.0032066498
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5641368e-08
Norm of the params: 6.962286
                Loss: fixed 159 labels. Loss 0.00321. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07271995
Train loss (w/o reg) on all data: 0.069014266
Test loss (w/o reg) on all data: 0.055391647
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.9736527e-06
Norm of the params: 8.608927
              Random: fixed  13 labels. Loss 0.05539. Accuracy 0.979.
### Flips: 1230, rs: 20, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601084
Test loss (w/o reg) on all data: 0.002656039
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9610352e-08
Norm of the params: 6.092834
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601245
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4523715e-08
Norm of the params: 6.092808
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07159192
Train loss (w/o reg) on all data: 0.067934826
Test loss (w/o reg) on all data: 0.052790802
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4146497e-05
Norm of the params: 8.552303
              Random: fixed  19 labels. Loss 0.05279. Accuracy 0.979.
### Flips: 1230, rs: 20, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009945
Test loss (w/o reg) on all data: 0.0026560153
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6117293e-08
Norm of the params: 6.0928483
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013123
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8965865e-08
Norm of the params: 6.0927963
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0684374
Train loss (w/o reg) on all data: 0.06468736
Test loss (w/o reg) on all data: 0.04973486
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.0014706e-05
Norm of the params: 8.660299
              Random: fixed  29 labels. Loss 0.04973. Accuracy 0.983.
### Flips: 1230, rs: 20, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601011
Test loss (w/o reg) on all data: 0.0026560214
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6694594e-08
Norm of the params: 6.0928464
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8505572e-08
Norm of the params: 6.092812
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06663792
Train loss (w/o reg) on all data: 0.06304137
Test loss (w/o reg) on all data: 0.047205567
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.3912817e-05
Norm of the params: 8.481214
              Random: fixed  36 labels. Loss 0.04721. Accuracy 0.983.
### Flips: 1230, rs: 20, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4874511e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012146
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.868045e-09
Norm of the params: 6.092813
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06401689
Train loss (w/o reg) on all data: 0.060432363
Test loss (w/o reg) on all data: 0.044398442
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.0968844e-06
Norm of the params: 8.467035
              Random: fixed  44 labels. Loss 0.04440. Accuracy 0.985.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07690354
Train loss (w/o reg) on all data: 0.072773986
Test loss (w/o reg) on all data: 0.04995658
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.311009e-06
Norm of the params: 9.087963
Flipped loss: 0.04996. Accuracy: 0.987
### Flips: 1230, rs: 21, checks: 205
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031494584
Train loss (w/o reg) on all data: 0.026649771
Test loss (w/o reg) on all data: 0.01908325
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.151821e-06
Norm of the params: 9.84359
     Influence (LOO): fixed 111 labels. Loss 0.01908. Accuracy 0.993.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013184118
Train loss (w/o reg) on all data: 0.008819135
Test loss (w/o reg) on all data: 0.01377222
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8533906e-06
Norm of the params: 9.34343
                Loss: fixed 137 labels. Loss 0.01377. Accuracy 0.995.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0732554
Train loss (w/o reg) on all data: 0.06906314
Test loss (w/o reg) on all data: 0.045984887
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.3222534e-06
Norm of the params: 9.156694
              Random: fixed  14 labels. Loss 0.04598. Accuracy 0.989.
### Flips: 1230, rs: 21, checks: 410
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073291035
Train loss (w/o reg) on all data: 0.004746671
Test loss (w/o reg) on all data: 0.004552567
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2808002e-07
Norm of the params: 7.1867003
     Influence (LOO): fixed 154 labels. Loss 0.00455. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004784142
Train loss (w/o reg) on all data: 0.002072491
Test loss (w/o reg) on all data: 0.0046908003
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3295684e-08
Norm of the params: 7.3643074
                Loss: fixed 157 labels. Loss 0.00469. Accuracy 0.999.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070657745
Train loss (w/o reg) on all data: 0.06652643
Test loss (w/o reg) on all data: 0.042263992
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7438605e-06
Norm of the params: 9.089905
              Random: fixed  25 labels. Loss 0.04226. Accuracy 0.992.
### Flips: 1230, rs: 21, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011546
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8067562e-08
Norm of the params: 6.0928226
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012733
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6088682e-08
Norm of the params: 6.092803
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068176985
Train loss (w/o reg) on all data: 0.06418793
Test loss (w/o reg) on all data: 0.039877586
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.640007e-06
Norm of the params: 8.932027
              Random: fixed  32 labels. Loss 0.03988. Accuracy 0.991.
### Flips: 1230, rs: 21, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601276
Test loss (w/o reg) on all data: 0.0026560803
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9238265e-08
Norm of the params: 6.092803
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7804194e-08
Norm of the params: 6.092825
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06634216
Train loss (w/o reg) on all data: 0.062216915
Test loss (w/o reg) on all data: 0.038161583
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.813559e-06
Norm of the params: 9.083221
              Random: fixed  40 labels. Loss 0.03816. Accuracy 0.990.
### Flips: 1230, rs: 21, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601247
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3844402e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601125
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.264494e-08
Norm of the params: 6.0928273
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06487237
Train loss (w/o reg) on all data: 0.060709428
Test loss (w/o reg) on all data: 0.035960622
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.412069e-06
Norm of the params: 9.124629
              Random: fixed  48 labels. Loss 0.03596. Accuracy 0.997.
### Flips: 1230, rs: 21, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601123
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4524399e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960127
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0657522e-08
Norm of the params: 6.0928035
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06364223
Train loss (w/o reg) on all data: 0.059501547
Test loss (w/o reg) on all data: 0.034662794
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.0567302e-05
Norm of the params: 9.100203
              Random: fixed  54 labels. Loss 0.03466. Accuracy 0.997.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07839013
Train loss (w/o reg) on all data: 0.07474868
Test loss (w/o reg) on all data: 0.056548737
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.1823875e-05
Norm of the params: 8.533987
Flipped loss: 0.05655. Accuracy: 0.983
### Flips: 1230, rs: 22, checks: 205
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02877021
Train loss (w/o reg) on all data: 0.023263501
Test loss (w/o reg) on all data: 0.018927764
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.9072684e-07
Norm of the params: 10.494484
     Influence (LOO): fixed 112 labels. Loss 0.01893. Accuracy 0.991.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012694837
Train loss (w/o reg) on all data: 0.008144766
Test loss (w/o reg) on all data: 0.010159828
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.732511e-07
Norm of the params: 9.539467
                Loss: fixed 142 labels. Loss 0.01016. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07766784
Train loss (w/o reg) on all data: 0.07403886
Test loss (w/o reg) on all data: 0.055413634
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.518403e-06
Norm of the params: 8.519364
              Random: fixed   4 labels. Loss 0.05541. Accuracy 0.983.
### Flips: 1230, rs: 22, checks: 410
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010131921
Train loss (w/o reg) on all data: 0.006512271
Test loss (w/o reg) on all data: 0.007977588
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.385603e-07
Norm of the params: 8.508408
     Influence (LOO): fixed 154 labels. Loss 0.00798. Accuracy 0.997.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043639843
Train loss (w/o reg) on all data: 0.001749215
Test loss (w/o reg) on all data: 0.004259003
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.809301e-08
Norm of the params: 7.2315555
                Loss: fixed 163 labels. Loss 0.00426. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074423455
Train loss (w/o reg) on all data: 0.07080115
Test loss (w/o reg) on all data: 0.05081423
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.8098144e-05
Norm of the params: 8.511529
              Random: fixed  17 labels. Loss 0.05081. Accuracy 0.987.
### Flips: 1230, rs: 22, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4972207e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1480677e-08
Norm of the params: 6.0928097
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07420436
Train loss (w/o reg) on all data: 0.070504725
Test loss (w/o reg) on all data: 0.048680216
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1234012e-05
Norm of the params: 8.601907
              Random: fixed  21 labels. Loss 0.04868. Accuracy 0.988.
### Flips: 1230, rs: 22, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601301
Test loss (w/o reg) on all data: 0.0026560887
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.493293e-08
Norm of the params: 6.092798
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.41247405e-08
Norm of the params: 6.0928183
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071318366
Train loss (w/o reg) on all data: 0.06738637
Test loss (w/o reg) on all data: 0.04569754
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.7179767e-06
Norm of the params: 8.867921
              Random: fixed  30 labels. Loss 0.04570. Accuracy 0.988.
### Flips: 1230, rs: 22, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.19772e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010795
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.025686e-08
Norm of the params: 6.092835
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06948355
Train loss (w/o reg) on all data: 0.065438405
Test loss (w/o reg) on all data: 0.041256577
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.600684e-06
Norm of the params: 8.994603
              Random: fixed  37 labels. Loss 0.04126. Accuracy 0.992.
### Flips: 1230, rs: 22, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.002578e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.549251e-09
Norm of the params: 6.092817
                Loss: fixed 168 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067506716
Train loss (w/o reg) on all data: 0.063477606
Test loss (w/o reg) on all data: 0.039351247
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.9116265e-06
Norm of the params: 8.976763
              Random: fixed  44 labels. Loss 0.03935. Accuracy 0.994.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07112586
Train loss (w/o reg) on all data: 0.06636443
Test loss (w/o reg) on all data: 0.054489672
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.686173e-05
Norm of the params: 9.758514
Flipped loss: 0.05449. Accuracy: 0.986
### Flips: 1230, rs: 23, checks: 205
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025119092
Train loss (w/o reg) on all data: 0.020636627
Test loss (w/o reg) on all data: 0.015107925
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.165709e-07
Norm of the params: 9.46833
     Influence (LOO): fixed 117 labels. Loss 0.01511. Accuracy 0.994.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01332921
Train loss (w/o reg) on all data: 0.008172815
Test loss (w/o reg) on all data: 0.01737522
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.105201e-07
Norm of the params: 10.155191
                Loss: fixed 129 labels. Loss 0.01738. Accuracy 0.994.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06947167
Train loss (w/o reg) on all data: 0.06475169
Test loss (w/o reg) on all data: 0.04964015
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.155538e-06
Norm of the params: 9.715944
              Random: fixed   8 labels. Loss 0.04964. Accuracy 0.989.
### Flips: 1230, rs: 23, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011086
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2602343e-08
Norm of the params: 6.09283
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005750915
Train loss (w/o reg) on all data: 0.0027806258
Test loss (w/o reg) on all data: 0.0047121076
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.15175226e-07
Norm of the params: 7.7075152
                Loss: fixed 149 labels. Loss 0.00471. Accuracy 0.999.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06696126
Train loss (w/o reg) on all data: 0.062378827
Test loss (w/o reg) on all data: 0.04803406
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.075746e-06
Norm of the params: 9.573326
              Random: fixed  17 labels. Loss 0.04803. Accuracy 0.989.
### Flips: 1230, rs: 23, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8099866e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2789079e-08
Norm of the params: 6.092823
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06613192
Train loss (w/o reg) on all data: 0.06142361
Test loss (w/o reg) on all data: 0.045873396
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.855143e-06
Norm of the params: 9.703929
              Random: fixed  23 labels. Loss 0.04587. Accuracy 0.989.
### Flips: 1230, rs: 23, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6578562e-08
Norm of the params: 6.092802
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7790727e-08
Norm of the params: 6.092826
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06450397
Train loss (w/o reg) on all data: 0.05979849
Test loss (w/o reg) on all data: 0.042823695
Train acc on all data:  0.974957451981522
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.5703355e-06
Norm of the params: 9.701005
              Random: fixed  30 labels. Loss 0.04282. Accuracy 0.989.
### Flips: 1230, rs: 23, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010923
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.561788e-08
Norm of the params: 6.092833
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601281
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3371666e-08
Norm of the params: 6.092801
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060517542
Train loss (w/o reg) on all data: 0.05549952
Test loss (w/o reg) on all data: 0.03941301
Train acc on all data:  0.9773887673231219
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.0095564e-05
Norm of the params: 10.018004
              Random: fixed  40 labels. Loss 0.03941. Accuracy 0.991.
### Flips: 1230, rs: 23, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096009014
Test loss (w/o reg) on all data: 0.0026560125
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.401715e-08
Norm of the params: 6.0928636
     Influence (LOO): fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.342386e-08
Norm of the params: 6.0928154
                Loss: fixed 153 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057987843
Train loss (w/o reg) on all data: 0.053082444
Test loss (w/o reg) on all data: 0.037298337
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.3414283e-06
Norm of the params: 9.904946
              Random: fixed  50 labels. Loss 0.03730. Accuracy 0.990.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07970206
Train loss (w/o reg) on all data: 0.075259976
Test loss (w/o reg) on all data: 0.058492955
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.2197975e-06
Norm of the params: 9.425585
Flipped loss: 0.05849. Accuracy: 0.983
### Flips: 1230, rs: 24, checks: 205
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034028724
Train loss (w/o reg) on all data: 0.029300703
Test loss (w/o reg) on all data: 0.020816045
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4695545e-06
Norm of the params: 9.724219
     Influence (LOO): fixed 114 labels. Loss 0.02082. Accuracy 0.993.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015053286
Train loss (w/o reg) on all data: 0.009466318
Test loss (w/o reg) on all data: 0.009216539
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0744167e-06
Norm of the params: 10.570684
                Loss: fixed 146 labels. Loss 0.00922. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07759972
Train loss (w/o reg) on all data: 0.07286534
Test loss (w/o reg) on all data: 0.056402236
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.6187656e-06
Norm of the params: 9.73076
              Random: fixed   7 labels. Loss 0.05640. Accuracy 0.985.
### Flips: 1230, rs: 24, checks: 410
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010757925
Train loss (w/o reg) on all data: 0.0073300824
Test loss (w/o reg) on all data: 0.0053773792
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.45327e-07
Norm of the params: 8.279907
     Influence (LOO): fixed 160 labels. Loss 0.00538. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047204765
Train loss (w/o reg) on all data: 0.001968339
Test loss (w/o reg) on all data: 0.0033460408
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7253765e-07
Norm of the params: 7.4190803
                Loss: fixed 169 labels. Loss 0.00335. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07499002
Train loss (w/o reg) on all data: 0.07032585
Test loss (w/o reg) on all data: 0.05014665
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.26649e-06
Norm of the params: 9.658328
              Random: fixed  19 labels. Loss 0.05015. Accuracy 0.989.
### Flips: 1230, rs: 24, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162387
Train loss (w/o reg) on all data: 0.000960111
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0592434e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601274
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6582266e-08
Norm of the params: 6.0928035
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07412608
Train loss (w/o reg) on all data: 0.06944648
Test loss (w/o reg) on all data: 0.04782522
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3910088e-05
Norm of the params: 9.674294
              Random: fixed  25 labels. Loss 0.04783. Accuracy 0.991.
### Flips: 1230, rs: 24, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601242
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6087427e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8051528e-08
Norm of the params: 6.09282
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07189498
Train loss (w/o reg) on all data: 0.06736534
Test loss (w/o reg) on all data: 0.044131853
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.7868438e-06
Norm of the params: 9.5180235
              Random: fixed  36 labels. Loss 0.04413. Accuracy 0.992.
### Flips: 1230, rs: 24, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.686902e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.15827e-09
Norm of the params: 6.09282
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069848984
Train loss (w/o reg) on all data: 0.065286666
Test loss (w/o reg) on all data: 0.042778585
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.9947753e-06
Norm of the params: 9.552296
              Random: fixed  42 labels. Loss 0.04278. Accuracy 0.992.
### Flips: 1230, rs: 24, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1539151e-08
Norm of the params: 6.0928044
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601211
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3726825e-08
Norm of the params: 6.0928135
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065844625
Train loss (w/o reg) on all data: 0.061266813
Test loss (w/o reg) on all data: 0.037297558
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.2778878e-06
Norm of the params: 9.568504
              Random: fixed  56 labels. Loss 0.03730. Accuracy 0.994.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07482512
Train loss (w/o reg) on all data: 0.070885435
Test loss (w/o reg) on all data: 0.05604739
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.05611e-06
Norm of the params: 8.876581
Flipped loss: 0.05605. Accuracy: 0.981
### Flips: 1230, rs: 25, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02607821
Train loss (w/o reg) on all data: 0.021031376
Test loss (w/o reg) on all data: 0.017713552
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.478003e-07
Norm of the params: 10.046722
     Influence (LOO): fixed 113 labels. Loss 0.01771. Accuracy 0.995.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013908594
Train loss (w/o reg) on all data: 0.008896624
Test loss (w/o reg) on all data: 0.011037355
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.445447e-07
Norm of the params: 10.011963
                Loss: fixed 136 labels. Loss 0.01104. Accuracy 0.998.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07384715
Train loss (w/o reg) on all data: 0.06983461
Test loss (w/o reg) on all data: 0.053669337
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.5321587e-06
Norm of the params: 8.958278
              Random: fixed   7 labels. Loss 0.05367. Accuracy 0.981.
### Flips: 1230, rs: 25, checks: 410
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060481727
Train loss (w/o reg) on all data: 0.003643641
Test loss (w/o reg) on all data: 0.0043807435
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9887363e-07
Norm of the params: 6.934741
     Influence (LOO): fixed 154 labels. Loss 0.00438. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004491089
Train loss (w/o reg) on all data: 0.0020253449
Test loss (w/o reg) on all data: 0.0045721573
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8955845e-07
Norm of the params: 7.022455
                Loss: fixed 156 labels. Loss 0.00457. Accuracy 0.999.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07200367
Train loss (w/o reg) on all data: 0.068031386
Test loss (w/o reg) on all data: 0.05052061
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.1877706e-05
Norm of the params: 8.913229
              Random: fixed  17 labels. Loss 0.05052. Accuracy 0.984.
### Flips: 1230, rs: 25, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.002656077
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4834077e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5864542e-08
Norm of the params: 6.09282
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06895357
Train loss (w/o reg) on all data: 0.06491241
Test loss (w/o reg) on all data: 0.04768032
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.3607877e-06
Norm of the params: 8.990176
              Random: fixed  26 labels. Loss 0.04768. Accuracy 0.986.
### Flips: 1230, rs: 25, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.002656046
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.719021e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5599928e-08
Norm of the params: 6.0928106
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06650868
Train loss (w/o reg) on all data: 0.06238702
Test loss (w/o reg) on all data: 0.043390755
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.0500363e-06
Norm of the params: 9.079271
              Random: fixed  34 labels. Loss 0.04339. Accuracy 0.988.
### Flips: 1230, rs: 25, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601111
Test loss (w/o reg) on all data: 0.0026560433
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2805695e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4074822e-08
Norm of the params: 6.0928154
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06495738
Train loss (w/o reg) on all data: 0.060827818
Test loss (w/o reg) on all data: 0.039929718
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.842345e-06
Norm of the params: 9.087977
              Random: fixed  42 labels. Loss 0.03993. Accuracy 0.991.
### Flips: 1230, rs: 25, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601166
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3994332e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1355157e-08
Norm of the params: 6.092814
                Loss: fixed 159 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05955383
Train loss (w/o reg) on all data: 0.055392068
Test loss (w/o reg) on all data: 0.033771332
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.9152654e-06
Norm of the params: 9.123339
              Random: fixed  57 labels. Loss 0.03377. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081453994
Train loss (w/o reg) on all data: 0.07738796
Test loss (w/o reg) on all data: 0.06546413
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.5807456e-06
Norm of the params: 9.017801
Flipped loss: 0.06546. Accuracy: 0.975
### Flips: 1230, rs: 26, checks: 205
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038676724
Train loss (w/o reg) on all data: 0.033086285
Test loss (w/o reg) on all data: 0.027884504
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.053938e-06
Norm of the params: 10.573965
     Influence (LOO): fixed 112 labels. Loss 0.02788. Accuracy 0.987.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019818591
Train loss (w/o reg) on all data: 0.014020266
Test loss (w/o reg) on all data: 0.017819878
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.6888244e-07
Norm of the params: 10.768775
                Loss: fixed 143 labels. Loss 0.01782. Accuracy 0.993.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07966481
Train loss (w/o reg) on all data: 0.07554409
Test loss (w/o reg) on all data: 0.063676625
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5143974e-05
Norm of the params: 9.0782385
              Random: fixed   6 labels. Loss 0.06368. Accuracy 0.976.
### Flips: 1230, rs: 26, checks: 410
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010621879
Train loss (w/o reg) on all data: 0.0071807145
Test loss (w/o reg) on all data: 0.008383492
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3095032e-06
Norm of the params: 8.29598
     Influence (LOO): fixed 166 labels. Loss 0.00838. Accuracy 0.998.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034181955
Train loss (w/o reg) on all data: 0.0012643981
Test loss (w/o reg) on all data: 0.0029283264
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2205216e-08
Norm of the params: 6.5632267
                Loss: fixed 175 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078126825
Train loss (w/o reg) on all data: 0.07388615
Test loss (w/o reg) on all data: 0.06200253
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.580492e-05
Norm of the params: 9.209426
              Random: fixed  13 labels. Loss 0.06200. Accuracy 0.975.
### Flips: 1230, rs: 26, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560463
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0951953e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601299
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9127196e-08
Norm of the params: 6.092798
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07560426
Train loss (w/o reg) on all data: 0.07131174
Test loss (w/o reg) on all data: 0.05676816
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.3553086e-06
Norm of the params: 9.265549
              Random: fixed  26 labels. Loss 0.05677. Accuracy 0.978.
### Flips: 1230, rs: 26, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.173563e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2155211e-08
Norm of the params: 6.0928173
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07118443
Train loss (w/o reg) on all data: 0.0667648
Test loss (w/o reg) on all data: 0.054867215
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.1948027e-06
Norm of the params: 9.401728
              Random: fixed  39 labels. Loss 0.05487. Accuracy 0.978.
### Flips: 1230, rs: 26, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010685
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5998312e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1520112e-08
Norm of the params: 6.092809
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070037775
Train loss (w/o reg) on all data: 0.06567564
Test loss (w/o reg) on all data: 0.053013906
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.859764e-06
Norm of the params: 9.340379
              Random: fixed  44 labels. Loss 0.05301. Accuracy 0.982.
### Flips: 1230, rs: 26, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011115
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.630176e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560652
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3746746e-08
Norm of the params: 6.092812
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06735137
Train loss (w/o reg) on all data: 0.06281766
Test loss (w/o reg) on all data: 0.050868854
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0746867e-05
Norm of the params: 9.5223
              Random: fixed  53 labels. Loss 0.05087. Accuracy 0.983.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0754194
Train loss (w/o reg) on all data: 0.071705475
Test loss (w/o reg) on all data: 0.06424917
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.341592e-06
Norm of the params: 8.618502
Flipped loss: 0.06425. Accuracy: 0.975
### Flips: 1230, rs: 27, checks: 205
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026831329
Train loss (w/o reg) on all data: 0.021545311
Test loss (w/o reg) on all data: 0.025933664
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.520634e-06
Norm of the params: 10.282041
     Influence (LOO): fixed 117 labels. Loss 0.02593. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014635401
Train loss (w/o reg) on all data: 0.009179234
Test loss (w/o reg) on all data: 0.02177567
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.7125417e-06
Norm of the params: 10.446212
                Loss: fixed 133 labels. Loss 0.02178. Accuracy 0.993.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07293438
Train loss (w/o reg) on all data: 0.06924206
Test loss (w/o reg) on all data: 0.059222426
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 7.201673e-06
Norm of the params: 8.593396
              Random: fixed   9 labels. Loss 0.05922. Accuracy 0.974.
### Flips: 1230, rs: 27, checks: 410
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068540224
Train loss (w/o reg) on all data: 0.0038199893
Test loss (w/o reg) on all data: 0.010442855
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.230298e-07
Norm of the params: 7.7897797
     Influence (LOO): fixed 163 labels. Loss 0.01044. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059948713
Train loss (w/o reg) on all data: 0.0029418936
Test loss (w/o reg) on all data: 0.012362934
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8068577e-07
Norm of the params: 7.8140616
                Loss: fixed 160 labels. Loss 0.01236. Accuracy 0.995.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07110469
Train loss (w/o reg) on all data: 0.06755493
Test loss (w/o reg) on all data: 0.054923702
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 7.3236833e-06
Norm of the params: 8.425865
              Random: fixed  16 labels. Loss 0.05492. Accuracy 0.976.
### Flips: 1230, rs: 27, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0792836e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601164
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4415018e-08
Norm of the params: 6.0928206
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06975771
Train loss (w/o reg) on all data: 0.06620145
Test loss (w/o reg) on all data: 0.05183724
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.4578196e-06
Norm of the params: 8.433571
              Random: fixed  25 labels. Loss 0.05184. Accuracy 0.981.
### Flips: 1230, rs: 27, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.16416246e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162387
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0943547e-08
Norm of the params: 6.0928197
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06753485
Train loss (w/o reg) on all data: 0.06386709
Test loss (w/o reg) on all data: 0.04928186
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.8987429e-06
Norm of the params: 8.564766
              Random: fixed  31 labels. Loss 0.04928. Accuracy 0.980.
### Flips: 1230, rs: 27, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.901832e-09
Norm of the params: 6.0928216
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2611822e-08
Norm of the params: 6.092809
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06505429
Train loss (w/o reg) on all data: 0.06126459
Test loss (w/o reg) on all data: 0.044651326
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 7.832465e-06
Norm of the params: 8.705979
              Random: fixed  43 labels. Loss 0.04465. Accuracy 0.983.
### Flips: 1230, rs: 27, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5183255e-09
Norm of the params: 6.0928187
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1991145e-08
Norm of the params: 6.092815
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06341805
Train loss (w/o reg) on all data: 0.059679497
Test loss (w/o reg) on all data: 0.039216712
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.440289e-06
Norm of the params: 8.647027
              Random: fixed  54 labels. Loss 0.03922. Accuracy 0.988.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07547489
Train loss (w/o reg) on all data: 0.0712412
Test loss (w/o reg) on all data: 0.051402595
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.9699837e-05
Norm of the params: 9.201833
Flipped loss: 0.05140. Accuracy: 0.990
### Flips: 1230, rs: 28, checks: 205
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024517048
Train loss (w/o reg) on all data: 0.020063264
Test loss (w/o reg) on all data: 0.014123546
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1190234e-06
Norm of the params: 9.43799
     Influence (LOO): fixed 115 labels. Loss 0.01412. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009109773
Train loss (w/o reg) on all data: 0.005065827
Test loss (w/o reg) on all data: 0.008364573
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.4422418e-07
Norm of the params: 8.993271
                Loss: fixed 136 labels. Loss 0.00836. Accuracy 0.998.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07407996
Train loss (w/o reg) on all data: 0.070008524
Test loss (w/o reg) on all data: 0.046517048
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7600803e-05
Norm of the params: 9.023787
              Random: fixed   9 labels. Loss 0.04652. Accuracy 0.992.
### Flips: 1230, rs: 28, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046222876
Train loss (w/o reg) on all data: 0.0024850073
Test loss (w/o reg) on all data: 0.0035039948
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.437281e-08
Norm of the params: 6.538012
     Influence (LOO): fixed 148 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005008865
Train loss (w/o reg) on all data: 0.0022014035
Test loss (w/o reg) on all data: 0.004203753
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2483562e-07
Norm of the params: 7.4932795
                Loss: fixed 147 labels. Loss 0.00420. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07215466
Train loss (w/o reg) on all data: 0.06799739
Test loss (w/o reg) on all data: 0.045521125
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0966285e-05
Norm of the params: 9.11842
              Random: fixed  16 labels. Loss 0.04552. Accuracy 0.990.
### Flips: 1230, rs: 28, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010853
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9724596e-08
Norm of the params: 6.092834
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1310593e-08
Norm of the params: 6.0928144
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07117073
Train loss (w/o reg) on all data: 0.06703966
Test loss (w/o reg) on all data: 0.042899553
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.3830726e-06
Norm of the params: 9.089635
              Random: fixed  22 labels. Loss 0.04290. Accuracy 0.993.
### Flips: 1230, rs: 28, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010667
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.014465e-08
Norm of the params: 6.092836
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4395814e-08
Norm of the params: 6.09281
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068481915
Train loss (w/o reg) on all data: 0.064350925
Test loss (w/o reg) on all data: 0.040056165
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.992376e-06
Norm of the params: 9.089541
              Random: fixed  32 labels. Loss 0.04006. Accuracy 0.995.
### Flips: 1230, rs: 28, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011575
Test loss (w/o reg) on all data: 0.0026560472
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3763357e-08
Norm of the params: 6.092822
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601259
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7303269e-08
Norm of the params: 6.092805
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065116905
Train loss (w/o reg) on all data: 0.060767803
Test loss (w/o reg) on all data: 0.037358325
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.254573e-06
Norm of the params: 9.326413
              Random: fixed  42 labels. Loss 0.03736. Accuracy 0.994.
### Flips: 1230, rs: 28, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601284
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3753285e-08
Norm of the params: 6.092801
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601119
Test loss (w/o reg) on all data: 0.0026560503
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4820393e-08
Norm of the params: 6.0928283
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06132643
Train loss (w/o reg) on all data: 0.056939416
Test loss (w/o reg) on all data: 0.033657838
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.954706e-06
Norm of the params: 9.36698
              Random: fixed  52 labels. Loss 0.03366. Accuracy 0.996.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07919802
Train loss (w/o reg) on all data: 0.075714916
Test loss (w/o reg) on all data: 0.062896036
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 6.145563e-06
Norm of the params: 8.34638
Flipped loss: 0.06290. Accuracy: 0.980
### Flips: 1230, rs: 29, checks: 205
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032000013
Train loss (w/o reg) on all data: 0.027106274
Test loss (w/o reg) on all data: 0.021856029
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1277448e-06
Norm of the params: 9.8931675
     Influence (LOO): fixed 111 labels. Loss 0.02186. Accuracy 0.996.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012921028
Train loss (w/o reg) on all data: 0.008082569
Test loss (w/o reg) on all data: 0.016696539
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.4733915e-07
Norm of the params: 9.837132
                Loss: fixed 143 labels. Loss 0.01670. Accuracy 0.994.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07775037
Train loss (w/o reg) on all data: 0.07444144
Test loss (w/o reg) on all data: 0.056440756
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.3910408e-05
Norm of the params: 8.135025
              Random: fixed  12 labels. Loss 0.05644. Accuracy 0.982.
### Flips: 1230, rs: 29, checks: 410
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004261485
Train loss (w/o reg) on all data: 0.0017424048
Test loss (w/o reg) on all data: 0.0053821085
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.164581e-07
Norm of the params: 7.098
     Influence (LOO): fixed 163 labels. Loss 0.00538. Accuracy 0.998.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046902904
Train loss (w/o reg) on all data: 0.001913468
Test loss (w/o reg) on all data: 0.007516666
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.089195e-07
Norm of the params: 7.452278
                Loss: fixed 163 labels. Loss 0.00752. Accuracy 0.998.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07562267
Train loss (w/o reg) on all data: 0.07229384
Test loss (w/o reg) on all data: 0.052173983
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.436644e-06
Norm of the params: 8.159447
              Random: fixed  23 labels. Loss 0.05217. Accuracy 0.984.
### Flips: 1230, rs: 29, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.34008955e-08
Norm of the params: 6.092814
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.128905e-09
Norm of the params: 6.092818
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07304166
Train loss (w/o reg) on all data: 0.06951502
Test loss (w/o reg) on all data: 0.05022111
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.503058e-06
Norm of the params: 8.398386
              Random: fixed  31 labels. Loss 0.05022. Accuracy 0.983.
### Flips: 1230, rs: 29, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6952098e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.795804e-09
Norm of the params: 6.092823
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0720726
Train loss (w/o reg) on all data: 0.0684858
Test loss (w/o reg) on all data: 0.049375914
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.2759984e-06
Norm of the params: 8.469719
              Random: fixed  36 labels. Loss 0.04938. Accuracy 0.983.
### Flips: 1230, rs: 29, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601539
Test loss (w/o reg) on all data: 0.002656111
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0038345e-08
Norm of the params: 6.09276
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601112
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3925975e-08
Norm of the params: 6.0928297
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06824356
Train loss (w/o reg) on all data: 0.06458453
Test loss (w/o reg) on all data: 0.0469208
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 7.0883557e-06
Norm of the params: 8.554566
              Random: fixed  44 labels. Loss 0.04692. Accuracy 0.983.
### Flips: 1230, rs: 29, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2488462e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601223
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.168177e-09
Norm of the params: 6.0928116
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0644753
Train loss (w/o reg) on all data: 0.0606591
Test loss (w/o reg) on all data: 0.043404654
Train acc on all data:  0.973741794310722
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.0820777e-06
Norm of the params: 8.736361
              Random: fixed  57 labels. Loss 0.04340. Accuracy 0.984.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07707898
Train loss (w/o reg) on all data: 0.07307763
Test loss (w/o reg) on all data: 0.057978284
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.9235995e-06
Norm of the params: 8.945788
Flipped loss: 0.05798. Accuracy: 0.982
### Flips: 1230, rs: 30, checks: 205
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027053952
Train loss (w/o reg) on all data: 0.021942804
Test loss (w/o reg) on all data: 0.018781448
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2071451e-06
Norm of the params: 10.1105385
     Influence (LOO): fixed 118 labels. Loss 0.01878. Accuracy 0.996.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017459694
Train loss (w/o reg) on all data: 0.011355953
Test loss (w/o reg) on all data: 0.011087522
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.4746115e-07
Norm of the params: 11.048749
                Loss: fixed 138 labels. Loss 0.01109. Accuracy 0.997.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07566427
Train loss (w/o reg) on all data: 0.07180571
Test loss (w/o reg) on all data: 0.055792272
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.8439184e-06
Norm of the params: 8.784709
              Random: fixed   9 labels. Loss 0.05579. Accuracy 0.980.
### Flips: 1230, rs: 30, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059539955
Train loss (w/o reg) on all data: 0.0031734484
Test loss (w/o reg) on all data: 0.0036008356
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0942973e-07
Norm of the params: 7.4572744
     Influence (LOO): fixed 161 labels. Loss 0.00360. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051697223
Train loss (w/o reg) on all data: 0.002310902
Test loss (w/o reg) on all data: 0.0038383328
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8016593e-07
Norm of the params: 7.5615087
                Loss: fixed 161 labels. Loss 0.00384. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07394372
Train loss (w/o reg) on all data: 0.070219606
Test loss (w/o reg) on all data: 0.05449698
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.4572124e-06
Norm of the params: 8.630313
              Random: fixed  19 labels. Loss 0.05450. Accuracy 0.982.
### Flips: 1230, rs: 30, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4059293e-08
Norm of the params: 6.09282
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0552214e-08
Norm of the params: 6.092813
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071278304
Train loss (w/o reg) on all data: 0.06735536
Test loss (w/o reg) on all data: 0.050409216
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.24852e-06
Norm of the params: 8.857704
              Random: fixed  28 labels. Loss 0.05041. Accuracy 0.981.
### Flips: 1230, rs: 30, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010766
Test loss (w/o reg) on all data: 0.0026560326
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7277058e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601205
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.2378335e-09
Norm of the params: 6.092814
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06828344
Train loss (w/o reg) on all data: 0.064195536
Test loss (w/o reg) on all data: 0.04479749
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.6060756e-05
Norm of the params: 9.042019
              Random: fixed  38 labels. Loss 0.04480. Accuracy 0.985.
### Flips: 1230, rs: 30, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7506956e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7529773e-08
Norm of the params: 6.092804
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06465627
Train loss (w/o reg) on all data: 0.060500257
Test loss (w/o reg) on all data: 0.03884169
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4653731e-05
Norm of the params: 9.117037
              Random: fixed  50 labels. Loss 0.03884. Accuracy 0.993.
### Flips: 1230, rs: 30, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096009806
Test loss (w/o reg) on all data: 0.0026560302
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3920255e-08
Norm of the params: 6.0928516
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.28389575e-08
Norm of the params: 6.0928135
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061830986
Train loss (w/o reg) on all data: 0.057665676
Test loss (w/o reg) on all data: 0.03531696
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.305937e-06
Norm of the params: 9.127225
              Random: fixed  60 labels. Loss 0.03532. Accuracy 0.994.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0750172
Train loss (w/o reg) on all data: 0.07096076
Test loss (w/o reg) on all data: 0.055362098
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.218301e-06
Norm of the params: 9.00715
Flipped loss: 0.05536. Accuracy: 0.983
### Flips: 1230, rs: 31, checks: 205
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022837406
Train loss (w/o reg) on all data: 0.018199235
Test loss (w/o reg) on all data: 0.014946091
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.5231127e-07
Norm of the params: 9.631376
     Influence (LOO): fixed 113 labels. Loss 0.01495. Accuracy 0.997.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059754243
Train loss (w/o reg) on all data: 0.0027509963
Test loss (w/o reg) on all data: 0.005530269
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6894554e-07
Norm of the params: 8.030477
                Loss: fixed 135 labels. Loss 0.00553. Accuracy 0.999.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07302418
Train loss (w/o reg) on all data: 0.06900399
Test loss (w/o reg) on all data: 0.054322656
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 7.149249e-06
Norm of the params: 8.966821
              Random: fixed   6 labels. Loss 0.05432. Accuracy 0.981.
### Flips: 1230, rs: 31, checks: 410
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012163
Test loss (w/o reg) on all data: 0.002656068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8767876e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035241682
Train loss (w/o reg) on all data: 0.0013204734
Test loss (w/o reg) on all data: 0.003181884
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2535387e-08
Norm of the params: 6.6388173
                Loss: fixed 141 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071674235
Train loss (w/o reg) on all data: 0.06788503
Test loss (w/o reg) on all data: 0.049252376
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.5652085e-06
Norm of the params: 8.705411
              Random: fixed  16 labels. Loss 0.04925. Accuracy 0.983.
### Flips: 1230, rs: 31, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9330344e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0672218e-08
Norm of the params: 6.092821
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06974988
Train loss (w/o reg) on all data: 0.065826096
Test loss (w/o reg) on all data: 0.046999644
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.5296446e-06
Norm of the params: 8.858651
              Random: fixed  23 labels. Loss 0.04700. Accuracy 0.984.
### Flips: 1230, rs: 31, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601225
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.46681165e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5843165e-09
Norm of the params: 6.0928144
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06829208
Train loss (w/o reg) on all data: 0.0643103
Test loss (w/o reg) on all data: 0.045386788
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 7.6212864e-06
Norm of the params: 8.923881
              Random: fixed  27 labels. Loss 0.04539. Accuracy 0.985.
### Flips: 1230, rs: 31, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1288756e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.635583e-09
Norm of the params: 6.0928144
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06622797
Train loss (w/o reg) on all data: 0.06243639
Test loss (w/o reg) on all data: 0.041723274
Train acc on all data:  0.973741794310722
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2547321e-05
Norm of the params: 8.708133
              Random: fixed  36 labels. Loss 0.04172. Accuracy 0.989.
### Flips: 1230, rs: 31, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1458801e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1054632e-08
Norm of the params: 6.092824
                Loss: fixed 142 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062348906
Train loss (w/o reg) on all data: 0.058570247
Test loss (w/o reg) on all data: 0.037329048
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9672775e-06
Norm of the params: 8.693285
              Random: fixed  46 labels. Loss 0.03733. Accuracy 0.991.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0779284
Train loss (w/o reg) on all data: 0.07385599
Test loss (w/o reg) on all data: 0.06266779
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.687103e-06
Norm of the params: 9.024865
Flipped loss: 0.06267. Accuracy: 0.978
### Flips: 1230, rs: 32, checks: 205
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030670542
Train loss (w/o reg) on all data: 0.02607336
Test loss (w/o reg) on all data: 0.022348773
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6416406e-06
Norm of the params: 9.588723
     Influence (LOO): fixed 115 labels. Loss 0.02235. Accuracy 0.993.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013852546
Train loss (w/o reg) on all data: 0.008692275
Test loss (w/o reg) on all data: 0.017385857
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.5475317e-06
Norm of the params: 10.159008
                Loss: fixed 144 labels. Loss 0.01739. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077169284
Train loss (w/o reg) on all data: 0.07316748
Test loss (w/o reg) on all data: 0.05693962
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0884888e-05
Norm of the params: 8.946292
              Random: fixed   9 labels. Loss 0.05694. Accuracy 0.983.
### Flips: 1230, rs: 32, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065209037
Train loss (w/o reg) on all data: 0.003811461
Test loss (w/o reg) on all data: 0.007006659
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.4565e-07
Norm of the params: 7.3613076
     Influence (LOO): fixed 164 labels. Loss 0.00701. Accuracy 0.998.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034339027
Train loss (w/o reg) on all data: 0.0012599605
Test loss (w/o reg) on all data: 0.0029031988
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.88417e-08
Norm of the params: 6.5938497
                Loss: fixed 169 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07609598
Train loss (w/o reg) on all data: 0.072292164
Test loss (w/o reg) on all data: 0.05350079
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8441718e-06
Norm of the params: 8.722179
              Random: fixed  19 labels. Loss 0.05350. Accuracy 0.984.
### Flips: 1230, rs: 32, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601087
Test loss (w/o reg) on all data: 0.0026560402
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6835414e-08
Norm of the params: 6.092834
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8999486e-08
Norm of the params: 6.0928082
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07313522
Train loss (w/o reg) on all data: 0.06932205
Test loss (w/o reg) on all data: 0.050457712
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.5008296e-06
Norm of the params: 8.732893
              Random: fixed  29 labels. Loss 0.05046. Accuracy 0.989.
### Flips: 1230, rs: 32, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7386474e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.348231e-08
Norm of the params: 6.092812
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07023054
Train loss (w/o reg) on all data: 0.066145375
Test loss (w/o reg) on all data: 0.050102085
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.9801703e-06
Norm of the params: 9.038994
              Random: fixed  36 labels. Loss 0.05010. Accuracy 0.986.
### Flips: 1230, rs: 32, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9965443e-08
Norm of the params: 6.0928116
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601107
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.032377e-08
Norm of the params: 6.09283
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06888743
Train loss (w/o reg) on all data: 0.064771645
Test loss (w/o reg) on all data: 0.04615981
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5373003e-05
Norm of the params: 9.072796
              Random: fixed  45 labels. Loss 0.04616. Accuracy 0.990.
### Flips: 1230, rs: 32, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010556
Test loss (w/o reg) on all data: 0.002656039
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.497596e-08
Norm of the params: 6.092838
     Influence (LOO): fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.880623e-09
Norm of the params: 6.0928144
                Loss: fixed 170 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06535182
Train loss (w/o reg) on all data: 0.061226796
Test loss (w/o reg) on all data: 0.04009311
Train acc on all data:  0.973984925844882
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.5256223e-06
Norm of the params: 9.082982
              Random: fixed  58 labels. Loss 0.04009. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075502165
Train loss (w/o reg) on all data: 0.071824506
Test loss (w/o reg) on all data: 0.06261935
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.8623294e-06
Norm of the params: 8.576318
Flipped loss: 0.06262. Accuracy: 0.983
### Flips: 1230, rs: 33, checks: 205
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028837264
Train loss (w/o reg) on all data: 0.024259232
Test loss (w/o reg) on all data: 0.024229782
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.7356736e-07
Norm of the params: 9.568732
     Influence (LOO): fixed 114 labels. Loss 0.02423. Accuracy 0.993.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014296826
Train loss (w/o reg) on all data: 0.009597784
Test loss (w/o reg) on all data: 0.016295113
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.145607e-07
Norm of the params: 9.694372
                Loss: fixed 139 labels. Loss 0.01630. Accuracy 0.996.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074254036
Train loss (w/o reg) on all data: 0.07072581
Test loss (w/o reg) on all data: 0.059529044
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.56123e-06
Norm of the params: 8.400264
              Random: fixed   7 labels. Loss 0.05953. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 410
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00614543
Train loss (w/o reg) on all data: 0.0035369832
Test loss (w/o reg) on all data: 0.005597968
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3649653e-07
Norm of the params: 7.222806
     Influence (LOO): fixed 159 labels. Loss 0.00560. Accuracy 0.997.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044962754
Train loss (w/o reg) on all data: 0.0020996179
Test loss (w/o reg) on all data: 0.00425223
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0665374e-07
Norm of the params: 6.923377
                Loss: fixed 161 labels. Loss 0.00425. Accuracy 0.999.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07020405
Train loss (w/o reg) on all data: 0.0663774
Test loss (w/o reg) on all data: 0.05647339
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.424919e-06
Norm of the params: 8.748315
              Random: fixed  20 labels. Loss 0.05647. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032313403
Train loss (w/o reg) on all data: 0.0013606071
Test loss (w/o reg) on all data: 0.0032806927
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.9630455e-09
Norm of the params: 6.116752
     Influence (LOO): fixed 164 labels. Loss 0.00328. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601289
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.713618e-08
Norm of the params: 6.0927997
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06797937
Train loss (w/o reg) on all data: 0.0642078
Test loss (w/o reg) on all data: 0.05264383
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4713998e-05
Norm of the params: 8.685129
              Random: fixed  30 labels. Loss 0.05264. Accuracy 0.985.
### Flips: 1230, rs: 33, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.655602e-09
Norm of the params: 6.0928154
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.077486e-09
Norm of the params: 6.0928183
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064477645
Train loss (w/o reg) on all data: 0.060472786
Test loss (w/o reg) on all data: 0.049720127
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.400273e-06
Norm of the params: 8.949702
              Random: fixed  37 labels. Loss 0.04972. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012565
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.899171e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601166
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2453947e-08
Norm of the params: 6.09282
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061284203
Train loss (w/o reg) on all data: 0.057194173
Test loss (w/o reg) on all data: 0.048155114
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.650222e-06
Norm of the params: 9.04437
              Random: fixed  47 labels. Loss 0.04816. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009600952
Test loss (w/o reg) on all data: 0.0026560274
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3548198e-08
Norm of the params: 6.0928555
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.177505e-09
Norm of the params: 6.092819
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056384373
Train loss (w/o reg) on all data: 0.05215319
Test loss (w/o reg) on all data: 0.04474841
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.2893045e-06
Norm of the params: 9.199115
              Random: fixed  59 labels. Loss 0.04475. Accuracy 0.985.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07685618
Train loss (w/o reg) on all data: 0.07280922
Test loss (w/o reg) on all data: 0.06341417
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.472272e-06
Norm of the params: 8.996622
Flipped loss: 0.06341. Accuracy: 0.978
### Flips: 1230, rs: 34, checks: 205
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027749205
Train loss (w/o reg) on all data: 0.02296389
Test loss (w/o reg) on all data: 0.018999303
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8145173e-06
Norm of the params: 9.782958
     Influence (LOO): fixed 114 labels. Loss 0.01900. Accuracy 0.994.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012232065
Train loss (w/o reg) on all data: 0.007304389
Test loss (w/o reg) on all data: 0.01723428
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.6391996e-07
Norm of the params: 9.927413
                Loss: fixed 138 labels. Loss 0.01723. Accuracy 0.993.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07543803
Train loss (w/o reg) on all data: 0.07155018
Test loss (w/o reg) on all data: 0.060437586
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.1144073e-06
Norm of the params: 8.817993
              Random: fixed   9 labels. Loss 0.06044. Accuracy 0.984.
### Flips: 1230, rs: 34, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004687014
Train loss (w/o reg) on all data: 0.0021034796
Test loss (w/o reg) on all data: 0.007174544
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0162261e-07
Norm of the params: 7.188232
     Influence (LOO): fixed 156 labels. Loss 0.00717. Accuracy 0.997.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005912548
Train loss (w/o reg) on all data: 0.0028338847
Test loss (w/o reg) on all data: 0.008330196
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.385028e-08
Norm of the params: 7.8468633
                Loss: fixed 154 labels. Loss 0.00833. Accuracy 0.995.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07480734
Train loss (w/o reg) on all data: 0.071057364
Test loss (w/o reg) on all data: 0.057982925
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.7083e-06
Norm of the params: 8.66022
              Random: fixed  16 labels. Loss 0.05798. Accuracy 0.984.
### Flips: 1230, rs: 34, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.06459535e-08
Norm of the params: 6.092815
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6737463e-09
Norm of the params: 6.092818
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07309719
Train loss (w/o reg) on all data: 0.06917243
Test loss (w/o reg) on all data: 0.054112628
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.3675863e-06
Norm of the params: 8.859756
              Random: fixed  23 labels. Loss 0.05411. Accuracy 0.987.
### Flips: 1230, rs: 34, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012396
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9696762e-08
Norm of the params: 6.092808
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601072
Test loss (w/o reg) on all data: 0.002656044
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.103073e-08
Norm of the params: 6.0928354
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07222584
Train loss (w/o reg) on all data: 0.068430856
Test loss (w/o reg) on all data: 0.049501475
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.223698e-06
Norm of the params: 8.712037
              Random: fixed  28 labels. Loss 0.04950. Accuracy 0.989.
### Flips: 1230, rs: 34, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601148
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2072233e-08
Norm of the params: 6.092823
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601236
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1423685e-08
Norm of the params: 6.0928097
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06936943
Train loss (w/o reg) on all data: 0.06544525
Test loss (w/o reg) on all data: 0.04565864
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.490212e-06
Norm of the params: 8.859092
              Random: fixed  37 labels. Loss 0.04566. Accuracy 0.988.
### Flips: 1230, rs: 34, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9974552e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.257798e-09
Norm of the params: 6.0928135
                Loss: fixed 160 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06758998
Train loss (w/o reg) on all data: 0.06363782
Test loss (w/o reg) on all data: 0.043837424
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.7998726e-06
Norm of the params: 8.890625
              Random: fixed  42 labels. Loss 0.04384. Accuracy 0.988.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07526851
Train loss (w/o reg) on all data: 0.07149797
Test loss (w/o reg) on all data: 0.05792128
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 8.949759e-06
Norm of the params: 8.68394
Flipped loss: 0.05792. Accuracy: 0.975
### Flips: 1230, rs: 35, checks: 205
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022068784
Train loss (w/o reg) on all data: 0.017560102
Test loss (w/o reg) on all data: 0.015913388
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.5047406e-07
Norm of the params: 9.49598
     Influence (LOO): fixed 116 labels. Loss 0.01591. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008208477
Train loss (w/o reg) on all data: 0.0045109265
Test loss (w/o reg) on all data: 0.009233651
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6265818e-07
Norm of the params: 8.599477
                Loss: fixed 139 labels. Loss 0.00923. Accuracy 0.998.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074783444
Train loss (w/o reg) on all data: 0.07102589
Test loss (w/o reg) on all data: 0.056069754
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.705526e-05
Norm of the params: 8.668972
              Random: fixed   4 labels. Loss 0.05607. Accuracy 0.977.
### Flips: 1230, rs: 35, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004566919
Train loss (w/o reg) on all data: 0.002076857
Test loss (w/o reg) on all data: 0.0043577785
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1783884e-07
Norm of the params: 7.0569997
     Influence (LOO): fixed 149 labels. Loss 0.00436. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9922364e-08
Norm of the params: 6.092811
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07269198
Train loss (w/o reg) on all data: 0.06883942
Test loss (w/o reg) on all data: 0.053321823
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.4954763e-06
Norm of the params: 8.777878
              Random: fixed  11 labels. Loss 0.05332. Accuracy 0.979.
### Flips: 1230, rs: 35, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.53871e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8170009e-08
Norm of the params: 6.0928197
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070999786
Train loss (w/o reg) on all data: 0.06711895
Test loss (w/o reg) on all data: 0.050165076
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.08639e-06
Norm of the params: 8.810032
              Random: fixed  19 labels. Loss 0.05017. Accuracy 0.983.
### Flips: 1230, rs: 35, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601251
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.38488305e-08
Norm of the params: 6.092806
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.783536e-09
Norm of the params: 6.092815
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06833619
Train loss (w/o reg) on all data: 0.06414138
Test loss (w/o reg) on all data: 0.048257876
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.1251093e-06
Norm of the params: 9.159486
              Random: fixed  27 labels. Loss 0.04826. Accuracy 0.984.
### Flips: 1230, rs: 35, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.307233e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3226589e-08
Norm of the params: 6.092815
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06569443
Train loss (w/o reg) on all data: 0.06144425
Test loss (w/o reg) on all data: 0.045872614
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.7122764e-06
Norm of the params: 9.219737
              Random: fixed  34 labels. Loss 0.04587. Accuracy 0.988.
### Flips: 1230, rs: 35, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9044555e-08
Norm of the params: 6.092806
     Influence (LOO): fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3656407e-08
Norm of the params: 6.092828
                Loss: fixed 150 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063834645
Train loss (w/o reg) on all data: 0.059449956
Test loss (w/o reg) on all data: 0.04430247
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.9530014e-06
Norm of the params: 9.364493
              Random: fixed  38 labels. Loss 0.04430. Accuracy 0.987.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074320294
Train loss (w/o reg) on all data: 0.069713354
Test loss (w/o reg) on all data: 0.05654638
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.4520638e-06
Norm of the params: 9.598898
Flipped loss: 0.05655. Accuracy: 0.979
### Flips: 1230, rs: 36, checks: 205
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026777014
Train loss (w/o reg) on all data: 0.021854999
Test loss (w/o reg) on all data: 0.019690016
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6268026e-06
Norm of the params: 9.921708
     Influence (LOO): fixed 110 labels. Loss 0.01969. Accuracy 0.993.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01382146
Train loss (w/o reg) on all data: 0.009031116
Test loss (w/o reg) on all data: 0.01733146
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7636385e-07
Norm of the params: 9.7881
                Loss: fixed 131 labels. Loss 0.01733. Accuracy 0.994.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07338371
Train loss (w/o reg) on all data: 0.06855479
Test loss (w/o reg) on all data: 0.055354163
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.517388e-06
Norm of the params: 9.827431
              Random: fixed   4 labels. Loss 0.05535. Accuracy 0.979.
### Flips: 1230, rs: 36, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040400336
Train loss (w/o reg) on all data: 0.001842451
Test loss (w/o reg) on all data: 0.0032254753
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.860301e-08
Norm of the params: 6.629603
     Influence (LOO): fixed 153 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042141834
Train loss (w/o reg) on all data: 0.00190096
Test loss (w/o reg) on all data: 0.004869334
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.787218e-08
Norm of the params: 6.8017993
                Loss: fixed 152 labels. Loss 0.00487. Accuracy 0.998.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07124012
Train loss (w/o reg) on all data: 0.06643161
Test loss (w/o reg) on all data: 0.051220555
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.502772e-05
Norm of the params: 9.806641
              Random: fixed  15 labels. Loss 0.05122. Accuracy 0.983.
### Flips: 1230, rs: 36, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2058614e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.124312e-09
Norm of the params: 6.0928197
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06918101
Train loss (w/o reg) on all data: 0.064354606
Test loss (w/o reg) on all data: 0.047722604
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.9257415e-06
Norm of the params: 9.824873
              Random: fixed  24 labels. Loss 0.04772. Accuracy 0.985.
### Flips: 1230, rs: 36, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560551
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7355028e-08
Norm of the params: 6.0928316
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.653117e-09
Norm of the params: 6.0928144
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0664555
Train loss (w/o reg) on all data: 0.061665572
Test loss (w/o reg) on all data: 0.044749744
Train acc on all data:  0.973741794310722
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.3121871e-05
Norm of the params: 9.787671
              Random: fixed  32 labels. Loss 0.04475. Accuracy 0.989.
### Flips: 1230, rs: 36, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6362769e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2530028e-08
Norm of the params: 6.0928154
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064474285
Train loss (w/o reg) on all data: 0.059762448
Test loss (w/o reg) on all data: 0.042413343
Train acc on all data:  0.975686846584002
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.3920373e-06
Norm of the params: 9.707562
              Random: fixed  40 labels. Loss 0.04241. Accuracy 0.990.
### Flips: 1230, rs: 36, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960133
Test loss (w/o reg) on all data: 0.0026560791
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7356709e-08
Norm of the params: 6.0927935
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601111
Test loss (w/o reg) on all data: 0.002656048
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0768745e-08
Norm of the params: 6.0928297
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06254338
Train loss (w/o reg) on all data: 0.05799036
Test loss (w/o reg) on all data: 0.038088124
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.455978e-06
Norm of the params: 9.542551
              Random: fixed  48 labels. Loss 0.03809. Accuracy 0.991.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08030715
Train loss (w/o reg) on all data: 0.0764813
Test loss (w/o reg) on all data: 0.057150666
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 6.816389e-06
Norm of the params: 8.747402
Flipped loss: 0.05715. Accuracy: 0.983
### Flips: 1230, rs: 37, checks: 205
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036565475
Train loss (w/o reg) on all data: 0.03165874
Test loss (w/o reg) on all data: 0.02314757
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0862328e-06
Norm of the params: 9.906296
     Influence (LOO): fixed 107 labels. Loss 0.02315. Accuracy 0.993.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014348218
Train loss (w/o reg) on all data: 0.00969086
Test loss (w/o reg) on all data: 0.015796809
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.941201e-07
Norm of the params: 9.651278
                Loss: fixed 144 labels. Loss 0.01580. Accuracy 0.995.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080151126
Train loss (w/o reg) on all data: 0.076318525
Test loss (w/o reg) on all data: 0.056865968
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.597371e-05
Norm of the params: 8.755118
              Random: fixed   1 labels. Loss 0.05687. Accuracy 0.983.
### Flips: 1230, rs: 37, checks: 410
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077814623
Train loss (w/o reg) on all data: 0.005077724
Test loss (w/o reg) on all data: 0.007703697
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.9652125e-08
Norm of the params: 7.353555
     Influence (LOO): fixed 165 labels. Loss 0.00770. Accuracy 0.998.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004738326
Train loss (w/o reg) on all data: 0.0019250192
Test loss (w/o reg) on all data: 0.0036105323
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.0465775e-08
Norm of the params: 7.501076
                Loss: fixed 166 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07740649
Train loss (w/o reg) on all data: 0.07344235
Test loss (w/o reg) on all data: 0.052408915
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.0183761e-05
Norm of the params: 8.904091
              Random: fixed  15 labels. Loss 0.05241. Accuracy 0.983.
### Flips: 1230, rs: 37, checks: 615
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9743592e-08
Norm of the params: 6.092827
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601261
Test loss (w/o reg) on all data: 0.0026560638
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6942499e-08
Norm of the params: 6.0928054
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07552141
Train loss (w/o reg) on all data: 0.071465194
Test loss (w/o reg) on all data: 0.04934852
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.3572547e-05
Norm of the params: 9.0069065
              Random: fixed  22 labels. Loss 0.04935. Accuracy 0.984.
### Flips: 1230, rs: 37, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960114
Test loss (w/o reg) on all data: 0.0026560414
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1386727e-08
Norm of the params: 6.0928245
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8226219e-08
Norm of the params: 6.0928063
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07413857
Train loss (w/o reg) on all data: 0.070189826
Test loss (w/o reg) on all data: 0.046617646
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.021331e-06
Norm of the params: 8.886775
              Random: fixed  30 labels. Loss 0.04662. Accuracy 0.985.
### Flips: 1230, rs: 37, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.0026560307
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6676584e-08
Norm of the params: 6.09283
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601269
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3451227e-08
Norm of the params: 6.0928035
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07245326
Train loss (w/o reg) on all data: 0.06840279
Test loss (w/o reg) on all data: 0.043835334
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.382213e-06
Norm of the params: 9.00052
              Random: fixed  38 labels. Loss 0.04384. Accuracy 0.989.
### Flips: 1230, rs: 37, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5014276e-08
Norm of the params: 6.0928135
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6656365e-08
Norm of the params: 6.0928254
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069519654
Train loss (w/o reg) on all data: 0.06558182
Test loss (w/o reg) on all data: 0.039909896
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.599971e-06
Norm of the params: 8.874497
              Random: fixed  47 labels. Loss 0.03991. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07665099
Train loss (w/o reg) on all data: 0.07294166
Test loss (w/o reg) on all data: 0.05568446
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.9346306e-06
Norm of the params: 8.613168
Flipped loss: 0.05568. Accuracy: 0.983
### Flips: 1230, rs: 38, checks: 205
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027032923
Train loss (w/o reg) on all data: 0.02257038
Test loss (w/o reg) on all data: 0.016350575
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.2370075e-06
Norm of the params: 9.447269
     Influence (LOO): fixed 114 labels. Loss 0.01635. Accuracy 0.995.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012289573
Train loss (w/o reg) on all data: 0.007664801
Test loss (w/o reg) on all data: 0.0077538495
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.0955754e-07
Norm of the params: 9.6174555
                Loss: fixed 139 labels. Loss 0.00775. Accuracy 0.999.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07546928
Train loss (w/o reg) on all data: 0.07170514
Test loss (w/o reg) on all data: 0.0522198
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.522436e-06
Norm of the params: 8.676567
              Random: fixed   6 labels. Loss 0.05222. Accuracy 0.985.
### Flips: 1230, rs: 38, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006865816
Train loss (w/o reg) on all data: 0.0038464174
Test loss (w/o reg) on all data: 0.0043379525
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.478099e-08
Norm of the params: 7.7709694
     Influence (LOO): fixed 151 labels. Loss 0.00434. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601646
Test loss (w/o reg) on all data: 0.0026561404
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.738063e-08
Norm of the params: 6.092741
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07313155
Train loss (w/o reg) on all data: 0.0693853
Test loss (w/o reg) on all data: 0.050997157
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.2682135e-06
Norm of the params: 8.655924
              Random: fixed  15 labels. Loss 0.05100. Accuracy 0.984.
### Flips: 1230, rs: 38, checks: 615
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096013857
Test loss (w/o reg) on all data: 0.0026560957
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9126498e-08
Norm of the params: 6.092784
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3580816e-08
Norm of the params: 6.09281
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071332164
Train loss (w/o reg) on all data: 0.06750571
Test loss (w/o reg) on all data: 0.047406644
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1972715e-05
Norm of the params: 8.748089
              Random: fixed  23 labels. Loss 0.04741. Accuracy 0.987.
### Flips: 1230, rs: 38, checks: 820
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.997404e-09
Norm of the params: 6.0928154
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.702942e-09
Norm of the params: 6.092818
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07030183
Train loss (w/o reg) on all data: 0.06649059
Test loss (w/o reg) on all data: 0.046380673
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.69475e-06
Norm of the params: 8.730682
              Random: fixed  28 labels. Loss 0.04638. Accuracy 0.987.
### Flips: 1230, rs: 38, checks: 1025
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601238
Test loss (w/o reg) on all data: 0.0026560763
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3102782e-08
Norm of the params: 6.092809
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560677
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4135083e-08
Norm of the params: 6.09282
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068878576
Train loss (w/o reg) on all data: 0.06491175
Test loss (w/o reg) on all data: 0.043691725
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.5158733e-06
Norm of the params: 8.907104
              Random: fixed  35 labels. Loss 0.04369. Accuracy 0.992.
### Flips: 1230, rs: 38, checks: 1230
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4211593e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6552825e-08
Norm of the params: 6.0928235
                Loss: fixed 155 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0673044
Train loss (w/o reg) on all data: 0.06332014
Test loss (w/o reg) on all data: 0.041595843
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.6105205e-06
Norm of the params: 8.926666
              Random: fixed  40 labels. Loss 0.04160. Accuracy 0.991.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0799897
Train loss (w/o reg) on all data: 0.07603038
Test loss (w/o reg) on all data: 0.06921464
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 5.7024195e-06
Norm of the params: 8.898675
Flipped loss: 0.06921. Accuracy: 0.973
### Flips: 1230, rs: 39, checks: 205
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036195476
Train loss (w/o reg) on all data: 0.0312882
Test loss (w/o reg) on all data: 0.030231498
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 5.793235e-06
Norm of the params: 9.906843
     Influence (LOO): fixed 113 labels. Loss 0.03023. Accuracy 0.985.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017701434
Train loss (w/o reg) on all data: 0.011371323
Test loss (w/o reg) on all data: 0.028618593
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.914839e-07
Norm of the params: 11.251765
                Loss: fixed 142 labels. Loss 0.02862. Accuracy 0.991.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078307964
Train loss (w/o reg) on all data: 0.07437876
Test loss (w/o reg) on all data: 0.06475467
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 8.414543e-06
Norm of the params: 8.864769
              Random: fixed  10 labels. Loss 0.06475. Accuracy 0.975.
### Flips: 1230, rs: 39, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0080124345
Train loss (w/o reg) on all data: 0.004825133
Test loss (w/o reg) on all data: 0.0069084666
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 7.681753e-07
Norm of the params: 7.984111
     Influence (LOO): fixed 168 labels. Loss 0.00691. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063981814
Train loss (w/o reg) on all data: 0.0029696513
Test loss (w/o reg) on all data: 0.0065995683
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.13557604e-07
Norm of the params: 8.280736
                Loss: fixed 168 labels. Loss 0.00660. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07592901
Train loss (w/o reg) on all data: 0.07193204
Test loss (w/o reg) on all data: 0.06347165
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 7.4411923e-06
Norm of the params: 8.940883
              Random: fixed  18 labels. Loss 0.06347. Accuracy 0.977.
### Flips: 1230, rs: 39, checks: 615
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3738651e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1497013e-08
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0746285
Train loss (w/o reg) on all data: 0.070705496
Test loss (w/o reg) on all data: 0.06132116
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 3.238947e-06
Norm of the params: 8.857776
              Random: fixed  26 labels. Loss 0.06132. Accuracy 0.976.
### Flips: 1230, rs: 39, checks: 820
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0324611e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601204
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.220485e-09
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07223909
Train loss (w/o reg) on all data: 0.06817174
Test loss (w/o reg) on all data: 0.06087047
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.9183135e-06
Norm of the params: 9.019259
              Random: fixed  32 labels. Loss 0.06087. Accuracy 0.976.
### Flips: 1230, rs: 39, checks: 1025
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7630112e-08
Norm of the params: 6.092803
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601111
Test loss (w/o reg) on all data: 0.0026560372
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3811758e-08
Norm of the params: 6.092829
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06989592
Train loss (w/o reg) on all data: 0.06590059
Test loss (w/o reg) on all data: 0.054868978
Train acc on all data:  0.9727692681740822
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4915514e-06
Norm of the params: 8.939054
              Random: fixed  43 labels. Loss 0.05487. Accuracy 0.980.
### Flips: 1230, rs: 39, checks: 1230
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011156
Test loss (w/o reg) on all data: 0.0026560307
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6524235e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601298
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8710204e-08
Norm of the params: 6.092799
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06887781
Train loss (w/o reg) on all data: 0.06498273
Test loss (w/o reg) on all data: 0.052470367
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.453182e-06
Norm of the params: 8.82619
              Random: fixed  50 labels. Loss 0.05247. Accuracy 0.983.
