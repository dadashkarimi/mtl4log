nohup: ignoring input
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-07 13:20:35.319653: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-07 13:20:41.696144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-02-07 13:20:41.696221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Total number of parameters: 359
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6562918e-08
Norm of the params: 6.092818
Orig loss: 0.00266. Accuracy: 1.000
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14746325
Train loss (w/o reg) on all data: 0.14003877
Test loss (w/o reg) on all data: 0.04943
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2762089e-06
Norm of the params: 12.185622
Flipped loss: 0.04943. Accuracy: 0.997
### Flips: 205, rs: 0, checks: 205
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012215031
Train loss (w/o reg) on all data: 0.009216191
Test loss (w/o reg) on all data: 0.0056127
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.047849e-07
Norm of the params: 7.744469
     Influence (LOO): fixed 162 labels. Loss 0.00561. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601366
Test loss (w/o reg) on all data: 0.002656104
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8070396e-08
Norm of the params: 6.092788
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14394565
Train loss (w/o reg) on all data: 0.13649075
Test loss (w/o reg) on all data: 0.047974426
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.2253835e-06
Norm of the params: 12.21057
              Random: fixed   6 labels. Loss 0.04797. Accuracy 0.998.
### Flips: 205, rs: 0, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504633
Test loss (w/o reg) on all data: 0.0026118031
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.24132e-08
Norm of the params: 5.9419866
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731076e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13681322
Train loss (w/o reg) on all data: 0.12925468
Test loss (w/o reg) on all data: 0.045991246
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9184587e-06
Norm of the params: 12.29515
              Random: fixed  17 labels. Loss 0.04599. Accuracy 0.999.
### Flips: 205, rs: 0, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504508
Test loss (w/o reg) on all data: 0.002611776
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1765437e-08
Norm of the params: 5.942008
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.76235e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13053265
Train loss (w/o reg) on all data: 0.122811876
Test loss (w/o reg) on all data: 0.043263417
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.964521e-06
Norm of the params: 12.426399
              Random: fixed  24 labels. Loss 0.04326. Accuracy 0.998.
### Flips: 205, rs: 0, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504505
Test loss (w/o reg) on all data: 0.0026117684
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4754028e-08
Norm of the params: 5.942009
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.75303e-09
Norm of the params: 6.092823
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12423544
Train loss (w/o reg) on all data: 0.1162994
Test loss (w/o reg) on all data: 0.039731827
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.012726e-06
Norm of the params: 12.598443
              Random: fixed  33 labels. Loss 0.03973. Accuracy 0.999.
### Flips: 205, rs: 0, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504557
Test loss (w/o reg) on all data: 0.002611784
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.36520475e-08
Norm of the params: 5.942
     Influence (LOO): fixed 170 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.76273e-09
Norm of the params: 6.092822
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1198003
Train loss (w/o reg) on all data: 0.11195653
Test loss (w/o reg) on all data: 0.03841893
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8871073e-06
Norm of the params: 12.524995
              Random: fixed  39 labels. Loss 0.03842. Accuracy 1.000.
### Flips: 205, rs: 0, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011336
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8473217e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8927218e-08
Norm of the params: 6.0928073
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114245586
Train loss (w/o reg) on all data: 0.10591586
Test loss (w/o reg) on all data: 0.037875254
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.95876e-06
Norm of the params: 12.90715
              Random: fixed  45 labels. Loss 0.03788. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15214607
Train loss (w/o reg) on all data: 0.1443241
Test loss (w/o reg) on all data: 0.058973573
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.065919e-06
Norm of the params: 12.5075865
Flipped loss: 0.05897. Accuracy: 0.995
### Flips: 205, rs: 1, checks: 205
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027910754
Train loss (w/o reg) on all data: 0.023743287
Test loss (w/o reg) on all data: 0.009311504
Train acc on all data:  0.9934354485776805
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5787713e-07
Norm of the params: 9.129586
     Influence (LOO): fixed 156 labels. Loss 0.00931. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656055
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1455481e-08
Norm of the params: 6.09282
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14677769
Train loss (w/o reg) on all data: 0.13937938
Test loss (w/o reg) on all data: 0.053774808
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.26402e-06
Norm of the params: 12.164136
              Random: fixed  10 labels. Loss 0.05377. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008329673
Train loss (w/o reg) on all data: 0.005911144
Test loss (w/o reg) on all data: 0.0048076273
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.277178e-08
Norm of the params: 6.954896
     Influence (LOO): fixed 179 labels. Loss 0.00481. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560712
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.90094e-08
Norm of the params: 6.092823
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13806294
Train loss (w/o reg) on all data: 0.13103472
Test loss (w/o reg) on all data: 0.049991384
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.7577604e-06
Norm of the params: 11.855993
              Random: fixed  24 labels. Loss 0.04999. Accuracy 0.997.
### Flips: 205, rs: 1, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011366
Test loss (w/o reg) on all data: 0.0026560451
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7418213e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601183
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4440071e-08
Norm of the params: 6.0928183
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1299691
Train loss (w/o reg) on all data: 0.12297804
Test loss (w/o reg) on all data: 0.044716805
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9901445e-05
Norm of the params: 11.824608
              Random: fixed  33 labels. Loss 0.04472. Accuracy 0.997.
### Flips: 205, rs: 1, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6702189e-08
Norm of the params: 6.0928082
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.002656051
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1170272e-08
Norm of the params: 6.0928216
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12288923
Train loss (w/o reg) on all data: 0.11595607
Test loss (w/o reg) on all data: 0.042318493
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8257866e-06
Norm of the params: 11.775536
              Random: fixed  43 labels. Loss 0.04232. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601117
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7162398e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.002656069
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0041727e-08
Norm of the params: 6.0928135
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11641246
Train loss (w/o reg) on all data: 0.109613016
Test loss (w/o reg) on all data: 0.040204614
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.110032e-06
Norm of the params: 11.661425
              Random: fixed  53 labels. Loss 0.04020. Accuracy 0.998.
### Flips: 205, rs: 1, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5200216e-08
Norm of the params: 6.0928173
     Influence (LOO): fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1836029e-08
Norm of the params: 6.0928144
                Loss: fixed 184 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10961706
Train loss (w/o reg) on all data: 0.10268221
Test loss (w/o reg) on all data: 0.03775513
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3572558e-06
Norm of the params: 11.776973
              Random: fixed  61 labels. Loss 0.03776. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14758961
Train loss (w/o reg) on all data: 0.13962196
Test loss (w/o reg) on all data: 0.05584788
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.443878e-05
Norm of the params: 12.623514
Flipped loss: 0.05585. Accuracy: 0.997
### Flips: 205, rs: 2, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021763504
Train loss (w/o reg) on all data: 0.018040495
Test loss (w/o reg) on all data: 0.0083019305
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8211537e-07
Norm of the params: 8.629031
     Influence (LOO): fixed 155 labels. Loss 0.00830. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601114
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2552403e-08
Norm of the params: 6.0928288
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14333582
Train loss (w/o reg) on all data: 0.13537374
Test loss (w/o reg) on all data: 0.05429086
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0916929e-06
Norm of the params: 12.619097
              Random: fixed   7 labels. Loss 0.05429. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008255255
Train loss (w/o reg) on all data: 0.005896547
Test loss (w/o reg) on all data: 0.003465293
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.164515e-07
Norm of the params: 6.8683457
     Influence (LOO): fixed 171 labels. Loss 0.00347. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560843
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6402997e-08
Norm of the params: 6.092812
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13855897
Train loss (w/o reg) on all data: 0.13072585
Test loss (w/o reg) on all data: 0.0524415
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.340019e-06
Norm of the params: 12.516489
              Random: fixed  14 labels. Loss 0.05244. Accuracy 0.997.
### Flips: 205, rs: 2, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955855
Test loss (w/o reg) on all data: 0.002633726
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.094854e-08
Norm of the params: 5.928658
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2106033e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13371286
Train loss (w/o reg) on all data: 0.12592302
Test loss (w/o reg) on all data: 0.05091204
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.931628e-06
Norm of the params: 12.481858
              Random: fixed  21 labels. Loss 0.05091. Accuracy 0.997.
### Flips: 205, rs: 2, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955864
Test loss (w/o reg) on all data: 0.0026337446
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5478593e-08
Norm of the params: 5.928656
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206395e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12626632
Train loss (w/o reg) on all data: 0.11861563
Test loss (w/o reg) on all data: 0.049003042
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.34264865e-05
Norm of the params: 12.369867
              Random: fixed  32 labels. Loss 0.04900. Accuracy 0.998.
### Flips: 205, rs: 2, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253034
Train loss (w/o reg) on all data: 0.002495588
Test loss (w/o reg) on all data: 0.0026337574
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1525635e-08
Norm of the params: 5.928653
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220603e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117915064
Train loss (w/o reg) on all data: 0.110238835
Test loss (w/o reg) on all data: 0.04886133
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5116971e-05
Norm of the params: 12.390504
              Random: fixed  42 labels. Loss 0.04886. Accuracy 0.994.
### Flips: 205, rs: 2, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3557391e-08
Norm of the params: 6.0928187
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601207
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0715513e-08
Norm of the params: 6.0928144
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10396526
Train loss (w/o reg) on all data: 0.09599339
Test loss (w/o reg) on all data: 0.04627632
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5502225e-06
Norm of the params: 12.626848
              Random: fixed  58 labels. Loss 0.04628. Accuracy 0.991.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14673169
Train loss (w/o reg) on all data: 0.13939223
Test loss (w/o reg) on all data: 0.060583636
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.144917e-06
Norm of the params: 12.115662
Flipped loss: 0.06058. Accuracy: 0.992
### Flips: 205, rs: 3, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025572121
Train loss (w/o reg) on all data: 0.021881562
Test loss (w/o reg) on all data: 0.009345742
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5266289e-06
Norm of the params: 8.591344
     Influence (LOO): fixed 149 labels. Loss 0.00935. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601347
Test loss (w/o reg) on all data: 0.002656123
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.323685e-08
Norm of the params: 6.09279
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14357509
Train loss (w/o reg) on all data: 0.13630389
Test loss (w/o reg) on all data: 0.05832157
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.479907e-05
Norm of the params: 12.059191
              Random: fixed   6 labels. Loss 0.05832. Accuracy 0.993.
### Flips: 205, rs: 3, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011170188
Train loss (w/o reg) on all data: 0.008652756
Test loss (w/o reg) on all data: 0.0041082655
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8400772e-08
Norm of the params: 7.0956783
     Influence (LOO): fixed 165 labels. Loss 0.00411. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013065
Test loss (w/o reg) on all data: 0.002656081
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5604008e-08
Norm of the params: 6.0927978
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13720451
Train loss (w/o reg) on all data: 0.12967515
Test loss (w/o reg) on all data: 0.057211593
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.2613713e-06
Norm of the params: 12.271406
              Random: fixed  14 labels. Loss 0.05721. Accuracy 0.991.
### Flips: 205, rs: 3, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047741877
Train loss (w/o reg) on all data: 0.0029861776
Test loss (w/o reg) on all data: 0.0027003486
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0289414e-08
Norm of the params: 5.979984
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0851126e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1285666
Train loss (w/o reg) on all data: 0.12119982
Test loss (w/o reg) on all data: 0.052910924
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.457695e-06
Norm of the params: 12.138184
              Random: fixed  27 labels. Loss 0.05291. Accuracy 0.993.
### Flips: 205, rs: 3, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004774188
Train loss (w/o reg) on all data: 0.0029861876
Test loss (w/o reg) on all data: 0.0027003814
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.444252e-08
Norm of the params: 5.9799676
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.083742e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1209854
Train loss (w/o reg) on all data: 0.11361735
Test loss (w/o reg) on all data: 0.049155857
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.0027495e-06
Norm of the params: 12.139233
              Random: fixed  37 labels. Loss 0.04916. Accuracy 0.992.
### Flips: 205, rs: 3, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047741877
Train loss (w/o reg) on all data: 0.0029861764
Test loss (w/o reg) on all data: 0.0027003256
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.33182e-08
Norm of the params: 5.979985
     Influence (LOO): fixed 170 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0851837e-08
Norm of the params: 6.09282
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11461708
Train loss (w/o reg) on all data: 0.107158825
Test loss (w/o reg) on all data: 0.046072364
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6623574e-06
Norm of the params: 12.213313
              Random: fixed  46 labels. Loss 0.04607. Accuracy 0.994.
### Flips: 205, rs: 3, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011144
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05870575e-08
Norm of the params: 6.092829
     Influence (LOO): fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012105
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.872463e-09
Norm of the params: 6.092814
                Loss: fixed 173 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10629926
Train loss (w/o reg) on all data: 0.09862622
Test loss (w/o reg) on all data: 0.042743556
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7396175e-06
Norm of the params: 12.387928
              Random: fixed  55 labels. Loss 0.04274. Accuracy 0.994.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1527687
Train loss (w/o reg) on all data: 0.14443466
Test loss (w/o reg) on all data: 0.060442235
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5907648e-05
Norm of the params: 12.910489
Flipped loss: 0.06044. Accuracy: 0.995
### Flips: 205, rs: 4, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026102923
Train loss (w/o reg) on all data: 0.022025216
Test loss (w/o reg) on all data: 0.00781811
Train acc on all data:  0.9941648431801605
Test acc on all data:   1.0
Norm of the mean of gradients: 6.00827e-07
Norm of the params: 9.030732
     Influence (LOO): fixed 153 labels. Loss 0.00782. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037321304
Train loss (w/o reg) on all data: 0.001472317
Test loss (w/o reg) on all data: 0.0043935357
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.041491e-08
Norm of the params: 6.722817
                Loss: fixed 176 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14565831
Train loss (w/o reg) on all data: 0.13731441
Test loss (w/o reg) on all data: 0.05695057
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.365775e-05
Norm of the params: 12.918127
              Random: fixed  10 labels. Loss 0.05695. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01216815
Train loss (w/o reg) on all data: 0.00939793
Test loss (w/o reg) on all data: 0.0041964795
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 8.704036e-08
Norm of the params: 7.4434133
     Influence (LOO): fixed 169 labels. Loss 0.00420. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0346076e-08
Norm of the params: 6.0928254
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13808686
Train loss (w/o reg) on all data: 0.12973608
Test loss (w/o reg) on all data: 0.05404239
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1688368e-05
Norm of the params: 12.923446
              Random: fixed  21 labels. Loss 0.05404. Accuracy 0.996.
### Flips: 205, rs: 4, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052877134
Train loss (w/o reg) on all data: 0.0033059153
Test loss (w/o reg) on all data: 0.0031720628
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.770521e-08
Norm of the params: 6.29571
     Influence (LOO): fixed 175 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010126
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0928022e-08
Norm of the params: 6.092846
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13504784
Train loss (w/o reg) on all data: 0.12656166
Test loss (w/o reg) on all data: 0.052925978
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9111193e-06
Norm of the params: 13.027803
              Random: fixed  25 labels. Loss 0.05293. Accuracy 0.996.
### Flips: 205, rs: 4, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504559
Test loss (w/o reg) on all data: 0.0026117736
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7469621e-08
Norm of the params: 5.9419994
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.747265e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827441
Train loss (w/o reg) on all data: 0.11985343
Test loss (w/o reg) on all data: 0.051056806
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.5213125e-06
Norm of the params: 12.9776535
              Random: fixed  35 labels. Loss 0.05106. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504502
Test loss (w/o reg) on all data: 0.0026117524
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.848492e-08
Norm of the params: 5.942009
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.063494e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12234576
Train loss (w/o reg) on all data: 0.11383327
Test loss (w/o reg) on all data: 0.04927255
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.0876636e-06
Norm of the params: 13.047982
              Random: fixed  42 labels. Loss 0.04927. Accuracy 0.995.
### Flips: 205, rs: 4, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504603
Test loss (w/o reg) on all data: 0.0026117875
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.546194e-08
Norm of the params: 5.941992
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8543715e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11623016
Train loss (w/o reg) on all data: 0.10789985
Test loss (w/o reg) on all data: 0.046549577
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.3869954e-06
Norm of the params: 12.9075985
              Random: fixed  51 labels. Loss 0.04655. Accuracy 0.994.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15645537
Train loss (w/o reg) on all data: 0.14860697
Test loss (w/o reg) on all data: 0.053917103
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.316464e-06
Norm of the params: 12.5286875
Flipped loss: 0.05392. Accuracy: 0.999
### Flips: 205, rs: 5, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031957533
Train loss (w/o reg) on all data: 0.026946709
Test loss (w/o reg) on all data: 0.009561849
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 2.998839e-07
Norm of the params: 10.010818
     Influence (LOO): fixed 156 labels. Loss 0.00956. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004204713
Train loss (w/o reg) on all data: 0.0018032985
Test loss (w/o reg) on all data: 0.0026193098
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1118668e-08
Norm of the params: 6.930244
                Loss: fixed 184 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15046507
Train loss (w/o reg) on all data: 0.14279473
Test loss (w/o reg) on all data: 0.052390873
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.12266935e-05
Norm of the params: 12.385756
              Random: fixed  10 labels. Loss 0.05239. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 410
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013089961
Train loss (w/o reg) on all data: 0.009844487
Test loss (w/o reg) on all data: 0.005514727
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3653731e-07
Norm of the params: 8.0566435
     Influence (LOO): fixed 175 labels. Loss 0.00551. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7011557e-08
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14281358
Train loss (w/o reg) on all data: 0.13502558
Test loss (w/o reg) on all data: 0.049484648
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2637541e-05
Norm of the params: 12.4803915
              Random: fixed  22 labels. Loss 0.04948. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158226
Train loss (w/o reg) on all data: 0.0018504526
Test loss (w/o reg) on all data: 0.0026117773
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0336637e-08
Norm of the params: 5.9420033
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.730823e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13648514
Train loss (w/o reg) on all data: 0.1284526
Test loss (w/o reg) on all data: 0.048000865
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2247367e-06
Norm of the params: 12.67481
              Random: fixed  31 labels. Loss 0.04800. Accuracy 0.999.
### Flips: 205, rs: 5, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504582
Test loss (w/o reg) on all data: 0.0026117833
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7926473e-08
Norm of the params: 5.941996
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728658e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1274223
Train loss (w/o reg) on all data: 0.11936968
Test loss (w/o reg) on all data: 0.046057448
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.282631e-06
Norm of the params: 12.690647
              Random: fixed  44 labels. Loss 0.04606. Accuracy 0.998.
### Flips: 205, rs: 5, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850457
Test loss (w/o reg) on all data: 0.0026117756
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3221379e-08
Norm of the params: 5.941997
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728813e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12568733
Train loss (w/o reg) on all data: 0.11751579
Test loss (w/o reg) on all data: 0.04585979
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3054959e-06
Norm of the params: 12.784011
              Random: fixed  46 labels. Loss 0.04586. Accuracy 0.998.
### Flips: 205, rs: 5, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504573
Test loss (w/o reg) on all data: 0.0026117803
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.965797e-08
Norm of the params: 5.941998
     Influence (LOO): fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.729064e-09
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12146976
Train loss (w/o reg) on all data: 0.113652214
Test loss (w/o reg) on all data: 0.04462981
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.9107748e-06
Norm of the params: 12.504032
              Random: fixed  52 labels. Loss 0.04463. Accuracy 0.998.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15137523
Train loss (w/o reg) on all data: 0.14406559
Test loss (w/o reg) on all data: 0.051338986
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8263864e-06
Norm of the params: 12.0910225
Flipped loss: 0.05134. Accuracy: 0.998
### Flips: 205, rs: 6, checks: 205
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031134473
Train loss (w/o reg) on all data: 0.027011639
Test loss (w/o reg) on all data: 0.008974958
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3233275e-07
Norm of the params: 9.080566
     Influence (LOO): fixed 150 labels. Loss 0.00897. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035984488
Train loss (w/o reg) on all data: 0.0013201104
Test loss (w/o reg) on all data: 0.0026171214
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1239665e-08
Norm of the params: 6.750316
                Loss: fixed 175 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14406028
Train loss (w/o reg) on all data: 0.13690503
Test loss (w/o reg) on all data: 0.047983944
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4252889e-05
Norm of the params: 11.962654
              Random: fixed  11 labels. Loss 0.04798. Accuracy 0.997.
### Flips: 205, rs: 6, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072451057
Train loss (w/o reg) on all data: 0.0045594047
Test loss (w/o reg) on all data: 0.0030997014
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.004338e-08
Norm of the params: 7.328985
     Influence (LOO): fixed 172 labels. Loss 0.00310. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601282
Test loss (w/o reg) on all data: 0.0026560775
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8235575e-08
Norm of the params: 6.092801
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13776314
Train loss (w/o reg) on all data: 0.13048138
Test loss (w/o reg) on all data: 0.045633167
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3191477e-06
Norm of the params: 12.067943
              Random: fixed  20 labels. Loss 0.04563. Accuracy 0.996.
### Flips: 205, rs: 6, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6525914e-08
Norm of the params: 6.092824
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7186096e-08
Norm of the params: 6.0928164
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13209423
Train loss (w/o reg) on all data: 0.12503184
Test loss (w/o reg) on all data: 0.043742575
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1471793e-06
Norm of the params: 11.884778
              Random: fixed  29 labels. Loss 0.04374. Accuracy 0.998.
### Flips: 205, rs: 6, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010714
Test loss (w/o reg) on all data: 0.0026560419
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.577511e-08
Norm of the params: 6.0928354
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560498
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6113153e-08
Norm of the params: 6.0928183
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057384
Train loss (w/o reg) on all data: 0.11314392
Test loss (w/o reg) on all data: 0.039867368
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.191733e-06
Norm of the params: 12.190095
              Random: fixed  43 labels. Loss 0.03987. Accuracy 0.999.
### Flips: 205, rs: 6, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.282446e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3689336e-08
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117737435
Train loss (w/o reg) on all data: 0.11043406
Test loss (w/o reg) on all data: 0.038877927
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 8.71363e-06
Norm of the params: 12.085836
              Random: fixed  48 labels. Loss 0.03888. Accuracy 1.000.
### Flips: 205, rs: 6, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7728928e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.054155e-08
Norm of the params: 6.0928197
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11022441
Train loss (w/o reg) on all data: 0.102998056
Test loss (w/o reg) on all data: 0.036190104
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 9.382165e-06
Norm of the params: 12.021943
              Random: fixed  58 labels. Loss 0.03619. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15016375
Train loss (w/o reg) on all data: 0.14224552
Test loss (w/o reg) on all data: 0.05962749
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9316598e-06
Norm of the params: 12.584305
Flipped loss: 0.05963. Accuracy: 0.996
### Flips: 205, rs: 7, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023539642
Train loss (w/o reg) on all data: 0.01933927
Test loss (w/o reg) on all data: 0.007721191
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 5.481149e-07
Norm of the params: 9.165555
     Influence (LOO): fixed 159 labels. Loss 0.00772. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601185
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6309177e-08
Norm of the params: 6.0928164
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1423648
Train loss (w/o reg) on all data: 0.13457808
Test loss (w/o reg) on all data: 0.055801634
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.0029536e-06
Norm of the params: 12.479365
              Random: fixed  13 labels. Loss 0.05580. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010779976
Train loss (w/o reg) on all data: 0.008207416
Test loss (w/o reg) on all data: 0.0047272174
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.08192424e-07
Norm of the params: 7.172949
     Influence (LOO): fixed 173 labels. Loss 0.00473. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011296
Test loss (w/o reg) on all data: 0.0026560435
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4729762e-08
Norm of the params: 6.092827
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1367524
Train loss (w/o reg) on all data: 0.12907888
Test loss (w/o reg) on all data: 0.052163392
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.139683e-06
Norm of the params: 12.388314
              Random: fixed  22 labels. Loss 0.05216. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052877134
Train loss (w/o reg) on all data: 0.0033059306
Test loss (w/o reg) on all data: 0.0031721182
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.701231e-08
Norm of the params: 6.295686
     Influence (LOO): fixed 178 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010114
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1165887e-08
Norm of the params: 6.092846
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13119881
Train loss (w/o reg) on all data: 0.12395802
Test loss (w/o reg) on all data: 0.049260106
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2134077e-05
Norm of the params: 12.033937
              Random: fixed  30 labels. Loss 0.04926. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504579
Test loss (w/o reg) on all data: 0.0026117868
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5632403e-08
Norm of the params: 5.941996
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731034e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1272409
Train loss (w/o reg) on all data: 0.11992321
Test loss (w/o reg) on all data: 0.048865005
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6754446e-06
Norm of the params: 12.097674
              Random: fixed  35 labels. Loss 0.04887. Accuracy 0.997.
### Flips: 205, rs: 7, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504614
Test loss (w/o reg) on all data: 0.0026117987
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.625872e-08
Norm of the params: 5.9419904
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.761564e-09
Norm of the params: 6.0928226
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11807056
Train loss (w/o reg) on all data: 0.111068785
Test loss (w/o reg) on all data: 0.04492802
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8688417e-06
Norm of the params: 11.833654
              Random: fixed  49 labels. Loss 0.04493. Accuracy 0.999.
### Flips: 205, rs: 7, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560675
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5915399e-08
Norm of the params: 6.0928125
     Influence (LOO): fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5217665e-09
Norm of the params: 6.0928164
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11266378
Train loss (w/o reg) on all data: 0.10594401
Test loss (w/o reg) on all data: 0.041050155
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.189788e-06
Norm of the params: 11.592906
              Random: fixed  58 labels. Loss 0.04105. Accuracy 0.998.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15190074
Train loss (w/o reg) on all data: 0.14401592
Test loss (w/o reg) on all data: 0.055768475
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.34834045e-05
Norm of the params: 12.557719
Flipped loss: 0.05577. Accuracy: 0.994
### Flips: 205, rs: 8, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025170423
Train loss (w/o reg) on all data: 0.021468578
Test loss (w/o reg) on all data: 0.007006257
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1514344e-07
Norm of the params: 8.60447
     Influence (LOO): fixed 154 labels. Loss 0.00701. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009613
Test loss (w/o reg) on all data: 0.0026560265
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0787174e-08
Norm of the params: 6.092854
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14482458
Train loss (w/o reg) on all data: 0.13673638
Test loss (w/o reg) on all data: 0.052890673
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.8962655e-06
Norm of the params: 12.718651
              Random: fixed  11 labels. Loss 0.05289. Accuracy 0.995.
### Flips: 205, rs: 8, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011204874
Train loss (w/o reg) on all data: 0.008550508
Test loss (w/o reg) on all data: 0.0040084324
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.511886e-07
Norm of the params: 7.2861047
     Influence (LOO): fixed 169 labels. Loss 0.00401. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3575199e-08
Norm of the params: 6.092823
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14018668
Train loss (w/o reg) on all data: 0.13223648
Test loss (w/o reg) on all data: 0.051358733
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.21216e-06
Norm of the params: 12.609684
              Random: fixed  18 labels. Loss 0.05136. Accuracy 0.994.
### Flips: 205, rs: 8, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955876
Test loss (w/o reg) on all data: 0.0026337167
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8867007e-08
Norm of the params: 5.928655
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2201604e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13116667
Train loss (w/o reg) on all data: 0.12345488
Test loss (w/o reg) on all data: 0.04593494
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4616976e-05
Norm of the params: 12.419161
              Random: fixed  32 labels. Loss 0.04593. Accuracy 0.997.
### Flips: 205, rs: 8, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337288
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9655855e-09
Norm of the params: 5.9286604
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220678e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12608457
Train loss (w/o reg) on all data: 0.118441775
Test loss (w/o reg) on all data: 0.042867105
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4512483e-06
Norm of the params: 12.363488
              Random: fixed  39 labels. Loss 0.04287. Accuracy 0.997.
### Flips: 205, rs: 8, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955845
Test loss (w/o reg) on all data: 0.002633702
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.742841e-08
Norm of the params: 5.9286604
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2201357e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11831042
Train loss (w/o reg) on all data: 0.11089893
Test loss (w/o reg) on all data: 0.039526746
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0673094e-05
Norm of the params: 12.174968
              Random: fixed  50 labels. Loss 0.03953. Accuracy 0.999.
### Flips: 205, rs: 8, checks: 1230
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955843
Test loss (w/o reg) on all data: 0.0026337327
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.054915e-09
Norm of the params: 5.92866
     Influence (LOO): fixed 174 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206782e-08
Norm of the params: 6.0928364
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10580227
Train loss (w/o reg) on all data: 0.098210245
Test loss (w/o reg) on all data: 0.034743622
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.154321e-06
Norm of the params: 12.322356
              Random: fixed  64 labels. Loss 0.03474. Accuracy 0.998.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15368278
Train loss (w/o reg) on all data: 0.14664002
Test loss (w/o reg) on all data: 0.059793707
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.7101175e-06
Norm of the params: 11.868242
Flipped loss: 0.05979. Accuracy: 0.994
### Flips: 205, rs: 9, checks: 205
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027419899
Train loss (w/o reg) on all data: 0.023185026
Test loss (w/o reg) on all data: 0.009511523
Train acc on all data:  0.9941648431801605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.748997e-06
Norm of the params: 9.203122
     Influence (LOO): fixed 157 labels. Loss 0.00951. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3345499e-08
Norm of the params: 6.0928073
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1496464
Train loss (w/o reg) on all data: 0.14263406
Test loss (w/o reg) on all data: 0.056860767
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3801005e-06
Norm of the params: 11.842586
              Random: fixed   7 labels. Loss 0.05686. Accuracy 0.996.
### Flips: 205, rs: 9, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01236745
Train loss (w/o reg) on all data: 0.009524697
Test loss (w/o reg) on all data: 0.0048036193
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.00504e-07
Norm of the params: 7.540229
     Influence (LOO): fixed 173 labels. Loss 0.00480. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8414939e-08
Norm of the params: 6.092809
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14375621
Train loss (w/o reg) on all data: 0.13683271
Test loss (w/o reg) on all data: 0.05482786
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5334024e-05
Norm of the params: 11.767325
              Random: fixed  16 labels. Loss 0.05483. Accuracy 0.998.
### Flips: 205, rs: 9, checks: 615
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075198645
Train loss (w/o reg) on all data: 0.0051086047
Test loss (w/o reg) on all data: 0.0039977697
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6631958e-07
Norm of the params: 6.944436
     Influence (LOO): fixed 177 labels. Loss 0.00400. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7551924e-08
Norm of the params: 6.092808
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13994014
Train loss (w/o reg) on all data: 0.13303263
Test loss (w/o reg) on all data: 0.054212846
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.8131426e-06
Norm of the params: 11.753735
              Random: fixed  21 labels. Loss 0.05421. Accuracy 0.997.
### Flips: 205, rs: 9, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011895
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9564993e-08
Norm of the params: 6.092816
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011884
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0902038e-08
Norm of the params: 6.0928164
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1322828
Train loss (w/o reg) on all data: 0.12537248
Test loss (w/o reg) on all data: 0.05189293
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0515862e-06
Norm of the params: 11.756113
              Random: fixed  32 labels. Loss 0.05189. Accuracy 0.998.
### Flips: 205, rs: 9, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601206
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3011957e-08
Norm of the params: 6.092813
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011953
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0498732e-08
Norm of the params: 6.0928154
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12004843
Train loss (w/o reg) on all data: 0.11360133
Test loss (w/o reg) on all data: 0.04881934
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.594907e-06
Norm of the params: 11.35527
              Random: fixed  49 labels. Loss 0.04882. Accuracy 0.997.
### Flips: 205, rs: 9, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096010097
Test loss (w/o reg) on all data: 0.002656031
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0356432e-08
Norm of the params: 6.092845
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601342
Test loss (w/o reg) on all data: 0.0026560805
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.856148e-08
Norm of the params: 6.092792
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114170305
Train loss (w/o reg) on all data: 0.107728325
Test loss (w/o reg) on all data: 0.045190167
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.8514555e-06
Norm of the params: 11.350756
              Random: fixed  58 labels. Loss 0.04519. Accuracy 0.997.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15537003
Train loss (w/o reg) on all data: 0.14846717
Test loss (w/o reg) on all data: 0.053709332
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.2653744e-06
Norm of the params: 11.749772
Flipped loss: 0.05371. Accuracy: 0.997
### Flips: 205, rs: 10, checks: 205
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020039259
Train loss (w/o reg) on all data: 0.016562084
Test loss (w/o reg) on all data: 0.006494309
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 7.348645e-07
Norm of the params: 8.339273
     Influence (LOO): fixed 163 labels. Loss 0.00649. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601118
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.57337e-08
Norm of the params: 6.092829
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14898719
Train loss (w/o reg) on all data: 0.1423499
Test loss (w/o reg) on all data: 0.051429026
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8103767e-06
Norm of the params: 11.521542
              Random: fixed  11 labels. Loss 0.05143. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004444846
Train loss (w/o reg) on all data: 0.0023350522
Test loss (w/o reg) on all data: 0.0028284572
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0102033e-07
Norm of the params: 6.495835
     Influence (LOO): fixed 177 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096007605
Test loss (w/o reg) on all data: 0.002656007
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.00118e-08
Norm of the params: 6.0928874
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14307219
Train loss (w/o reg) on all data: 0.13644025
Test loss (w/o reg) on all data: 0.04910121
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.69739e-06
Norm of the params: 11.516892
              Random: fixed  20 labels. Loss 0.04910. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601339
Test loss (w/o reg) on all data: 0.0026560933
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1682347e-08
Norm of the params: 6.092793
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2512832e-08
Norm of the params: 6.09281
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14082384
Train loss (w/o reg) on all data: 0.13423471
Test loss (w/o reg) on all data: 0.048013657
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.403219e-06
Norm of the params: 11.479661
              Random: fixed  23 labels. Loss 0.04801. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601331
Test loss (w/o reg) on all data: 0.0026560929
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1393377e-08
Norm of the params: 6.092794
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012297
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2383267e-08
Norm of the params: 6.09281
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13271481
Train loss (w/o reg) on all data: 0.12602916
Test loss (w/o reg) on all data: 0.0456534
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.52501e-06
Norm of the params: 11.563428
              Random: fixed  34 labels. Loss 0.04565. Accuracy 0.998.
### Flips: 205, rs: 10, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.098753e-09
Norm of the params: 6.0928216
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.942134e-09
Norm of the params: 6.0928197
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12521608
Train loss (w/o reg) on all data: 0.118542805
Test loss (w/o reg) on all data: 0.042287946
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.5357934e-06
Norm of the params: 11.552731
              Random: fixed  45 labels. Loss 0.04229. Accuracy 0.999.
### Flips: 205, rs: 10, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.098854e-09
Norm of the params: 6.0928135
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1068353e-08
Norm of the params: 6.0928187
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12149283
Train loss (w/o reg) on all data: 0.114869125
Test loss (w/o reg) on all data: 0.040981498
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.5020336e-06
Norm of the params: 11.509738
              Random: fixed  49 labels. Loss 0.04098. Accuracy 0.998.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14722434
Train loss (w/o reg) on all data: 0.13970323
Test loss (w/o reg) on all data: 0.060312495
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.211264e-06
Norm of the params: 12.264671
Flipped loss: 0.06031. Accuracy: 0.993
### Flips: 205, rs: 11, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02437058
Train loss (w/o reg) on all data: 0.020810982
Test loss (w/o reg) on all data: 0.009848819
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2777393e-07
Norm of the params: 8.437534
     Influence (LOO): fixed 159 labels. Loss 0.00985. Accuracy 0.999.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031825108
Train loss (w/o reg) on all data: 0.0011578884
Test loss (w/o reg) on all data: 0.003246974
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.310772e-08
Norm of the params: 6.363368
                Loss: fixed 180 labels. Loss 0.00325. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14096715
Train loss (w/o reg) on all data: 0.13297358
Test loss (w/o reg) on all data: 0.05782353
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.3894627e-06
Norm of the params: 12.644019
              Random: fixed   9 labels. Loss 0.05782. Accuracy 0.992.
### Flips: 205, rs: 11, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009134753
Train loss (w/o reg) on all data: 0.0068351137
Test loss (w/o reg) on all data: 0.0034082844
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1862254e-08
Norm of the params: 6.7817984
     Influence (LOO): fixed 176 labels. Loss 0.00341. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601259
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.271511e-08
Norm of the params: 6.0928054
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12887998
Train loss (w/o reg) on all data: 0.12104137
Test loss (w/o reg) on all data: 0.05299015
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.623192e-06
Norm of the params: 12.520865
              Random: fixed  25 labels. Loss 0.05299. Accuracy 0.993.
### Flips: 205, rs: 11, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024956064
Test loss (w/o reg) on all data: 0.0026337847
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7576255e-08
Norm of the params: 5.928623
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2207159e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12494074
Train loss (w/o reg) on all data: 0.117078386
Test loss (w/o reg) on all data: 0.051795468
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1931561e-05
Norm of the params: 12.539815
              Random: fixed  31 labels. Loss 0.05180. Accuracy 0.994.
### Flips: 205, rs: 11, checks: 820
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337274
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.943602e-08
Norm of the params: 5.928661
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206416e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118966684
Train loss (w/o reg) on all data: 0.111153394
Test loss (w/o reg) on all data: 0.04831586
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.5408661e-06
Norm of the params: 12.500633
              Random: fixed  40 labels. Loss 0.04832. Accuracy 0.994.
### Flips: 205, rs: 11, checks: 1025
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955852
Test loss (w/o reg) on all data: 0.0026337323
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0936147e-08
Norm of the params: 5.9286585
     Influence (LOO): fixed 179 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220678e-08
Norm of the params: 6.0928364
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11410168
Train loss (w/o reg) on all data: 0.10652218
Test loss (w/o reg) on all data: 0.045787532
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.9980674e-06
Norm of the params: 12.312184
              Random: fixed  48 labels. Loss 0.04579. Accuracy 0.995.
### Flips: 205, rs: 11, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601316
Test loss (w/o reg) on all data: 0.0026561206
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0141744e-08
Norm of the params: 6.092797
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601217
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5608543e-08
Norm of the params: 6.0928116
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10782409
Train loss (w/o reg) on all data: 0.100119635
Test loss (w/o reg) on all data: 0.04399001
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.9323453e-06
Norm of the params: 12.413263
              Random: fixed  57 labels. Loss 0.04399. Accuracy 0.994.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14600107
Train loss (w/o reg) on all data: 0.13802862
Test loss (w/o reg) on all data: 0.056865886
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.3806594e-06
Norm of the params: 12.627308
Flipped loss: 0.05687. Accuracy: 0.994
### Flips: 205, rs: 12, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024331708
Train loss (w/o reg) on all data: 0.020762222
Test loss (w/o reg) on all data: 0.007927941
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2692048e-07
Norm of the params: 8.449243
     Influence (LOO): fixed 156 labels. Loss 0.00793. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009600375
Test loss (w/o reg) on all data: 0.0026559534
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.159342e-08
Norm of the params: 6.0929503
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1380284
Train loss (w/o reg) on all data: 0.13016012
Test loss (w/o reg) on all data: 0.052108757
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.332967e-05
Norm of the params: 12.544541
              Random: fixed  12 labels. Loss 0.05211. Accuracy 0.997.
### Flips: 205, rs: 12, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075422074
Train loss (w/o reg) on all data: 0.005263719
Test loss (w/o reg) on all data: 0.0035499514
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.562765e-08
Norm of the params: 6.7505383
     Influence (LOO): fixed 173 labels. Loss 0.00355. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560505
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1997786e-08
Norm of the params: 6.092831
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13115759
Train loss (w/o reg) on all data: 0.123343796
Test loss (w/o reg) on all data: 0.050956942
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0822065e-06
Norm of the params: 12.501042
              Random: fixed  22 labels. Loss 0.05096. Accuracy 0.995.
### Flips: 205, rs: 12, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601149
Test loss (w/o reg) on all data: 0.0026560624
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.845333e-09
Norm of the params: 6.092823
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601198
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.993965e-09
Norm of the params: 6.0928154
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12722006
Train loss (w/o reg) on all data: 0.11925713
Test loss (w/o reg) on all data: 0.049603254
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.404998e-06
Norm of the params: 12.619777
              Random: fixed  28 labels. Loss 0.04960. Accuracy 0.995.
### Flips: 205, rs: 12, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0276655e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5622895e-08
Norm of the params: 6.0928135
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11915162
Train loss (w/o reg) on all data: 0.111008614
Test loss (w/o reg) on all data: 0.046897274
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.026539e-06
Norm of the params: 12.761667
              Random: fixed  37 labels. Loss 0.04690. Accuracy 0.996.
### Flips: 205, rs: 12, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8422766e-08
Norm of the params: 6.0928254
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601172
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0793106e-08
Norm of the params: 6.0928197
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11443847
Train loss (w/o reg) on all data: 0.10678701
Test loss (w/o reg) on all data: 0.04406971
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.458585e-06
Norm of the params: 12.370492
              Random: fixed  45 labels. Loss 0.04407. Accuracy 0.997.
### Flips: 205, rs: 12, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601189
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3332833e-08
Norm of the params: 6.0928164
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.596531e-09
Norm of the params: 6.0928173
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10610682
Train loss (w/o reg) on all data: 0.09865291
Test loss (w/o reg) on all data: 0.039242588
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.123753e-06
Norm of the params: 12.209758
              Random: fixed  56 labels. Loss 0.03924. Accuracy 0.997.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15917075
Train loss (w/o reg) on all data: 0.15092692
Test loss (w/o reg) on all data: 0.05835071
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.807252e-06
Norm of the params: 12.840426
Flipped loss: 0.05835. Accuracy: 0.997
### Flips: 205, rs: 13, checks: 205
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024086323
Train loss (w/o reg) on all data: 0.020344052
Test loss (w/o reg) on all data: 0.0074513964
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.968774e-07
Norm of the params: 8.651324
     Influence (LOO): fixed 162 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034720325
Train loss (w/o reg) on all data: 0.0013089796
Test loss (w/o reg) on all data: 0.0026516
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.131236e-08
Norm of the params: 6.577314
                Loss: fixed 181 labels. Loss 0.00265. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15527901
Train loss (w/o reg) on all data: 0.146837
Test loss (w/o reg) on all data: 0.057903506
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.3910917e-06
Norm of the params: 12.993853
              Random: fixed   5 labels. Loss 0.05790. Accuracy 0.995.
### Flips: 205, rs: 13, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011810001
Train loss (w/o reg) on all data: 0.009169164
Test loss (w/o reg) on all data: 0.004081058
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1368695e-07
Norm of the params: 7.2675133
     Influence (LOO): fixed 175 labels. Loss 0.00408. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601385
Test loss (w/o reg) on all data: 0.0026561038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.439357e-08
Norm of the params: 6.0927844
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14265278
Train loss (w/o reg) on all data: 0.13369419
Test loss (w/o reg) on all data: 0.05544995
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4836933e-06
Norm of the params: 13.385509
              Random: fixed  21 labels. Loss 0.05545. Accuracy 0.995.
### Flips: 205, rs: 13, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044110185
Train loss (w/o reg) on all data: 0.002656308
Test loss (w/o reg) on all data: 0.0030831222
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1809856e-08
Norm of the params: 5.924037
     Influence (LOO): fixed 180 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.684556e-09
Norm of the params: 6.0928206
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14004824
Train loss (w/o reg) on all data: 0.13114545
Test loss (w/o reg) on all data: 0.053470578
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.367604e-06
Norm of the params: 13.343752
              Random: fixed  25 labels. Loss 0.05347. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 820
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004411018
Train loss (w/o reg) on all data: 0.0026563173
Test loss (w/o reg) on all data: 0.0030831401
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9995932e-08
Norm of the params: 5.92402
     Influence (LOO): fixed 180 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.683457e-09
Norm of the params: 6.0928206
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13247743
Train loss (w/o reg) on all data: 0.1237754
Test loss (w/o reg) on all data: 0.051727798
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.309022e-06
Norm of the params: 13.19244
              Random: fixed  35 labels. Loss 0.05173. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504439
Test loss (w/o reg) on all data: 0.0026117289
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8722512e-08
Norm of the params: 5.9420204
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.878847e-09
Norm of the params: 6.092823
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12565787
Train loss (w/o reg) on all data: 0.11670604
Test loss (w/o reg) on all data: 0.049633615
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.355258e-06
Norm of the params: 13.38045
              Random: fixed  42 labels. Loss 0.04963. Accuracy 0.996.
### Flips: 205, rs: 13, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504409
Test loss (w/o reg) on all data: 0.002611739
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5161303e-08
Norm of the params: 5.9420257
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.748127e-09
Norm of the params: 6.092822
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117641255
Train loss (w/o reg) on all data: 0.10889488
Test loss (w/o reg) on all data: 0.04387686
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5740514e-05
Norm of the params: 13.22602
              Random: fixed  55 labels. Loss 0.04388. Accuracy 0.997.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1491856
Train loss (w/o reg) on all data: 0.14189848
Test loss (w/o reg) on all data: 0.05630681
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.4673806e-06
Norm of the params: 12.072372
Flipped loss: 0.05631. Accuracy: 0.997
### Flips: 205, rs: 14, checks: 205
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025843825
Train loss (w/o reg) on all data: 0.021613788
Test loss (w/o reg) on all data: 0.0076286662
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3473125e-07
Norm of the params: 9.197866
     Influence (LOO): fixed 153 labels. Loss 0.00763. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3971882e-08
Norm of the params: 6.0928154
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14161445
Train loss (w/o reg) on all data: 0.13429968
Test loss (w/o reg) on all data: 0.054148473
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.3725695e-06
Norm of the params: 12.0952635
              Random: fixed  11 labels. Loss 0.05415. Accuracy 0.997.
### Flips: 205, rs: 14, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007972587
Train loss (w/o reg) on all data: 0.005465749
Test loss (w/o reg) on all data: 0.0034979412
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.438387e-08
Norm of the params: 7.0807304
     Influence (LOO): fixed 171 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1442318e-08
Norm of the params: 6.092814
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13505554
Train loss (w/o reg) on all data: 0.12779638
Test loss (w/o reg) on all data: 0.049734958
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.834761e-06
Norm of the params: 12.049206
              Random: fixed  22 labels. Loss 0.04973. Accuracy 0.996.
### Flips: 205, rs: 14, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.632353e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011924
Test loss (w/o reg) on all data: 0.002656057
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2325296e-09
Norm of the params: 6.092816
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13206862
Train loss (w/o reg) on all data: 0.12490506
Test loss (w/o reg) on all data: 0.04730367
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9558298e-06
Norm of the params: 11.969593
              Random: fixed  27 labels. Loss 0.04730. Accuracy 0.997.
### Flips: 205, rs: 14, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.540817e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.704617e-09
Norm of the params: 6.0928164
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12618925
Train loss (w/o reg) on all data: 0.11921868
Test loss (w/o reg) on all data: 0.04579937
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.0371093e-06
Norm of the params: 11.807266
              Random: fixed  37 labels. Loss 0.04580. Accuracy 0.995.
### Flips: 205, rs: 14, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3562348e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2952092e-08
Norm of the params: 6.092823
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11661656
Train loss (w/o reg) on all data: 0.1093715
Test loss (w/o reg) on all data: 0.043280184
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.676076e-06
Norm of the params: 12.037492
              Random: fixed  47 labels. Loss 0.04328. Accuracy 0.996.
### Flips: 205, rs: 14, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601187
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8462718e-08
Norm of the params: 6.092817
     Influence (LOO): fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.796023e-08
Norm of the params: 6.0928164
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11175411
Train loss (w/o reg) on all data: 0.10439996
Test loss (w/o reg) on all data: 0.04185074
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6606167e-06
Norm of the params: 12.127786
              Random: fixed  53 labels. Loss 0.04185. Accuracy 0.997.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1519027
Train loss (w/o reg) on all data: 0.1435581
Test loss (w/o reg) on all data: 0.060315073
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9572419e-05
Norm of the params: 12.918676
Flipped loss: 0.06032. Accuracy: 0.997
### Flips: 205, rs: 15, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01978244
Train loss (w/o reg) on all data: 0.015877172
Test loss (w/o reg) on all data: 0.007533836
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0411002e-07
Norm of the params: 8.837725
     Influence (LOO): fixed 162 labels. Loss 0.00753. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560621
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.176813e-08
Norm of the params: 6.0928206
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14775518
Train loss (w/o reg) on all data: 0.13943878
Test loss (w/o reg) on all data: 0.05812758
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.2572494e-06
Norm of the params: 12.896816
              Random: fixed   7 labels. Loss 0.05813. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 410
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044808155
Train loss (w/o reg) on all data: 0.0023802605
Test loss (w/o reg) on all data: 0.0032515945
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.124035e-08
Norm of the params: 6.4815974
     Influence (LOO): fixed 177 labels. Loss 0.00325. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011674
Test loss (w/o reg) on all data: 0.0026560554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4448122e-08
Norm of the params: 6.09282
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14161864
Train loss (w/o reg) on all data: 0.13319428
Test loss (w/o reg) on all data: 0.056494296
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.560936e-06
Norm of the params: 12.98026
              Random: fixed  15 labels. Loss 0.05649. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601132
Test loss (w/o reg) on all data: 0.0026560477
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5402471e-08
Norm of the params: 6.092826
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601312
Test loss (w/o reg) on all data: 0.0026560815
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.793714e-08
Norm of the params: 6.092797
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1387628
Train loss (w/o reg) on all data: 0.13066918
Test loss (w/o reg) on all data: 0.05575968
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0318819e-05
Norm of the params: 12.722918
              Random: fixed  20 labels. Loss 0.05576. Accuracy 0.996.
### Flips: 205, rs: 15, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011645
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5276376e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601219
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0380908e-08
Norm of the params: 6.092812
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1335524
Train loss (w/o reg) on all data: 0.12542297
Test loss (w/o reg) on all data: 0.054443516
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.381982e-06
Norm of the params: 12.751031
              Random: fixed  28 labels. Loss 0.05444. Accuracy 0.996.
### Flips: 205, rs: 15, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012437
Test loss (w/o reg) on all data: 0.0026560698
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8999367e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5777406e-08
Norm of the params: 6.092827
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123914376
Train loss (w/o reg) on all data: 0.115511775
Test loss (w/o reg) on all data: 0.048473377
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0667861e-06
Norm of the params: 12.963489
              Random: fixed  39 labels. Loss 0.04847. Accuracy 0.997.
### Flips: 205, rs: 15, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.0026560468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2011568e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601145
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5203314e-08
Norm of the params: 6.0928235
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119277515
Train loss (w/o reg) on all data: 0.11094209
Test loss (w/o reg) on all data: 0.04688304
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6078586e-06
Norm of the params: 12.911565
              Random: fixed  45 labels. Loss 0.04688. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15457976
Train loss (w/o reg) on all data: 0.14759602
Test loss (w/o reg) on all data: 0.05155092
Train acc on all data:  0.9567225869195235
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.010005e-05
Norm of the params: 11.818406
Flipped loss: 0.05155. Accuracy: 0.998
### Flips: 205, rs: 16, checks: 205
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022360174
Train loss (w/o reg) on all data: 0.018753361
Test loss (w/o reg) on all data: 0.006905674
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.926149e-07
Norm of the params: 8.493306
     Influence (LOO): fixed 163 labels. Loss 0.00691. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601097
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5620297e-08
Norm of the params: 6.0928316
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14772604
Train loss (w/o reg) on all data: 0.14062703
Test loss (w/o reg) on all data: 0.04884862
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.378445e-06
Norm of the params: 11.91555
              Random: fixed   9 labels. Loss 0.04885. Accuracy 0.998.
### Flips: 205, rs: 16, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009440983
Train loss (w/o reg) on all data: 0.0069445204
Test loss (w/o reg) on all data: 0.003374648
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 9.4254744e-08
Norm of the params: 7.0660625
     Influence (LOO): fixed 175 labels. Loss 0.00337. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7662058e-08
Norm of the params: 6.0928173
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14206852
Train loss (w/o reg) on all data: 0.13488036
Test loss (w/o reg) on all data: 0.04568456
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1030344e-06
Norm of the params: 11.990121
              Random: fixed  18 labels. Loss 0.04568. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005343829
Train loss (w/o reg) on all data: 0.0033799442
Test loss (w/o reg) on all data: 0.0027326322
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9451867e-08
Norm of the params: 6.2671924
     Influence (LOO): fixed 178 labels. Loss 0.00273. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3384983e-08
Norm of the params: 6.09282
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13601278
Train loss (w/o reg) on all data: 0.12858194
Test loss (w/o reg) on all data: 0.043699507
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1702295e-06
Norm of the params: 12.190843
              Random: fixed  25 labels. Loss 0.04370. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.001850456
Test loss (w/o reg) on all data: 0.0026117815
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.949185e-08
Norm of the params: 5.9419994
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.728317e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12930982
Train loss (w/o reg) on all data: 0.12164675
Test loss (w/o reg) on all data: 0.04113787
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9927166e-06
Norm of the params: 12.379881
              Random: fixed  33 labels. Loss 0.04114. Accuracy 0.999.
### Flips: 205, rs: 16, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504529
Test loss (w/o reg) on all data: 0.0026117729
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5420595e-08
Norm of the params: 5.942004
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.759053e-09
Norm of the params: 6.0928226
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120397106
Train loss (w/o reg) on all data: 0.11276833
Test loss (w/o reg) on all data: 0.03800741
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7659414e-06
Norm of the params: 12.352146
              Random: fixed  44 labels. Loss 0.03801. Accuracy 1.000.
### Flips: 205, rs: 16, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504587
Test loss (w/o reg) on all data: 0.0026117875
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9627092e-08
Norm of the params: 5.941995
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7315195e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110241205
Train loss (w/o reg) on all data: 0.10261454
Test loss (w/o reg) on all data: 0.035128932
Train acc on all data:  0.9708242159008024
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1670683e-06
Norm of the params: 12.350443
              Random: fixed  58 labels. Loss 0.03513. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15350856
Train loss (w/o reg) on all data: 0.1464743
Test loss (w/o reg) on all data: 0.058764905
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.6474446e-06
Norm of the params: 11.8610735
Flipped loss: 0.05876. Accuracy: 0.993
### Flips: 205, rs: 17, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024084397
Train loss (w/o reg) on all data: 0.020789312
Test loss (w/o reg) on all data: 0.0078227585
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.08591e-07
Norm of the params: 8.117985
     Influence (LOO): fixed 160 labels. Loss 0.00782. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601153
Test loss (w/o reg) on all data: 0.0026560694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.242222e-08
Norm of the params: 6.0928216
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14685203
Train loss (w/o reg) on all data: 0.13973992
Test loss (w/o reg) on all data: 0.055284563
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8026864e-06
Norm of the params: 11.9265375
              Random: fixed   7 labels. Loss 0.05528. Accuracy 0.995.
### Flips: 205, rs: 17, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010026994
Train loss (w/o reg) on all data: 0.0076727653
Test loss (w/o reg) on all data: 0.004125128
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4389918e-07
Norm of the params: 6.86182
     Influence (LOO): fixed 175 labels. Loss 0.00413. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601092
Test loss (w/o reg) on all data: 0.0026560402
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9777484e-08
Norm of the params: 6.092833
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13897248
Train loss (w/o reg) on all data: 0.13157168
Test loss (w/o reg) on all data: 0.05158632
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8236109e-06
Norm of the params: 12.166176
              Random: fixed  18 labels. Loss 0.05159. Accuracy 0.993.
### Flips: 205, rs: 17, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061893994
Train loss (w/o reg) on all data: 0.004204819
Test loss (w/o reg) on all data: 0.0038925172
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 3.693286e-08
Norm of the params: 6.3001275
     Influence (LOO): fixed 178 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2742347e-08
Norm of the params: 6.0928173
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13163696
Train loss (w/o reg) on all data: 0.12426247
Test loss (w/o reg) on all data: 0.049715716
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.9035483e-06
Norm of the params: 12.144541
              Random: fixed  29 labels. Loss 0.04972. Accuracy 0.993.
### Flips: 205, rs: 17, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004397688
Train loss (w/o reg) on all data: 0.0025526027
Test loss (w/o reg) on all data: 0.0035622478
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.868599e-09
Norm of the params: 6.074677
     Influence (LOO): fixed 179 labels. Loss 0.00356. Accuracy 0.999.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601241
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1447471e-08
Norm of the params: 6.0928082
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12805043
Train loss (w/o reg) on all data: 0.12061098
Test loss (w/o reg) on all data: 0.04831364
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.337688e-06
Norm of the params: 12.197911
              Random: fixed  34 labels. Loss 0.04831. Accuracy 0.994.
### Flips: 205, rs: 17, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096013705
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9293073e-08
Norm of the params: 6.0927877
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601186
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.070051e-08
Norm of the params: 6.0928173
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124070965
Train loss (w/o reg) on all data: 0.11673909
Test loss (w/o reg) on all data: 0.04645923
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.651792e-06
Norm of the params: 12.109399
              Random: fixed  40 labels. Loss 0.04646. Accuracy 0.996.
### Flips: 205, rs: 17, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601051
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1677824e-08
Norm of the params: 6.0928397
     Influence (LOO): fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601133
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.103543e-08
Norm of the params: 6.0928264
                Loss: fixed 181 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118922755
Train loss (w/o reg) on all data: 0.11161943
Test loss (w/o reg) on all data: 0.044428345
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.9139286e-06
Norm of the params: 12.0858
              Random: fixed  47 labels. Loss 0.04443. Accuracy 0.995.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1538879
Train loss (w/o reg) on all data: 0.146827
Test loss (w/o reg) on all data: 0.051355463
Train acc on all data:  0.9581813761244834
Test acc on all data:   1.0
Norm of the mean of gradients: 5.189194e-06
Norm of the params: 11.883518
Flipped loss: 0.05136. Accuracy: 1.000
### Flips: 205, rs: 18, checks: 205
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024114968
Train loss (w/o reg) on all data: 0.020244388
Test loss (w/o reg) on all data: 0.007813525
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.2884025e-07
Norm of the params: 8.798387
     Influence (LOO): fixed 157 labels. Loss 0.00781. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7351394e-08
Norm of the params: 6.09282
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14567252
Train loss (w/o reg) on all data: 0.13859652
Test loss (w/o reg) on all data: 0.0478819
Train acc on all data:  0.9608558230002431
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6328377e-06
Norm of the params: 11.896219
              Random: fixed  13 labels. Loss 0.04788. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 410
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077338563
Train loss (w/o reg) on all data: 0.005121056
Test loss (w/o reg) on all data: 0.0038490607
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5207006e-07
Norm of the params: 7.228832
     Influence (LOO): fixed 171 labels. Loss 0.00385. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601175
Test loss (w/o reg) on all data: 0.0026560696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.778143e-08
Norm of the params: 6.0928197
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14080445
Train loss (w/o reg) on all data: 0.13373698
Test loss (w/o reg) on all data: 0.0462636
Train acc on all data:  0.963044006807683
Test acc on all data:   1.0
Norm of the mean of gradients: 2.06149e-06
Norm of the params: 11.889045
              Random: fixed  20 labels. Loss 0.04626. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 615
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960112
Test loss (w/o reg) on all data: 0.002656067
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4257764e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7127785e-08
Norm of the params: 6.0928187
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13211213
Train loss (w/o reg) on all data: 0.124742106
Test loss (w/o reg) on all data: 0.043496743
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 8.61242e-06
Norm of the params: 12.140866
              Random: fixed  30 labels. Loss 0.04350. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601053
Test loss (w/o reg) on all data: 0.0026560314
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7223174e-08
Norm of the params: 6.0928392
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560645
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2990206e-08
Norm of the params: 6.0928135
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12793559
Train loss (w/o reg) on all data: 0.120807976
Test loss (w/o reg) on all data: 0.0420324
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5027188e-06
Norm of the params: 11.939524
              Random: fixed  37 labels. Loss 0.04203. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011313
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5084886e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5925325e-08
Norm of the params: 6.0928183
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12239237
Train loss (w/o reg) on all data: 0.11523071
Test loss (w/o reg) on all data: 0.040481117
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6194302e-06
Norm of the params: 11.968008
              Random: fixed  44 labels. Loss 0.04048. Accuracy 1.000.
### Flips: 205, rs: 18, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601246
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3077852e-08
Norm of the params: 6.0928073
     Influence (LOO): fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560666
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0470602e-08
Norm of the params: 6.0928173
                Loss: fixed 175 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11665807
Train loss (w/o reg) on all data: 0.10945505
Test loss (w/o reg) on all data: 0.03808704
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.4323086e-06
Norm of the params: 12.002518
              Random: fixed  52 labels. Loss 0.03809. Accuracy 0.998.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14850424
Train loss (w/o reg) on all data: 0.14103451
Test loss (w/o reg) on all data: 0.05388513
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6955247e-06
Norm of the params: 12.222709
Flipped loss: 0.05389. Accuracy: 0.998
### Flips: 205, rs: 19, checks: 205
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017814392
Train loss (w/o reg) on all data: 0.014555738
Test loss (w/o reg) on all data: 0.0065638553
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7830885e-07
Norm of the params: 8.072985
     Influence (LOO): fixed 158 labels. Loss 0.00656. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033283643
Train loss (w/o reg) on all data: 0.0012077944
Test loss (w/o reg) on all data: 0.002752565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9595896e-08
Norm of the params: 6.5124035
                Loss: fixed 170 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14598416
Train loss (w/o reg) on all data: 0.13866454
Test loss (w/o reg) on all data: 0.052571524
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7053056e-06
Norm of the params: 12.099269
              Random: fixed   4 labels. Loss 0.05257. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 410
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011825
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5324364e-08
Norm of the params: 6.092818
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.897586e-09
Norm of the params: 6.0928187
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14014596
Train loss (w/o reg) on all data: 0.13259806
Test loss (w/o reg) on all data: 0.05051935
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.1253182e-06
Norm of the params: 12.286499
              Random: fixed  12 labels. Loss 0.05052. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601178
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.584796e-08
Norm of the params: 6.0928183
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.002656056
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0589744e-08
Norm of the params: 6.0928164
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1331778
Train loss (w/o reg) on all data: 0.12565489
Test loss (w/o reg) on all data: 0.0473626
Train acc on all data:  0.9647459275468028
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0884344e-06
Norm of the params: 12.266143
              Random: fixed  22 labels. Loss 0.04736. Accuracy 1.000.
### Flips: 205, rs: 19, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0990945e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3383194e-08
Norm of the params: 6.092814
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12564749
Train loss (w/o reg) on all data: 0.11799055
Test loss (w/o reg) on all data: 0.044407878
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.268679e-06
Norm of the params: 12.374925
              Random: fixed  32 labels. Loss 0.04441. Accuracy 0.999.
### Flips: 205, rs: 19, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9186758e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2791973e-08
Norm of the params: 6.0928144
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11683214
Train loss (w/o reg) on all data: 0.10929491
Test loss (w/o reg) on all data: 0.040663514
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 6.500554e-06
Norm of the params: 12.2778015
              Random: fixed  44 labels. Loss 0.04066. Accuracy 1.000.
### Flips: 205, rs: 19, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011936
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2668292e-08
Norm of the params: 6.0928154
     Influence (LOO): fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011866
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.48866715e-08
Norm of the params: 6.0928173
                Loss: fixed 171 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10905245
Train loss (w/o reg) on all data: 0.101687245
Test loss (w/o reg) on all data: 0.036972947
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0735138e-06
Norm of the params: 12.136886
              Random: fixed  56 labels. Loss 0.03697. Accuracy 0.999.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674938
Train loss (w/o reg) on all data: 0.13941208
Test loss (w/o reg) on all data: 0.05953477
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.513124e-06
Norm of the params: 12.113879
Flipped loss: 0.05953. Accuracy: 0.991
### Flips: 205, rs: 20, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019693159
Train loss (w/o reg) on all data: 0.01590908
Test loss (w/o reg) on all data: 0.0064801835
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6907003e-07
Norm of the params: 8.699516
     Influence (LOO): fixed 156 labels. Loss 0.00648. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032601203
Train loss (w/o reg) on all data: 0.0011785391
Test loss (w/o reg) on all data: 0.0035768996
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.831603e-08
Norm of the params: 6.4522576
                Loss: fixed 171 labels. Loss 0.00358. Accuracy 0.999.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14177418
Train loss (w/o reg) on all data: 0.13422298
Test loss (w/o reg) on all data: 0.057748925
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.5391993e-06
Norm of the params: 12.289175
              Random: fixed   7 labels. Loss 0.05775. Accuracy 0.991.
### Flips: 205, rs: 20, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005544957
Train loss (w/o reg) on all data: 0.0031628942
Test loss (w/o reg) on all data: 0.0036075618
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.711655e-08
Norm of the params: 6.9022646
     Influence (LOO): fixed 170 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560628
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0922905e-08
Norm of the params: 6.092822
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13313074
Train loss (w/o reg) on all data: 0.12567414
Test loss (w/o reg) on all data: 0.05443859
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.044582e-06
Norm of the params: 12.211957
              Random: fixed  19 labels. Loss 0.05444. Accuracy 0.992.
### Flips: 205, rs: 20, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601192
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.04709e-09
Norm of the params: 6.0928164
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17505365e-08
Norm of the params: 6.092818
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12409037
Train loss (w/o reg) on all data: 0.1165605
Test loss (w/o reg) on all data: 0.04984649
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.7867363e-06
Norm of the params: 12.271819
              Random: fixed  33 labels. Loss 0.04985. Accuracy 0.993.
### Flips: 205, rs: 20, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4569557e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601201
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1261345e-08
Norm of the params: 6.0928144
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12013184
Train loss (w/o reg) on all data: 0.11243568
Test loss (w/o reg) on all data: 0.047257084
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6320724e-05
Norm of the params: 12.406582
              Random: fixed  39 labels. Loss 0.04726. Accuracy 0.995.
### Flips: 205, rs: 20, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011173
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3321073e-08
Norm of the params: 6.0928283
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601229
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4818721e-08
Norm of the params: 6.09281
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11342108
Train loss (w/o reg) on all data: 0.10570815
Test loss (w/o reg) on all data: 0.044515193
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9025184e-06
Norm of the params: 12.420088
              Random: fixed  48 labels. Loss 0.04452. Accuracy 0.996.
### Flips: 205, rs: 20, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601161
Test loss (w/o reg) on all data: 0.0026560472
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9288548e-08
Norm of the params: 6.092821
     Influence (LOO): fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7897562e-08
Norm of the params: 6.092803
                Loss: fixed 172 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10837091
Train loss (w/o reg) on all data: 0.10061339
Test loss (w/o reg) on all data: 0.042197425
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4526598e-05
Norm of the params: 12.455935
              Random: fixed  54 labels. Loss 0.04220. Accuracy 0.996.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1524428
Train loss (w/o reg) on all data: 0.14548843
Test loss (w/o reg) on all data: 0.055862892
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.4019283e-06
Norm of the params: 11.793533
Flipped loss: 0.05586. Accuracy: 0.996
### Flips: 205, rs: 21, checks: 205
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023490254
Train loss (w/o reg) on all data: 0.019839881
Test loss (w/o reg) on all data: 0.0078148525
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3009766e-07
Norm of the params: 8.54444
     Influence (LOO): fixed 158 labels. Loss 0.00781. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096012675
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3168264e-08
Norm of the params: 6.0928035
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14612143
Train loss (w/o reg) on all data: 0.1390317
Test loss (w/o reg) on all data: 0.052810404
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.8252347e-06
Norm of the params: 11.907755
              Random: fixed   8 labels. Loss 0.05281. Accuracy 0.996.
### Flips: 205, rs: 21, checks: 410
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007722269
Train loss (w/o reg) on all data: 0.0053861677
Test loss (w/o reg) on all data: 0.0033958124
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5971805e-08
Norm of the params: 6.835351
     Influence (LOO): fixed 173 labels. Loss 0.00340. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.0026560577
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9222263e-08
Norm of the params: 6.0928335
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13928033
Train loss (w/o reg) on all data: 0.13184024
Test loss (w/o reg) on all data: 0.04977057
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.540087e-06
Norm of the params: 12.198429
              Random: fixed  17 labels. Loss 0.04977. Accuracy 0.999.
### Flips: 205, rs: 21, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005203963
Train loss (w/o reg) on all data: 0.0031816773
Test loss (w/o reg) on all data: 0.003155274
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.494782e-08
Norm of the params: 6.359694
     Influence (LOO): fixed 175 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560647
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.626557e-08
Norm of the params: 6.092824
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13131359
Train loss (w/o reg) on all data: 0.123763196
Test loss (w/o reg) on all data: 0.046663627
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.082331e-05
Norm of the params: 12.28853
              Random: fixed  29 labels. Loss 0.04666. Accuracy 0.999.
### Flips: 205, rs: 21, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504589
Test loss (w/o reg) on all data: 0.0026117754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4751369e-08
Norm of the params: 5.941995
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.72891e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827243
Train loss (w/o reg) on all data: 0.120875485
Test loss (w/o reg) on all data: 0.045686223
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.324908e-06
Norm of the params: 12.163008
              Random: fixed  33 labels. Loss 0.04569. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158243
Train loss (w/o reg) on all data: 0.0018504628
Test loss (w/o reg) on all data: 0.002611797
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.510842e-08
Norm of the params: 5.941989
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.03876e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12168763
Train loss (w/o reg) on all data: 0.11425366
Test loss (w/o reg) on all data: 0.042780984
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.08405275e-05
Norm of the params: 12.193416
              Random: fixed  41 labels. Loss 0.04278. Accuracy 0.998.
### Flips: 205, rs: 21, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850459
Test loss (w/o reg) on all data: 0.0026117868
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6962058e-08
Norm of the params: 5.9419937
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.867316e-09
Norm of the params: 6.092823
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11437007
Train loss (w/o reg) on all data: 0.10699672
Test loss (w/o reg) on all data: 0.040296935
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.050395e-05
Norm of the params: 12.143597
              Random: fixed  52 labels. Loss 0.04030. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15215708
Train loss (w/o reg) on all data: 0.14384404
Test loss (w/o reg) on all data: 0.05285658
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.8808832e-06
Norm of the params: 12.894215
Flipped loss: 0.05286. Accuracy: 0.999
### Flips: 205, rs: 22, checks: 205
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022134848
Train loss (w/o reg) on all data: 0.018623319
Test loss (w/o reg) on all data: 0.007503975
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 5.671178e-07
Norm of the params: 8.38037
     Influence (LOO): fixed 160 labels. Loss 0.00750. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601081
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7303657e-08
Norm of the params: 6.092834
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14607418
Train loss (w/o reg) on all data: 0.13782579
Test loss (w/o reg) on all data: 0.05047724
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.0680967e-06
Norm of the params: 12.843972
              Random: fixed  10 labels. Loss 0.05048. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008224226
Train loss (w/o reg) on all data: 0.0054489607
Test loss (w/o reg) on all data: 0.0040640174
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 6.636406e-08
Norm of the params: 7.4501877
     Influence (LOO): fixed 172 labels. Loss 0.00406. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5046318e-08
Norm of the params: 6.0928173
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14002043
Train loss (w/o reg) on all data: 0.13164835
Test loss (w/o reg) on all data: 0.04766421
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.683769e-06
Norm of the params: 12.939928
              Random: fixed  18 labels. Loss 0.04766. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044750604
Train loss (w/o reg) on all data: 0.0023959503
Test loss (w/o reg) on all data: 0.003228123
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8390233e-08
Norm of the params: 6.4484262
     Influence (LOO): fixed 176 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1842617e-08
Norm of the params: 6.0928183
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13174908
Train loss (w/o reg) on all data: 0.12354091
Test loss (w/o reg) on all data: 0.045059264
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8402094e-05
Norm of the params: 12.812632
              Random: fixed  30 labels. Loss 0.04506. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601222
Test loss (w/o reg) on all data: 0.002656072
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5931592e-08
Norm of the params: 6.092812
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3282896e-08
Norm of the params: 6.0928154
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124434136
Train loss (w/o reg) on all data: 0.11613761
Test loss (w/o reg) on all data: 0.042394865
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.983371e-06
Norm of the params: 12.881404
              Random: fixed  39 labels. Loss 0.04239. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601432
Test loss (w/o reg) on all data: 0.0026561
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9631754e-08
Norm of the params: 6.0927763
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601283
Test loss (w/o reg) on all data: 0.0026560416
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.832678e-08
Norm of the params: 6.092801
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12337101
Train loss (w/o reg) on all data: 0.115055665
Test loss (w/o reg) on all data: 0.042083763
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8862352e-06
Norm of the params: 12.896003
              Random: fixed  41 labels. Loss 0.04208. Accuracy 0.999.
### Flips: 205, rs: 22, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096013176
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3374115e-08
Norm of the params: 6.0927954
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012396
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9224592e-08
Norm of the params: 6.0928082
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11844201
Train loss (w/o reg) on all data: 0.1103691
Test loss (w/o reg) on all data: 0.039559923
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3135006e-06
Norm of the params: 12.706618
              Random: fixed  49 labels. Loss 0.03956. Accuracy 0.999.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1563925
Train loss (w/o reg) on all data: 0.14929827
Test loss (w/o reg) on all data: 0.054498468
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.147867e-06
Norm of the params: 11.911533
Flipped loss: 0.05450. Accuracy: 0.997
### Flips: 205, rs: 23, checks: 205
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023606693
Train loss (w/o reg) on all data: 0.019577892
Test loss (w/o reg) on all data: 0.0073490622
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3012096e-06
Norm of the params: 8.976414
     Influence (LOO): fixed 158 labels. Loss 0.00735. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960096
Test loss (w/o reg) on all data: 0.0026560093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.126295e-08
Norm of the params: 6.0928535
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15226199
Train loss (w/o reg) on all data: 0.14522699
Test loss (w/o reg) on all data: 0.05369074
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.0083514e-06
Norm of the params: 11.861706
              Random: fixed   5 labels. Loss 0.05369. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 410
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012583791
Train loss (w/o reg) on all data: 0.009779329
Test loss (w/o reg) on all data: 0.004227249
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4272103e-07
Norm of the params: 7.4892745
     Influence (LOO): fixed 170 labels. Loss 0.00423. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601126
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19674715e-08
Norm of the params: 6.092826
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14455415
Train loss (w/o reg) on all data: 0.13744967
Test loss (w/o reg) on all data: 0.050884604
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.2681574e-05
Norm of the params: 11.920136
              Random: fixed  15 labels. Loss 0.05088. Accuracy 0.998.
### Flips: 205, rs: 23, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.00263374
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2536286e-08
Norm of the params: 5.928661
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206072e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13496986
Train loss (w/o reg) on all data: 0.12756452
Test loss (w/o reg) on all data: 0.048502274
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.01080295e-05
Norm of the params: 12.169912
              Random: fixed  28 labels. Loss 0.04850. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955887
Test loss (w/o reg) on all data: 0.002633732
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9300872e-08
Norm of the params: 5.928653
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2202066e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12886952
Train loss (w/o reg) on all data: 0.12190286
Test loss (w/o reg) on all data: 0.043978903
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0303042e-06
Norm of the params: 11.803945
              Random: fixed  38 labels. Loss 0.04398. Accuracy 0.997.
### Flips: 205, rs: 23, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955869
Test loss (w/o reg) on all data: 0.0026337395
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1966693e-08
Norm of the params: 5.928655
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206445e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12046012
Train loss (w/o reg) on all data: 0.11374559
Test loss (w/o reg) on all data: 0.040882304
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2403834e-05
Norm of the params: 11.588381
              Random: fixed  50 labels. Loss 0.04088. Accuracy 0.995.
### Flips: 205, rs: 23, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955862
Test loss (w/o reg) on all data: 0.002633741
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1626343e-08
Norm of the params: 5.9286575
     Influence (LOO): fixed 176 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206423e-08
Norm of the params: 6.0928364
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11541239
Train loss (w/o reg) on all data: 0.10875023
Test loss (w/o reg) on all data: 0.036202747
Train acc on all data:  0.9713104789691223
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.923951e-06
Norm of the params: 11.543104
              Random: fixed  58 labels. Loss 0.03620. Accuracy 0.999.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15707289
Train loss (w/o reg) on all data: 0.14941399
Test loss (w/o reg) on all data: 0.05264052
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.3926203e-06
Norm of the params: 12.376513
Flipped loss: 0.05264. Accuracy: 0.996
### Flips: 205, rs: 24, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022013575
Train loss (w/o reg) on all data: 0.018774675
Test loss (w/o reg) on all data: 0.006296098
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 1.062807e-06
Norm of the params: 8.048477
     Influence (LOO): fixed 164 labels. Loss 0.00630. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012064
Test loss (w/o reg) on all data: 0.0026560312
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7852696e-08
Norm of the params: 6.0928135
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15077044
Train loss (w/o reg) on all data: 0.14299463
Test loss (w/o reg) on all data: 0.050253477
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.6338853e-06
Norm of the params: 12.4706135
              Random: fixed   9 labels. Loss 0.05025. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 410
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01008534
Train loss (w/o reg) on all data: 0.007626568
Test loss (w/o reg) on all data: 0.0036699192
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5011245e-07
Norm of the params: 7.012521
     Influence (LOO): fixed 177 labels. Loss 0.00367. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6648931e-08
Norm of the params: 6.092802
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14398524
Train loss (w/o reg) on all data: 0.1362255
Test loss (w/o reg) on all data: 0.04824341
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.398177e-06
Norm of the params: 12.457722
              Random: fixed  18 labels. Loss 0.04824. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059347358
Train loss (w/o reg) on all data: 0.0039639543
Test loss (w/o reg) on all data: 0.0031905966
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.643278e-08
Norm of the params: 6.2781863
     Influence (LOO): fixed 180 labels. Loss 0.00319. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1025606e-08
Norm of the params: 6.092812
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13955954
Train loss (w/o reg) on all data: 0.13190183
Test loss (w/o reg) on all data: 0.047152217
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.097326e-05
Norm of the params: 12.375542
              Random: fixed  24 labels. Loss 0.04715. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 820
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495589
Test loss (w/o reg) on all data: 0.0026337483
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4547457e-08
Norm of the params: 5.928652
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206786e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1334579
Train loss (w/o reg) on all data: 0.12573394
Test loss (w/o reg) on all data: 0.043785457
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9369044e-06
Norm of the params: 12.428967
              Random: fixed  33 labels. Loss 0.04379. Accuracy 0.996.
### Flips: 205, rs: 24, checks: 1025
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955864
Test loss (w/o reg) on all data: 0.0026337455
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6321754e-08
Norm of the params: 5.928657
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206375e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12306404
Train loss (w/o reg) on all data: 0.11572788
Test loss (w/o reg) on all data: 0.038446747
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.601585e-06
Norm of the params: 12.112938
              Random: fixed  46 labels. Loss 0.03845. Accuracy 0.997.
### Flips: 205, rs: 24, checks: 1230
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.0024955922
Test loss (w/o reg) on all data: 0.0026337502
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8581776e-08
Norm of the params: 5.928646
     Influence (LOO): fixed 181 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206441e-08
Norm of the params: 6.0928364
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11481981
Train loss (w/o reg) on all data: 0.10731756
Test loss (w/o reg) on all data: 0.03607945
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4843766e-06
Norm of the params: 12.249288
              Random: fixed  57 labels. Loss 0.03608. Accuracy 0.997.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149245
Train loss (w/o reg) on all data: 0.14129798
Test loss (w/o reg) on all data: 0.052479126
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.297086e-05
Norm of the params: 12.607147
Flipped loss: 0.05248. Accuracy: 0.999
### Flips: 205, rs: 25, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022829743
Train loss (w/o reg) on all data: 0.01959975
Test loss (w/o reg) on all data: 0.00679966
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7965235e-07
Norm of the params: 8.037403
     Influence (LOO): fixed 154 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7693596e-08
Norm of the params: 6.0928154
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14308906
Train loss (w/o reg) on all data: 0.13517466
Test loss (w/o reg) on all data: 0.049661923
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.121804e-06
Norm of the params: 12.581246
              Random: fixed   9 labels. Loss 0.04966. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011982778
Train loss (w/o reg) on all data: 0.009404367
Test loss (w/o reg) on all data: 0.004437299
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4059846e-07
Norm of the params: 7.1811028
     Influence (LOO): fixed 165 labels. Loss 0.00444. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601252
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4087346e-08
Norm of the params: 6.0928063
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13575596
Train loss (w/o reg) on all data: 0.12765515
Test loss (w/o reg) on all data: 0.04661248
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4515324e-06
Norm of the params: 12.728561
              Random: fixed  18 labels. Loss 0.04661. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049584694
Train loss (w/o reg) on all data: 0.0031965852
Test loss (w/o reg) on all data: 0.0030820032
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7978317e-08
Norm of the params: 5.9361334
     Influence (LOO): fixed 171 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560363
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.117875e-08
Norm of the params: 6.092826
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13286054
Train loss (w/o reg) on all data: 0.12466574
Test loss (w/o reg) on all data: 0.045655828
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5066654e-06
Norm of the params: 12.802184
              Random: fixed  22 labels. Loss 0.04566. Accuracy 0.999.
### Flips: 205, rs: 25, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495587
Test loss (w/o reg) on all data: 0.002633734
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.669143e-08
Norm of the params: 5.9286556
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206983e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486732
Train loss (w/o reg) on all data: 0.11663292
Test loss (w/o reg) on all data: 0.043952625
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9882364e-06
Norm of the params: 12.83308
              Random: fixed  32 labels. Loss 0.04395. Accuracy 0.998.
### Flips: 205, rs: 25, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253034
Train loss (w/o reg) on all data: 0.0024955892
Test loss (w/o reg) on all data: 0.0026337388
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6424226e-08
Norm of the params: 5.9286504
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206367e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116093144
Train loss (w/o reg) on all data: 0.10787307
Test loss (w/o reg) on all data: 0.040869415
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.8846427e-06
Norm of the params: 12.821914
              Random: fixed  44 labels. Loss 0.04087. Accuracy 0.998.
### Flips: 205, rs: 25, checks: 1230
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024956
Test loss (w/o reg) on all data: 0.0026337663
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2700733e-08
Norm of the params: 5.928634
     Influence (LOO): fixed 172 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206393e-08
Norm of the params: 6.0928364
                Loss: fixed 174 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111387074
Train loss (w/o reg) on all data: 0.103204854
Test loss (w/o reg) on all data: 0.038362898
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4158984e-06
Norm of the params: 12.792358
              Random: fixed  51 labels. Loss 0.03836. Accuracy 0.997.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15231165
Train loss (w/o reg) on all data: 0.14423004
Test loss (w/o reg) on all data: 0.05365009
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 9.318822e-06
Norm of the params: 12.713471
Flipped loss: 0.05365. Accuracy: 1.000
### Flips: 205, rs: 26, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021986462
Train loss (w/o reg) on all data: 0.018138109
Test loss (w/o reg) on all data: 0.0061742286
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4177316e-07
Norm of the params: 8.7730875
     Influence (LOO): fixed 159 labels. Loss 0.00617. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601147
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1246674e-08
Norm of the params: 6.092824
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15110828
Train loss (w/o reg) on all data: 0.14305823
Test loss (w/o reg) on all data: 0.05293997
Train acc on all data:  0.9574519815220034
Test acc on all data:   1.0
Norm of the mean of gradients: 4.187163e-06
Norm of the params: 12.688621
              Random: fixed   2 labels. Loss 0.05294. Accuracy 1.000.
### Flips: 205, rs: 26, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006315386
Train loss (w/o reg) on all data: 0.004266466
Test loss (w/o reg) on all data: 0.0030553227
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8286038e-08
Norm of the params: 6.4014378
     Influence (LOO): fixed 175 labels. Loss 0.00306. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011494
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5009359e-08
Norm of the params: 6.092823
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14387055
Train loss (w/o reg) on all data: 0.13596365
Test loss (w/o reg) on all data: 0.050668757
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6891087e-06
Norm of the params: 12.5752945
              Random: fixed  11 labels. Loss 0.05067. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504544
Test loss (w/o reg) on all data: 0.0026117754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5849056e-08
Norm of the params: 5.942002
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.736335e-09
Norm of the params: 6.092822
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1377862
Train loss (w/o reg) on all data: 0.13005044
Test loss (w/o reg) on all data: 0.047196418
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.832724e-06
Norm of the params: 12.438456
              Random: fixed  21 labels. Loss 0.04720. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504563
Test loss (w/o reg) on all data: 0.0026117552
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.499046e-08
Norm of the params: 5.941999
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.877148e-09
Norm of the params: 6.092823
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13268241
Train loss (w/o reg) on all data: 0.12514776
Test loss (w/o reg) on all data: 0.04551049
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.1192127e-06
Norm of the params: 12.275708
              Random: fixed  29 labels. Loss 0.04551. Accuracy 0.998.
### Flips: 205, rs: 26, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.001850451
Test loss (w/o reg) on all data: 0.0026117729
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.161472e-09
Norm of the params: 5.942007
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.748401e-09
Norm of the params: 6.0928226
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12498231
Train loss (w/o reg) on all data: 0.117363654
Test loss (w/o reg) on all data: 0.042006064
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3049103e-06
Norm of the params: 12.34395
              Random: fixed  40 labels. Loss 0.04201. Accuracy 0.999.
### Flips: 205, rs: 26, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158245
Train loss (w/o reg) on all data: 0.001850468
Test loss (w/o reg) on all data: 0.0026118055
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1013283e-07
Norm of the params: 5.9419804
     Influence (LOO): fixed 177 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.318285e-09
Norm of the params: 6.0928216
                Loss: fixed 178 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11933407
Train loss (w/o reg) on all data: 0.11148314
Test loss (w/o reg) on all data: 0.03876385
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.913852e-06
Norm of the params: 12.530707
              Random: fixed  48 labels. Loss 0.03876. Accuracy 0.999.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14791891
Train loss (w/o reg) on all data: 0.13914073
Test loss (w/o reg) on all data: 0.057765994
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0497197e-06
Norm of the params: 13.250046
Flipped loss: 0.05777. Accuracy: 0.998
### Flips: 205, rs: 27, checks: 205
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027919877
Train loss (w/o reg) on all data: 0.023289967
Test loss (w/o reg) on all data: 0.011536727
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7944156e-06
Norm of the params: 9.622796
     Influence (LOO): fixed 151 labels. Loss 0.01154. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2970449e-08
Norm of the params: 6.092827
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1444006
Train loss (w/o reg) on all data: 0.13557099
Test loss (w/o reg) on all data: 0.054627012
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1424938e-06
Norm of the params: 13.288803
              Random: fixed   7 labels. Loss 0.05463. Accuracy 0.998.
### Flips: 205, rs: 27, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009190837
Train loss (w/o reg) on all data: 0.006267865
Test loss (w/o reg) on all data: 0.004813948
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1799963e-07
Norm of the params: 7.645877
     Influence (LOO): fixed 173 labels. Loss 0.00481. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5480905e-08
Norm of the params: 6.0928235
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13978696
Train loss (w/o reg) on all data: 0.13096584
Test loss (w/o reg) on all data: 0.051766984
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.934628e-06
Norm of the params: 13.2824
              Random: fixed  14 labels. Loss 0.05177. Accuracy 0.998.
### Flips: 205, rs: 27, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560368
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5889847e-08
Norm of the params: 6.0928264
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2311472e-08
Norm of the params: 6.09282
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13237263
Train loss (w/o reg) on all data: 0.123935096
Test loss (w/o reg) on all data: 0.04620541
Train acc on all data:  0.963287138341843
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6571397e-06
Norm of the params: 12.99041
              Random: fixed  26 labels. Loss 0.04621. Accuracy 1.000.
### Flips: 205, rs: 27, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2517765e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6401635e-09
Norm of the params: 6.0928164
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12741716
Train loss (w/o reg) on all data: 0.11925719
Test loss (w/o reg) on all data: 0.04474575
Train acc on all data:  0.9649890590809628
Test acc on all data:   1.0
Norm of the mean of gradients: 4.538861e-06
Norm of the params: 12.77496
              Random: fixed  33 labels. Loss 0.04475. Accuracy 1.000.
### Flips: 205, rs: 27, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601263
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6718272e-08
Norm of the params: 6.0928054
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560423
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6552063e-08
Norm of the params: 6.0928154
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120117486
Train loss (w/o reg) on all data: 0.11227598
Test loss (w/o reg) on all data: 0.04139336
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.1176723e-06
Norm of the params: 12.523186
              Random: fixed  44 labels. Loss 0.04139. Accuracy 0.999.
### Flips: 205, rs: 27, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960111
Test loss (w/o reg) on all data: 0.002656037
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5199931e-08
Norm of the params: 6.092831
     Influence (LOO): fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4841617e-08
Norm of the params: 6.0928197
                Loss: fixed 179 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11380543
Train loss (w/o reg) on all data: 0.10584418
Test loss (w/o reg) on all data: 0.0391335
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.157427e-06
Norm of the params: 12.618438
              Random: fixed  52 labels. Loss 0.03913. Accuracy 0.999.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15432999
Train loss (w/o reg) on all data: 0.14618419
Test loss (w/o reg) on all data: 0.05598647
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6282688e-06
Norm of the params: 12.763853
Flipped loss: 0.05599. Accuracy: 0.996
### Flips: 205, rs: 28, checks: 205
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029001717
Train loss (w/o reg) on all data: 0.024803732
Test loss (w/o reg) on all data: 0.0084646335
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2745827e-07
Norm of the params: 9.162951
     Influence (LOO): fixed 158 labels. Loss 0.00846. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601213
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4850593e-08
Norm of the params: 6.092813
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641382
Train loss (w/o reg) on all data: 0.1382602
Test loss (w/o reg) on all data: 0.053372744
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.254207e-06
Norm of the params: 12.769977
              Random: fixed  12 labels. Loss 0.05337. Accuracy 0.995.
### Flips: 205, rs: 28, checks: 410
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00926818
Train loss (w/o reg) on all data: 0.006729458
Test loss (w/o reg) on all data: 0.0041651586
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3247583e-07
Norm of the params: 7.1256185
     Influence (LOO): fixed 180 labels. Loss 0.00417. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601196
Test loss (w/o reg) on all data: 0.0026560829
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8422418e-08
Norm of the params: 6.092816
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14200687
Train loss (w/o reg) on all data: 0.13373877
Test loss (w/o reg) on all data: 0.051658027
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1111896e-06
Norm of the params: 12.859321
              Random: fixed  18 labels. Loss 0.05166. Accuracy 0.996.
### Flips: 205, rs: 28, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011284
Test loss (w/o reg) on all data: 0.0026560382
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1649123e-08
Norm of the params: 6.092827
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601158
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0737202e-08
Norm of the params: 6.092822
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1357198
Train loss (w/o reg) on all data: 0.12775639
Test loss (w/o reg) on all data: 0.04904387
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.672101e-06
Norm of the params: 12.620155
              Random: fixed  26 labels. Loss 0.04904. Accuracy 0.993.
### Flips: 205, rs: 28, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012774
Test loss (w/o reg) on all data: 0.0026560896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2771107e-08
Norm of the params: 6.092802
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7824375e-08
Norm of the params: 6.092812
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12787609
Train loss (w/o reg) on all data: 0.11982573
Test loss (w/o reg) on all data: 0.04521261
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.488376e-06
Norm of the params: 12.68886
              Random: fixed  38 labels. Loss 0.04521. Accuracy 0.994.
### Flips: 205, rs: 28, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3102523e-08
Norm of the params: 6.09281
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601203
Test loss (w/o reg) on all data: 0.0026560584
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.04054e-09
Norm of the params: 6.0928144
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12545997
Train loss (w/o reg) on all data: 0.11735973
Test loss (w/o reg) on all data: 0.04467833
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.642898e-06
Norm of the params: 12.728111
              Random: fixed  41 labels. Loss 0.04468. Accuracy 0.995.
### Flips: 205, rs: 28, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012943
Test loss (w/o reg) on all data: 0.0026560905
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4897362e-08
Norm of the params: 6.0928
     Influence (LOO): fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560582
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5916312e-08
Norm of the params: 6.0928097
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11620463
Train loss (w/o reg) on all data: 0.10826476
Test loss (w/o reg) on all data: 0.04163079
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.5287707e-06
Norm of the params: 12.601482
              Random: fixed  53 labels. Loss 0.04163. Accuracy 0.995.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15801403
Train loss (w/o reg) on all data: 0.15047182
Test loss (w/o reg) on all data: 0.058964565
Train acc on all data:  0.9542912715779237
Test acc on all data:   1.0
Norm of the mean of gradients: 8.669933e-06
Norm of the params: 12.281859
Flipped loss: 0.05896. Accuracy: 1.000
### Flips: 205, rs: 29, checks: 205
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031075552
Train loss (w/o reg) on all data: 0.02667748
Test loss (w/o reg) on all data: 0.011665696
Train acc on all data:  0.9934354485776805
Test acc on all data:   1.0
Norm of the mean of gradients: 5.288962e-07
Norm of the params: 9.378776
     Influence (LOO): fixed 162 labels. Loss 0.01167. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011354
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2255799e-08
Norm of the params: 6.0928254
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15248086
Train loss (w/o reg) on all data: 0.1449311
Test loss (w/o reg) on all data: 0.05549891
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.3410214e-06
Norm of the params: 12.288013
              Random: fixed  10 labels. Loss 0.05550. Accuracy 0.999.
### Flips: 205, rs: 29, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00979028
Train loss (w/o reg) on all data: 0.0072648316
Test loss (w/o reg) on all data: 0.004573706
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2546237e-07
Norm of the params: 7.106966
     Influence (LOO): fixed 183 labels. Loss 0.00457. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011837
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.41020235e-08
Norm of the params: 6.092818
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14625071
Train loss (w/o reg) on all data: 0.13857917
Test loss (w/o reg) on all data: 0.05359805
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.951495e-06
Norm of the params: 12.386713
              Random: fixed  18 labels. Loss 0.05360. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005400412
Train loss (w/o reg) on all data: 0.0033894903
Test loss (w/o reg) on all data: 0.0030398264
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7605718e-08
Norm of the params: 6.3418007
     Influence (LOO): fixed 187 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560642
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0643679e-08
Norm of the params: 6.0928187
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13712192
Train loss (w/o reg) on all data: 0.12950017
Test loss (w/o reg) on all data: 0.04938849
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.4829745e-06
Norm of the params: 12.34646
              Random: fixed  31 labels. Loss 0.04939. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011965
Test loss (w/o reg) on all data: 0.0026560612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.423926e-08
Norm of the params: 6.092815
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601191
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.931866e-08
Norm of the params: 6.0928173
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13426757
Train loss (w/o reg) on all data: 0.12693255
Test loss (w/o reg) on all data: 0.048061226
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.1214944e-06
Norm of the params: 12.111991
              Random: fixed  36 labels. Loss 0.04806. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560475
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3219263e-08
Norm of the params: 6.0928197
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560605
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6735454e-08
Norm of the params: 6.0928187
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128005
Train loss (w/o reg) on all data: 0.12045381
Test loss (w/o reg) on all data: 0.044965915
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4125386e-06
Norm of the params: 12.289175
              Random: fixed  46 labels. Loss 0.04497. Accuracy 0.998.
### Flips: 205, rs: 29, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013
Test loss (w/o reg) on all data: 0.0026560514
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.00826e-08
Norm of the params: 6.092799
     Influence (LOO): fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601216
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8773079e-08
Norm of the params: 6.092812
                Loss: fixed 189 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11858481
Train loss (w/o reg) on all data: 0.11075645
Test loss (w/o reg) on all data: 0.0413227
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0099508e-06
Norm of the params: 12.51268
              Random: fixed  57 labels. Loss 0.04132. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14261132
Train loss (w/o reg) on all data: 0.13404131
Test loss (w/o reg) on all data: 0.058534976
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8091993e-06
Norm of the params: 13.091999
Flipped loss: 0.05853. Accuracy: 0.994
### Flips: 205, rs: 30, checks: 205
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020355858
Train loss (w/o reg) on all data: 0.016649308
Test loss (w/o reg) on all data: 0.006942716
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0339433e-07
Norm of the params: 8.609935
     Influence (LOO): fixed 160 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031857602
Train loss (w/o reg) on all data: 0.0010902117
Test loss (w/o reg) on all data: 0.0027034166
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.731106e-07
Norm of the params: 6.4738684
                Loss: fixed 175 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13658328
Train loss (w/o reg) on all data: 0.12823322
Test loss (w/o reg) on all data: 0.055203356
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.4946775e-06
Norm of the params: 12.922888
              Random: fixed   9 labels. Loss 0.05520. Accuracy 0.994.
### Flips: 205, rs: 30, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0085628405
Train loss (w/o reg) on all data: 0.006310287
Test loss (w/o reg) on all data: 0.0038882298
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.00196246e-07
Norm of the params: 6.7120104
     Influence (LOO): fixed 172 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560882
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7112415e-08
Norm of the params: 6.0928044
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162075
Train loss (w/o reg) on all data: 0.12343766
Test loss (w/o reg) on all data: 0.05184074
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.1104336e-06
Norm of the params: 12.793042
              Random: fixed  17 labels. Loss 0.05184. Accuracy 0.996.
### Flips: 205, rs: 30, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615823
Train loss (w/o reg) on all data: 0.0018504523
Test loss (w/o reg) on all data: 0.0026117724
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2073078e-08
Norm of the params: 5.9420047
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.74498e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12947248
Train loss (w/o reg) on all data: 0.12136256
Test loss (w/o reg) on all data: 0.051247254
Train acc on all data:  0.9637734014101629
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.96093e-06
Norm of the params: 12.735718
              Random: fixed  20 labels. Loss 0.05125. Accuracy 0.995.
### Flips: 205, rs: 30, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615823
Train loss (w/o reg) on all data: 0.0018504527
Test loss (w/o reg) on all data: 0.0026117747
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0352521e-08
Norm of the params: 5.9420037
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.745494e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11850791
Train loss (w/o reg) on all data: 0.11037582
Test loss (w/o reg) on all data: 0.04511506
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.558581e-06
Norm of the params: 12.753106
              Random: fixed  37 labels. Loss 0.04512. Accuracy 0.997.
### Flips: 205, rs: 30, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504536
Test loss (w/o reg) on all data: 0.0026117996
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8830297e-08
Norm of the params: 5.942003
     Influence (LOO): fixed 175 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7303125e-09
Norm of the params: 6.092822
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11394794
Train loss (w/o reg) on all data: 0.105638504
Test loss (w/o reg) on all data: 0.04336075
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2775692e-06
Norm of the params: 12.89142
              Random: fixed  44 labels. Loss 0.04336. Accuracy 0.997.
### Flips: 205, rs: 30, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011383
Test loss (w/o reg) on all data: 0.0026560759
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.917458e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3103853e-08
Norm of the params: 6.0928173
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110772684
Train loss (w/o reg) on all data: 0.10254843
Test loss (w/o reg) on all data: 0.042212542
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.655376e-06
Norm of the params: 12.825178
              Random: fixed  48 labels. Loss 0.04221. Accuracy 0.997.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14750078
Train loss (w/o reg) on all data: 0.13956234
Test loss (w/o reg) on all data: 0.053526364
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.3489587e-06
Norm of the params: 12.600349
Flipped loss: 0.05353. Accuracy: 0.997
### Flips: 205, rs: 31, checks: 205
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027288599
Train loss (w/o reg) on all data: 0.022889892
Test loss (w/o reg) on all data: 0.008111198
Train acc on all data:  0.9941648431801605
Test acc on all data:   1.0
Norm of the mean of gradients: 2.989673e-07
Norm of the params: 9.379454
     Influence (LOO): fixed 152 labels. Loss 0.00811. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011604
Test loss (w/o reg) on all data: 0.0026560817
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.08826e-08
Norm of the params: 6.092821
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14429766
Train loss (w/o reg) on all data: 0.13635384
Test loss (w/o reg) on all data: 0.05243178
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2449345e-05
Norm of the params: 12.604618
              Random: fixed   5 labels. Loss 0.05243. Accuracy 0.997.
### Flips: 205, rs: 31, checks: 410
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010945506
Train loss (w/o reg) on all data: 0.008077371
Test loss (w/o reg) on all data: 0.003983864
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0144685e-07
Norm of the params: 7.5738173
     Influence (LOO): fixed 170 labels. Loss 0.00398. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7759023e-08
Norm of the params: 6.092809
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13390812
Train loss (w/o reg) on all data: 0.12602922
Test loss (w/o reg) on all data: 0.045337025
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.377179e-06
Norm of the params: 12.553011
              Random: fixed  21 labels. Loss 0.04534. Accuracy 0.997.
### Flips: 205, rs: 31, checks: 615
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044750604
Train loss (w/o reg) on all data: 0.0023959526
Test loss (w/o reg) on all data: 0.0032281077
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0208835e-08
Norm of the params: 6.448423
     Influence (LOO): fixed 175 labels. Loss 0.00323. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601181
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1841352e-08
Norm of the params: 6.0928183
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12852432
Train loss (w/o reg) on all data: 0.12086745
Test loss (w/o reg) on all data: 0.04332405
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4783162e-06
Norm of the params: 12.374869
              Random: fixed  30 labels. Loss 0.04332. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096009474
Test loss (w/o reg) on all data: 0.0026560526
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0892463e-08
Norm of the params: 6.0928564
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960117
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1248845e-08
Norm of the params: 6.092819
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12304264
Train loss (w/o reg) on all data: 0.115890026
Test loss (w/o reg) on all data: 0.04150952
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.274395e-05
Norm of the params: 11.96045
              Random: fixed  39 labels. Loss 0.04151. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1025
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600932
Test loss (w/o reg) on all data: 0.0026560274
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4669135e-08
Norm of the params: 6.092859
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2052258e-08
Norm of the params: 6.0928106
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10948162
Train loss (w/o reg) on all data: 0.10213783
Test loss (w/o reg) on all data: 0.03665744
Train acc on all data:  0.9722830051057623
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.223631e-06
Norm of the params: 12.119234
              Random: fixed  56 labels. Loss 0.03666. Accuracy 0.998.
### Flips: 205, rs: 31, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1461306e-08
Norm of the params: 6.092825
     Influence (LOO): fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.835559e-09
Norm of the params: 6.092818
                Loss: fixed 176 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10288662
Train loss (w/o reg) on all data: 0.095536985
Test loss (w/o reg) on all data: 0.033570196
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1719372e-06
Norm of the params: 12.124056
              Random: fixed  65 labels. Loss 0.03357. Accuracy 0.999.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14232507
Train loss (w/o reg) on all data: 0.13498518
Test loss (w/o reg) on all data: 0.05103116
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.982418e-06
Norm of the params: 12.116015
Flipped loss: 0.05103. Accuracy: 0.998
### Flips: 205, rs: 32, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014407062
Train loss (w/o reg) on all data: 0.011409637
Test loss (w/o reg) on all data: 0.0057144025
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0348999e-06
Norm of the params: 7.742642
     Influence (LOO): fixed 156 labels. Loss 0.00571. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601237
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1890927e-08
Norm of the params: 6.0928087
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13691807
Train loss (w/o reg) on all data: 0.12933688
Test loss (w/o reg) on all data: 0.049914252
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.8341923e-06
Norm of the params: 12.313564
              Random: fixed   7 labels. Loss 0.04991. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 410
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076951394
Train loss (w/o reg) on all data: 0.005304308
Test loss (w/o reg) on all data: 0.0034335647
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2287182e-07
Norm of the params: 6.9149566
     Influence (LOO): fixed 163 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.0026560663
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5703334e-08
Norm of the params: 6.0928235
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12833728
Train loss (w/o reg) on all data: 0.120584816
Test loss (w/o reg) on all data: 0.046653148
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9815252e-05
Norm of the params: 12.451873
              Random: fixed  19 labels. Loss 0.04665. Accuracy 0.997.
### Flips: 205, rs: 32, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504359
Test loss (w/o reg) on all data: 0.0026117344
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.685418e-08
Norm of the params: 5.942033
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.878408e-09
Norm of the params: 6.092823
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122740634
Train loss (w/o reg) on all data: 0.115008414
Test loss (w/o reg) on all data: 0.044663128
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.068064e-06
Norm of the params: 12.435612
              Random: fixed  27 labels. Loss 0.04466. Accuracy 0.996.
### Flips: 205, rs: 32, checks: 820
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504601
Test loss (w/o reg) on all data: 0.002611799
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7504776e-08
Norm of the params: 5.9419923
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762386e-09
Norm of the params: 6.092822
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11889994
Train loss (w/o reg) on all data: 0.1110321
Test loss (w/o reg) on all data: 0.042282846
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.1224596e-06
Norm of the params: 12.544197
              Random: fixed  32 labels. Loss 0.04228. Accuracy 0.996.
### Flips: 205, rs: 32, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504587
Test loss (w/o reg) on all data: 0.0026117794
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2521882e-08
Norm of the params: 5.9419947
     Influence (LOO): fixed 166 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762567e-09
Norm of the params: 6.092822
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112118766
Train loss (w/o reg) on all data: 0.10405958
Test loss (w/o reg) on all data: 0.03999034
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.995834e-06
Norm of the params: 12.695817
              Random: fixed  40 labels. Loss 0.03999. Accuracy 0.998.
### Flips: 205, rs: 32, checks: 1230
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9376595e-08
Norm of the params: 6.092819
     Influence (LOO): fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601168
Test loss (w/o reg) on all data: 0.0026560596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.480201e-09
Norm of the params: 6.0928206
                Loss: fixed 167 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109910205
Train loss (w/o reg) on all data: 0.10197274
Test loss (w/o reg) on all data: 0.03985508
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.867268e-06
Norm of the params: 12.599577
              Random: fixed  43 labels. Loss 0.03986. Accuracy 0.997.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14955904
Train loss (w/o reg) on all data: 0.14183101
Test loss (w/o reg) on all data: 0.052554905
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.183711e-06
Norm of the params: 12.432233
Flipped loss: 0.05255. Accuracy: 0.999
### Flips: 205, rs: 33, checks: 205
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025530916
Train loss (w/o reg) on all data: 0.02162569
Test loss (w/o reg) on all data: 0.0088130925
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 5.728396e-07
Norm of the params: 8.837677
     Influence (LOO): fixed 153 labels. Loss 0.00881. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034638732
Train loss (w/o reg) on all data: 0.0012580276
Test loss (w/o reg) on all data: 0.002846322
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4863928e-08
Norm of the params: 6.6420565
                Loss: fixed 175 labels. Loss 0.00285. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14115745
Train loss (w/o reg) on all data: 0.13349944
Test loss (w/o reg) on all data: 0.049533855
Train acc on all data:  0.9603695599319232
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0872365e-05
Norm of the params: 12.375787
              Random: fixed  13 labels. Loss 0.04953. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010438705
Train loss (w/o reg) on all data: 0.0076318583
Test loss (w/o reg) on all data: 0.004696422
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8130287e-07
Norm of the params: 7.4924574
     Influence (LOO): fixed 170 labels. Loss 0.00470. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3336745e-08
Norm of the params: 6.0928197
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13431644
Train loss (w/o reg) on all data: 0.12659676
Test loss (w/o reg) on all data: 0.046788964
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2888973e-06
Norm of the params: 12.425526
              Random: fixed  22 labels. Loss 0.04679. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504484
Test loss (w/o reg) on all data: 0.0026117587
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.769814e-08
Norm of the params: 5.9420114
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.63992e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12644744
Train loss (w/o reg) on all data: 0.11891763
Test loss (w/o reg) on all data: 0.043715898
Train acc on all data:  0.9659615852176027
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0798785e-06
Norm of the params: 12.271762
              Random: fixed  34 labels. Loss 0.04372. Accuracy 1.000.
### Flips: 205, rs: 33, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504615
Test loss (w/o reg) on all data: 0.0026117803
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5625675e-08
Norm of the params: 5.94199
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.729528e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057563
Train loss (w/o reg) on all data: 0.11325657
Test loss (w/o reg) on all data: 0.04182936
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.259625e-06
Norm of the params: 12.098809
              Random: fixed  43 labels. Loss 0.04183. Accuracy 0.999.
### Flips: 205, rs: 33, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158233
Train loss (w/o reg) on all data: 0.0018504601
Test loss (w/o reg) on all data: 0.0026117787
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.840639e-08
Norm of the params: 5.9419923
     Influence (LOO): fixed 176 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601152
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.548096e-09
Norm of the params: 6.092822
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11421097
Train loss (w/o reg) on all data: 0.10685645
Test loss (w/o reg) on all data: 0.039756276
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1045245e-06
Norm of the params: 12.128082
              Random: fixed  51 labels. Loss 0.03976. Accuracy 0.998.
### Flips: 205, rs: 33, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010865
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7522236e-08
Norm of the params: 6.0928335
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601139
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.106109e-08
Norm of the params: 6.092824
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10977122
Train loss (w/o reg) on all data: 0.10262023
Test loss (w/o reg) on all data: 0.038107358
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.7757775e-06
Norm of the params: 11.959088
              Random: fixed  60 labels. Loss 0.03811. Accuracy 0.998.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15682897
Train loss (w/o reg) on all data: 0.14925909
Test loss (w/o reg) on all data: 0.0554334
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.6115707e-06
Norm of the params: 12.304369
Flipped loss: 0.05543. Accuracy: 0.999
### Flips: 205, rs: 34, checks: 205
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024126211
Train loss (w/o reg) on all data: 0.02039363
Test loss (w/o reg) on all data: 0.009346639
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7283935e-07
Norm of the params: 8.640117
     Influence (LOO): fixed 161 labels. Loss 0.00935. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034086392
Train loss (w/o reg) on all data: 0.0011744701
Test loss (w/o reg) on all data: 0.00259981
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3106889e-08
Norm of the params: 6.6845627
                Loss: fixed 182 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15171422
Train loss (w/o reg) on all data: 0.14403154
Test loss (w/o reg) on all data: 0.0531209
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0699011e-05
Norm of the params: 12.395714
              Random: fixed   7 labels. Loss 0.05312. Accuracy 0.999.
### Flips: 205, rs: 34, checks: 410
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012925005
Train loss (w/o reg) on all data: 0.009879616
Test loss (w/o reg) on all data: 0.0057810023
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3431447e-07
Norm of the params: 7.8043427
     Influence (LOO): fixed 172 labels. Loss 0.00578. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.0009601297
Test loss (w/o reg) on all data: 0.0026561033
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0215295e-07
Norm of the params: 6.0928
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14696701
Train loss (w/o reg) on all data: 0.13948074
Test loss (w/o reg) on all data: 0.05209439
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.943453e-06
Norm of the params: 12.236237
              Random: fixed  13 labels. Loss 0.05209. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 615
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158243
Train loss (w/o reg) on all data: 0.0018504595
Test loss (w/o reg) on all data: 0.0026117929
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2485765e-08
Norm of the params: 5.941994
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.730752e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14155614
Train loss (w/o reg) on all data: 0.13406329
Test loss (w/o reg) on all data: 0.04959619
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0872277e-06
Norm of the params: 12.241619
              Random: fixed  20 labels. Loss 0.04960. Accuracy 0.997.
### Flips: 205, rs: 34, checks: 820
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504461
Test loss (w/o reg) on all data: 0.0026117738
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6355774e-08
Norm of the params: 5.942015
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.762713e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13799669
Train loss (w/o reg) on all data: 0.13061965
Test loss (w/o reg) on all data: 0.04865494
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.6786795e-06
Norm of the params: 12.146646
              Random: fixed  27 labels. Loss 0.04865. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158238
Train loss (w/o reg) on all data: 0.0018504618
Test loss (w/o reg) on all data: 0.0026117857
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7828473e-08
Norm of the params: 5.9419894
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.764824e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13059017
Train loss (w/o reg) on all data: 0.12326585
Test loss (w/o reg) on all data: 0.045447633
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.090757e-05
Norm of the params: 12.103155
              Random: fixed  38 labels. Loss 0.04545. Accuracy 0.998.
### Flips: 205, rs: 34, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504753
Test loss (w/o reg) on all data: 0.0026118155
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 8.3968466e-08
Norm of the params: 5.9419665
     Influence (LOO): fixed 182 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.485679e-09
Norm of the params: 6.092822
                Loss: fixed 183 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11986478
Train loss (w/o reg) on all data: 0.112273775
Test loss (w/o reg) on all data: 0.042020883
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.1080802e-06
Norm of the params: 12.32153
              Random: fixed  51 labels. Loss 0.04202. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1437891
Train loss (w/o reg) on all data: 0.1362636
Test loss (w/o reg) on all data: 0.04910796
Train acc on all data:  0.9598832968636032
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5903222e-05
Norm of the params: 12.26826
Flipped loss: 0.04911. Accuracy: 1.000
### Flips: 205, rs: 35, checks: 205
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016544513
Train loss (w/o reg) on all data: 0.013273043
Test loss (w/o reg) on all data: 0.0074469377
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.250707e-07
Norm of the params: 8.088846
     Influence (LOO): fixed 150 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033229268
Train loss (w/o reg) on all data: 0.0011508467
Test loss (w/o reg) on all data: 0.0027215527
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0271202e-08
Norm of the params: 6.5910244
                Loss: fixed 164 labels. Loss 0.00272. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13342851
Train loss (w/o reg) on all data: 0.12569225
Test loss (w/o reg) on all data: 0.04585224
Train acc on all data:  0.963044006807683
Test acc on all data:   1.0
Norm of the mean of gradients: 7.002721e-06
Norm of the params: 12.438869
              Random: fixed  13 labels. Loss 0.04585. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 410
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008086189
Train loss (w/o reg) on all data: 0.005982866
Test loss (w/o reg) on all data: 0.0035166158
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9128077e-08
Norm of the params: 6.4858656
     Influence (LOO): fixed 160 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601167
Test loss (w/o reg) on all data: 0.0026560703
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0578272e-08
Norm of the params: 6.0928197
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123731166
Train loss (w/o reg) on all data: 0.11617768
Test loss (w/o reg) on all data: 0.04255239
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7862272e-06
Norm of the params: 12.291044
              Random: fixed  28 labels. Loss 0.04255. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004958469
Train loss (w/o reg) on all data: 0.00319655
Test loss (w/o reg) on all data: 0.0030819448
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8320072e-08
Norm of the params: 5.936192
     Influence (LOO): fixed 162 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960113
Test loss (w/o reg) on all data: 0.0026560363
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1337e-08
Norm of the params: 6.092826
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119389966
Train loss (w/o reg) on all data: 0.11189491
Test loss (w/o reg) on all data: 0.041776624
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9031693e-06
Norm of the params: 12.243406
              Random: fixed  34 labels. Loss 0.04178. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495584
Test loss (w/o reg) on all data: 0.0026337267
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.708223e-08
Norm of the params: 5.9286604
     Influence (LOO): fixed 163 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220679e-08
Norm of the params: 6.0928364
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11398816
Train loss (w/o reg) on all data: 0.10645762
Test loss (w/o reg) on all data: 0.039467815
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.863687e-06
Norm of the params: 12.272359
              Random: fixed  42 labels. Loss 0.03947. Accuracy 0.999.
### Flips: 205, rs: 35, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004253035
Train loss (w/o reg) on all data: 0.0024955897
Test loss (w/o reg) on all data: 0.0026337346
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5994373e-08
Norm of the params: 5.928651
     Influence (LOO): fixed 163 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206786e-08
Norm of the params: 6.0928364
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10604519
Train loss (w/o reg) on all data: 0.09839299
Test loss (w/o reg) on all data: 0.03616611
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1329645e-06
Norm of the params: 12.371089
              Random: fixed  52 labels. Loss 0.03617. Accuracy 1.000.
### Flips: 205, rs: 35, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012047
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05554e-08
Norm of the params: 6.0928144
     Influence (LOO): fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011773
Test loss (w/o reg) on all data: 0.0026560598
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2948558e-08
Norm of the params: 6.092819
                Loss: fixed 165 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10128443
Train loss (w/o reg) on all data: 0.0937064
Test loss (w/o reg) on all data: 0.03422315
Train acc on all data:  0.9744711889132021
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9924863e-06
Norm of the params: 12.310997
              Random: fixed  59 labels. Loss 0.03422. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1490481
Train loss (w/o reg) on all data: 0.14188944
Test loss (w/o reg) on all data: 0.05307124
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.386671e-06
Norm of the params: 11.96551
Flipped loss: 0.05307. Accuracy: 0.999
### Flips: 205, rs: 36, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026248686
Train loss (w/o reg) on all data: 0.022200681
Test loss (w/o reg) on all data: 0.008307454
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6998668e-07
Norm of the params: 8.997783
     Influence (LOO): fixed 155 labels. Loss 0.00831. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601233
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7423571e-08
Norm of the params: 6.0928087
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14057972
Train loss (w/o reg) on all data: 0.13334899
Test loss (w/o reg) on all data: 0.04967067
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.497477e-06
Norm of the params: 12.025579
              Random: fixed  11 labels. Loss 0.04967. Accuracy 0.999.
### Flips: 205, rs: 36, checks: 410
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010936346
Train loss (w/o reg) on all data: 0.008243079
Test loss (w/o reg) on all data: 0.004195696
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.702127e-07
Norm of the params: 7.3393
     Influence (LOO): fixed 170 labels. Loss 0.00420. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012355
Test loss (w/o reg) on all data: 0.0026560568
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0769257e-08
Norm of the params: 6.092809
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13654345
Train loss (w/o reg) on all data: 0.12941308
Test loss (w/o reg) on all data: 0.04835813
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3980455e-06
Norm of the params: 11.9418335
              Random: fixed  18 labels. Loss 0.04836. Accuracy 0.999.
### Flips: 205, rs: 36, checks: 615
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037485412
Train loss (w/o reg) on all data: 0.001961789
Test loss (w/o reg) on all data: 0.003131893
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1360674e-08
Norm of the params: 5.9778795
     Influence (LOO): fixed 176 labels. Loss 0.00313. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601165
Test loss (w/o reg) on all data: 0.00265606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3848758e-08
Norm of the params: 6.092821
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12943149
Train loss (w/o reg) on all data: 0.12245324
Test loss (w/o reg) on all data: 0.04634614
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7588332e-05
Norm of the params: 11.813763
              Random: fixed  29 labels. Loss 0.04635. Accuracy 0.998.
### Flips: 205, rs: 36, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.0026560521
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6090288e-08
Norm of the params: 6.092811
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2670304e-08
Norm of the params: 6.0928164
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12228916
Train loss (w/o reg) on all data: 0.11511959
Test loss (w/o reg) on all data: 0.043396477
Train acc on all data:  0.9669341113542427
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.501661e-06
Norm of the params: 11.974614
              Random: fixed  40 labels. Loss 0.04340. Accuracy 0.998.
### Flips: 205, rs: 36, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601364
Test loss (w/o reg) on all data: 0.0026560458
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.7989996e-08
Norm of the params: 6.092789
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560724
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8580282e-08
Norm of the params: 6.0928164
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11256223
Train loss (w/o reg) on all data: 0.10545327
Test loss (w/o reg) on all data: 0.040321626
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9616706e-06
Norm of the params: 11.923894
              Random: fixed  52 labels. Loss 0.04032. Accuracy 0.997.
### Flips: 205, rs: 36, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960109
Test loss (w/o reg) on all data: 0.0026560428
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3412248e-08
Norm of the params: 6.092832
     Influence (LOO): fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.0026560607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.324502e-09
Norm of the params: 6.092819
                Loss: fixed 177 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10715308
Train loss (w/o reg) on all data: 0.100291125
Test loss (w/o reg) on all data: 0.038548026
Train acc on all data:  0.9720398735716023
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5376237e-06
Norm of the params: 11.714908
              Random: fixed  60 labels. Loss 0.03855. Accuracy 0.997.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15037176
Train loss (w/o reg) on all data: 0.14322904
Test loss (w/o reg) on all data: 0.058860786
Train acc on all data:  0.9550206661804036
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.1491246e-06
Norm of the params: 11.95217
Flipped loss: 0.05886. Accuracy: 0.993
### Flips: 205, rs: 37, checks: 205
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02317163
Train loss (w/o reg) on all data: 0.019280186
Test loss (w/o reg) on all data: 0.007608566
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9641858e-07
Norm of the params: 8.822068
     Influence (LOO): fixed 165 labels. Loss 0.00761. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003648779
Train loss (w/o reg) on all data: 0.0012964649
Test loss (w/o reg) on all data: 0.0026098024
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6037633e-08
Norm of the params: 6.859029
                Loss: fixed 184 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14463307
Train loss (w/o reg) on all data: 0.13754736
Test loss (w/o reg) on all data: 0.056057442
Train acc on all data:  0.9574519815220034
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8717028e-06
Norm of the params: 11.904379
              Random: fixed   9 labels. Loss 0.05606. Accuracy 0.993.
### Flips: 205, rs: 37, checks: 410
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006950848
Train loss (w/o reg) on all data: 0.0047367346
Test loss (w/o reg) on all data: 0.0032268448
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2376516e-07
Norm of the params: 6.654493
     Influence (LOO): fixed 181 labels. Loss 0.00323. Accuracy 0.999.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4120439e-08
Norm of the params: 6.092798
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13675772
Train loss (w/o reg) on all data: 0.12959231
Test loss (w/o reg) on all data: 0.0536065
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.306609e-06
Norm of the params: 11.971128
              Random: fixed  20 labels. Loss 0.05361. Accuracy 0.992.
### Flips: 205, rs: 37, checks: 615
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00585163
Train loss (w/o reg) on all data: 0.003902881
Test loss (w/o reg) on all data: 0.00263413
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.995331e-08
Norm of the params: 6.2429943
     Influence (LOO): fixed 182 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [12] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012536
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8650067e-08
Norm of the params: 6.092806
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12924357
Train loss (w/o reg) on all data: 0.12198695
Test loss (w/o reg) on all data: 0.050088834
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.079621e-06
Norm of the params: 12.0470915
              Random: fixed  32 labels. Loss 0.05009. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 820
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024956204
Test loss (w/o reg) on all data: 0.002633803
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5012472e-07
Norm of the params: 5.9286
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2211122e-08
Norm of the params: 6.092837
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120071955
Train loss (w/o reg) on all data: 0.11289175
Test loss (w/o reg) on all data: 0.046491623
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.36374e-06
Norm of the params: 11.983493
              Random: fixed  45 labels. Loss 0.04649. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 1025
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530354
Train loss (w/o reg) on all data: 0.0024955992
Test loss (w/o reg) on all data: 0.0026337588
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6811865e-08
Norm of the params: 5.928636
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206757e-08
Norm of the params: 6.0928364
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11366162
Train loss (w/o reg) on all data: 0.10675054
Test loss (w/o reg) on all data: 0.043876275
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.031673e-06
Norm of the params: 11.756762
              Random: fixed  55 labels. Loss 0.04388. Accuracy 0.994.
### Flips: 205, rs: 37, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042530345
Train loss (w/o reg) on all data: 0.002495593
Test loss (w/o reg) on all data: 0.0026337535
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.195757e-08
Norm of the params: 5.928645
     Influence (LOO): fixed 183 labels. Loss 0.00263. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601065
Test loss (w/o reg) on all data: 0.0026560482
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2206537e-08
Norm of the params: 6.0928364
                Loss: fixed 185 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1073278
Train loss (w/o reg) on all data: 0.10059153
Test loss (w/o reg) on all data: 0.040455338
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.1217086e-06
Norm of the params: 11.607122
              Random: fixed  65 labels. Loss 0.04046. Accuracy 0.995.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15258206
Train loss (w/o reg) on all data: 0.14449997
Test loss (w/o reg) on all data: 0.054844934
Train acc on all data:  0.9567225869195235
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.5578597e-06
Norm of the params: 12.713846
Flipped loss: 0.05484. Accuracy: 0.996
### Flips: 205, rs: 38, checks: 205
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02774588
Train loss (w/o reg) on all data: 0.024021132
Test loss (w/o reg) on all data: 0.008100698
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 9.632937e-07
Norm of the params: 8.631045
     Influence (LOO): fixed 157 labels. Loss 0.00810. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9867554e-08
Norm of the params: 6.092816
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14695767
Train loss (w/o reg) on all data: 0.13922475
Test loss (w/o reg) on all data: 0.05010067
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.165784e-06
Norm of the params: 12.436169
              Random: fixed  11 labels. Loss 0.05010. Accuracy 0.997.
### Flips: 205, rs: 38, checks: 410
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011193203
Train loss (w/o reg) on all data: 0.008365347
Test loss (w/o reg) on all data: 0.004557652
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4087607e-07
Norm of the params: 7.520446
     Influence (LOO): fixed 174 labels. Loss 0.00456. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011796
Test loss (w/o reg) on all data: 0.0026560517
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0064675e-08
Norm of the params: 6.092818
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14122464
Train loss (w/o reg) on all data: 0.13329704
Test loss (w/o reg) on all data: 0.04753311
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.084417e-06
Norm of the params: 12.591741
              Random: fixed  20 labels. Loss 0.04753. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 615
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005185454
Train loss (w/o reg) on all data: 0.0031549383
Test loss (w/o reg) on all data: 0.002764501
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.811367e-08
Norm of the params: 6.372623
     Influence (LOO): fixed 180 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.0026560843
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6614562e-08
Norm of the params: 6.0928035
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13330211
Train loss (w/o reg) on all data: 0.12532657
Test loss (w/o reg) on all data: 0.04536608
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5725385e-06
Norm of the params: 12.629758
              Random: fixed  31 labels. Loss 0.04537. Accuracy 0.997.
### Flips: 205, rs: 38, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158229
Train loss (w/o reg) on all data: 0.0018504655
Test loss (w/o reg) on all data: 0.0026118115
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1845156e-07
Norm of the params: 5.941982
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.0026560538
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.355286e-09
Norm of the params: 6.0928216
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126138
Train loss (w/o reg) on all data: 0.11773671
Test loss (w/o reg) on all data: 0.042688157
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0474584e-06
Norm of the params: 12.96248
              Random: fixed  40 labels. Loss 0.04269. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 1025
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504472
Test loss (w/o reg) on all data: 0.002611764
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9746907e-08
Norm of the params: 5.9420147
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.759256e-09
Norm of the params: 6.0928226
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11818192
Train loss (w/o reg) on all data: 0.10953548
Test loss (w/o reg) on all data: 0.040184274
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.555101e-06
Norm of the params: 13.150241
              Random: fixed  50 labels. Loss 0.04018. Accuracy 0.998.
### Flips: 205, rs: 38, checks: 1230
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504434
Test loss (w/o reg) on all data: 0.0026117489
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3866026e-08
Norm of the params: 5.94202
     Influence (LOO): fixed 181 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.760823e-09
Norm of the params: 6.0928226
                Loss: fixed 182 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11225788
Train loss (w/o reg) on all data: 0.103635505
Test loss (w/o reg) on all data: 0.03792237
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0194194e-06
Norm of the params: 13.13193
              Random: fixed  59 labels. Loss 0.03792. Accuracy 0.996.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14908299
Train loss (w/o reg) on all data: 0.1408788
Test loss (w/o reg) on all data: 0.058730535
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4942915e-05
Norm of the params: 12.809526
Flipped loss: 0.05873. Accuracy: 0.993
### Flips: 205, rs: 39, checks: 205
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026079673
Train loss (w/o reg) on all data: 0.022140503
Test loss (w/o reg) on all data: 0.0084669385
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9962476e-07
Norm of the params: 8.876001
     Influence (LOO): fixed 158 labels. Loss 0.00847. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041751335
Train loss (w/o reg) on all data: 0.001507792
Test loss (w/o reg) on all data: 0.0029857864
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0793803e-08
Norm of the params: 7.303891
                Loss: fixed 178 labels. Loss 0.00299. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14340506
Train loss (w/o reg) on all data: 0.13530831
Test loss (w/o reg) on all data: 0.05607891
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.601407e-06
Norm of the params: 12.725375
              Random: fixed   9 labels. Loss 0.05608. Accuracy 0.993.
### Flips: 205, rs: 39, checks: 410
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008578552
Train loss (w/o reg) on all data: 0.006254995
Test loss (w/o reg) on all data: 0.0034803962
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2295357e-07
Norm of the params: 6.816974
     Influence (LOO): fixed 175 labels. Loss 0.00348. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003409193
Train loss (w/o reg) on all data: 0.0011793401
Test loss (w/o reg) on all data: 0.00290908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.854557e-08
Norm of the params: 6.678103
                Loss: fixed 179 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13937418
Train loss (w/o reg) on all data: 0.13112715
Test loss (w/o reg) on all data: 0.05495811
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.819819e-06
Norm of the params: 12.842925
              Random: fixed  14 labels. Loss 0.05496. Accuracy 0.994.
### Flips: 205, rs: 39, checks: 615
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004411018
Train loss (w/o reg) on all data: 0.0026563052
Test loss (w/o reg) on all data: 0.0030831718
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3375827e-08
Norm of the params: 5.924041
     Influence (LOO): fixed 178 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [13] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.0026560589
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.684781e-09
Norm of the params: 6.0928206
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13471098
Train loss (w/o reg) on all data: 0.12645623
Test loss (w/o reg) on all data: 0.052003134
Train acc on all data:  0.963044006807683
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.13408e-06
Norm of the params: 12.848935
              Random: fixed  22 labels. Loss 0.05200. Accuracy 0.995.
### Flips: 205, rs: 39, checks: 820
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.0018504543
Test loss (w/o reg) on all data: 0.0026117642
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8237366e-08
Norm of the params: 5.942003
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.0026560533
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.731e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13198864
Train loss (w/o reg) on all data: 0.123981364
Test loss (w/o reg) on all data: 0.05043054
Train acc on all data:  0.9635302698760029
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.708718e-06
Norm of the params: 12.654862
              Random: fixed  27 labels. Loss 0.05043. Accuracy 0.995.
### Flips: 205, rs: 39, checks: 1025
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003615824
Train loss (w/o reg) on all data: 0.001850454
Test loss (w/o reg) on all data: 0.002611766
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7538698e-08
Norm of the params: 5.9420037
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601151
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.756744e-09
Norm of the params: 6.092823
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12661248
Train loss (w/o reg) on all data: 0.11854873
Test loss (w/o reg) on all data: 0.047692012
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2984175e-06
Norm of the params: 12.699419
              Random: fixed  35 labels. Loss 0.04769. Accuracy 0.997.
### Flips: 205, rs: 39, checks: 1230
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036158236
Train loss (w/o reg) on all data: 0.0018504568
Test loss (w/o reg) on all data: 0.0026117817
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1200028e-08
Norm of the params: 5.941998
     Influence (LOO): fixed 179 labels. Loss 0.00261. Accuracy 1.000.
Using normal model
LBFGS training took [11] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011517
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.747286e-09
Norm of the params: 6.092822
                Loss: fixed 180 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11931413
Train loss (w/o reg) on all data: 0.11162967
Test loss (w/o reg) on all data: 0.04260809
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.93369e-06
Norm of the params: 12.397139
              Random: fixed  46 labels. Loss 0.04261. Accuracy 0.997.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25523645
Train loss (w/o reg) on all data: 0.24803415
Test loss (w/o reg) on all data: 0.10766977
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8555374e-05
Norm of the params: 12.001919
Flipped loss: 0.10767. Accuracy: 0.998
### Flips: 410, rs: 0, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13130566
Train loss (w/o reg) on all data: 0.122753516
Test loss (w/o reg) on all data: 0.05256282
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7597678e-06
Norm of the params: 13.078341
     Influence (LOO): fixed 193 labels. Loss 0.05256. Accuracy 0.998.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09930131
Train loss (w/o reg) on all data: 0.08495253
Test loss (w/o reg) on all data: 0.048298262
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.3725288e-06
Norm of the params: 16.940353
                Loss: fixed 205 labels. Loss 0.04830. Accuracy 0.991.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24459328
Train loss (w/o reg) on all data: 0.2372749
Test loss (w/o reg) on all data: 0.102220505
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5454294e-05
Norm of the params: 12.098246
              Random: fixed  20 labels. Loss 0.10222. Accuracy 0.998.
### Flips: 410, rs: 0, checks: 410
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06630761
Train loss (w/o reg) on all data: 0.060275584
Test loss (w/o reg) on all data: 0.021161921
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0043522e-06
Norm of the params: 10.983651
     Influence (LOO): fixed 288 labels. Loss 0.02116. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051846434
Train loss (w/o reg) on all data: 0.002095114
Test loss (w/o reg) on all data: 0.0044906554
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7256527e-07
Norm of the params: 7.860699
                Loss: fixed 360 labels. Loss 0.00449. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23756522
Train loss (w/o reg) on all data: 0.2303418
Test loss (w/o reg) on all data: 0.09703169
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.9505205e-06
Norm of the params: 12.019497
              Random: fixed  36 labels. Loss 0.09703. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 615
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036535148
Train loss (w/o reg) on all data: 0.031735066
Test loss (w/o reg) on all data: 0.012031098
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9959552e-06
Norm of the params: 9.798042
     Influence (LOO): fixed 327 labels. Loss 0.01203. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601264
Test loss (w/o reg) on all data: 0.0026560964
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1399366e-08
Norm of the params: 6.092804
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23084138
Train loss (w/o reg) on all data: 0.22359115
Test loss (w/o reg) on all data: 0.0939563
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.2520763e-06
Norm of the params: 12.041786
              Random: fixed  48 labels. Loss 0.09396. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 820
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028479658
Train loss (w/o reg) on all data: 0.024165418
Test loss (w/o reg) on all data: 0.009117891
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2868575e-06
Norm of the params: 9.28896
     Influence (LOO): fixed 337 labels. Loss 0.00912. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560875
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1252175e-08
Norm of the params: 6.0928054
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22095974
Train loss (w/o reg) on all data: 0.21359016
Test loss (w/o reg) on all data: 0.09003835
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.549912e-05
Norm of the params: 12.140495
              Random: fixed  64 labels. Loss 0.09004. Accuracy 0.997.
### Flips: 410, rs: 0, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021611078
Train loss (w/o reg) on all data: 0.018191414
Test loss (w/o reg) on all data: 0.007345348
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0442336e-07
Norm of the params: 8.270022
     Influence (LOO): fixed 346 labels. Loss 0.00735. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601174
Test loss (w/o reg) on all data: 0.0026560794
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.999378e-08
Norm of the params: 6.092819
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21417393
Train loss (w/o reg) on all data: 0.20689332
Test loss (w/o reg) on all data: 0.085499056
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.1139148e-06
Norm of the params: 12.066983
              Random: fixed  79 labels. Loss 0.08550. Accuracy 0.999.
### Flips: 410, rs: 0, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013939117
Train loss (w/o reg) on all data: 0.010914678
Test loss (w/o reg) on all data: 0.00548902
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6708872e-07
Norm of the params: 7.7774544
     Influence (LOO): fixed 353 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560707
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5042599e-08
Norm of the params: 6.0928173
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20234562
Train loss (w/o reg) on all data: 0.19471772
Test loss (w/o reg) on all data: 0.081512906
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2574812e-05
Norm of the params: 12.351438
              Random: fixed  97 labels. Loss 0.08151. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25862622
Train loss (w/o reg) on all data: 0.2525715
Test loss (w/o reg) on all data: 0.09979753
Train acc on all data:  0.9136883053732069
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2164931e-05
Norm of the params: 11.004312
Flipped loss: 0.09980. Accuracy: 0.994
### Flips: 410, rs: 1, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13420631
Train loss (w/o reg) on all data: 0.12676823
Test loss (w/o reg) on all data: 0.05078655
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.1096272e-06
Norm of the params: 12.196789
     Influence (LOO): fixed 185 labels. Loss 0.05079. Accuracy 0.996.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09785621
Train loss (w/o reg) on all data: 0.08566165
Test loss (w/o reg) on all data: 0.04991578
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.8039811e-06
Norm of the params: 15.617014
                Loss: fixed 205 labels. Loss 0.04992. Accuracy 0.987.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24757746
Train loss (w/o reg) on all data: 0.24141659
Test loss (w/o reg) on all data: 0.09514955
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1550451e-05
Norm of the params: 11.100333
              Random: fixed  20 labels. Loss 0.09515. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 410
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06053633
Train loss (w/o reg) on all data: 0.05448155
Test loss (w/o reg) on all data: 0.02110532
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4570435e-06
Norm of the params: 11.004343
     Influence (LOO): fixed 293 labels. Loss 0.02111. Accuracy 0.997.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003922875
Train loss (w/o reg) on all data: 0.00144176
Test loss (w/o reg) on all data: 0.0034984222
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9747233e-08
Norm of the params: 7.0443096
                Loss: fixed 362 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23996203
Train loss (w/o reg) on all data: 0.23399083
Test loss (w/o reg) on all data: 0.09056494
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0508943e-05
Norm of the params: 10.9281225
              Random: fixed  37 labels. Loss 0.09056. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 615
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036397137
Train loss (w/o reg) on all data: 0.031761892
Test loss (w/o reg) on all data: 0.011444766
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1436892e-06
Norm of the params: 9.628336
     Influence (LOO): fixed 329 labels. Loss 0.01144. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162405
Train loss (w/o reg) on all data: 0.00096012285
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7778722e-08
Norm of the params: 6.0928116
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2336551
Train loss (w/o reg) on all data: 0.22759132
Test loss (w/o reg) on all data: 0.08852755
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.9304886e-06
Norm of the params: 11.012509
              Random: fixed  49 labels. Loss 0.08853. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 820
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023217816
Train loss (w/o reg) on all data: 0.019642439
Test loss (w/o reg) on all data: 0.006884773
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 8.530848e-07
Norm of the params: 8.456213
     Influence (LOO): fixed 344 labels. Loss 0.00688. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013024
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7376364e-08
Norm of the params: 6.0927978
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22070703
Train loss (w/o reg) on all data: 0.21448787
Test loss (w/o reg) on all data: 0.08339383
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.577973e-06
Norm of the params: 11.152724
              Random: fixed  71 labels. Loss 0.08339. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015237164
Train loss (w/o reg) on all data: 0.012444638
Test loss (w/o reg) on all data: 0.005293291
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 5.861574e-07
Norm of the params: 7.4733205
     Influence (LOO): fixed 352 labels. Loss 0.00529. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560626
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3153007e-08
Norm of the params: 6.0928044
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21171792
Train loss (w/o reg) on all data: 0.20561786
Test loss (w/o reg) on all data: 0.078739166
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.901641e-06
Norm of the params: 11.045415
              Random: fixed  89 labels. Loss 0.07874. Accuracy 0.994.
### Flips: 410, rs: 1, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010898982
Train loss (w/o reg) on all data: 0.008480981
Test loss (w/o reg) on all data: 0.0042993464
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6052916e-07
Norm of the params: 6.9541373
     Influence (LOO): fixed 356 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601226
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2835355e-08
Norm of the params: 6.092811
                Loss: fixed 364 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20208895
Train loss (w/o reg) on all data: 0.19585897
Test loss (w/o reg) on all data: 0.07495119
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.503241e-06
Norm of the params: 11.1624155
              Random: fixed 105 labels. Loss 0.07495. Accuracy 0.995.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25830045
Train loss (w/o reg) on all data: 0.25230798
Test loss (w/o reg) on all data: 0.10343826
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.5170375e-06
Norm of the params: 10.947587
Flipped loss: 0.10344. Accuracy: 0.998
### Flips: 410, rs: 2, checks: 205
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13530183
Train loss (w/o reg) on all data: 0.12746467
Test loss (w/o reg) on all data: 0.04925188
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1548345e-06
Norm of the params: 12.519711
     Influence (LOO): fixed 183 labels. Loss 0.04925. Accuracy 0.998.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094670855
Train loss (w/o reg) on all data: 0.081186466
Test loss (w/o reg) on all data: 0.050859205
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.2890628e-06
Norm of the params: 16.422173
                Loss: fixed 205 labels. Loss 0.05086. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2480415
Train loss (w/o reg) on all data: 0.24215403
Test loss (w/o reg) on all data: 0.09616817
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.189141e-06
Norm of the params: 10.85124
              Random: fixed  22 labels. Loss 0.09617. Accuracy 0.999.
### Flips: 410, rs: 2, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05551047
Train loss (w/o reg) on all data: 0.049580667
Test loss (w/o reg) on all data: 0.01952885
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2793679e-06
Norm of the params: 10.890179
     Influence (LOO): fixed 293 labels. Loss 0.01953. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00494643
Train loss (w/o reg) on all data: 0.0018304065
Test loss (w/o reg) on all data: 0.002939558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.274901e-07
Norm of the params: 7.8943324
                Loss: fixed 353 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24026887
Train loss (w/o reg) on all data: 0.2343221
Test loss (w/o reg) on all data: 0.09334626
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2539467e-05
Norm of the params: 10.905753
              Random: fixed  36 labels. Loss 0.09335. Accuracy 0.999.
### Flips: 410, rs: 2, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03403237
Train loss (w/o reg) on all data: 0.029644413
Test loss (w/o reg) on all data: 0.011336462
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 5.446441e-07
Norm of the params: 9.367986
     Influence (LOO): fixed 321 labels. Loss 0.01134. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004213181
Train loss (w/o reg) on all data: 0.0015274356
Test loss (w/o reg) on all data: 0.0027694916
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.117165e-08
Norm of the params: 7.3290453
                Loss: fixed 354 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22682106
Train loss (w/o reg) on all data: 0.22058284
Test loss (w/o reg) on all data: 0.08764404
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.3158637e-06
Norm of the params: 11.1698065
              Random: fixed  57 labels. Loss 0.08764. Accuracy 0.998.
### Flips: 410, rs: 2, checks: 820
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018264985
Train loss (w/o reg) on all data: 0.01457352
Test loss (w/o reg) on all data: 0.007419482
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2779386e-07
Norm of the params: 8.592399
     Influence (LOO): fixed 339 labels. Loss 0.00742. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.000960118
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6456484e-08
Norm of the params: 6.092818
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21983181
Train loss (w/o reg) on all data: 0.21347345
Test loss (w/o reg) on all data: 0.08282021
Train acc on all data:  0.9326525650376853
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.436444e-06
Norm of the params: 11.276834
              Random: fixed  71 labels. Loss 0.08282. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 1025
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0137434825
Train loss (w/o reg) on all data: 0.01055903
Test loss (w/o reg) on all data: 0.0056614727
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6235527e-07
Norm of the params: 7.9805417
     Influence (LOO): fixed 345 labels. Loss 0.00566. Accuracy 0.999.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096010946
Test loss (w/o reg) on all data: 0.0026560484
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1958495e-08
Norm of the params: 6.092833
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20902833
Train loss (w/o reg) on all data: 0.20278026
Test loss (w/o reg) on all data: 0.07799059
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3779326e-05
Norm of the params: 11.17861
              Random: fixed  89 labels. Loss 0.07799. Accuracy 0.997.
### Flips: 410, rs: 2, checks: 1230
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011455864
Train loss (w/o reg) on all data: 0.008622796
Test loss (w/o reg) on all data: 0.005164671
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.5674208e-07
Norm of the params: 7.527373
     Influence (LOO): fixed 348 labels. Loss 0.00516. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656045
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3022347e-08
Norm of the params: 6.092825
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19902143
Train loss (w/o reg) on all data: 0.19267622
Test loss (w/o reg) on all data: 0.07267795
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2772614e-06
Norm of the params: 11.265173
              Random: fixed 105 labels. Loss 0.07268. Accuracy 0.998.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25349134
Train loss (w/o reg) on all data: 0.24708457
Test loss (w/o reg) on all data: 0.10995053
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.5197585e-05
Norm of the params: 11.319681
Flipped loss: 0.10995. Accuracy: 0.989
### Flips: 410, rs: 3, checks: 205
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1303013
Train loss (w/o reg) on all data: 0.122977465
Test loss (w/o reg) on all data: 0.051216707
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.129854e-06
Norm of the params: 12.102751
     Influence (LOO): fixed 188 labels. Loss 0.05122. Accuracy 0.997.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09316138
Train loss (w/o reg) on all data: 0.07860265
Test loss (w/o reg) on all data: 0.055279415
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6503562e-06
Norm of the params: 17.06384
                Loss: fixed 205 labels. Loss 0.05528. Accuracy 0.986.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2458947
Train loss (w/o reg) on all data: 0.23933038
Test loss (w/o reg) on all data: 0.10506093
Train acc on all data:  0.9180646729880866
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.194149e-06
Norm of the params: 11.45803
              Random: fixed  14 labels. Loss 0.10506. Accuracy 0.991.
### Flips: 410, rs: 3, checks: 410
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057633705
Train loss (w/o reg) on all data: 0.05172163
Test loss (w/o reg) on all data: 0.021165796
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 1.156927e-06
Norm of the params: 10.873893
     Influence (LOO): fixed 289 labels. Loss 0.02117. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036910544
Train loss (w/o reg) on all data: 0.0013854748
Test loss (w/o reg) on all data: 0.003928767
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4304754e-08
Norm of the params: 6.7905517
                Loss: fixed 354 labels. Loss 0.00393. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23601922
Train loss (w/o reg) on all data: 0.22933416
Test loss (w/o reg) on all data: 0.0976306
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.935454e-06
Norm of the params: 11.562927
              Random: fixed  34 labels. Loss 0.09763. Accuracy 0.990.
### Flips: 410, rs: 3, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030733708
Train loss (w/o reg) on all data: 0.02622241
Test loss (w/o reg) on all data: 0.010734413
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6199736e-07
Norm of the params: 9.4987335
     Influence (LOO): fixed 326 labels. Loss 0.01073. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215604
Train loss (w/o reg) on all data: 0.0011097618
Test loss (w/o reg) on all data: 0.003636158
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3475931e-08
Norm of the params: 6.1835246
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22426838
Train loss (w/o reg) on all data: 0.21722792
Test loss (w/o reg) on all data: 0.091780216
Train acc on all data:  0.9273036712861659
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.142248e-06
Norm of the params: 11.866299
              Random: fixed  53 labels. Loss 0.09178. Accuracy 0.991.
### Flips: 410, rs: 3, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023745526
Train loss (w/o reg) on all data: 0.0198362
Test loss (w/o reg) on all data: 0.008700648
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9584197e-07
Norm of the params: 8.842314
     Influence (LOO): fixed 334 labels. Loss 0.00870. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215606
Train loss (w/o reg) on all data: 0.001109767
Test loss (w/o reg) on all data: 0.0036361704
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4006531e-08
Norm of the params: 6.1835155
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21144046
Train loss (w/o reg) on all data: 0.20444487
Test loss (w/o reg) on all data: 0.08263318
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3539791e-05
Norm of the params: 11.828428
              Random: fixed  76 labels. Loss 0.08263. Accuracy 0.994.
### Flips: 410, rs: 3, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017742459
Train loss (w/o reg) on all data: 0.014550173
Test loss (w/o reg) on all data: 0.0062349266
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 8.42047e-07
Norm of the params: 7.990351
     Influence (LOO): fixed 342 labels. Loss 0.00623. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003021561
Train loss (w/o reg) on all data: 0.0011097529
Test loss (w/o reg) on all data: 0.003636184
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7293419e-08
Norm of the params: 6.18354
                Loss: fixed 355 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19539934
Train loss (w/o reg) on all data: 0.18812588
Test loss (w/o reg) on all data: 0.076274164
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.129854e-06
Norm of the params: 12.0610695
              Random: fixed 101 labels. Loss 0.07627. Accuracy 0.994.
### Flips: 410, rs: 3, checks: 1230
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014840838
Train loss (w/o reg) on all data: 0.011649381
Test loss (w/o reg) on all data: 0.005507901
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 5.057419e-07
Norm of the params: 7.989313
     Influence (LOO): fixed 345 labels. Loss 0.00551. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560747
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.992155e-08
Norm of the params: 6.092813
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18813469
Train loss (w/o reg) on all data: 0.1809695
Test loss (w/o reg) on all data: 0.07208275
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.1074637e-06
Norm of the params: 11.970946
              Random: fixed 115 labels. Loss 0.07208. Accuracy 0.995.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2551855
Train loss (w/o reg) on all data: 0.2477695
Test loss (w/o reg) on all data: 0.10506089
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.12041e-06
Norm of the params: 12.178689
Flipped loss: 0.10506. Accuracy: 0.997
### Flips: 410, rs: 4, checks: 205
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13221157
Train loss (w/o reg) on all data: 0.12461631
Test loss (w/o reg) on all data: 0.047705904
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.995539e-06
Norm of the params: 12.324985
     Influence (LOO): fixed 187 labels. Loss 0.04771. Accuracy 0.999.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08951993
Train loss (w/o reg) on all data: 0.07578612
Test loss (w/o reg) on all data: 0.049734544
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.6029096e-06
Norm of the params: 16.57336
                Loss: fixed 205 labels. Loss 0.04973. Accuracy 0.990.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24005543
Train loss (w/o reg) on all data: 0.23264185
Test loss (w/o reg) on all data: 0.097127825
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2331367e-05
Norm of the params: 12.17668
              Random: fixed  28 labels. Loss 0.09713. Accuracy 0.996.
### Flips: 410, rs: 4, checks: 410
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056454163
Train loss (w/o reg) on all data: 0.051382735
Test loss (w/o reg) on all data: 0.017217526
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1321572e-06
Norm of the params: 10.071175
     Influence (LOO): fixed 289 labels. Loss 0.01722. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044024875
Train loss (w/o reg) on all data: 0.0016265044
Test loss (w/o reg) on all data: 0.0034203057
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.397427e-08
Norm of the params: 7.451152
                Loss: fixed 346 labels. Loss 0.00342. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23010626
Train loss (w/o reg) on all data: 0.22256093
Test loss (w/o reg) on all data: 0.09229041
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.927061e-06
Norm of the params: 12.284414
              Random: fixed  45 labels. Loss 0.09229. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03412841
Train loss (w/o reg) on all data: 0.029648872
Test loss (w/o reg) on all data: 0.011435192
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7835804e-07
Norm of the params: 9.46524
     Influence (LOO): fixed 313 labels. Loss 0.01144. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009600618
Test loss (w/o reg) on all data: 0.0026560167
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.364404e-08
Norm of the params: 6.09291
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22020397
Train loss (w/o reg) on all data: 0.21263748
Test loss (w/o reg) on all data: 0.087850556
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.411059e-06
Norm of the params: 12.301616
              Random: fixed  62 labels. Loss 0.08785. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019171745
Train loss (w/o reg) on all data: 0.015713526
Test loss (w/o reg) on all data: 0.0061681555
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0371539e-07
Norm of the params: 8.316513
     Influence (LOO): fixed 332 labels. Loss 0.00617. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6603077e-08
Norm of the params: 6.0928073
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20818183
Train loss (w/o reg) on all data: 0.20061333
Test loss (w/o reg) on all data: 0.08026649
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.987085e-06
Norm of the params: 12.303241
              Random: fixed  85 labels. Loss 0.08027. Accuracy 0.995.
### Flips: 410, rs: 4, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018230796
Train loss (w/o reg) on all data: 0.01486977
Test loss (w/o reg) on all data: 0.0057230974
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1517665e-07
Norm of the params: 8.198813
     Influence (LOO): fixed 333 labels. Loss 0.00572. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.0026560838
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4861585e-08
Norm of the params: 6.0928073
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1981974
Train loss (w/o reg) on all data: 0.19081151
Test loss (w/o reg) on all data: 0.07478207
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.068085e-06
Norm of the params: 12.153907
              Random: fixed 105 labels. Loss 0.07478. Accuracy 0.996.
### Flips: 410, rs: 4, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014088709
Train loss (w/o reg) on all data: 0.010883041
Test loss (w/o reg) on all data: 0.004572537
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7848454e-07
Norm of the params: 8.007083
     Influence (LOO): fixed 337 labels. Loss 0.00457. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011814
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2982443e-08
Norm of the params: 6.0928173
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19057003
Train loss (w/o reg) on all data: 0.18311961
Test loss (w/o reg) on all data: 0.071328945
Train acc on all data:  0.9452954048140044
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.4941088e-05
Norm of the params: 12.2069
              Random: fixed 119 labels. Loss 0.07133. Accuracy 0.996.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26561734
Train loss (w/o reg) on all data: 0.25907508
Test loss (w/o reg) on all data: 0.11129985
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.073754e-06
Norm of the params: 11.438757
Flipped loss: 0.11130. Accuracy: 0.992
### Flips: 410, rs: 5, checks: 205
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13873862
Train loss (w/o reg) on all data: 0.1309015
Test loss (w/o reg) on all data: 0.053143278
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.152817e-06
Norm of the params: 12.519679
     Influence (LOO): fixed 192 labels. Loss 0.05314. Accuracy 0.997.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10560593
Train loss (w/o reg) on all data: 0.09196085
Test loss (w/o reg) on all data: 0.0562249
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.5275636e-06
Norm of the params: 16.519737
                Loss: fixed 205 labels. Loss 0.05622. Accuracy 0.986.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2563793
Train loss (w/o reg) on all data: 0.24945195
Test loss (w/o reg) on all data: 0.106644936
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.252399e-06
Norm of the params: 11.770599
              Random: fixed  15 labels. Loss 0.10664. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 410
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065506056
Train loss (w/o reg) on all data: 0.0597877
Test loss (w/o reg) on all data: 0.022748234
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6896762e-06
Norm of the params: 10.694259
     Influence (LOO): fixed 298 labels. Loss 0.02275. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053693987
Train loss (w/o reg) on all data: 0.0020991317
Test loss (w/o reg) on all data: 0.004326598
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.037658e-08
Norm of the params: 8.0873575
                Loss: fixed 360 labels. Loss 0.00433. Accuracy 0.999.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24615759
Train loss (w/o reg) on all data: 0.2392386
Test loss (w/o reg) on all data: 0.100449346
Train acc on all data:  0.9212253829321663
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0163299e-05
Norm of the params: 11.763491
              Random: fixed  35 labels. Loss 0.10045. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034503758
Train loss (w/o reg) on all data: 0.030089479
Test loss (w/o reg) on all data: 0.010770337
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1389803e-07
Norm of the params: 9.396042
     Influence (LOO): fixed 335 labels. Loss 0.01077. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003956434
Train loss (w/o reg) on all data: 0.0014328536
Test loss (w/o reg) on all data: 0.0027366797
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2798365e-08
Norm of the params: 7.1043367
                Loss: fixed 364 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23716065
Train loss (w/o reg) on all data: 0.23020992
Test loss (w/o reg) on all data: 0.096931435
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.235566e-05
Norm of the params: 11.790448
              Random: fixed  50 labels. Loss 0.09693. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023849845
Train loss (w/o reg) on all data: 0.01994175
Test loss (w/o reg) on all data: 0.00794856
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0907866e-07
Norm of the params: 8.840922
     Influence (LOO): fixed 346 labels. Loss 0.00795. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033851552
Train loss (w/o reg) on all data: 0.0011814169
Test loss (w/o reg) on all data: 0.0026828814
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0536918e-08
Norm of the params: 6.638883
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22469766
Train loss (w/o reg) on all data: 0.21768865
Test loss (w/o reg) on all data: 0.091372
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.3592995e-06
Norm of the params: 11.839772
              Random: fixed  71 labels. Loss 0.09137. Accuracy 0.990.
### Flips: 410, rs: 5, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017598504
Train loss (w/o reg) on all data: 0.01417165
Test loss (w/o reg) on all data: 0.0069286297
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0208931e-07
Norm of the params: 8.278712
     Influence (LOO): fixed 351 labels. Loss 0.00693. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003385156
Train loss (w/o reg) on all data: 0.0011814034
Test loss (w/o reg) on all data: 0.0026828905
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6899983e-08
Norm of the params: 6.638904
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21205805
Train loss (w/o reg) on all data: 0.20444314
Test loss (w/o reg) on all data: 0.08569933
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.9351654e-06
Norm of the params: 12.340912
              Random: fixed  91 labels. Loss 0.08570. Accuracy 0.991.
### Flips: 410, rs: 5, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013721576
Train loss (w/o reg) on all data: 0.010820518
Test loss (w/o reg) on all data: 0.005500337
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.75208e-07
Norm of the params: 7.617162
     Influence (LOO): fixed 356 labels. Loss 0.00550. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033851555
Train loss (w/o reg) on all data: 0.0011814303
Test loss (w/o reg) on all data: 0.0026828966
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3006895e-08
Norm of the params: 6.638863
                Loss: fixed 365 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20286204
Train loss (w/o reg) on all data: 0.19502361
Test loss (w/o reg) on all data: 0.08227229
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1330026e-05
Norm of the params: 12.520727
              Random: fixed 106 labels. Loss 0.08227. Accuracy 0.990.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24926001
Train loss (w/o reg) on all data: 0.24184582
Test loss (w/o reg) on all data: 0.09720137
Train acc on all data:  0.9180646729880866
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1146099e-05
Norm of the params: 12.177189
Flipped loss: 0.09720. Accuracy: 0.997
### Flips: 410, rs: 6, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12681569
Train loss (w/o reg) on all data: 0.11874727
Test loss (w/o reg) on all data: 0.044391397
Train acc on all data:  0.962314612205203
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8505285e-06
Norm of the params: 12.703083
     Influence (LOO): fixed 187 labels. Loss 0.04439. Accuracy 1.000.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088542975
Train loss (w/o reg) on all data: 0.07494849
Test loss (w/o reg) on all data: 0.040766653
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9264747e-06
Norm of the params: 16.489079
                Loss: fixed 205 labels. Loss 0.04077. Accuracy 0.991.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24193045
Train loss (w/o reg) on all data: 0.23454101
Test loss (w/o reg) on all data: 0.093500435
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0772815e-06
Norm of the params: 12.156843
              Random: fixed  13 labels. Loss 0.09350. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053272627
Train loss (w/o reg) on all data: 0.04768785
Test loss (w/o reg) on all data: 0.017203182
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1489393e-06
Norm of the params: 10.568611
     Influence (LOO): fixed 287 labels. Loss 0.01720. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030876363
Train loss (w/o reg) on all data: 0.0011732262
Test loss (w/o reg) on all data: 0.0034910596
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6848682e-08
Norm of the params: 6.1877465
                Loss: fixed 343 labels. Loss 0.00349. Accuracy 0.999.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2329195
Train loss (w/o reg) on all data: 0.22504129
Test loss (w/o reg) on all data: 0.09042545
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1392383e-05
Norm of the params: 12.552464
              Random: fixed  27 labels. Loss 0.09043. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 615
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030077577
Train loss (w/o reg) on all data: 0.025683774
Test loss (w/o reg) on all data: 0.009943739
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1032486e-06
Norm of the params: 9.374224
     Influence (LOO): fixed 316 labels. Loss 0.00994. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601049
Test loss (w/o reg) on all data: 0.002656041
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5826461e-08
Norm of the params: 6.0928392
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22026765
Train loss (w/o reg) on all data: 0.21239218
Test loss (w/o reg) on all data: 0.08405248
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.671283e-06
Norm of the params: 12.5502825
              Random: fixed  48 labels. Loss 0.08405. Accuracy 0.997.
### Flips: 410, rs: 6, checks: 820
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020893287
Train loss (w/o reg) on all data: 0.01740151
Test loss (w/o reg) on all data: 0.007111695
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 4.371058e-07
Norm of the params: 8.356766
     Influence (LOO): fixed 326 labels. Loss 0.00711. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601179
Test loss (w/o reg) on all data: 0.002656066
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9108068e-08
Norm of the params: 6.0928183
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21022092
Train loss (w/o reg) on all data: 0.20205383
Test loss (w/o reg) on all data: 0.08028014
Train acc on all data:  0.9331388281060053
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0005826e-05
Norm of the params: 12.780521
              Random: fixed  64 labels. Loss 0.08028. Accuracy 1.000.
### Flips: 410, rs: 6, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019053467
Train loss (w/o reg) on all data: 0.015647057
Test loss (w/o reg) on all data: 0.00679639
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0359059e-06
Norm of the params: 8.253981
     Influence (LOO): fixed 328 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.543897e-08
Norm of the params: 6.092816
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2014307
Train loss (w/o reg) on all data: 0.19335607
Test loss (w/o reg) on all data: 0.07522045
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.4694367e-05
Norm of the params: 12.707971
              Random: fixed  81 labels. Loss 0.07522. Accuracy 0.999.
### Flips: 410, rs: 6, checks: 1230
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011262059
Train loss (w/o reg) on all data: 0.008743714
Test loss (w/o reg) on all data: 0.0044752113
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0401663e-07
Norm of the params: 7.096966
     Influence (LOO): fixed 336 labels. Loss 0.00448. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010976
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9536435e-08
Norm of the params: 6.0928316
                Loss: fixed 344 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18912181
Train loss (w/o reg) on all data: 0.18111242
Test loss (w/o reg) on all data: 0.07033256
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.2229e-06
Norm of the params: 12.656529
              Random: fixed 100 labels. Loss 0.07033. Accuracy 0.998.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25872687
Train loss (w/o reg) on all data: 0.25095102
Test loss (w/o reg) on all data: 0.105985746
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4203169e-05
Norm of the params: 12.470644
Flipped loss: 0.10599. Accuracy: 0.991
### Flips: 410, rs: 7, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13766958
Train loss (w/o reg) on all data: 0.12919296
Test loss (w/o reg) on all data: 0.04855617
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.3462067e-06
Norm of the params: 13.020459
     Influence (LOO): fixed 186 labels. Loss 0.04856. Accuracy 0.995.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10214672
Train loss (w/o reg) on all data: 0.08661491
Test loss (w/o reg) on all data: 0.04867358
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.740381e-06
Norm of the params: 17.624878
                Loss: fixed 205 labels. Loss 0.04867. Accuracy 0.987.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2478135
Train loss (w/o reg) on all data: 0.23996277
Test loss (w/o reg) on all data: 0.10053801
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.5274736e-06
Norm of the params: 12.530542
              Random: fixed  20 labels. Loss 0.10054. Accuracy 0.991.
### Flips: 410, rs: 7, checks: 410
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063230895
Train loss (w/o reg) on all data: 0.057293825
Test loss (w/o reg) on all data: 0.019736791
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9419075e-06
Norm of the params: 10.896855
     Influence (LOO): fixed 289 labels. Loss 0.01974. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0045051407
Train loss (w/o reg) on all data: 0.0016956896
Test loss (w/o reg) on all data: 0.0033519873
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.430138e-08
Norm of the params: 7.495934
                Loss: fixed 356 labels. Loss 0.00335. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2377275
Train loss (w/o reg) on all data: 0.23010406
Test loss (w/o reg) on all data: 0.09568931
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.0220887e-06
Norm of the params: 12.347826
              Random: fixed  40 labels. Loss 0.09569. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 615
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033242594
Train loss (w/o reg) on all data: 0.02833054
Test loss (w/o reg) on all data: 0.010517066
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7256322e-07
Norm of the params: 9.911663
     Influence (LOO): fixed 326 labels. Loss 0.01052. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012856
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5317483e-08
Norm of the params: 6.0928
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22850865
Train loss (w/o reg) on all data: 0.2207529
Test loss (w/o reg) on all data: 0.09081966
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.646561e-06
Norm of the params: 12.454522
              Random: fixed  57 labels. Loss 0.09082. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024869734
Train loss (w/o reg) on all data: 0.020640604
Test loss (w/o reg) on all data: 0.0076960605
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1692375e-06
Norm of the params: 9.19688
     Influence (LOO): fixed 337 labels. Loss 0.00770. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.185351e-08
Norm of the params: 6.0928173
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2198059
Train loss (w/o reg) on all data: 0.21175839
Test loss (w/o reg) on all data: 0.08887551
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.8488076e-06
Norm of the params: 12.686609
              Random: fixed  72 labels. Loss 0.08888. Accuracy 0.991.
### Flips: 410, rs: 7, checks: 1025
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016052507
Train loss (w/o reg) on all data: 0.013031181
Test loss (w/o reg) on all data: 0.005488721
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.704746e-07
Norm of the params: 7.7734494
     Influence (LOO): fixed 347 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601069
Test loss (w/o reg) on all data: 0.002656012
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.17659134e-07
Norm of the params: 6.0928364
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20982409
Train loss (w/o reg) on all data: 0.20171875
Test loss (w/o reg) on all data: 0.08469228
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2670647e-05
Norm of the params: 12.732116
              Random: fixed  87 labels. Loss 0.08469. Accuracy 0.993.
### Flips: 410, rs: 7, checks: 1230
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008600752
Train loss (w/o reg) on all data: 0.006122649
Test loss (w/o reg) on all data: 0.0037523431
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3459811e-07
Norm of the params: 7.040032
     Influence (LOO): fixed 354 labels. Loss 0.00375. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012966
Test loss (w/o reg) on all data: 0.0026560945
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4376943e-08
Norm of the params: 6.092799
                Loss: fixed 359 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19675033
Train loss (w/o reg) on all data: 0.18882538
Test loss (w/o reg) on all data: 0.076124586
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3295345e-05
Norm of the params: 12.589634
              Random: fixed 111 labels. Loss 0.07612. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2674558
Train loss (w/o reg) on all data: 0.2608183
Test loss (w/o reg) on all data: 0.11405583
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7288901e-05
Norm of the params: 11.5217
Flipped loss: 0.11406. Accuracy: 0.991
### Flips: 410, rs: 8, checks: 205
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1404631
Train loss (w/o reg) on all data: 0.1328421
Test loss (w/o reg) on all data: 0.051402498
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 8.682295e-06
Norm of the params: 12.345849
     Influence (LOO): fixed 186 labels. Loss 0.05140. Accuracy 1.000.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10980301
Train loss (w/o reg) on all data: 0.09603023
Test loss (w/o reg) on all data: 0.06303755
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.542552e-06
Norm of the params: 16.596857
                Loss: fixed 205 labels. Loss 0.06304. Accuracy 0.980.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25829247
Train loss (w/o reg) on all data: 0.2515882
Test loss (w/o reg) on all data: 0.10501762
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0833722e-05
Norm of the params: 11.579516
              Random: fixed  19 labels. Loss 0.10502. Accuracy 0.994.
### Flips: 410, rs: 8, checks: 410
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07308923
Train loss (w/o reg) on all data: 0.066555835
Test loss (w/o reg) on all data: 0.023928728
Train acc on all data:  0.9795769511305616
Test acc on all data:   1.0
Norm of the mean of gradients: 2.723342e-06
Norm of the params: 11.431002
     Influence (LOO): fixed 283 labels. Loss 0.02393. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004793172
Train loss (w/o reg) on all data: 0.0018280229
Test loss (w/o reg) on all data: 0.0028480175
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.389044e-08
Norm of the params: 7.7008424
                Loss: fixed 364 labels. Loss 0.00285. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2475658
Train loss (w/o reg) on all data: 0.2404353
Test loss (w/o reg) on all data: 0.10136002
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.726356e-06
Norm of the params: 11.941944
              Random: fixed  39 labels. Loss 0.10136. Accuracy 0.995.
### Flips: 410, rs: 8, checks: 615
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042018835
Train loss (w/o reg) on all data: 0.036631417
Test loss (w/o reg) on all data: 0.012820052
Train acc on all data:  0.9888159494286409
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0993377e-06
Norm of the params: 10.38019
     Influence (LOO): fixed 322 labels. Loss 0.01282. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362238
Train loss (w/o reg) on all data: 0.0011012006
Test loss (w/o reg) on all data: 0.0029767947
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9011715e-08
Norm of the params: 6.2209697
                Loss: fixed 367 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23607427
Train loss (w/o reg) on all data: 0.2288576
Test loss (w/o reg) on all data: 0.096494295
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3410675e-05
Norm of the params: 12.013875
              Random: fixed  58 labels. Loss 0.09649. Accuracy 0.994.
### Flips: 410, rs: 8, checks: 820
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03043916
Train loss (w/o reg) on all data: 0.02555912
Test loss (w/o reg) on all data: 0.0091750305
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3935888e-06
Norm of the params: 9.879313
     Influence (LOO): fixed 335 labels. Loss 0.00918. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362234
Train loss (w/o reg) on all data: 0.001101186
Test loss (w/o reg) on all data: 0.0029767468
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1771277e-08
Norm of the params: 6.220993
                Loss: fixed 367 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22651874
Train loss (w/o reg) on all data: 0.21927977
Test loss (w/o reg) on all data: 0.08926321
Train acc on all data:  0.9307075127644056
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.9776097e-06
Norm of the params: 12.032431
              Random: fixed  76 labels. Loss 0.08926. Accuracy 0.995.
### Flips: 410, rs: 8, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023780392
Train loss (w/o reg) on all data: 0.01936193
Test loss (w/o reg) on all data: 0.0074190605
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4123352e-06
Norm of the params: 9.400492
     Influence (LOO): fixed 346 labels. Loss 0.00742. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601184
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1787191e-08
Norm of the params: 6.0928173
                Loss: fixed 368 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21547769
Train loss (w/o reg) on all data: 0.20797038
Test loss (w/o reg) on all data: 0.08328582
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.703114e-06
Norm of the params: 12.253419
              Random: fixed  95 labels. Loss 0.08329. Accuracy 0.997.
### Flips: 410, rs: 8, checks: 1230
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013425832
Train loss (w/o reg) on all data: 0.009664701
Test loss (w/o reg) on all data: 0.0062467805
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0603454e-07
Norm of the params: 8.673097
     Influence (LOO): fixed 356 labels. Loss 0.00625. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601315
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3917169e-08
Norm of the params: 6.0927963
                Loss: fixed 368 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20315722
Train loss (w/o reg) on all data: 0.19516347
Test loss (w/o reg) on all data: 0.078828804
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0556141e-05
Norm of the params: 12.644164
              Random: fixed 114 labels. Loss 0.07883. Accuracy 0.997.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25410336
Train loss (w/o reg) on all data: 0.2469687
Test loss (w/o reg) on all data: 0.1127436
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.9427892e-06
Norm of the params: 11.945424
Flipped loss: 0.11274. Accuracy: 0.991
### Flips: 410, rs: 9, checks: 205
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162269
Train loss (w/o reg) on all data: 0.12396108
Test loss (w/o reg) on all data: 0.048317563
Train acc on all data:  0.9598832968636032
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0023801e-06
Norm of the params: 12.378696
     Influence (LOO): fixed 183 labels. Loss 0.04832. Accuracy 1.000.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09498345
Train loss (w/o reg) on all data: 0.082210645
Test loss (w/o reg) on all data: 0.06307003
Train acc on all data:  0.9688791636275225
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.093052e-06
Norm of the params: 15.982995
                Loss: fixed 205 labels. Loss 0.06307. Accuracy 0.984.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2445014
Train loss (w/o reg) on all data: 0.23753862
Test loss (w/o reg) on all data: 0.10675287
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.586392e-05
Norm of the params: 11.8006525
              Random: fixed  19 labels. Loss 0.10675. Accuracy 0.989.
### Flips: 410, rs: 9, checks: 410
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065983824
Train loss (w/o reg) on all data: 0.060406175
Test loss (w/o reg) on all data: 0.021034718
Train acc on all data:  0.9815220034038414
Test acc on all data:   1.0
Norm of the mean of gradients: 2.002588e-06
Norm of the params: 10.561868
     Influence (LOO): fixed 276 labels. Loss 0.02103. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039115977
Train loss (w/o reg) on all data: 0.0014237999
Test loss (w/o reg) on all data: 0.0030563816
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.00728e-08
Norm of the params: 7.0537906
                Loss: fixed 350 labels. Loss 0.00306. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23593026
Train loss (w/o reg) on all data: 0.22913688
Test loss (w/o reg) on all data: 0.09965847
Train acc on all data:  0.9238998298079261
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.696591e-06
Norm of the params: 11.656222
              Random: fixed  35 labels. Loss 0.09966. Accuracy 0.993.
### Flips: 410, rs: 9, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031906396
Train loss (w/o reg) on all data: 0.027977232
Test loss (w/o reg) on all data: 0.009951627
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 9.510859e-07
Norm of the params: 8.864721
     Influence (LOO): fixed 319 labels. Loss 0.00995. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003568763
Train loss (w/o reg) on all data: 0.0012876174
Test loss (w/o reg) on all data: 0.002966254
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1328484e-08
Norm of the params: 6.7544737
                Loss: fixed 351 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22412612
Train loss (w/o reg) on all data: 0.21708584
Test loss (w/o reg) on all data: 0.0941211
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.6286766e-06
Norm of the params: 11.866148
              Random: fixed  57 labels. Loss 0.09412. Accuracy 0.993.
### Flips: 410, rs: 9, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020153126
Train loss (w/o reg) on all data: 0.016690176
Test loss (w/o reg) on all data: 0.0070985416
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.017998e-07
Norm of the params: 8.322199
     Influence (LOO): fixed 332 labels. Loss 0.00710. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601106
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.088401e-08
Norm of the params: 6.09283
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21554361
Train loss (w/o reg) on all data: 0.20835356
Test loss (w/o reg) on all data: 0.09257937
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3659386e-05
Norm of the params: 11.991699
              Random: fixed  70 labels. Loss 0.09258. Accuracy 0.994.
### Flips: 410, rs: 9, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017565424
Train loss (w/o reg) on all data: 0.014431402
Test loss (w/o reg) on all data: 0.006150506
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 4.199295e-07
Norm of the params: 7.9170966
     Influence (LOO): fixed 336 labels. Loss 0.00615. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2509418e-08
Norm of the params: 6.092819
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20713645
Train loss (w/o reg) on all data: 0.20003118
Test loss (w/o reg) on all data: 0.087262034
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.3190353e-06
Norm of the params: 11.920808
              Random: fixed  85 labels. Loss 0.08726. Accuracy 0.995.
### Flips: 410, rs: 9, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013036148
Train loss (w/o reg) on all data: 0.010354011
Test loss (w/o reg) on all data: 0.004653919
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 5.91036e-07
Norm of the params: 7.324119
     Influence (LOO): fixed 342 labels. Loss 0.00465. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011424
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9263565e-09
Norm of the params: 6.0928245
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19235231
Train loss (w/o reg) on all data: 0.1848912
Test loss (w/o reg) on all data: 0.081101246
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.415073e-05
Norm of the params: 12.215661
              Random: fixed 108 labels. Loss 0.08110. Accuracy 0.991.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26166177
Train loss (w/o reg) on all data: 0.25554898
Test loss (w/o reg) on all data: 0.10660727
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.526223e-06
Norm of the params: 11.056919
Flipped loss: 0.10661. Accuracy: 0.994
### Flips: 410, rs: 10, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13290103
Train loss (w/o reg) on all data: 0.1253525
Test loss (w/o reg) on all data: 0.0482012
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.7715037e-06
Norm of the params: 12.287008
     Influence (LOO): fixed 192 labels. Loss 0.04820. Accuracy 0.999.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101995654
Train loss (w/o reg) on all data: 0.08994317
Test loss (w/o reg) on all data: 0.0536952
Train acc on all data:  0.9652321906151228
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.055021e-06
Norm of the params: 15.525776
                Loss: fixed 205 labels. Loss 0.05370. Accuracy 0.988.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25438538
Train loss (w/o reg) on all data: 0.2480981
Test loss (w/o reg) on all data: 0.102665134
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0276858e-05
Norm of the params: 11.2136345
              Random: fixed  14 labels. Loss 0.10267. Accuracy 0.994.
### Flips: 410, rs: 10, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06467296
Train loss (w/o reg) on all data: 0.058659736
Test loss (w/o reg) on all data: 0.020363286
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0978672e-06
Norm of the params: 10.966516
     Influence (LOO): fixed 285 labels. Loss 0.02036. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036582989
Train loss (w/o reg) on all data: 0.0013216871
Test loss (w/o reg) on all data: 0.0027576704
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8408412e-08
Norm of the params: 6.8360977
                Loss: fixed 356 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24275084
Train loss (w/o reg) on all data: 0.2359959
Test loss (w/o reg) on all data: 0.09833892
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.877843e-06
Norm of the params: 11.623201
              Random: fixed  32 labels. Loss 0.09834. Accuracy 0.993.
### Flips: 410, rs: 10, checks: 615
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03681906
Train loss (w/o reg) on all data: 0.032404795
Test loss (w/o reg) on all data: 0.011715905
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.0538866e-07
Norm of the params: 9.396024
     Influence (LOO): fixed 321 labels. Loss 0.01172. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601095
Test loss (w/o reg) on all data: 0.0026560437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0348352e-08
Norm of the params: 6.092832
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23393074
Train loss (w/o reg) on all data: 0.22699465
Test loss (w/o reg) on all data: 0.09423596
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.3707542e-05
Norm of the params: 11.778026
              Random: fixed  48 labels. Loss 0.09424. Accuracy 0.993.
### Flips: 410, rs: 10, checks: 820
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024954291
Train loss (w/o reg) on all data: 0.021547956
Test loss (w/o reg) on all data: 0.008080009
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3724845e-07
Norm of the params: 8.253889
     Influence (LOO): fixed 334 labels. Loss 0.00808. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012495
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8419045e-08
Norm of the params: 6.0928063
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22339454
Train loss (w/o reg) on all data: 0.21634896
Test loss (w/o reg) on all data: 0.09078423
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3810866e-05
Norm of the params: 11.87062
              Random: fixed  67 labels. Loss 0.09078. Accuracy 0.994.
### Flips: 410, rs: 10, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015792727
Train loss (w/o reg) on all data: 0.012723757
Test loss (w/o reg) on all data: 0.0064069196
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.941751e-07
Norm of the params: 7.8345027
     Influence (LOO): fixed 344 labels. Loss 0.00641. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601169
Test loss (w/o reg) on all data: 0.0026560687
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0992249e-08
Norm of the params: 6.092819
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20364778
Train loss (w/o reg) on all data: 0.19620498
Test loss (w/o reg) on all data: 0.08355314
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8256566e-06
Norm of the params: 12.200655
              Random: fixed  97 labels. Loss 0.08355. Accuracy 0.995.
### Flips: 410, rs: 10, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012424065
Train loss (w/o reg) on all data: 0.009636871
Test loss (w/o reg) on all data: 0.004619457
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3255829e-07
Norm of the params: 7.4661837
     Influence (LOO): fixed 348 labels. Loss 0.00462. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012384
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.196809e-08
Norm of the params: 6.0928087
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19383927
Train loss (w/o reg) on all data: 0.18615887
Test loss (w/o reg) on all data: 0.079391055
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.514407e-06
Norm of the params: 12.393868
              Random: fixed 114 labels. Loss 0.07939. Accuracy 0.994.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25515696
Train loss (w/o reg) on all data: 0.24793547
Test loss (w/o reg) on all data: 0.102636695
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4302984e-05
Norm of the params: 12.017884
Flipped loss: 0.10264. Accuracy: 0.993
### Flips: 410, rs: 11, checks: 205
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12655441
Train loss (w/o reg) on all data: 0.1190021
Test loss (w/o reg) on all data: 0.047797136
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.78982e-06
Norm of the params: 12.290098
     Influence (LOO): fixed 188 labels. Loss 0.04780. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09531691
Train loss (w/o reg) on all data: 0.08221996
Test loss (w/o reg) on all data: 0.047168892
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0274392e-05
Norm of the params: 16.184532
                Loss: fixed 205 labels. Loss 0.04717. Accuracy 0.989.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2443588
Train loss (w/o reg) on all data: 0.23716421
Test loss (w/o reg) on all data: 0.096290275
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.1137855e-05
Norm of the params: 11.995483
              Random: fixed  22 labels. Loss 0.09629. Accuracy 0.993.
### Flips: 410, rs: 11, checks: 410
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059211984
Train loss (w/o reg) on all data: 0.05353714
Test loss (w/o reg) on all data: 0.020565156
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.6329395e-07
Norm of the params: 10.653491
     Influence (LOO): fixed 283 labels. Loss 0.02057. Accuracy 0.997.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004058118
Train loss (w/o reg) on all data: 0.0016741744
Test loss (w/o reg) on all data: 0.006336596
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5692857e-08
Norm of the params: 6.9049892
                Loss: fixed 347 labels. Loss 0.00634. Accuracy 0.998.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2344961
Train loss (w/o reg) on all data: 0.22747758
Test loss (w/o reg) on all data: 0.0914016
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.1676875e-06
Norm of the params: 11.847798
              Random: fixed  40 labels. Loss 0.09140. Accuracy 0.995.
### Flips: 410, rs: 11, checks: 615
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030928846
Train loss (w/o reg) on all data: 0.026270214
Test loss (w/o reg) on all data: 0.010373851
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.609001e-07
Norm of the params: 9.652597
     Influence (LOO): fixed 319 labels. Loss 0.01037. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004080574
Train loss (w/o reg) on all data: 0.0017832168
Test loss (w/o reg) on all data: 0.0051734773
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1398664e-08
Norm of the params: 6.778433
                Loss: fixed 348 labels. Loss 0.00517. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22787213
Train loss (w/o reg) on all data: 0.22073781
Test loss (w/o reg) on all data: 0.08766448
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0156698e-05
Norm of the params: 11.945133
              Random: fixed  52 labels. Loss 0.08766. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02236653
Train loss (w/o reg) on all data: 0.018633006
Test loss (w/o reg) on all data: 0.0072230105
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6567432e-07
Norm of the params: 8.641209
     Influence (LOO): fixed 333 labels. Loss 0.00722. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601138
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1219576e-08
Norm of the params: 6.092824
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21544637
Train loss (w/o reg) on all data: 0.20827948
Test loss (w/o reg) on all data: 0.08111696
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4135468e-05
Norm of the params: 11.972383
              Random: fixed  73 labels. Loss 0.08112. Accuracy 0.995.
### Flips: 410, rs: 11, checks: 1025
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017240398
Train loss (w/o reg) on all data: 0.014130356
Test loss (w/o reg) on all data: 0.005700778
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9994022e-07
Norm of the params: 7.8867507
     Influence (LOO): fixed 338 labels. Loss 0.00570. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601339
Test loss (w/o reg) on all data: 0.0026560822
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.287558e-08
Norm of the params: 6.0927916
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20350209
Train loss (w/o reg) on all data: 0.19624797
Test loss (w/o reg) on all data: 0.07689924
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.2690447e-06
Norm of the params: 12.045013
              Random: fixed  92 labels. Loss 0.07690. Accuracy 0.996.
### Flips: 410, rs: 11, checks: 1230
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01315003
Train loss (w/o reg) on all data: 0.01035781
Test loss (w/o reg) on all data: 0.0046353536
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 9.9758076e-08
Norm of the params: 7.4729104
     Influence (LOO): fixed 342 labels. Loss 0.00464. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601273
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6800117e-08
Norm of the params: 6.092802
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19384868
Train loss (w/o reg) on all data: 0.18633437
Test loss (w/o reg) on all data: 0.07459577
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.225175e-06
Norm of the params: 12.2591305
              Random: fixed 108 labels. Loss 0.07460. Accuracy 0.993.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2561247
Train loss (w/o reg) on all data: 0.2500148
Test loss (w/o reg) on all data: 0.10450805
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5483089e-05
Norm of the params: 11.054316
Flipped loss: 0.10451. Accuracy: 0.992
### Flips: 410, rs: 12, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12846212
Train loss (w/o reg) on all data: 0.120937705
Test loss (w/o reg) on all data: 0.050013848
Train acc on all data:  0.9603695599319232
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.488433e-06
Norm of the params: 12.267364
     Influence (LOO): fixed 188 labels. Loss 0.05001. Accuracy 0.997.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09707989
Train loss (w/o reg) on all data: 0.08341408
Test loss (w/o reg) on all data: 0.05775892
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.9610209e-06
Norm of the params: 16.532278
                Loss: fixed 205 labels. Loss 0.05776. Accuracy 0.983.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663137
Train loss (w/o reg) on all data: 0.24050388
Test loss (w/o reg) on all data: 0.09825076
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.159725e-06
Norm of the params: 11.070228
              Random: fixed  19 labels. Loss 0.09825. Accuracy 0.993.
### Flips: 410, rs: 12, checks: 410
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06347528
Train loss (w/o reg) on all data: 0.057422508
Test loss (w/o reg) on all data: 0.020379012
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4107118e-06
Norm of the params: 11.0025215
     Influence (LOO): fixed 283 labels. Loss 0.02038. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004505326
Train loss (w/o reg) on all data: 0.0016892343
Test loss (w/o reg) on all data: 0.005855336
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0074363e-07
Norm of the params: 7.504788
                Loss: fixed 353 labels. Loss 0.00586. Accuracy 0.999.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23414852
Train loss (w/o reg) on all data: 0.22796604
Test loss (w/o reg) on all data: 0.093013085
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.8036455e-06
Norm of the params: 11.119781
              Random: fixed  44 labels. Loss 0.09301. Accuracy 0.993.
### Flips: 410, rs: 12, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042376745
Train loss (w/o reg) on all data: 0.037634432
Test loss (w/o reg) on all data: 0.013100368
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 9.598125e-07
Norm of the params: 9.738904
     Influence (LOO): fixed 312 labels. Loss 0.01310. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004271215
Train loss (w/o reg) on all data: 0.0016281672
Test loss (w/o reg) on all data: 0.003601487
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.5815905e-08
Norm of the params: 7.2705526
                Loss: fixed 354 labels. Loss 0.00360. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2217754
Train loss (w/o reg) on all data: 0.21548787
Test loss (w/o reg) on all data: 0.08561192
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.7275934e-06
Norm of the params: 11.2138605
              Random: fixed  66 labels. Loss 0.08561. Accuracy 0.995.
### Flips: 410, rs: 12, checks: 820
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02369006
Train loss (w/o reg) on all data: 0.01997855
Test loss (w/o reg) on all data: 0.0083575435
Train acc on all data:  0.9948942377826404
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7803244e-07
Norm of the params: 8.615695
     Influence (LOO): fixed 335 labels. Loss 0.00836. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960123
Test loss (w/o reg) on all data: 0.0026560915
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9174466e-08
Norm of the params: 6.0928097
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21592812
Train loss (w/o reg) on all data: 0.20938471
Test loss (w/o reg) on all data: 0.08066438
Train acc on all data:  0.9328956965718453
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8172295e-05
Norm of the params: 11.439769
              Random: fixed  76 labels. Loss 0.08066. Accuracy 0.994.
### Flips: 410, rs: 12, checks: 1025
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016811008
Train loss (w/o reg) on all data: 0.013684295
Test loss (w/o reg) on all data: 0.0052719093
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 5.6085605e-07
Norm of the params: 7.90786
     Influence (LOO): fixed 343 labels. Loss 0.00527. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096010877
Test loss (w/o reg) on all data: 0.002656047
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9911107e-08
Norm of the params: 6.0928326
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20539494
Train loss (w/o reg) on all data: 0.1988753
Test loss (w/o reg) on all data: 0.075201236
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.1807724e-06
Norm of the params: 11.418975
              Random: fixed  96 labels. Loss 0.07520. Accuracy 0.995.
### Flips: 410, rs: 12, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009572148
Train loss (w/o reg) on all data: 0.006916615
Test loss (w/o reg) on all data: 0.0037767359
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2859555e-07
Norm of the params: 7.287707
     Influence (LOO): fixed 350 labels. Loss 0.00378. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560246
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.517469e-08
Norm of the params: 6.092826
                Loss: fixed 356 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19344065
Train loss (w/o reg) on all data: 0.18667638
Test loss (w/o reg) on all data: 0.070779584
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.4189727e-06
Norm of the params: 11.631213
              Random: fixed 114 labels. Loss 0.07078. Accuracy 0.996.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2554243
Train loss (w/o reg) on all data: 0.24870035
Test loss (w/o reg) on all data: 0.108607136
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.4648518e-05
Norm of the params: 11.596494
Flipped loss: 0.10861. Accuracy: 0.995
### Flips: 410, rs: 13, checks: 205
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12880795
Train loss (w/o reg) on all data: 0.12102279
Test loss (w/o reg) on all data: 0.04963486
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.7286795e-06
Norm of the params: 12.478102
     Influence (LOO): fixed 186 labels. Loss 0.04963. Accuracy 0.998.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09589897
Train loss (w/o reg) on all data: 0.083383754
Test loss (w/o reg) on all data: 0.056857537
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1535018e-06
Norm of the params: 15.821008
                Loss: fixed 205 labels. Loss 0.05686. Accuracy 0.985.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24577297
Train loss (w/o reg) on all data: 0.2390783
Test loss (w/o reg) on all data: 0.10295658
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.355284e-06
Norm of the params: 11.571235
              Random: fixed  19 labels. Loss 0.10296. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 410
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0669365
Train loss (w/o reg) on all data: 0.061555587
Test loss (w/o reg) on all data: 0.022117797
Train acc on all data:  0.9810357403355215
Test acc on all data:   1.0
Norm of the mean of gradients: 1.172899e-06
Norm of the params: 10.373925
     Influence (LOO): fixed 273 labels. Loss 0.02212. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003617968
Train loss (w/o reg) on all data: 0.0013908554
Test loss (w/o reg) on all data: 0.0029409816
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2568818e-08
Norm of the params: 6.6739984
                Loss: fixed 349 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23608287
Train loss (w/o reg) on all data: 0.22953415
Test loss (w/o reg) on all data: 0.094850324
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6326856e-05
Norm of the params: 11.444402
              Random: fixed  37 labels. Loss 0.09485. Accuracy 0.996.
### Flips: 410, rs: 13, checks: 615
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036632586
Train loss (w/o reg) on all data: 0.0324258
Test loss (w/o reg) on all data: 0.011334552
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 5.593343e-07
Norm of the params: 9.172556
     Influence (LOO): fixed 315 labels. Loss 0.01133. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362243
Train loss (w/o reg) on all data: 0.0011012148
Test loss (w/o reg) on all data: 0.0029768138
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7874469e-08
Norm of the params: 6.2209473
                Loss: fixed 350 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23033363
Train loss (w/o reg) on all data: 0.22369859
Test loss (w/o reg) on all data: 0.09022912
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.2341354e-06
Norm of the params: 11.519588
              Random: fixed  50 labels. Loss 0.09023. Accuracy 0.995.
### Flips: 410, rs: 13, checks: 820
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01843165
Train loss (w/o reg) on all data: 0.015194491
Test loss (w/o reg) on all data: 0.006452514
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3603594e-07
Norm of the params: 8.046317
     Influence (LOO): fixed 336 labels. Loss 0.00645. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601212
Test loss (w/o reg) on all data: 0.0026560486
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9027102e-08
Norm of the params: 6.0928125
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22090831
Train loss (w/o reg) on all data: 0.21414483
Test loss (w/o reg) on all data: 0.08575598
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4823215e-05
Norm of the params: 11.63055
              Random: fixed  66 labels. Loss 0.08576. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 1025
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014430757
Train loss (w/o reg) on all data: 0.011547027
Test loss (w/o reg) on all data: 0.0058039357
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2356868e-07
Norm of the params: 7.5943794
     Influence (LOO): fixed 340 labels. Loss 0.00580. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.0026560575
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.47910795e-08
Norm of the params: 6.092808
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20787986
Train loss (w/o reg) on all data: 0.20097925
Test loss (w/o reg) on all data: 0.08022348
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.2874624e-06
Norm of the params: 11.747854
              Random: fixed  88 labels. Loss 0.08022. Accuracy 0.994.
### Flips: 410, rs: 13, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013546163
Train loss (w/o reg) on all data: 0.010669395
Test loss (w/o reg) on all data: 0.0057120523
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6591786e-07
Norm of the params: 7.5852075
     Influence (LOO): fixed 341 labels. Loss 0.00571. Accuracy 0.999.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096015376
Test loss (w/o reg) on all data: 0.0026561131
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4990547e-08
Norm of the params: 6.092759
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19406033
Train loss (w/o reg) on all data: 0.1869886
Test loss (w/o reg) on all data: 0.07466652
Train acc on all data:  0.9416484318016046
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.4725274e-06
Norm of the params: 11.89262
              Random: fixed 109 labels. Loss 0.07467. Accuracy 0.995.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25434342
Train loss (w/o reg) on all data: 0.24799095
Test loss (w/o reg) on all data: 0.102974676
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.5080834e-06
Norm of the params: 11.271613
Flipped loss: 0.10297. Accuracy: 0.993
### Flips: 410, rs: 14, checks: 205
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12938707
Train loss (w/o reg) on all data: 0.12173038
Test loss (w/o reg) on all data: 0.04972999
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.957315e-06
Norm of the params: 12.374723
     Influence (LOO): fixed 188 labels. Loss 0.04973. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097147174
Train loss (w/o reg) on all data: 0.08471329
Test loss (w/o reg) on all data: 0.049278818
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6902798e-06
Norm of the params: 15.769521
                Loss: fixed 205 labels. Loss 0.04928. Accuracy 0.989.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24214324
Train loss (w/o reg) on all data: 0.23557962
Test loss (w/o reg) on all data: 0.096113525
Train acc on all data:  0.9209822513980063
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.410586e-05
Norm of the params: 11.45742
              Random: fixed  25 labels. Loss 0.09611. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06260011
Train loss (w/o reg) on all data: 0.056250922
Test loss (w/o reg) on all data: 0.021169204
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4106426e-06
Norm of the params: 11.268713
     Influence (LOO): fixed 282 labels. Loss 0.02117. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043190126
Train loss (w/o reg) on all data: 0.0016405802
Test loss (w/o reg) on all data: 0.003020673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.246847e-08
Norm of the params: 7.3190603
                Loss: fixed 352 labels. Loss 0.00302. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23401707
Train loss (w/o reg) on all data: 0.227464
Test loss (w/o reg) on all data: 0.09008491
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.020781e-06
Norm of the params: 11.448204
              Random: fixed  41 labels. Loss 0.09008. Accuracy 0.997.
### Flips: 410, rs: 14, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040599015
Train loss (w/o reg) on all data: 0.035313033
Test loss (w/o reg) on all data: 0.013175004
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6580778e-06
Norm of the params: 10.282007
     Influence (LOO): fixed 314 labels. Loss 0.01318. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034945277
Train loss (w/o reg) on all data: 0.0012287492
Test loss (w/o reg) on all data: 0.0029084976
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3436157e-08
Norm of the params: 6.7316837
                Loss: fixed 354 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22319196
Train loss (w/o reg) on all data: 0.21632883
Test loss (w/o reg) on all data: 0.08774332
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.00975185e-05
Norm of the params: 11.715915
              Random: fixed  59 labels. Loss 0.08774. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025757672
Train loss (w/o reg) on all data: 0.021627147
Test loss (w/o reg) on all data: 0.009161429
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0693747e-07
Norm of the params: 9.089032
     Influence (LOO): fixed 330 labels. Loss 0.00916. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011744
Test loss (w/o reg) on all data: 0.0026560733
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1715699e-08
Norm of the params: 6.0928183
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2129719
Train loss (w/o reg) on all data: 0.2060425
Test loss (w/o reg) on all data: 0.08376145
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8294768e-05
Norm of the params: 11.772345
              Random: fixed  77 labels. Loss 0.08376. Accuracy 0.992.
### Flips: 410, rs: 14, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019448884
Train loss (w/o reg) on all data: 0.01581744
Test loss (w/o reg) on all data: 0.007358261
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2809988e-06
Norm of the params: 8.522257
     Influence (LOO): fixed 338 labels. Loss 0.00736. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560493
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7702996e-08
Norm of the params: 6.0928125
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2020965
Train loss (w/o reg) on all data: 0.19493039
Test loss (w/o reg) on all data: 0.07819982
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.27694175e-05
Norm of the params: 11.971736
              Random: fixed  94 labels. Loss 0.07820. Accuracy 0.994.
### Flips: 410, rs: 14, checks: 1230
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015013838
Train loss (w/o reg) on all data: 0.011860354
Test loss (w/o reg) on all data: 0.005357086
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7546425e-07
Norm of the params: 7.941643
     Influence (LOO): fixed 343 labels. Loss 0.00536. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7812539e-08
Norm of the params: 6.092808
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19239068
Train loss (w/o reg) on all data: 0.1853513
Test loss (w/o reg) on all data: 0.07469279
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6153217e-05
Norm of the params: 11.865402
              Random: fixed 111 labels. Loss 0.07469. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24474128
Train loss (w/o reg) on all data: 0.23825666
Test loss (w/o reg) on all data: 0.103660144
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2817045e-05
Norm of the params: 11.388247
Flipped loss: 0.10366. Accuracy: 0.992
### Flips: 410, rs: 15, checks: 205
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120572984
Train loss (w/o reg) on all data: 0.1127928
Test loss (w/o reg) on all data: 0.043237507
Train acc on all data:  0.963287138341843
Test acc on all data:   1.0
Norm of the mean of gradients: 2.438915e-06
Norm of the params: 12.474122
     Influence (LOO): fixed 186 labels. Loss 0.04324. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08361422
Train loss (w/o reg) on all data: 0.07178736
Test loss (w/o reg) on all data: 0.04128702
Train acc on all data:  0.9717967420374423
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.99625e-06
Norm of the params: 15.37977
                Loss: fixed 205 labels. Loss 0.04129. Accuracy 0.991.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23335488
Train loss (w/o reg) on all data: 0.22708762
Test loss (w/o reg) on all data: 0.0958112
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.48969e-06
Norm of the params: 11.1957655
              Random: fixed  21 labels. Loss 0.09581. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058236957
Train loss (w/o reg) on all data: 0.052934352
Test loss (w/o reg) on all data: 0.020029804
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4529454e-06
Norm of the params: 10.2981615
     Influence (LOO): fixed 272 labels. Loss 0.02003. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011785
Test loss (w/o reg) on all data: 0.002656065
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.665617e-09
Norm of the params: 6.0928183
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22389615
Train loss (w/o reg) on all data: 0.21765986
Test loss (w/o reg) on all data: 0.09071464
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9339312e-05
Norm of the params: 11.168062
              Random: fixed  38 labels. Loss 0.09071. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031334348
Train loss (w/o reg) on all data: 0.026680999
Test loss (w/o reg) on all data: 0.0105447555
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7647644e-07
Norm of the params: 9.647124
     Influence (LOO): fixed 304 labels. Loss 0.01054. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601253
Test loss (w/o reg) on all data: 0.0026560922
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7256501e-08
Norm of the params: 6.0928054
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21808015
Train loss (w/o reg) on all data: 0.211835
Test loss (w/o reg) on all data: 0.08811317
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.449747e-06
Norm of the params: 11.176002
              Random: fixed  49 labels. Loss 0.08811. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 820
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019018406
Train loss (w/o reg) on all data: 0.015538865
Test loss (w/o reg) on all data: 0.007281367
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.635723e-07
Norm of the params: 8.342113
     Influence (LOO): fixed 319 labels. Loss 0.00728. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6000294e-08
Norm of the params: 6.0928183
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20997122
Train loss (w/o reg) on all data: 0.20363784
Test loss (w/o reg) on all data: 0.08190765
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2642165e-05
Norm of the params: 11.254668
              Random: fixed  65 labels. Loss 0.08191. Accuracy 0.994.
### Flips: 410, rs: 15, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01180543
Train loss (w/o reg) on all data: 0.008920355
Test loss (w/o reg) on all data: 0.0049419333
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2824353e-07
Norm of the params: 7.596151
     Influence (LOO): fixed 327 labels. Loss 0.00494. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011016
Test loss (w/o reg) on all data: 0.0026560633
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.842749e-08
Norm of the params: 6.092831
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20000784
Train loss (w/o reg) on all data: 0.19361854
Test loss (w/o reg) on all data: 0.07690241
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.144943e-05
Norm of the params: 11.304252
              Random: fixed  84 labels. Loss 0.07690. Accuracy 0.993.
### Flips: 410, rs: 15, checks: 1230
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0052803727
Train loss (w/o reg) on all data: 0.003324088
Test loss (w/o reg) on all data: 0.0034012077
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.9171114e-08
Norm of the params: 6.2550535
     Influence (LOO): fixed 333 labels. Loss 0.00340. Accuracy 0.999.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601265
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9191483e-08
Norm of the params: 6.092804
                Loss: fixed 336 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1890282
Train loss (w/o reg) on all data: 0.18236805
Test loss (w/o reg) on all data: 0.074181795
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0001005e-05
Norm of the params: 11.541368
              Random: fixed 100 labels. Loss 0.07418. Accuracy 0.994.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25694406
Train loss (w/o reg) on all data: 0.24885897
Test loss (w/o reg) on all data: 0.10700756
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9940273e-05
Norm of the params: 12.716206
Flipped loss: 0.10701. Accuracy: 0.994
### Flips: 410, rs: 16, checks: 205
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14264248
Train loss (w/o reg) on all data: 0.13510408
Test loss (w/o reg) on all data: 0.051400803
Train acc on all data:  0.9562363238512035
Test acc on all data:   1.0
Norm of the mean of gradients: 1.291057e-05
Norm of the params: 12.278766
     Influence (LOO): fixed 181 labels. Loss 0.05140. Accuracy 1.000.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09630237
Train loss (w/o reg) on all data: 0.08197375
Test loss (w/o reg) on all data: 0.056519356
Train acc on all data:  0.9679066374908826
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.9801902e-06
Norm of the params: 16.92845
                Loss: fixed 205 labels. Loss 0.05652. Accuracy 0.989.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663492
Train loss (w/o reg) on all data: 0.2384774
Test loss (w/o reg) on all data: 0.10180003
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0794061e-05
Norm of the params: 12.773031
              Random: fixed  20 labels. Loss 0.10180. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 410
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061258044
Train loss (w/o reg) on all data: 0.05588539
Test loss (w/o reg) on all data: 0.020003099
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4665235e-06
Norm of the params: 10.365959
     Influence (LOO): fixed 294 labels. Loss 0.02000. Accuracy 1.000.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043888255
Train loss (w/o reg) on all data: 0.0017180558
Test loss (w/o reg) on all data: 0.0030720462
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5664376e-08
Norm of the params: 7.3085837
                Loss: fixed 358 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23751959
Train loss (w/o reg) on all data: 0.22942844
Test loss (w/o reg) on all data: 0.09642658
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.1863461e-05
Norm of the params: 12.72097
              Random: fixed  40 labels. Loss 0.09643. Accuracy 0.995.
### Flips: 410, rs: 16, checks: 615
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032260105
Train loss (w/o reg) on all data: 0.028070873
Test loss (w/o reg) on all data: 0.01082571
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 7.668008e-07
Norm of the params: 9.153395
     Influence (LOO): fixed 331 labels. Loss 0.01083. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037317416
Train loss (w/o reg) on all data: 0.0013234058
Test loss (w/o reg) on all data: 0.003028588
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9595588e-07
Norm of the params: 6.9402246
                Loss: fixed 360 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22754186
Train loss (w/o reg) on all data: 0.21954712
Test loss (w/o reg) on all data: 0.08879475
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.557125e-05
Norm of the params: 12.644953
              Random: fixed  58 labels. Loss 0.08879. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018013952
Train loss (w/o reg) on all data: 0.014750923
Test loss (w/o reg) on all data: 0.0066504073
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3635044e-07
Norm of the params: 8.078403
     Influence (LOO): fixed 348 labels. Loss 0.00665. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033632657
Train loss (w/o reg) on all data: 0.0011526126
Test loss (w/o reg) on all data: 0.002782596
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.479897e-08
Norm of the params: 6.64929
                Loss: fixed 361 labels. Loss 0.00278. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21594101
Train loss (w/o reg) on all data: 0.20784067
Test loss (w/o reg) on all data: 0.08459838
Train acc on all data:  0.9326525650376853
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0910398e-05
Norm of the params: 12.728197
              Random: fixed  76 labels. Loss 0.08460. Accuracy 0.996.
### Flips: 410, rs: 16, checks: 1025
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007759409
Train loss (w/o reg) on all data: 0.0053913775
Test loss (w/o reg) on all data: 0.0035661561
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.928383e-07
Norm of the params: 6.8819056
     Influence (LOO): fixed 358 labels. Loss 0.00357. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033632652
Train loss (w/o reg) on all data: 0.0011526241
Test loss (w/o reg) on all data: 0.0027826016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0424872e-08
Norm of the params: 6.649273
                Loss: fixed 361 labels. Loss 0.00278. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20650177
Train loss (w/o reg) on all data: 0.19885686
Test loss (w/o reg) on all data: 0.078417145
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.2532857e-06
Norm of the params: 12.3652
              Random: fixed  95 labels. Loss 0.07842. Accuracy 0.997.
### Flips: 410, rs: 16, checks: 1230
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006447875
Train loss (w/o reg) on all data: 0.0041685193
Test loss (w/o reg) on all data: 0.0031612068
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 7.636697e-08
Norm of the params: 6.7518225
     Influence (LOO): fixed 359 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010923
Test loss (w/o reg) on all data: 0.0026560545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5823296e-08
Norm of the params: 6.0928326
                Loss: fixed 362 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19797464
Train loss (w/o reg) on all data: 0.19009282
Test loss (w/o reg) on all data: 0.07453119
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.992252e-06
Norm of the params: 12.555338
              Random: fixed 108 labels. Loss 0.07453. Accuracy 0.996.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25478342
Train loss (w/o reg) on all data: 0.24738257
Test loss (w/o reg) on all data: 0.1105826
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6608887e-05
Norm of the params: 12.16624
Flipped loss: 0.11058. Accuracy: 0.993
### Flips: 410, rs: 17, checks: 205
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13392216
Train loss (w/o reg) on all data: 0.12535918
Test loss (w/o reg) on all data: 0.051784266
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.8386926e-06
Norm of the params: 13.086624
     Influence (LOO): fixed 186 labels. Loss 0.05178. Accuracy 0.999.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09658618
Train loss (w/o reg) on all data: 0.0838256
Test loss (w/o reg) on all data: 0.053118665
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.8356813e-06
Norm of the params: 15.975344
                Loss: fixed 205 labels. Loss 0.05312. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24707802
Train loss (w/o reg) on all data: 0.23961768
Test loss (w/o reg) on all data: 0.106315956
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.301437e-06
Norm of the params: 12.21503
              Random: fixed  17 labels. Loss 0.10632. Accuracy 0.992.
### Flips: 410, rs: 17, checks: 410
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06777035
Train loss (w/o reg) on all data: 0.06098608
Test loss (w/o reg) on all data: 0.023453088
Train acc on all data:  0.9803063457330415
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6452329e-06
Norm of the params: 11.648405
     Influence (LOO): fixed 276 labels. Loss 0.02345. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034164607
Train loss (w/o reg) on all data: 0.001185735
Test loss (w/o reg) on all data: 0.002793187
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4835712e-08
Norm of the params: 6.6794105
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23433392
Train loss (w/o reg) on all data: 0.2268454
Test loss (w/o reg) on all data: 0.09909341
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.771476e-05
Norm of the params: 12.238076
              Random: fixed  41 labels. Loss 0.09909. Accuracy 0.993.
### Flips: 410, rs: 17, checks: 615
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037856795
Train loss (w/o reg) on all data: 0.033126675
Test loss (w/o reg) on all data: 0.011772476
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7553945e-06
Norm of the params: 9.726376
     Influence (LOO): fixed 314 labels. Loss 0.01177. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.0011857381
Test loss (w/o reg) on all data: 0.0027932397
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2075942e-07
Norm of the params: 6.679405
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22525139
Train loss (w/o reg) on all data: 0.21809894
Test loss (w/o reg) on all data: 0.093641914
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7791388e-05
Norm of the params: 11.960315
              Random: fixed  57 labels. Loss 0.09364. Accuracy 0.995.
### Flips: 410, rs: 17, checks: 820
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026952539
Train loss (w/o reg) on all data: 0.022983007
Test loss (w/o reg) on all data: 0.008661916
Train acc on all data:  0.9931923170435205
Test acc on all data:   1.0
Norm of the mean of gradients: 3.753621e-07
Norm of the params: 8.910143
     Influence (LOO): fixed 329 labels. Loss 0.00866. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.001185731
Test loss (w/o reg) on all data: 0.00279322
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.205449e-08
Norm of the params: 6.679416
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21278222
Train loss (w/o reg) on all data: 0.20593616
Test loss (w/o reg) on all data: 0.08628278
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.7528682e-05
Norm of the params: 11.701326
              Random: fixed  82 labels. Loss 0.08628. Accuracy 0.994.
### Flips: 410, rs: 17, checks: 1025
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021668179
Train loss (w/o reg) on all data: 0.017952338
Test loss (w/o reg) on all data: 0.007932504
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2434433e-06
Norm of the params: 8.620722
     Influence (LOO): fixed 335 labels. Loss 0.00793. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003416461
Train loss (w/o reg) on all data: 0.0011857323
Test loss (w/o reg) on all data: 0.0027931978
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8063558e-08
Norm of the params: 6.6794143
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20401
Train loss (w/o reg) on all data: 0.1970889
Test loss (w/o reg) on all data: 0.08222325
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.53593e-06
Norm of the params: 11.765291
              Random: fixed  97 labels. Loss 0.08222. Accuracy 0.992.
### Flips: 410, rs: 17, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01840568
Train loss (w/o reg) on all data: 0.014784864
Test loss (w/o reg) on all data: 0.007239685
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.683333e-07
Norm of the params: 8.509777
     Influence (LOO): fixed 339 labels. Loss 0.00724. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034164607
Train loss (w/o reg) on all data: 0.0011857427
Test loss (w/o reg) on all data: 0.002793216
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2160923e-08
Norm of the params: 6.679399
                Loss: fixed 356 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19838107
Train loss (w/o reg) on all data: 0.1915035
Test loss (w/o reg) on all data: 0.079794556
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3021382e-05
Norm of the params: 11.728234
              Random: fixed 108 labels. Loss 0.07979. Accuracy 0.994.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25573057
Train loss (w/o reg) on all data: 0.24779142
Test loss (w/o reg) on all data: 0.105706885
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.7693665e-06
Norm of the params: 12.600916
Flipped loss: 0.10571. Accuracy: 0.996
### Flips: 410, rs: 18, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12831704
Train loss (w/o reg) on all data: 0.120708734
Test loss (w/o reg) on all data: 0.047362585
Train acc on all data:  0.9613420860685631
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.823938e-06
Norm of the params: 12.335567
     Influence (LOO): fixed 192 labels. Loss 0.04736. Accuracy 0.999.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09983897
Train loss (w/o reg) on all data: 0.085442655
Test loss (w/o reg) on all data: 0.06227054
Train acc on all data:  0.9659615852176027
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.091876e-06
Norm of the params: 16.96839
                Loss: fixed 205 labels. Loss 0.06227. Accuracy 0.981.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24548505
Train loss (w/o reg) on all data: 0.23741287
Test loss (w/o reg) on all data: 0.100782305
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.135671e-05
Norm of the params: 12.706043
              Random: fixed  21 labels. Loss 0.10078. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 410
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058938023
Train loss (w/o reg) on all data: 0.05241019
Test loss (w/o reg) on all data: 0.021247793
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3567461e-06
Norm of the params: 11.426139
     Influence (LOO): fixed 288 labels. Loss 0.02125. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601221
Test loss (w/o reg) on all data: 0.0026560721
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.47208175e-08
Norm of the params: 6.092811
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23608105
Train loss (w/o reg) on all data: 0.22796425
Test loss (w/o reg) on all data: 0.0963363
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.2945013e-06
Norm of the params: 12.741117
              Random: fixed  39 labels. Loss 0.09634. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036161788
Train loss (w/o reg) on all data: 0.03129027
Test loss (w/o reg) on all data: 0.01242114
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 7.194767e-07
Norm of the params: 9.870682
     Influence (LOO): fixed 318 labels. Loss 0.01242. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011756
Test loss (w/o reg) on all data: 0.002656079
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4886357e-08
Norm of the params: 6.092818
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22277647
Train loss (w/o reg) on all data: 0.21460766
Test loss (w/o reg) on all data: 0.08891321
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.4544144e-05
Norm of the params: 12.781876
              Random: fixed  61 labels. Loss 0.08891. Accuracy 0.995.
### Flips: 410, rs: 18, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022436354
Train loss (w/o reg) on all data: 0.018278737
Test loss (w/o reg) on all data: 0.009079499
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9741304e-07
Norm of the params: 9.118791
     Influence (LOO): fixed 334 labels. Loss 0.00908. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096013007
Test loss (w/o reg) on all data: 0.0026561068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.387208e-08
Norm of the params: 6.0927973
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21359226
Train loss (w/o reg) on all data: 0.20541327
Test loss (w/o reg) on all data: 0.08348867
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.8340145e-06
Norm of the params: 12.789838
              Random: fixed  77 labels. Loss 0.08349. Accuracy 0.996.
### Flips: 410, rs: 18, checks: 1025
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01619261
Train loss (w/o reg) on all data: 0.012532984
Test loss (w/o reg) on all data: 0.0072979196
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2276332e-07
Norm of the params: 8.555263
     Influence (LOO): fixed 341 labels. Loss 0.00730. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601098
Test loss (w/o reg) on all data: 0.0026560528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.724627e-09
Norm of the params: 6.092831
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20290026
Train loss (w/o reg) on all data: 0.19460475
Test loss (w/o reg) on all data: 0.07962734
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2332086e-05
Norm of the params: 12.880616
              Random: fixed  96 labels. Loss 0.07963. Accuracy 0.997.
### Flips: 410, rs: 18, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013928646
Train loss (w/o reg) on all data: 0.01067166
Test loss (w/o reg) on all data: 0.0054207193
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6267903e-07
Norm of the params: 8.070917
     Influence (LOO): fixed 344 labels. Loss 0.00542. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012134
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5085986e-08
Norm of the params: 6.0928116
                Loss: fixed 357 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19400242
Train loss (w/o reg) on all data: 0.18560871
Test loss (w/o reg) on all data: 0.07618251
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.2951877e-06
Norm of the params: 12.956623
              Random: fixed 108 labels. Loss 0.07618. Accuracy 0.997.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25778025
Train loss (w/o reg) on all data: 0.24950606
Test loss (w/o reg) on all data: 0.109375484
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.0082576e-06
Norm of the params: 12.864052
Flipped loss: 0.10938. Accuracy: 0.991
### Flips: 410, rs: 19, checks: 205
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14493515
Train loss (w/o reg) on all data: 0.13556015
Test loss (w/o reg) on all data: 0.05347704
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6722823e-06
Norm of the params: 13.693057
     Influence (LOO): fixed 180 labels. Loss 0.05348. Accuracy 0.998.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10043174
Train loss (w/o reg) on all data: 0.08420347
Test loss (w/o reg) on all data: 0.053273827
Train acc on all data:  0.9686360320933625
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.933989e-06
Norm of the params: 18.015701
                Loss: fixed 205 labels. Loss 0.05327. Accuracy 0.988.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24992155
Train loss (w/o reg) on all data: 0.24155961
Test loss (w/o reg) on all data: 0.105673894
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.0347934e-06
Norm of the params: 12.932082
              Random: fixed  16 labels. Loss 0.10567. Accuracy 0.991.
### Flips: 410, rs: 19, checks: 410
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070937276
Train loss (w/o reg) on all data: 0.06409713
Test loss (w/o reg) on all data: 0.02423835
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8701304e-06
Norm of the params: 11.6962805
     Influence (LOO): fixed 284 labels. Loss 0.02424. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062756455
Train loss (w/o reg) on all data: 0.0025406284
Test loss (w/o reg) on all data: 0.004079333
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.07692806e-07
Norm of the params: 8.642938
                Loss: fixed 361 labels. Loss 0.00408. Accuracy 0.999.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2413469
Train loss (w/o reg) on all data: 0.23308928
Test loss (w/o reg) on all data: 0.1006007
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.353095e-05
Norm of the params: 12.851156
              Random: fixed  36 labels. Loss 0.10060. Accuracy 0.992.
### Flips: 410, rs: 19, checks: 615
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040407475
Train loss (w/o reg) on all data: 0.03571745
Test loss (w/o reg) on all data: 0.0121774925
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4225435e-06
Norm of the params: 9.685064
     Influence (LOO): fixed 324 labels. Loss 0.01218. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049146777
Train loss (w/o reg) on all data: 0.0018619669
Test loss (w/o reg) on all data: 0.0030098122
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0966807e-08
Norm of the params: 7.8137193
                Loss: fixed 363 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23681065
Train loss (w/o reg) on all data: 0.22841978
Test loss (w/o reg) on all data: 0.09724891
Train acc on all data:  0.9238998298079261
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.393487e-06
Norm of the params: 12.954443
              Random: fixed  45 labels. Loss 0.09725. Accuracy 0.992.
### Flips: 410, rs: 19, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023565471
Train loss (w/o reg) on all data: 0.019974016
Test loss (w/o reg) on all data: 0.0076889447
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 5.343001e-07
Norm of the params: 8.475206
     Influence (LOO): fixed 342 labels. Loss 0.00769. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042070076
Train loss (w/o reg) on all data: 0.0014974382
Test loss (w/o reg) on all data: 0.0029399137
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.018341e-08
Norm of the params: 7.3614798
                Loss: fixed 364 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22834271
Train loss (w/o reg) on all data: 0.21994591
Test loss (w/o reg) on all data: 0.09388252
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4466659e-05
Norm of the params: 12.95902
              Random: fixed  60 labels. Loss 0.09388. Accuracy 0.991.
### Flips: 410, rs: 19, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018428551
Train loss (w/o reg) on all data: 0.01517321
Test loss (w/o reg) on all data: 0.0060286527
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 4.94169e-07
Norm of the params: 8.06888
     Influence (LOO): fixed 349 labels. Loss 0.00603. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004207007
Train loss (w/o reg) on all data: 0.0014974519
Test loss (w/o reg) on all data: 0.0029399253
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.968601e-08
Norm of the params: 7.361461
                Loss: fixed 364 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22304478
Train loss (w/o reg) on all data: 0.21487732
Test loss (w/o reg) on all data: 0.0890047
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.510151e-06
Norm of the params: 12.780808
              Random: fixed  71 labels. Loss 0.08900. Accuracy 0.993.
### Flips: 410, rs: 19, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015542087
Train loss (w/o reg) on all data: 0.01264157
Test loss (w/o reg) on all data: 0.0051254006
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9867436e-07
Norm of the params: 7.6164527
     Influence (LOO): fixed 352 labels. Loss 0.00513. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960125
Test loss (w/o reg) on all data: 0.00265607
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0024045e-08
Norm of the params: 6.0928063
                Loss: fixed 365 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20670223
Train loss (w/o reg) on all data: 0.19819693
Test loss (w/o reg) on all data: 0.079579584
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.576955e-06
Norm of the params: 13.04247
              Random: fixed  99 labels. Loss 0.07958. Accuracy 0.995.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25772235
Train loss (w/o reg) on all data: 0.2510065
Test loss (w/o reg) on all data: 0.105817825
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4811283e-05
Norm of the params: 11.5895195
Flipped loss: 0.10582. Accuracy: 0.997
### Flips: 410, rs: 20, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12858395
Train loss (w/o reg) on all data: 0.12110803
Test loss (w/o reg) on all data: 0.04676972
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 3.428336e-06
Norm of the params: 12.227777
     Influence (LOO): fixed 189 labels. Loss 0.04677. Accuracy 1.000.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1045179
Train loss (w/o reg) on all data: 0.092356846
Test loss (w/o reg) on all data: 0.041753437
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.6486105e-06
Norm of the params: 15.595547
                Loss: fixed 205 labels. Loss 0.04175. Accuracy 0.995.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25058562
Train loss (w/o reg) on all data: 0.24376576
Test loss (w/o reg) on all data: 0.1013527
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3947829e-05
Norm of the params: 11.67891
              Random: fixed  13 labels. Loss 0.10135. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062468752
Train loss (w/o reg) on all data: 0.05610851
Test loss (w/o reg) on all data: 0.022217225
Train acc on all data:  0.9827376610746413
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1489595e-06
Norm of the params: 11.278515
     Influence (LOO): fixed 284 labels. Loss 0.02222. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.000960116
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.69562e-08
Norm of the params: 6.0928226
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24523962
Train loss (w/o reg) on all data: 0.23860349
Test loss (w/o reg) on all data: 0.097257495
Train acc on all data:  0.9202528567955264
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.649961e-06
Norm of the params: 11.520521
              Random: fixed  24 labels. Loss 0.09726. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 615
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042479046
Train loss (w/o reg) on all data: 0.03717001
Test loss (w/o reg) on all data: 0.013490532
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5612534e-06
Norm of the params: 10.3043995
     Influence (LOO): fixed 311 labels. Loss 0.01349. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601154
Test loss (w/o reg) on all data: 0.0026560556
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6678793e-08
Norm of the params: 6.092823
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23798276
Train loss (w/o reg) on all data: 0.2313028
Test loss (w/o reg) on all data: 0.094274685
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6687596e-05
Norm of the params: 11.558517
              Random: fixed  37 labels. Loss 0.09427. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 820
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02788859
Train loss (w/o reg) on all data: 0.023536716
Test loss (w/o reg) on all data: 0.009508087
Train acc on all data:  0.9939217116460005
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3797252e-07
Norm of the params: 9.32939
     Influence (LOO): fixed 330 labels. Loss 0.00951. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012274
Test loss (w/o reg) on all data: 0.0026560845
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.068558e-08
Norm of the params: 6.0928097
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23081371
Train loss (w/o reg) on all data: 0.22399394
Test loss (w/o reg) on all data: 0.09091023
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.5726464e-06
Norm of the params: 11.678844
              Random: fixed  50 labels. Loss 0.09091. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 1025
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0171281
Train loss (w/o reg) on all data: 0.013935887
Test loss (w/o reg) on all data: 0.005405835
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1699816e-07
Norm of the params: 7.9902616
     Influence (LOO): fixed 342 labels. Loss 0.00541. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.0026560656
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.524549e-08
Norm of the params: 6.092823
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2190699
Train loss (w/o reg) on all data: 0.21203434
Test loss (w/o reg) on all data: 0.08551419
Train acc on all data:  0.9316800389010454
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4805309e-05
Norm of the params: 11.862167
              Random: fixed  70 labels. Loss 0.08551. Accuracy 0.998.
### Flips: 410, rs: 20, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012182314
Train loss (w/o reg) on all data: 0.009330464
Test loss (w/o reg) on all data: 0.0040464043
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 3.217562e-07
Norm of the params: 7.5522847
     Influence (LOO): fixed 346 labels. Loss 0.00405. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601109
Test loss (w/o reg) on all data: 0.0026560512
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5043318e-08
Norm of the params: 6.0928297
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20651191
Train loss (w/o reg) on all data: 0.19940941
Test loss (w/o reg) on all data: 0.07952391
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.224841e-06
Norm of the params: 11.918476
              Random: fixed  92 labels. Loss 0.07952. Accuracy 0.998.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26383978
Train loss (w/o reg) on all data: 0.25647992
Test loss (w/o reg) on all data: 0.1163839
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5417297e-05
Norm of the params: 12.132492
Flipped loss: 0.11638. Accuracy: 0.990
### Flips: 410, rs: 21, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13458185
Train loss (w/o reg) on all data: 0.12639692
Test loss (w/o reg) on all data: 0.057171617
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.948303e-06
Norm of the params: 12.79447
     Influence (LOO): fixed 187 labels. Loss 0.05717. Accuracy 0.994.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10324415
Train loss (w/o reg) on all data: 0.08867071
Test loss (w/o reg) on all data: 0.06828751
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8484667e-06
Norm of the params: 17.072458
                Loss: fixed 205 labels. Loss 0.06829. Accuracy 0.979.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25374714
Train loss (w/o reg) on all data: 0.24659239
Test loss (w/o reg) on all data: 0.110740595
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.1677133e-06
Norm of the params: 11.962236
              Random: fixed  21 labels. Loss 0.11074. Accuracy 0.992.
### Flips: 410, rs: 21, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056809038
Train loss (w/o reg) on all data: 0.051264323
Test loss (w/o reg) on all data: 0.020313524
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.3964372e-06
Norm of the params: 10.530634
     Influence (LOO): fixed 298 labels. Loss 0.02031. Accuracy 0.997.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044784034
Train loss (w/o reg) on all data: 0.0020163876
Test loss (w/o reg) on all data: 0.0039232173
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.158586e-08
Norm of the params: 7.017144
                Loss: fixed 361 labels. Loss 0.00392. Accuracy 0.999.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24282268
Train loss (w/o reg) on all data: 0.23565273
Test loss (w/o reg) on all data: 0.10393912
Train acc on all data:  0.9214685144663263
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9792196e-05
Norm of the params: 11.974929
              Random: fixed  39 labels. Loss 0.10394. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 615
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037960924
Train loss (w/o reg) on all data: 0.033218917
Test loss (w/o reg) on all data: 0.010748369
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 2.163576e-06
Norm of the params: 9.73859
     Influence (LOO): fixed 326 labels. Loss 0.01075. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601004
Test loss (w/o reg) on all data: 0.002656036
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6285306e-08
Norm of the params: 6.092848
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23386326
Train loss (w/o reg) on all data: 0.22655745
Test loss (w/o reg) on all data: 0.098585464
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1537913e-05
Norm of the params: 12.087853
              Random: fixed  59 labels. Loss 0.09859. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02523308
Train loss (w/o reg) on all data: 0.021666814
Test loss (w/o reg) on all data: 0.008275861
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0160223e-06
Norm of the params: 8.445433
     Influence (LOO): fixed 340 labels. Loss 0.00828. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601182
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5098596e-08
Norm of the params: 6.0928173
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22477953
Train loss (w/o reg) on all data: 0.21731775
Test loss (w/o reg) on all data: 0.09410843
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.45602e-06
Norm of the params: 12.216207
              Random: fixed  73 labels. Loss 0.09411. Accuracy 0.991.
### Flips: 410, rs: 21, checks: 1025
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01951294
Train loss (w/o reg) on all data: 0.016096659
Test loss (w/o reg) on all data: 0.007108913
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4887578e-07
Norm of the params: 8.265931
     Influence (LOO): fixed 346 labels. Loss 0.00711. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601017
Test loss (w/o reg) on all data: 0.0026560377
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5248665e-08
Norm of the params: 6.092844
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21390808
Train loss (w/o reg) on all data: 0.20643
Test loss (w/o reg) on all data: 0.08867182
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.292115e-05
Norm of the params: 12.229536
              Random: fixed  91 labels. Loss 0.08867. Accuracy 0.992.
### Flips: 410, rs: 21, checks: 1230
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012386427
Train loss (w/o reg) on all data: 0.009507898
Test loss (w/o reg) on all data: 0.0049428553
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.371723e-07
Norm of the params: 7.5875273
     Influence (LOO): fixed 353 labels. Loss 0.00494. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012023
Test loss (w/o reg) on all data: 0.0026560768
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7294848e-08
Norm of the params: 6.0928135
                Loss: fixed 363 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1982618
Train loss (w/o reg) on all data: 0.1909744
Test loss (w/o reg) on all data: 0.07792191
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.78709e-06
Norm of the params: 12.072605
              Random: fixed 118 labels. Loss 0.07792. Accuracy 0.995.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25118712
Train loss (w/o reg) on all data: 0.24412212
Test loss (w/o reg) on all data: 0.10185687
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9126555e-05
Norm of the params: 11.886975
Flipped loss: 0.10186. Accuracy: 0.997
### Flips: 410, rs: 22, checks: 205
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1287882
Train loss (w/o reg) on all data: 0.12145412
Test loss (w/o reg) on all data: 0.048397675
Train acc on all data:  0.9601264283977632
Test acc on all data:   1.0
Norm of the mean of gradients: 9.2905375e-06
Norm of the params: 12.111216
     Influence (LOO): fixed 183 labels. Loss 0.04840. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09297489
Train loss (w/o reg) on all data: 0.07908796
Test loss (w/o reg) on all data: 0.04918746
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.8652747e-06
Norm of the params: 16.66549
                Loss: fixed 205 labels. Loss 0.04919. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23948058
Train loss (w/o reg) on all data: 0.23224483
Test loss (w/o reg) on all data: 0.09624521
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.3501585e-06
Norm of the params: 12.029761
              Random: fixed  22 labels. Loss 0.09625. Accuracy 0.998.
### Flips: 410, rs: 22, checks: 410
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056695808
Train loss (w/o reg) on all data: 0.05135519
Test loss (w/o reg) on all data: 0.019305805
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2253021e-06
Norm of the params: 10.335007
     Influence (LOO): fixed 287 labels. Loss 0.01931. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031875302
Train loss (w/o reg) on all data: 0.001111962
Test loss (w/o reg) on all data: 0.0026435507
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2291717e-08
Norm of the params: 6.442931
                Loss: fixed 350 labels. Loss 0.00264. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22965056
Train loss (w/o reg) on all data: 0.22204968
Test loss (w/o reg) on all data: 0.09407031
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.538406e-05
Norm of the params: 12.329541
              Random: fixed  38 labels. Loss 0.09407. Accuracy 0.996.
### Flips: 410, rs: 22, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028868033
Train loss (w/o reg) on all data: 0.024937125
Test loss (w/o reg) on all data: 0.009347178
Train acc on all data:  0.9936785801118405
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7311327e-07
Norm of the params: 8.866688
     Influence (LOO): fixed 324 labels. Loss 0.00935. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031875297
Train loss (w/o reg) on all data: 0.0011119392
Test loss (w/o reg) on all data: 0.0026435694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1978644e-08
Norm of the params: 6.442966
                Loss: fixed 350 labels. Loss 0.00264. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21805017
Train loss (w/o reg) on all data: 0.21075822
Test loss (w/o reg) on all data: 0.08788458
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1199043e-05
Norm of the params: 12.076375
              Random: fixed  61 labels. Loss 0.08788. Accuracy 0.997.
### Flips: 410, rs: 22, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012872776
Train loss (w/o reg) on all data: 0.010114949
Test loss (w/o reg) on all data: 0.0045827697
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5444615e-07
Norm of the params: 7.426745
     Influence (LOO): fixed 342 labels. Loss 0.00458. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601017
Test loss (w/o reg) on all data: 0.0026560572
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.839716e-08
Norm of the params: 6.092845
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20716438
Train loss (w/o reg) on all data: 0.1996956
Test loss (w/o reg) on all data: 0.084742144
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.5850625e-05
Norm of the params: 12.221933
              Random: fixed  79 labels. Loss 0.08474. Accuracy 0.993.
### Flips: 410, rs: 22, checks: 1025
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009980339
Train loss (w/o reg) on all data: 0.007451889
Test loss (w/o reg) on all data: 0.003782885
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9543602e-08
Norm of the params: 7.1111875
     Influence (LOO): fixed 345 labels. Loss 0.00378. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011616
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6248642e-08
Norm of the params: 6.0928206
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2018888
Train loss (w/o reg) on all data: 0.19443427
Test loss (w/o reg) on all data: 0.082766235
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0551238e-05
Norm of the params: 12.210262
              Random: fixed  89 labels. Loss 0.08277. Accuracy 0.992.
### Flips: 410, rs: 22, checks: 1230
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005209132
Train loss (w/o reg) on all data: 0.0032494012
Test loss (w/o reg) on all data: 0.0026032256
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.515309e-08
Norm of the params: 6.26056
     Influence (LOO): fixed 349 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [14] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012314
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.115791e-08
Norm of the params: 6.09281
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19317676
Train loss (w/o reg) on all data: 0.18564822
Test loss (w/o reg) on all data: 0.07935723
Train acc on all data:  0.9401896425966448
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.7433523e-06
Norm of the params: 12.270729
              Random: fixed 103 labels. Loss 0.07936. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2550112
Train loss (w/o reg) on all data: 0.24701002
Test loss (w/o reg) on all data: 0.10179943
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.0737466e-06
Norm of the params: 12.65004
Flipped loss: 0.10180. Accuracy: 0.995
### Flips: 410, rs: 23, checks: 205
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13409652
Train loss (w/o reg) on all data: 0.12554045
Test loss (w/o reg) on all data: 0.05032599
Train acc on all data:  0.9576951130561634
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7799887e-06
Norm of the params: 13.081337
     Influence (LOO): fixed 184 labels. Loss 0.05033. Accuracy 0.999.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09904782
Train loss (w/o reg) on all data: 0.085092746
Test loss (w/o reg) on all data: 0.050174218
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.909561e-06
Norm of the params: 16.70633
                Loss: fixed 205 labels. Loss 0.05017. Accuracy 0.988.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24693692
Train loss (w/o reg) on all data: 0.2389284
Test loss (w/o reg) on all data: 0.09654963
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0495167e-05
Norm of the params: 12.655843
              Random: fixed  17 labels. Loss 0.09655. Accuracy 0.996.
### Flips: 410, rs: 23, checks: 410
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062384505
Train loss (w/o reg) on all data: 0.05629351
Test loss (w/o reg) on all data: 0.021463431
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 2.973896e-06
Norm of the params: 11.037203
     Influence (LOO): fixed 288 labels. Loss 0.02146. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00407672
Train loss (w/o reg) on all data: 0.0015444056
Test loss (w/o reg) on all data: 0.0029258814
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.822132e-08
Norm of the params: 7.116621
                Loss: fixed 358 labels. Loss 0.00293. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23902084
Train loss (w/o reg) on all data: 0.23113617
Test loss (w/o reg) on all data: 0.09269671
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4835983e-05
Norm of the params: 12.557601
              Random: fixed  36 labels. Loss 0.09270. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 615
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04017475
Train loss (w/o reg) on all data: 0.035299413
Test loss (w/o reg) on all data: 0.013774973
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 6.302765e-07
Norm of the params: 9.874549
     Influence (LOO): fixed 317 labels. Loss 0.01377. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00328265
Train loss (w/o reg) on all data: 0.0011477231
Test loss (w/o reg) on all data: 0.0027467462
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4710492e-08
Norm of the params: 6.534412
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23105977
Train loss (w/o reg) on all data: 0.2234593
Test loss (w/o reg) on all data: 0.08750807
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2172076e-06
Norm of the params: 12.329211
              Random: fixed  52 labels. Loss 0.08751. Accuracy 0.999.
### Flips: 410, rs: 23, checks: 820
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021723818
Train loss (w/o reg) on all data: 0.018323293
Test loss (w/o reg) on all data: 0.00795198
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 4.229386e-07
Norm of the params: 8.246848
     Influence (LOO): fixed 340 labels. Loss 0.00795. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032826501
Train loss (w/o reg) on all data: 0.0011477079
Test loss (w/o reg) on all data: 0.0027467087
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2433062e-08
Norm of the params: 6.5344353
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22071783
Train loss (w/o reg) on all data: 0.21314126
Test loss (w/o reg) on all data: 0.08276728
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6701388e-05
Norm of the params: 12.30981
              Random: fixed  72 labels. Loss 0.08277. Accuracy 0.999.
### Flips: 410, rs: 23, checks: 1025
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016098158
Train loss (w/o reg) on all data: 0.012923664
Test loss (w/o reg) on all data: 0.0060834023
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4630613e-06
Norm of the params: 7.9680533
     Influence (LOO): fixed 347 labels. Loss 0.00608. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00328265
Train loss (w/o reg) on all data: 0.0011477043
Test loss (w/o reg) on all data: 0.0027467173
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7849916e-08
Norm of the params: 6.534441
                Loss: fixed 359 labels. Loss 0.00275. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20779309
Train loss (w/o reg) on all data: 0.1998874
Test loss (w/o reg) on all data: 0.07858887
Train acc on all data:  0.9358132749817651
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.190219e-06
Norm of the params: 12.574338
              Random: fixed  92 labels. Loss 0.07859. Accuracy 0.997.
### Flips: 410, rs: 23, checks: 1230
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0107381195
Train loss (w/o reg) on all data: 0.00796477
Test loss (w/o reg) on all data: 0.004265464
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8483627e-07
Norm of the params: 7.447616
     Influence (LOO): fixed 352 labels. Loss 0.00427. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012425
Test loss (w/o reg) on all data: 0.0026560742
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5310694e-08
Norm of the params: 6.092808
                Loss: fixed 360 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19531375
Train loss (w/o reg) on all data: 0.18752703
Test loss (w/o reg) on all data: 0.07316263
Train acc on all data:  0.9411621687332847
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.0020047e-05
Norm of the params: 12.479364
              Random: fixed 112 labels. Loss 0.07316. Accuracy 0.996.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25400943
Train loss (w/o reg) on all data: 0.24632849
Test loss (w/o reg) on all data: 0.1069717
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.602898e-06
Norm of the params: 12.39431
Flipped loss: 0.10697. Accuracy: 0.989
### Flips: 410, rs: 24, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13333412
Train loss (w/o reg) on all data: 0.12550889
Test loss (w/o reg) on all data: 0.0507992
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.390357e-06
Norm of the params: 12.510173
     Influence (LOO): fixed 184 labels. Loss 0.05080. Accuracy 0.998.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09509601
Train loss (w/o reg) on all data: 0.081384145
Test loss (w/o reg) on all data: 0.06537487
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.622207e-06
Norm of the params: 16.56011
                Loss: fixed 205 labels. Loss 0.06537. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24103531
Train loss (w/o reg) on all data: 0.23331408
Test loss (w/o reg) on all data: 0.10040454
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7249246e-05
Norm of the params: 12.426775
              Random: fixed  23 labels. Loss 0.10040. Accuracy 0.991.
### Flips: 410, rs: 24, checks: 410
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06143188
Train loss (w/o reg) on all data: 0.055743065
Test loss (w/o reg) on all data: 0.020555442
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8322783e-06
Norm of the params: 10.6666
     Influence (LOO): fixed 286 labels. Loss 0.02056. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005332716
Train loss (w/o reg) on all data: 0.0021061
Test loss (w/o reg) on all data: 0.004485513
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1722238e-08
Norm of the params: 8.033201
                Loss: fixed 352 labels. Loss 0.00449. Accuracy 0.999.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23463544
Train loss (w/o reg) on all data: 0.22696447
Test loss (w/o reg) on all data: 0.096956745
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1526088e-05
Norm of the params: 12.386252
              Random: fixed  37 labels. Loss 0.09696. Accuracy 0.992.
### Flips: 410, rs: 24, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03581608
Train loss (w/o reg) on all data: 0.03125941
Test loss (w/o reg) on all data: 0.012131165
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7893404e-07
Norm of the params: 9.546383
     Influence (LOO): fixed 319 labels. Loss 0.01213. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012175
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.126004e-08
Norm of the params: 6.0928116
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22835298
Train loss (w/o reg) on all data: 0.220786
Test loss (w/o reg) on all data: 0.093321525
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.777812e-05
Norm of the params: 12.302015
              Random: fixed  50 labels. Loss 0.09332. Accuracy 0.993.
### Flips: 410, rs: 24, checks: 820
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023819733
Train loss (w/o reg) on all data: 0.019993916
Test loss (w/o reg) on all data: 0.008806992
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.391842e-07
Norm of the params: 8.747363
     Influence (LOO): fixed 332 labels. Loss 0.00881. Accuracy 0.998.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601107
Test loss (w/o reg) on all data: 0.0026560635
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.62632e-08
Norm of the params: 6.09283
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21594888
Train loss (w/o reg) on all data: 0.20815913
Test loss (w/o reg) on all data: 0.08767311
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.4900697e-05
Norm of the params: 12.481785
              Random: fixed  71 labels. Loss 0.08767. Accuracy 0.993.
### Flips: 410, rs: 24, checks: 1025
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016746504
Train loss (w/o reg) on all data: 0.013519156
Test loss (w/o reg) on all data: 0.0061173444
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5579822e-07
Norm of the params: 8.034112
     Influence (LOO): fixed 341 labels. Loss 0.00612. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560714
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4772039e-08
Norm of the params: 6.0928154
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21116029
Train loss (w/o reg) on all data: 0.20355415
Test loss (w/o reg) on all data: 0.08536841
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.895285e-05
Norm of the params: 12.333807
              Random: fixed  81 labels. Loss 0.08537. Accuracy 0.992.
### Flips: 410, rs: 24, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015807712
Train loss (w/o reg) on all data: 0.012552012
Test loss (w/o reg) on all data: 0.005930728
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.148859e-07
Norm of the params: 8.069326
     Influence (LOO): fixed 342 labels. Loss 0.00593. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601195
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.650852e-09
Norm of the params: 6.0928154
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1960952
Train loss (w/o reg) on all data: 0.18832862
Test loss (w/o reg) on all data: 0.07703201
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.209227e-06
Norm of the params: 12.463207
              Random: fixed 106 labels. Loss 0.07703. Accuracy 0.993.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2601337
Train loss (w/o reg) on all data: 0.25382137
Test loss (w/o reg) on all data: 0.10134542
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.776699e-06
Norm of the params: 11.235961
Flipped loss: 0.10135. Accuracy: 0.997
### Flips: 410, rs: 25, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13343248
Train loss (w/o reg) on all data: 0.12567984
Test loss (w/o reg) on all data: 0.04548019
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.99149e-06
Norm of the params: 12.452027
     Influence (LOO): fixed 186 labels. Loss 0.04548. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10118371
Train loss (w/o reg) on all data: 0.08861588
Test loss (w/o reg) on all data: 0.046485692
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.225282e-06
Norm of the params: 15.854232
                Loss: fixed 205 labels. Loss 0.04649. Accuracy 0.991.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24917185
Train loss (w/o reg) on all data: 0.24291888
Test loss (w/o reg) on all data: 0.09526062
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.3207058e-05
Norm of the params: 11.183002
              Random: fixed  21 labels. Loss 0.09526. Accuracy 0.996.
### Flips: 410, rs: 25, checks: 410
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067329615
Train loss (w/o reg) on all data: 0.061154764
Test loss (w/o reg) on all data: 0.023116605
Train acc on all data:  0.9815220034038414
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5293084e-06
Norm of the params: 11.112918
     Influence (LOO): fixed 279 labels. Loss 0.02312. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601156
Test loss (w/o reg) on all data: 0.002656053
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.684449e-08
Norm of the params: 6.0928216
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2405281
Train loss (w/o reg) on all data: 0.23407744
Test loss (w/o reg) on all data: 0.09164851
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.35219e-06
Norm of the params: 11.358406
              Random: fixed  37 labels. Loss 0.09165. Accuracy 0.995.
### Flips: 410, rs: 25, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03792333
Train loss (w/o reg) on all data: 0.0330715
Test loss (w/o reg) on all data: 0.012659164
Train acc on all data:  0.9907610017019207
Test acc on all data:   1.0
Norm of the mean of gradients: 5.345827e-07
Norm of the params: 9.850714
     Influence (LOO): fixed 317 labels. Loss 0.01266. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601202
Test loss (w/o reg) on all data: 0.0026560691
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8246979e-08
Norm of the params: 6.092814
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2286139
Train loss (w/o reg) on all data: 0.22237143
Test loss (w/o reg) on all data: 0.08632736
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1467056e-05
Norm of the params: 11.173603
              Random: fixed  57 labels. Loss 0.08633. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 820
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02372606
Train loss (w/o reg) on all data: 0.02023196
Test loss (w/o reg) on all data: 0.0070945793
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7255687e-07
Norm of the params: 8.359544
     Influence (LOO): fixed 337 labels. Loss 0.00709. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011633
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9719026e-08
Norm of the params: 6.092821
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21727529
Train loss (w/o reg) on all data: 0.210713
Test loss (w/o reg) on all data: 0.08134975
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5293262e-06
Norm of the params: 11.4562645
              Random: fixed  76 labels. Loss 0.08135. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 1025
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012038969
Train loss (w/o reg) on all data: 0.00923639
Test loss (w/o reg) on all data: 0.0043139653
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 2.867133e-07
Norm of the params: 7.4867616
     Influence (LOO): fixed 347 labels. Loss 0.00431. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096014456
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4156107e-07
Norm of the params: 6.092774
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20691726
Train loss (w/o reg) on all data: 0.20054892
Test loss (w/o reg) on all data: 0.07825197
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7689619e-05
Norm of the params: 11.285689
              Random: fixed  94 labels. Loss 0.07825. Accuracy 0.997.
### Flips: 410, rs: 25, checks: 1230
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0094948495
Train loss (w/o reg) on all data: 0.0070633874
Test loss (w/o reg) on all data: 0.0035140926
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.094279e-06
Norm of the params: 6.9734674
     Influence (LOO): fixed 350 labels. Loss 0.00351. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012215
Test loss (w/o reg) on all data: 0.002656073
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5497175e-08
Norm of the params: 6.092812
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20015472
Train loss (w/o reg) on all data: 0.19397084
Test loss (w/o reg) on all data: 0.07353827
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.090198e-06
Norm of the params: 11.121039
              Random: fixed 107 labels. Loss 0.07354. Accuracy 0.997.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25533602
Train loss (w/o reg) on all data: 0.247787
Test loss (w/o reg) on all data: 0.100642756
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.491213e-06
Norm of the params: 12.287414
Flipped loss: 0.10064. Accuracy: 0.997
### Flips: 410, rs: 26, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12712045
Train loss (w/o reg) on all data: 0.11958047
Test loss (w/o reg) on all data: 0.04469477
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.0268286e-06
Norm of the params: 12.280051
     Influence (LOO): fixed 189 labels. Loss 0.04469. Accuracy 0.999.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09676456
Train loss (w/o reg) on all data: 0.0834667
Test loss (w/o reg) on all data: 0.050094675
Train acc on all data:  0.9676635059567226
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.6104243e-06
Norm of the params: 16.308191
                Loss: fixed 205 labels. Loss 0.05009. Accuracy 0.993.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2479908
Train loss (w/o reg) on all data: 0.24022056
Test loss (w/o reg) on all data: 0.09657271
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8106359e-05
Norm of the params: 12.466149
              Random: fixed  14 labels. Loss 0.09657. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 410
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061434746
Train loss (w/o reg) on all data: 0.0550353
Test loss (w/o reg) on all data: 0.019702202
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6338716e-06
Norm of the params: 11.313219
     Influence (LOO): fixed 281 labels. Loss 0.01970. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042055305
Train loss (w/o reg) on all data: 0.0017542429
Test loss (w/o reg) on all data: 0.002879897
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9264466e-08
Norm of the params: 7.0018396
                Loss: fixed 346 labels. Loss 0.00288. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23578928
Train loss (w/o reg) on all data: 0.2282693
Test loss (w/o reg) on all data: 0.08863366
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.8137243e-06
Norm of the params: 12.263761
              Random: fixed  37 labels. Loss 0.08863. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 615
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036361393
Train loss (w/o reg) on all data: 0.031501863
Test loss (w/o reg) on all data: 0.011785929
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1916275e-07
Norm of the params: 9.85853
     Influence (LOO): fixed 311 labels. Loss 0.01179. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011226
Test loss (w/o reg) on all data: 0.0026560547
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7714336e-08
Norm of the params: 6.092828
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22055796
Train loss (w/o reg) on all data: 0.21255784
Test loss (w/o reg) on all data: 0.08140119
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.070568e-06
Norm of the params: 12.649208
              Random: fixed  62 labels. Loss 0.08140. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019989964
Train loss (w/o reg) on all data: 0.016126202
Test loss (w/o reg) on all data: 0.00877956
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.824115e-07
Norm of the params: 8.790632
     Influence (LOO): fixed 328 labels. Loss 0.00878. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096012367
Test loss (w/o reg) on all data: 0.002656074
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.985133e-08
Norm of the params: 6.092809
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20739199
Train loss (w/o reg) on all data: 0.19924165
Test loss (w/o reg) on all data: 0.07737561
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.825883e-06
Norm of the params: 12.767411
              Random: fixed  84 labels. Loss 0.07738. Accuracy 0.998.
### Flips: 410, rs: 26, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017712127
Train loss (w/o reg) on all data: 0.014135937
Test loss (w/o reg) on all data: 0.0069380715
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5414624e-07
Norm of the params: 8.457173
     Influence (LOO): fixed 331 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601263
Test loss (w/o reg) on all data: 0.0026559408
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3594147e-07
Norm of the params: 6.0928044
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19856508
Train loss (w/o reg) on all data: 0.19031085
Test loss (w/o reg) on all data: 0.073737904
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.7740644e-06
Norm of the params: 12.848521
              Random: fixed  98 labels. Loss 0.07374. Accuracy 0.999.
### Flips: 410, rs: 26, checks: 1230
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014886996
Train loss (w/o reg) on all data: 0.011702648
Test loss (w/o reg) on all data: 0.005891147
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.08621705e-07
Norm of the params: 7.9804106
     Influence (LOO): fixed 335 labels. Loss 0.00589. Accuracy 0.999.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012035
Test loss (w/o reg) on all data: 0.0026560558
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9750575e-08
Norm of the params: 6.092814
                Loss: fixed 347 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18618414
Train loss (w/o reg) on all data: 0.17802052
Test loss (w/o reg) on all data: 0.068934545
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.122654e-06
Norm of the params: 12.77781
              Random: fixed 119 labels. Loss 0.06893. Accuracy 0.998.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26493242
Train loss (w/o reg) on all data: 0.25823477
Test loss (w/o reg) on all data: 0.1117095
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.6984525e-06
Norm of the params: 11.573807
Flipped loss: 0.11171. Accuracy: 0.992
### Flips: 410, rs: 27, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14227295
Train loss (w/o reg) on all data: 0.13471097
Test loss (w/o reg) on all data: 0.05103013
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.201259e-06
Norm of the params: 12.297948
     Influence (LOO): fixed 186 labels. Loss 0.05103. Accuracy 0.997.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104103625
Train loss (w/o reg) on all data: 0.09097225
Test loss (w/o reg) on all data: 0.05716185
Train acc on all data:  0.962314612205203
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.476503e-06
Norm of the params: 16.205784
                Loss: fixed 205 labels. Loss 0.05716. Accuracy 0.987.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25709254
Train loss (w/o reg) on all data: 0.25013742
Test loss (w/o reg) on all data: 0.10702687
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.7449629e-05
Norm of the params: 11.794172
              Random: fixed  14 labels. Loss 0.10703. Accuracy 0.994.
### Flips: 410, rs: 27, checks: 410
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06240514
Train loss (w/o reg) on all data: 0.0561729
Test loss (w/o reg) on all data: 0.020582478
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1233837e-06
Norm of the params: 11.164443
     Influence (LOO): fixed 296 labels. Loss 0.02058. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013787
Test loss (w/o reg) on all data: 0.002656092
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2088485e-08
Norm of the params: 6.092786
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24656148
Train loss (w/o reg) on all data: 0.23938434
Test loss (w/o reg) on all data: 0.102391504
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.119695e-06
Norm of the params: 11.980943
              Random: fixed  32 labels. Loss 0.10239. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 615
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03906405
Train loss (w/o reg) on all data: 0.03394801
Test loss (w/o reg) on all data: 0.012725974
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4856926e-07
Norm of the params: 10.115371
     Influence (LOO): fixed 327 labels. Loss 0.01273. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012227
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8290842e-08
Norm of the params: 6.0928106
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23811398
Train loss (w/o reg) on all data: 0.23077612
Test loss (w/o reg) on all data: 0.09766239
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.0245983e-06
Norm of the params: 12.11435
              Random: fixed  47 labels. Loss 0.09766. Accuracy 0.993.
### Flips: 410, rs: 27, checks: 820
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019382179
Train loss (w/o reg) on all data: 0.015710674
Test loss (w/o reg) on all data: 0.0072240247
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5188803e-07
Norm of the params: 8.569135
     Influence (LOO): fixed 349 labels. Loss 0.00722. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012116
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2494596e-08
Norm of the params: 6.092812
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23274796
Train loss (w/o reg) on all data: 0.22506242
Test loss (w/o reg) on all data: 0.09475014
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.4567923e-06
Norm of the params: 12.398013
              Random: fixed  57 labels. Loss 0.09475. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017704261
Train loss (w/o reg) on all data: 0.014281721
Test loss (w/o reg) on all data: 0.006856118
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7139506e-07
Norm of the params: 8.2735
     Influence (LOO): fixed 351 labels. Loss 0.00686. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601244
Test loss (w/o reg) on all data: 0.00265609
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7666653e-08
Norm of the params: 6.092808
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22377726
Train loss (w/o reg) on all data: 0.21575214
Test loss (w/o reg) on all data: 0.09042826
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.1489558e-05
Norm of the params: 12.668959
              Random: fixed  72 labels. Loss 0.09043. Accuracy 0.995.
### Flips: 410, rs: 27, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014926717
Train loss (w/o reg) on all data: 0.011525811
Test loss (w/o reg) on all data: 0.005964403
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.744605e-08
Norm of the params: 8.24731
     Influence (LOO): fixed 354 labels. Loss 0.00596. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601254
Test loss (w/o reg) on all data: 0.002656089
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3304637e-08
Norm of the params: 6.092806
                Loss: fixed 366 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21419635
Train loss (w/o reg) on all data: 0.20626602
Test loss (w/o reg) on all data: 0.08518139
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.6922893e-05
Norm of the params: 12.593912
              Random: fixed  90 labels. Loss 0.08518. Accuracy 0.996.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273272
Train loss (w/o reg) on all data: 0.24575529
Test loss (w/o reg) on all data: 0.09667602
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2798084e-05
Norm of the params: 11.813069
Flipped loss: 0.09668. Accuracy: 0.999
### Flips: 410, rs: 28, checks: 205
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13106634
Train loss (w/o reg) on all data: 0.12346462
Test loss (w/o reg) on all data: 0.046660468
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.29268e-05
Norm of the params: 12.330219
     Influence (LOO): fixed 184 labels. Loss 0.04666. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09305568
Train loss (w/o reg) on all data: 0.078983076
Test loss (w/o reg) on all data: 0.0473574
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7377727e-06
Norm of the params: 16.776535
                Loss: fixed 205 labels. Loss 0.04736. Accuracy 0.988.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24358766
Train loss (w/o reg) on all data: 0.23656839
Test loss (w/o reg) on all data: 0.09282847
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.428879e-05
Norm of the params: 11.848434
              Random: fixed  19 labels. Loss 0.09283. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 410
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059832983
Train loss (w/o reg) on all data: 0.053177185
Test loss (w/o reg) on all data: 0.019721355
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7316884e-06
Norm of the params: 11.537587
     Influence (LOO): fixed 285 labels. Loss 0.01972. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0033914614
Train loss (w/o reg) on all data: 0.0011934079
Test loss (w/o reg) on all data: 0.0033463659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8860892e-08
Norm of the params: 6.6303144
                Loss: fixed 351 labels. Loss 0.00335. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23419854
Train loss (w/o reg) on all data: 0.22723374
Test loss (w/o reg) on all data: 0.08802067
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.1838892e-06
Norm of the params: 11.802381
              Random: fixed  38 labels. Loss 0.08802. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 615
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034894574
Train loss (w/o reg) on all data: 0.03026312
Test loss (w/o reg) on all data: 0.011361406
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 7.380184e-07
Norm of the params: 9.624401
     Influence (LOO): fixed 316 labels. Loss 0.01136. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.002656059
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2067952e-08
Norm of the params: 6.0928245
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2210008
Train loss (w/o reg) on all data: 0.21372089
Test loss (w/o reg) on all data: 0.08280411
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2772252e-05
Norm of the params: 12.066417
              Random: fixed  60 labels. Loss 0.08280. Accuracy 0.999.
### Flips: 410, rs: 28, checks: 820
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01739388
Train loss (w/o reg) on all data: 0.013936342
Test loss (w/o reg) on all data: 0.006804434
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1378895e-07
Norm of the params: 8.315694
     Influence (LOO): fixed 337 labels. Loss 0.00680. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600907
Test loss (w/o reg) on all data: 0.0026560198
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6239485e-08
Norm of the params: 6.092863
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20768917
Train loss (w/o reg) on all data: 0.20024218
Test loss (w/o reg) on all data: 0.07477768
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0409426e-06
Norm of the params: 12.204093
              Random: fixed  83 labels. Loss 0.07478. Accuracy 0.998.
### Flips: 410, rs: 28, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016433343
Train loss (w/o reg) on all data: 0.013139096
Test loss (w/o reg) on all data: 0.0062374435
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 5.121959e-07
Norm of the params: 8.116955
     Influence (LOO): fixed 338 labels. Loss 0.00624. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012966
Test loss (w/o reg) on all data: 0.0026560796
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4478436e-08
Norm of the params: 6.0927987
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20254397
Train loss (w/o reg) on all data: 0.19502208
Test loss (w/o reg) on all data: 0.07270536
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.073922e-06
Norm of the params: 12.26531
              Random: fixed  91 labels. Loss 0.07271. Accuracy 0.997.
### Flips: 410, rs: 28, checks: 1230
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012532744
Train loss (w/o reg) on all data: 0.0094568115
Test loss (w/o reg) on all data: 0.0049788174
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2401706e-07
Norm of the params: 7.8433824
     Influence (LOO): fixed 342 labels. Loss 0.00498. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601215
Test loss (w/o reg) on all data: 0.0026560389
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.81588e-08
Norm of the params: 6.0928125
                Loss: fixed 352 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19083975
Train loss (w/o reg) on all data: 0.18313812
Test loss (w/o reg) on all data: 0.0693932
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3827976e-06
Norm of the params: 12.410988
              Random: fixed 110 labels. Loss 0.06939. Accuracy 0.999.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2559794
Train loss (w/o reg) on all data: 0.24908748
Test loss (w/o reg) on all data: 0.104150034
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.363678e-05
Norm of the params: 11.740451
Flipped loss: 0.10415. Accuracy: 0.994
### Flips: 410, rs: 29, checks: 205
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13148803
Train loss (w/o reg) on all data: 0.124698326
Test loss (w/o reg) on all data: 0.04736683
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7677695e-06
Norm of the params: 11.653064
     Influence (LOO): fixed 188 labels. Loss 0.04737. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09293476
Train loss (w/o reg) on all data: 0.07792176
Test loss (w/o reg) on all data: 0.052289486
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.4088115e-06
Norm of the params: 17.32801
                Loss: fixed 205 labels. Loss 0.05229. Accuracy 0.989.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2498335
Train loss (w/o reg) on all data: 0.24282977
Test loss (w/o reg) on all data: 0.09987638
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.474035e-06
Norm of the params: 11.835311
              Random: fixed  13 labels. Loss 0.09988. Accuracy 0.995.
### Flips: 410, rs: 29, checks: 410
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061310485
Train loss (w/o reg) on all data: 0.055582024
Test loss (w/o reg) on all data: 0.01972594
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4926773e-06
Norm of the params: 10.7037
     Influence (LOO): fixed 285 labels. Loss 0.01973. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013234
Test loss (w/o reg) on all data: 0.0026560898
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2210744e-07
Norm of the params: 6.0927954
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24164647
Train loss (w/o reg) on all data: 0.23459834
Test loss (w/o reg) on all data: 0.094230615
Train acc on all data:  0.9219547775346463
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.974232e-06
Norm of the params: 11.872771
              Random: fixed  29 labels. Loss 0.09423. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033371042
Train loss (w/o reg) on all data: 0.028304871
Test loss (w/o reg) on all data: 0.011609036
Train acc on all data:  0.9917335278385606
Test acc on all data:   1.0
Norm of the mean of gradients: 2.445082e-06
Norm of the params: 10.065953
     Influence (LOO): fixed 318 labels. Loss 0.01161. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026560873
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9999555e-08
Norm of the params: 6.0928035
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23128188
Train loss (w/o reg) on all data: 0.22418663
Test loss (w/o reg) on all data: 0.08635063
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8804942e-05
Norm of the params: 11.912387
              Random: fixed  50 labels. Loss 0.08635. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 820
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021442223
Train loss (w/o reg) on all data: 0.017795224
Test loss (w/o reg) on all data: 0.0070919823
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 9.055992e-07
Norm of the params: 8.54049
     Influence (LOO): fixed 333 labels. Loss 0.00709. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096013024
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9468779e-08
Norm of the params: 6.0927973
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22182669
Train loss (w/o reg) on all data: 0.21462396
Test loss (w/o reg) on all data: 0.08205969
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.412324e-06
Norm of the params: 12.002273
              Random: fixed  67 labels. Loss 0.08206. Accuracy 0.995.
### Flips: 410, rs: 29, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015618267
Train loss (w/o reg) on all data: 0.012819793
Test loss (w/o reg) on all data: 0.005092704
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1668764e-07
Norm of the params: 7.4812756
     Influence (LOO): fixed 341 labels. Loss 0.00509. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560384
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.664949e-08
Norm of the params: 6.0928216
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21284074
Train loss (w/o reg) on all data: 0.2052431
Test loss (w/o reg) on all data: 0.07835488
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.6801755e-06
Norm of the params: 12.326909
              Random: fixed  81 labels. Loss 0.07835. Accuracy 0.996.
### Flips: 410, rs: 29, checks: 1230
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01305132
Train loss (w/o reg) on all data: 0.010355762
Test loss (w/o reg) on all data: 0.004635686
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4579947e-07
Norm of the params: 7.342421
     Influence (LOO): fixed 343 labels. Loss 0.00464. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012803
Test loss (w/o reg) on all data: 0.0026560673
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1044987e-08
Norm of the params: 6.092802
                Loss: fixed 353 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20524856
Train loss (w/o reg) on all data: 0.1976643
Test loss (w/o reg) on all data: 0.07412881
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1365865e-05
Norm of the params: 12.31605
              Random: fixed  93 labels. Loss 0.07413. Accuracy 0.996.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2583414
Train loss (w/o reg) on all data: 0.25108784
Test loss (w/o reg) on all data: 0.11119045
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4773954e-05
Norm of the params: 12.044555
Flipped loss: 0.11119. Accuracy: 0.990
### Flips: 410, rs: 30, checks: 205
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13846765
Train loss (w/o reg) on all data: 0.13165317
Test loss (w/o reg) on all data: 0.05322803
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8894573e-06
Norm of the params: 11.674318
     Influence (LOO): fixed 187 labels. Loss 0.05323. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097239144
Train loss (w/o reg) on all data: 0.08338629
Test loss (w/o reg) on all data: 0.0539788
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.804953e-06
Norm of the params: 16.645033
                Loss: fixed 205 labels. Loss 0.05398. Accuracy 0.988.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24944985
Train loss (w/o reg) on all data: 0.24215989
Test loss (w/o reg) on all data: 0.104089126
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.0252827e-06
Norm of the params: 12.074738
              Random: fixed  16 labels. Loss 0.10409. Accuracy 0.995.
### Flips: 410, rs: 30, checks: 410
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06080538
Train loss (w/o reg) on all data: 0.055046313
Test loss (w/o reg) on all data: 0.019848023
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 3.04083e-06
Norm of the params: 10.732258
     Influence (LOO): fixed 296 labels. Loss 0.01985. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292437
Train loss (w/o reg) on all data: 0.0011683411
Test loss (w/o reg) on all data: 0.0026929867
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.461696e-08
Norm of the params: 6.420129
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24282958
Train loss (w/o reg) on all data: 0.23570158
Test loss (w/o reg) on all data: 0.09905539
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.62537e-06
Norm of the params: 11.939857
              Random: fixed  31 labels. Loss 0.09906. Accuracy 0.995.
### Flips: 410, rs: 30, checks: 615
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028853063
Train loss (w/o reg) on all data: 0.024776382
Test loss (w/o reg) on all data: 0.010919233
Train acc on all data:  0.9931923170435205
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.674002e-07
Norm of the params: 9.029595
     Influence (LOO): fixed 335 labels. Loss 0.01092. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292441
Train loss (w/o reg) on all data: 0.0011683536
Test loss (w/o reg) on all data: 0.00269303
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.039576e-08
Norm of the params: 6.4201097
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23335695
Train loss (w/o reg) on all data: 0.22621605
Test loss (w/o reg) on all data: 0.09384468
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0983346e-05
Norm of the params: 11.950655
              Random: fixed  49 labels. Loss 0.09384. Accuracy 0.994.
### Flips: 410, rs: 30, checks: 820
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017330762
Train loss (w/o reg) on all data: 0.013976085
Test loss (w/o reg) on all data: 0.006149701
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 4.235622e-07
Norm of the params: 8.191065
     Influence (LOO): fixed 349 labels. Loss 0.00615. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003229244
Train loss (w/o reg) on all data: 0.0011683451
Test loss (w/o reg) on all data: 0.0026930142
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5851637e-08
Norm of the params: 6.420123
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21899912
Train loss (w/o reg) on all data: 0.21162371
Test loss (w/o reg) on all data: 0.087877944
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.784356e-06
Norm of the params: 12.145297
              Random: fixed  74 labels. Loss 0.08788. Accuracy 0.994.
### Flips: 410, rs: 30, checks: 1025
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015545596
Train loss (w/o reg) on all data: 0.012398883
Test loss (w/o reg) on all data: 0.005754164
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 9.819416e-07
Norm of the params: 7.933112
     Influence (LOO): fixed 351 labels. Loss 0.00575. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292446
Train loss (w/o reg) on all data: 0.0011683492
Test loss (w/o reg) on all data: 0.002693016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8081918e-08
Norm of the params: 6.420117
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21049903
Train loss (w/o reg) on all data: 0.20320013
Test loss (w/o reg) on all data: 0.08523093
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.816042e-06
Norm of the params: 12.082135
              Random: fixed  88 labels. Loss 0.08523. Accuracy 0.992.
### Flips: 410, rs: 30, checks: 1230
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013129339
Train loss (w/o reg) on all data: 0.01019923
Test loss (w/o reg) on all data: 0.00451622
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0648179e-07
Norm of the params: 7.655205
     Influence (LOO): fixed 353 labels. Loss 0.00452. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032292446
Train loss (w/o reg) on all data: 0.001168327
Test loss (w/o reg) on all data: 0.0026929914
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.364385e-08
Norm of the params: 6.4201517
                Loss: fixed 362 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1983077
Train loss (w/o reg) on all data: 0.19056268
Test loss (w/o reg) on all data: 0.07896512
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.0613585e-06
Norm of the params: 12.445893
              Random: fixed 108 labels. Loss 0.07897. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25331464
Train loss (w/o reg) on all data: 0.2465421
Test loss (w/o reg) on all data: 0.11560204
Train acc on all data:  0.9149039630440068
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.0779255e-06
Norm of the params: 11.63833
Flipped loss: 0.11560. Accuracy: 0.987
### Flips: 410, rs: 31, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12784053
Train loss (w/o reg) on all data: 0.12026626
Test loss (w/o reg) on all data: 0.050786287
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2069483e-06
Norm of the params: 12.307943
     Influence (LOO): fixed 189 labels. Loss 0.05079. Accuracy 0.997.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09373373
Train loss (w/o reg) on all data: 0.080644876
Test loss (w/o reg) on all data: 0.068807036
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.039801e-06
Norm of the params: 16.179522
                Loss: fixed 205 labels. Loss 0.06881. Accuracy 0.978.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2416932
Train loss (w/o reg) on all data: 0.23446877
Test loss (w/o reg) on all data: 0.10897207
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.1245547e-06
Norm of the params: 12.020333
              Random: fixed  20 labels. Loss 0.10897. Accuracy 0.987.
### Flips: 410, rs: 31, checks: 410
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059044465
Train loss (w/o reg) on all data: 0.053376455
Test loss (w/o reg) on all data: 0.020560581
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6528836e-06
Norm of the params: 10.647078
     Influence (LOO): fixed 287 labels. Loss 0.02056. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004704952
Train loss (w/o reg) on all data: 0.0018679488
Test loss (w/o reg) on all data: 0.0028698645
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.185139e-08
Norm of the params: 7.5326004
                Loss: fixed 352 labels. Loss 0.00287. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23385364
Train loss (w/o reg) on all data: 0.22653422
Test loss (w/o reg) on all data: 0.103687204
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.554703e-06
Norm of the params: 12.099105
              Random: fixed  38 labels. Loss 0.10369. Accuracy 0.989.
### Flips: 410, rs: 31, checks: 615
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033715323
Train loss (w/o reg) on all data: 0.029489772
Test loss (w/o reg) on all data: 0.010016438
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9759125e-06
Norm of the params: 9.192985
     Influence (LOO): fixed 319 labels. Loss 0.01002. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039846366
Train loss (w/o reg) on all data: 0.0015141792
Test loss (w/o reg) on all data: 0.0027021347
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5238217e-08
Norm of the params: 7.0291643
                Loss: fixed 353 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22517678
Train loss (w/o reg) on all data: 0.2177733
Test loss (w/o reg) on all data: 0.09831144
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0396753e-05
Norm of the params: 12.1683855
              Random: fixed  56 labels. Loss 0.09831. Accuracy 0.990.
### Flips: 410, rs: 31, checks: 820
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022427026
Train loss (w/o reg) on all data: 0.018688777
Test loss (w/o reg) on all data: 0.007260028
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 3.166484e-07
Norm of the params: 8.646674
     Influence (LOO): fixed 332 labels. Loss 0.00726. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039846366
Train loss (w/o reg) on all data: 0.0015141795
Test loss (w/o reg) on all data: 0.0027021507
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2193168e-08
Norm of the params: 7.029164
                Loss: fixed 353 labels. Loss 0.00270. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21628839
Train loss (w/o reg) on all data: 0.20857228
Test loss (w/o reg) on all data: 0.09442868
Train acc on all data:  0.9324094335035255
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0471274e-05
Norm of the params: 12.422646
              Random: fixed  70 labels. Loss 0.09443. Accuracy 0.992.
### Flips: 410, rs: 31, checks: 1025
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0165645
Train loss (w/o reg) on all data: 0.0129225915
Test loss (w/o reg) on all data: 0.006063933
Train acc on all data:  0.9961098954534403
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2271115e-06
Norm of the params: 8.534529
     Influence (LOO): fixed 338 labels. Loss 0.00606. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012617
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5379342e-08
Norm of the params: 6.0928054
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20558317
Train loss (w/o reg) on all data: 0.19775951
Test loss (w/o reg) on all data: 0.08336636
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.9885607e-06
Norm of the params: 12.508922
              Random: fixed  88 labels. Loss 0.08337. Accuracy 0.996.
### Flips: 410, rs: 31, checks: 1230
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013637911
Train loss (w/o reg) on all data: 0.010303925
Test loss (w/o reg) on all data: 0.005485638
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8298854e-07
Norm of the params: 8.165765
     Influence (LOO): fixed 342 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.0009601319
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7465571e-08
Norm of the params: 6.0927963
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19453818
Train loss (w/o reg) on all data: 0.18650286
Test loss (w/o reg) on all data: 0.07871983
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9351008e-05
Norm of the params: 12.677002
              Random: fixed 105 labels. Loss 0.07872. Accuracy 0.996.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2537571
Train loss (w/o reg) on all data: 0.2459469
Test loss (w/o reg) on all data: 0.10688553
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4703651e-05
Norm of the params: 12.498159
Flipped loss: 0.10689. Accuracy: 0.991
### Flips: 410, rs: 32, checks: 205
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1325059
Train loss (w/o reg) on all data: 0.124575295
Test loss (w/o reg) on all data: 0.05183015
Train acc on all data:  0.9596401653294432
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5229402e-06
Norm of the params: 12.594128
     Influence (LOO): fixed 183 labels. Loss 0.05183. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09862747
Train loss (w/o reg) on all data: 0.08403489
Test loss (w/o reg) on all data: 0.052363567
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6757285e-06
Norm of the params: 17.083666
                Loss: fixed 205 labels. Loss 0.05236. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24720791
Train loss (w/o reg) on all data: 0.23949586
Test loss (w/o reg) on all data: 0.10277758
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.913911e-06
Norm of the params: 12.419385
              Random: fixed  11 labels. Loss 0.10278. Accuracy 0.994.
### Flips: 410, rs: 32, checks: 410
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062813245
Train loss (w/o reg) on all data: 0.05667837
Test loss (w/o reg) on all data: 0.021556607
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6011996e-06
Norm of the params: 11.076893
     Influence (LOO): fixed 278 labels. Loss 0.02156. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043355636
Train loss (w/o reg) on all data: 0.0015491534
Test loss (w/o reg) on all data: 0.0028610437
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3935177e-08
Norm of the params: 7.4651327
                Loss: fixed 351 labels. Loss 0.00286. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24057461
Train loss (w/o reg) on all data: 0.23272973
Test loss (w/o reg) on all data: 0.09676865
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1124419e-05
Norm of the params: 12.525878
              Random: fixed  25 labels. Loss 0.09677. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 615
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03344281
Train loss (w/o reg) on all data: 0.029095007
Test loss (w/o reg) on all data: 0.011727686
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4576069e-06
Norm of the params: 9.325026
     Influence (LOO): fixed 318 labels. Loss 0.01173. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004053294
Train loss (w/o reg) on all data: 0.0014179826
Test loss (w/o reg) on all data: 0.0027558748
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3375686e-08
Norm of the params: 7.259906
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2320616
Train loss (w/o reg) on all data: 0.22428863
Test loss (w/o reg) on all data: 0.091961876
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.46882885e-05
Norm of the params: 12.468329
              Random: fixed  40 labels. Loss 0.09196. Accuracy 0.997.
### Flips: 410, rs: 32, checks: 820
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023756405
Train loss (w/o reg) on all data: 0.02004156
Test loss (w/o reg) on all data: 0.008338315
Train acc on all data:  0.9944079747143204
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3881446e-07
Norm of the params: 8.619565
     Influence (LOO): fixed 330 labels. Loss 0.00834. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040532937
Train loss (w/o reg) on all data: 0.0014179844
Test loss (w/o reg) on all data: 0.0027558943
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.911274e-08
Norm of the params: 7.2599025
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22279483
Train loss (w/o reg) on all data: 0.21475391
Test loss (w/o reg) on all data: 0.08715028
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8849758e-05
Norm of the params: 12.681423
              Random: fixed  56 labels. Loss 0.08715. Accuracy 0.996.
### Flips: 410, rs: 32, checks: 1025
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019394284
Train loss (w/o reg) on all data: 0.016010677
Test loss (w/o reg) on all data: 0.0063357283
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3060147e-07
Norm of the params: 8.226307
     Influence (LOO): fixed 335 labels. Loss 0.00634. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004053294
Train loss (w/o reg) on all data: 0.0014179612
Test loss (w/o reg) on all data: 0.0027558447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.05413456e-07
Norm of the params: 7.2599354
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21454017
Train loss (w/o reg) on all data: 0.20668764
Test loss (w/o reg) on all data: 0.08245578
Train acc on all data:  0.9331388281060053
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.5787e-06
Norm of the params: 12.531975
              Random: fixed  72 labels. Loss 0.08246. Accuracy 0.996.
### Flips: 410, rs: 32, checks: 1230
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016167764
Train loss (w/o reg) on all data: 0.013076389
Test loss (w/o reg) on all data: 0.00548775
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3500304e-07
Norm of the params: 7.8630447
     Influence (LOO): fixed 339 labels. Loss 0.00549. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040532947
Train loss (w/o reg) on all data: 0.0014179831
Test loss (w/o reg) on all data: 0.0027559102
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5020687e-08
Norm of the params: 7.2599053
                Loss: fixed 352 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20274001
Train loss (w/o reg) on all data: 0.1944991
Test loss (w/o reg) on all data: 0.07616059
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.172763e-06
Norm of the params: 12.838158
              Random: fixed  93 labels. Loss 0.07616. Accuracy 0.996.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2517675
Train loss (w/o reg) on all data: 0.24538694
Test loss (w/o reg) on all data: 0.10024726
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4918192e-05
Norm of the params: 11.296504
Flipped loss: 0.10025. Accuracy: 0.999
### Flips: 410, rs: 33, checks: 205
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12375203
Train loss (w/o reg) on all data: 0.11626287
Test loss (w/o reg) on all data: 0.047000486
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4095948e-06
Norm of the params: 12.238598
     Influence (LOO): fixed 191 labels. Loss 0.04700. Accuracy 0.998.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09255968
Train loss (w/o reg) on all data: 0.0793952
Test loss (w/o reg) on all data: 0.046074342
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.8862144e-06
Norm of the params: 16.2262
                Loss: fixed 205 labels. Loss 0.04607. Accuracy 0.988.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24341284
Train loss (w/o reg) on all data: 0.23670524
Test loss (w/o reg) on all data: 0.096495636
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.8853403e-06
Norm of the params: 11.582398
              Random: fixed  15 labels. Loss 0.09650. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 410
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05733099
Train loss (w/o reg) on all data: 0.051149763
Test loss (w/o reg) on all data: 0.019255916
Train acc on all data:  0.9839533187454412
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2047138e-06
Norm of the params: 11.118655
     Influence (LOO): fixed 285 labels. Loss 0.01926. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601131
Test loss (w/o reg) on all data: 0.0026560535
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9423098e-08
Norm of the params: 6.0928264
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23511274
Train loss (w/o reg) on all data: 0.22814128
Test loss (w/o reg) on all data: 0.092921786
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3673012e-05
Norm of the params: 11.808021
              Random: fixed  30 labels. Loss 0.09292. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 615
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034337707
Train loss (w/o reg) on all data: 0.029306395
Test loss (w/o reg) on all data: 0.01153015
Train acc on all data:  0.9914903963044007
Test acc on all data:   1.0
Norm of the mean of gradients: 1.850451e-06
Norm of the params: 10.031261
     Influence (LOO): fixed 316 labels. Loss 0.01153. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096010463
Test loss (w/o reg) on all data: 0.002656054
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2182372e-08
Norm of the params: 6.09284
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22851522
Train loss (w/o reg) on all data: 0.22148958
Test loss (w/o reg) on all data: 0.0885493
Train acc on all data:  0.925358619012886
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1609525e-05
Norm of the params: 11.853815
              Random: fixed  43 labels. Loss 0.08855. Accuracy 1.000.
### Flips: 410, rs: 33, checks: 820
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020319644
Train loss (w/o reg) on all data: 0.016843915
Test loss (w/o reg) on all data: 0.007383787
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 3.331077e-07
Norm of the params: 8.33754
     Influence (LOO): fixed 333 labels. Loss 0.00738. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560726
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4015326e-08
Norm of the params: 6.09283
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21827163
Train loss (w/o reg) on all data: 0.21111813
Test loss (w/o reg) on all data: 0.0845515
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0200183e-05
Norm of the params: 11.961184
              Random: fixed  60 labels. Loss 0.08455. Accuracy 0.999.
### Flips: 410, rs: 33, checks: 1025
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017233264
Train loss (w/o reg) on all data: 0.013822253
Test loss (w/o reg) on all data: 0.006660218
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3036547e-07
Norm of the params: 8.259554
     Influence (LOO): fixed 336 labels. Loss 0.00666. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012186
Test loss (w/o reg) on all data: 0.002656083
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.001641e-08
Norm of the params: 6.092811
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20798238
Train loss (w/o reg) on all data: 0.20046912
Test loss (w/o reg) on all data: 0.079408444
Train acc on all data:  0.9341113542426452
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4611454e-05
Norm of the params: 12.258267
              Random: fixed  78 labels. Loss 0.07941. Accuracy 1.000.
### Flips: 410, rs: 33, checks: 1230
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010986738
Train loss (w/o reg) on all data: 0.007871921
Test loss (w/o reg) on all data: 0.004904137
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6526584e-07
Norm of the params: 7.8928027
     Influence (LOO): fixed 342 labels. Loss 0.00490. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096011505
Test loss (w/o reg) on all data: 0.0026560447
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8697548e-08
Norm of the params: 6.092824
                Loss: fixed 351 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19209678
Train loss (w/o reg) on all data: 0.18434967
Test loss (w/o reg) on all data: 0.07111104
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.272994e-06
Norm of the params: 12.44758
              Random: fixed 105 labels. Loss 0.07111. Accuracy 0.999.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2604264
Train loss (w/o reg) on all data: 0.2536458
Test loss (w/o reg) on all data: 0.1021379
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.4645806e-06
Norm of the params: 11.645242
Flipped loss: 0.10214. Accuracy: 0.998
### Flips: 410, rs: 34, checks: 205
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13522886
Train loss (w/o reg) on all data: 0.12840044
Test loss (w/o reg) on all data: 0.04583744
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0861709e-06
Norm of the params: 11.686247
     Influence (LOO): fixed 187 labels. Loss 0.04584. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10094634
Train loss (w/o reg) on all data: 0.08817246
Test loss (w/o reg) on all data: 0.04319577
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.513721e-06
Norm of the params: 15.983667
                Loss: fixed 205 labels. Loss 0.04320. Accuracy 0.994.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2531841
Train loss (w/o reg) on all data: 0.24645878
Test loss (w/o reg) on all data: 0.0988309
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.269092e-06
Norm of the params: 11.597692
              Random: fixed  16 labels. Loss 0.09883. Accuracy 0.997.
### Flips: 410, rs: 34, checks: 410
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05971296
Train loss (w/o reg) on all data: 0.053914905
Test loss (w/o reg) on all data: 0.018287038
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1789062e-06
Norm of the params: 10.768526
     Influence (LOO): fixed 290 labels. Loss 0.01829. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002948957
Train loss (w/o reg) on all data: 0.0010428992
Test loss (w/o reg) on all data: 0.002615252
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.592763e-09
Norm of the params: 6.174233
                Loss: fixed 354 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24473308
Train loss (w/o reg) on all data: 0.23793106
Test loss (w/o reg) on all data: 0.09300198
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.749583e-06
Norm of the params: 11.663636
              Random: fixed  30 labels. Loss 0.09300. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 615
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034074932
Train loss (w/o reg) on all data: 0.029703591
Test loss (w/o reg) on all data: 0.010995328
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1072124e-07
Norm of the params: 9.35023
     Influence (LOO): fixed 324 labels. Loss 0.01100. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601271
Test loss (w/o reg) on all data: 0.0026560884
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.532488e-08
Norm of the params: 6.0928035
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2362288
Train loss (w/o reg) on all data: 0.22978692
Test loss (w/o reg) on all data: 0.08835586
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7671382e-05
Norm of the params: 11.350657
              Random: fixed  48 labels. Loss 0.08836. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023256253
Train loss (w/o reg) on all data: 0.019553686
Test loss (w/o reg) on all data: 0.0079036495
Train acc on all data:  0.9951373693168004
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8904176e-07
Norm of the params: 8.6053095
     Influence (LOO): fixed 335 labels. Loss 0.00790. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601173
Test loss (w/o reg) on all data: 0.002656071
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.685411e-08
Norm of the params: 6.0928183
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23055992
Train loss (w/o reg) on all data: 0.22407627
Test loss (w/o reg) on all data: 0.085427254
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3822216e-06
Norm of the params: 11.387394
              Random: fixed  57 labels. Loss 0.08543. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 1025
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013852169
Train loss (w/o reg) on all data: 0.011065086
Test loss (w/o reg) on all data: 0.0052689095
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.323769e-07
Norm of the params: 7.466033
     Influence (LOO): fixed 346 labels. Loss 0.00527. Accuracy 0.999.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601371
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2297685e-08
Norm of the params: 6.0927863
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2207875
Train loss (w/o reg) on all data: 0.21453919
Test loss (w/o reg) on all data: 0.080947325
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8767147e-05
Norm of the params: 11.178823
              Random: fixed  74 labels. Loss 0.08095. Accuracy 0.999.
### Flips: 410, rs: 34, checks: 1230
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008832557
Train loss (w/o reg) on all data: 0.006429615
Test loss (w/o reg) on all data: 0.0040574893
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 4.247463e-08
Norm of the params: 6.9324484
     Influence (LOO): fixed 350 labels. Loss 0.00406. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096009934
Test loss (w/o reg) on all data: 0.0026560223
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4932268e-08
Norm of the params: 6.0928483
                Loss: fixed 355 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20829259
Train loss (w/o reg) on all data: 0.20167416
Test loss (w/o reg) on all data: 0.07591304
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.866159e-05
Norm of the params: 11.5051565
              Random: fixed  94 labels. Loss 0.07591. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25422147
Train loss (w/o reg) on all data: 0.24646933
Test loss (w/o reg) on all data: 0.102671206
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.775432e-06
Norm of the params: 12.4516115
Flipped loss: 0.10267. Accuracy: 0.992
### Flips: 410, rs: 35, checks: 205
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13445461
Train loss (w/o reg) on all data: 0.12660873
Test loss (w/o reg) on all data: 0.046687637
Train acc on all data:  0.9596401653294432
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1319368e-06
Norm of the params: 12.526677
     Influence (LOO): fixed 186 labels. Loss 0.04669. Accuracy 1.000.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09805071
Train loss (w/o reg) on all data: 0.08476014
Test loss (w/o reg) on all data: 0.049299635
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.0847508e-06
Norm of the params: 16.30373
                Loss: fixed 205 labels. Loss 0.04930. Accuracy 0.988.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24499822
Train loss (w/o reg) on all data: 0.23735365
Test loss (w/o reg) on all data: 0.096766636
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.977465e-06
Norm of the params: 12.364927
              Random: fixed  17 labels. Loss 0.09677. Accuracy 0.993.
### Flips: 410, rs: 35, checks: 410
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06731211
Train loss (w/o reg) on all data: 0.06079167
Test loss (w/o reg) on all data: 0.022976585
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 2.334443e-06
Norm of the params: 11.419665
     Influence (LOO): fixed 278 labels. Loss 0.02298. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004261473
Train loss (w/o reg) on all data: 0.0016220029
Test loss (w/o reg) on all data: 0.0030519671
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0647104e-07
Norm of the params: 7.265632
                Loss: fixed 352 labels. Loss 0.00305. Accuracy 1.000.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23492403
Train loss (w/o reg) on all data: 0.22752741
Test loss (w/o reg) on all data: 0.08984869
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2139857e-05
Norm of the params: 12.1627445
              Random: fixed  37 labels. Loss 0.08985. Accuracy 0.996.
### Flips: 410, rs: 35, checks: 615
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038108397
Train loss (w/o reg) on all data: 0.032931704
Test loss (w/o reg) on all data: 0.013386318
Train acc on all data:  0.9902747386336008
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8210022e-06
Norm of the params: 10.17516
     Influence (LOO): fixed 314 labels. Loss 0.01339. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012745
Test loss (w/o reg) on all data: 0.0026560947
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.346562e-08
Norm of the params: 6.092803
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225369
Train loss (w/o reg) on all data: 0.21823904
Test loss (w/o reg) on all data: 0.08357685
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.901513e-05
Norm of the params: 11.941501
              Random: fixed  55 labels. Loss 0.08358. Accuracy 0.995.
### Flips: 410, rs: 35, checks: 820
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022223106
Train loss (w/o reg) on all data: 0.01892227
Test loss (w/o reg) on all data: 0.006839474
Train acc on all data:  0.9953805008509604
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5471896e-07
Norm of the params: 8.125067
     Influence (LOO): fixed 335 labels. Loss 0.00684. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601197
Test loss (w/o reg) on all data: 0.0026560752
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0539412e-08
Norm of the params: 6.0928154
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21790177
Train loss (w/o reg) on all data: 0.21073459
Test loss (w/o reg) on all data: 0.079942875
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5422967e-05
Norm of the params: 11.972621
              Random: fixed  67 labels. Loss 0.07994. Accuracy 0.994.
### Flips: 410, rs: 35, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014947945
Train loss (w/o reg) on all data: 0.0119584
Test loss (w/o reg) on all data: 0.0050669643
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 4.278558e-07
Norm of the params: 7.732458
     Influence (LOO): fixed 342 labels. Loss 0.00507. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012757
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1151457e-08
Norm of the params: 6.092803
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20746702
Train loss (w/o reg) on all data: 0.20043719
Test loss (w/o reg) on all data: 0.073818
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.248134e-06
Norm of the params: 11.8573475
              Random: fixed  87 labels. Loss 0.07382. Accuracy 0.995.
### Flips: 410, rs: 35, checks: 1230
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013043674
Train loss (w/o reg) on all data: 0.010146077
Test loss (w/o reg) on all data: 0.0046892175
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4502959e-07
Norm of the params: 7.612617
     Influence (LOO): fixed 344 labels. Loss 0.00469. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601371
Test loss (w/o reg) on all data: 0.0026560654
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1023057e-08
Norm of the params: 6.0927863
                Loss: fixed 354 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19612154
Train loss (w/o reg) on all data: 0.18915218
Test loss (w/o reg) on all data: 0.0691764
Train acc on all data:  0.9401896425966448
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5774112e-05
Norm of the params: 11.806238
              Random: fixed 107 labels. Loss 0.06918. Accuracy 0.997.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2528538
Train loss (w/o reg) on all data: 0.24611257
Test loss (w/o reg) on all data: 0.09927069
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.398753e-06
Norm of the params: 11.611412
Flipped loss: 0.09927. Accuracy: 0.995
### Flips: 410, rs: 36, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130347
Train loss (w/o reg) on all data: 0.12263896
Test loss (w/o reg) on all data: 0.04224347
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.225268e-06
Norm of the params: 12.416141
     Influence (LOO): fixed 188 labels. Loss 0.04224. Accuracy 0.999.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09428401
Train loss (w/o reg) on all data: 0.08150903
Test loss (w/o reg) on all data: 0.04477255
Train acc on all data:  0.9693654266958425
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1893106e-06
Norm of the params: 15.984355
                Loss: fixed 205 labels. Loss 0.04477. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24456257
Train loss (w/o reg) on all data: 0.23776443
Test loss (w/o reg) on all data: 0.096109845
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.696694e-06
Norm of the params: 11.660305
              Random: fixed  16 labels. Loss 0.09611. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 410
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05895624
Train loss (w/o reg) on all data: 0.0531572
Test loss (w/o reg) on all data: 0.01811825
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5524029e-06
Norm of the params: 10.769438
     Influence (LOO): fixed 284 labels. Loss 0.01812. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038724649
Train loss (w/o reg) on all data: 0.0013536076
Test loss (w/o reg) on all data: 0.0030390278
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.803137e-08
Norm of the params: 7.0976863
                Loss: fixed 346 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2345495
Train loss (w/o reg) on all data: 0.2273242
Test loss (w/o reg) on all data: 0.0922183
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.919423e-06
Norm of the params: 12.021063
              Random: fixed  31 labels. Loss 0.09222. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 615
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038628444
Train loss (w/o reg) on all data: 0.033837926
Test loss (w/o reg) on all data: 0.0122875925
Train acc on all data:  0.9907610017019207
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1254667e-07
Norm of the params: 9.788279
     Influence (LOO): fixed 310 labels. Loss 0.01229. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601171
Test loss (w/o reg) on all data: 0.0026560756
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7907091e-08
Norm of the params: 6.092819
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220737
Train loss (w/o reg) on all data: 0.21333893
Test loss (w/o reg) on all data: 0.08739305
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.957091e-06
Norm of the params: 12.163943
              Random: fixed  55 labels. Loss 0.08739. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 820
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018615961
Train loss (w/o reg) on all data: 0.015114321
Test loss (w/o reg) on all data: 0.006770409
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8940886e-07
Norm of the params: 8.368559
     Influence (LOO): fixed 333 labels. Loss 0.00677. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601249
Test loss (w/o reg) on all data: 0.0026560773
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1177193e-08
Norm of the params: 6.0928063
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2074761
Train loss (w/o reg) on all data: 0.20011505
Test loss (w/o reg) on all data: 0.08107157
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6895077e-05
Norm of the params: 12.133463
              Random: fixed  77 labels. Loss 0.08107. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 1025
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014722213
Train loss (w/o reg) on all data: 0.011599614
Test loss (w/o reg) on all data: 0.0055026445
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3302711e-07
Norm of the params: 7.902656
     Influence (LOO): fixed 337 labels. Loss 0.00550. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960093
Test loss (w/o reg) on all data: 0.0026560016
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.814835e-08
Norm of the params: 6.0928593
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193442
Train loss (w/o reg) on all data: 0.18585294
Test loss (w/o reg) on all data: 0.07361551
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.686554e-06
Norm of the params: 12.31995
              Random: fixed 102 labels. Loss 0.07362. Accuracy 0.995.
### Flips: 410, rs: 36, checks: 1230
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0105007645
Train loss (w/o reg) on all data: 0.007856365
Test loss (w/o reg) on all data: 0.0039303363
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 7.036326e-08
Norm of the params: 7.272413
     Influence (LOO): fixed 342 labels. Loss 0.00393. Accuracy 1.000.
Using normal model
LBFGS training took [15] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010853
Test loss (w/o reg) on all data: 0.0026559895
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1115302e-07
Norm of the params: 6.092834
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18629986
Train loss (w/o reg) on all data: 0.17859878
Test loss (w/o reg) on all data: 0.07054404
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.034741e-06
Norm of the params: 12.410546
              Random: fixed 113 labels. Loss 0.07054. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25064355
Train loss (w/o reg) on all data: 0.24326424
Test loss (w/o reg) on all data: 0.105955884
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.5682137e-06
Norm of the params: 12.148512
Flipped loss: 0.10596. Accuracy: 0.996
### Flips: 410, rs: 37, checks: 205
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131921
Train loss (w/o reg) on all data: 0.12449084
Test loss (w/o reg) on all data: 0.04732104
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4779571e-05
Norm of the params: 12.19029
     Influence (LOO): fixed 179 labels. Loss 0.04732. Accuracy 0.999.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093429185
Train loss (w/o reg) on all data: 0.079827875
Test loss (w/o reg) on all data: 0.052323572
Train acc on all data:  0.9703379528324824
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.4452785e-06
Norm of the params: 16.493216
                Loss: fixed 205 labels. Loss 0.05232. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24258688
Train loss (w/o reg) on all data: 0.23525557
Test loss (w/o reg) on all data: 0.1011777
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1072236e-05
Norm of the params: 12.108936
              Random: fixed  17 labels. Loss 0.10118. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 410
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055978257
Train loss (w/o reg) on all data: 0.050540876
Test loss (w/o reg) on all data: 0.019238515
Train acc on all data:  0.9841964502796012
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7090614e-06
Norm of the params: 10.428214
     Influence (LOO): fixed 283 labels. Loss 0.01924. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012466
Test loss (w/o reg) on all data: 0.0026560766
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.618067e-08
Norm of the params: 6.0928082
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23470905
Train loss (w/o reg) on all data: 0.22709945
Test loss (w/o reg) on all data: 0.09810214
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.175124e-06
Norm of the params: 12.336618
              Random: fixed  31 labels. Loss 0.09810. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 615
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03967921
Train loss (w/o reg) on all data: 0.035228964
Test loss (w/o reg) on all data: 0.012904937
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 5.472378e-07
Norm of the params: 9.434243
     Influence (LOO): fixed 305 labels. Loss 0.01290. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601146
Test loss (w/o reg) on all data: 0.00265608
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2755767e-08
Norm of the params: 6.092824
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2172667
Train loss (w/o reg) on all data: 0.20996958
Test loss (w/o reg) on all data: 0.08955322
Train acc on all data:  0.9309506442985656
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8254623e-05
Norm of the params: 12.080655
              Random: fixed  64 labels. Loss 0.08955. Accuracy 0.994.
### Flips: 410, rs: 37, checks: 820
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021773258
Train loss (w/o reg) on all data: 0.018249843
Test loss (w/o reg) on all data: 0.0070838947
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2504294e-06
Norm of the params: 8.394541
     Influence (LOO): fixed 326 labels. Loss 0.00708. Accuracy 1.000.
Using normal model
LBFGS training took [17] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560684
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.508448e-08
Norm of the params: 6.092812
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20923333
Train loss (w/o reg) on all data: 0.20215121
Test loss (w/o reg) on all data: 0.0844192
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2304388e-05
Norm of the params: 11.901366
              Random: fixed  79 labels. Loss 0.08442. Accuracy 0.996.
### Flips: 410, rs: 37, checks: 1025
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016129227
Train loss (w/o reg) on all data: 0.012651373
Test loss (w/o reg) on all data: 0.0058332514
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9338738e-07
Norm of the params: 8.340091
     Influence (LOO): fixed 333 labels. Loss 0.00583. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601123
Test loss (w/o reg) on all data: 0.0026560565
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1887276e-08
Norm of the params: 6.0928264
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19801573
Train loss (w/o reg) on all data: 0.19075723
Test loss (w/o reg) on all data: 0.077706076
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.633902e-06
Norm of the params: 12.04865
              Random: fixed  95 labels. Loss 0.07771. Accuracy 0.997.
### Flips: 410, rs: 37, checks: 1230
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01325745
Train loss (w/o reg) on all data: 0.010070429
Test loss (w/o reg) on all data: 0.0050037536
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5729628e-07
Norm of the params: 7.9837604
     Influence (LOO): fixed 337 labels. Loss 0.00500. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601193
Test loss (w/o reg) on all data: 0.0026560617
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2714345e-09
Norm of the params: 6.0928164
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18607786
Train loss (w/o reg) on all data: 0.1789737
Test loss (w/o reg) on all data: 0.071837254
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4115472e-05
Norm of the params: 11.919864
              Random: fixed 113 labels. Loss 0.07184. Accuracy 0.998.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25627744
Train loss (w/o reg) on all data: 0.24816233
Test loss (w/o reg) on all data: 0.11122607
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1375681e-05
Norm of the params: 12.739794
Flipped loss: 0.11123. Accuracy: 0.992
### Flips: 410, rs: 38, checks: 205
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14042783
Train loss (w/o reg) on all data: 0.13263574
Test loss (w/o reg) on all data: 0.05049358
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.070096e-06
Norm of the params: 12.483662
     Influence (LOO): fixed 184 labels. Loss 0.05049. Accuracy 0.999.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10218264
Train loss (w/o reg) on all data: 0.08677426
Test loss (w/o reg) on all data: 0.058082342
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.723787e-06
Norm of the params: 17.5547
                Loss: fixed 205 labels. Loss 0.05808. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2436281
Train loss (w/o reg) on all data: 0.23563164
Test loss (w/o reg) on all data: 0.10304717
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.2746456e-06
Norm of the params: 12.646302
              Random: fixed  26 labels. Loss 0.10305. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 410
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061719142
Train loss (w/o reg) on all data: 0.0554241
Test loss (w/o reg) on all data: 0.021908345
Train acc on all data:  0.9817651349380014
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2202074e-06
Norm of the params: 11.220553
     Influence (LOO): fixed 291 labels. Loss 0.02191. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003465999
Train loss (w/o reg) on all data: 0.0012043518
Test loss (w/o reg) on all data: 0.0029205487
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.19012206e-07
Norm of the params: 6.725545
                Loss: fixed 364 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23593923
Train loss (w/o reg) on all data: 0.22808333
Test loss (w/o reg) on all data: 0.09646624
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.232829e-06
Norm of the params: 12.5346775
              Random: fixed  43 labels. Loss 0.09647. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 615
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035401165
Train loss (w/o reg) on all data: 0.030557627
Test loss (w/o reg) on all data: 0.013017224
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8908858e-06
Norm of the params: 9.842295
     Influence (LOO): fixed 327 labels. Loss 0.01302. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034659994
Train loss (w/o reg) on all data: 0.0012043467
Test loss (w/o reg) on all data: 0.002920573
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.2723565e-08
Norm of the params: 6.7255516
                Loss: fixed 364 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2293061
Train loss (w/o reg) on all data: 0.2214412
Test loss (w/o reg) on all data: 0.09199799
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.321754e-06
Norm of the params: 12.541862
              Random: fixed  56 labels. Loss 0.09200. Accuracy 0.996.
### Flips: 410, rs: 38, checks: 820
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019169226
Train loss (w/o reg) on all data: 0.015990512
Test loss (w/o reg) on all data: 0.0060893637
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8945983e-07
Norm of the params: 7.9733486
     Influence (LOO): fixed 349 labels. Loss 0.00609. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003435465
Train loss (w/o reg) on all data: 0.0012970818
Test loss (w/o reg) on all data: 0.0028204008
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.317461e-08
Norm of the params: 6.539699
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21775985
Train loss (w/o reg) on all data: 0.2101904
Test loss (w/o reg) on all data: 0.08686041
Train acc on all data:  0.9302212496960856
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8683104e-05
Norm of the params: 12.30402
              Random: fixed  77 labels. Loss 0.08686. Accuracy 0.996.
### Flips: 410, rs: 38, checks: 1025
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019169224
Train loss (w/o reg) on all data: 0.015990337
Test loss (w/o reg) on all data: 0.00608937
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 6.551037e-07
Norm of the params: 7.9735656
     Influence (LOO): fixed 349 labels. Loss 0.00609. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034354653
Train loss (w/o reg) on all data: 0.0012970814
Test loss (w/o reg) on all data: 0.002820409
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.126762e-08
Norm of the params: 6.5397005
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2031061
Train loss (w/o reg) on all data: 0.19558701
Test loss (w/o reg) on all data: 0.07901009
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.868727e-06
Norm of the params: 12.263026
              Random: fixed 103 labels. Loss 0.07901. Accuracy 0.995.
### Flips: 410, rs: 38, checks: 1230
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016920315
Train loss (w/o reg) on all data: 0.013701835
Test loss (w/o reg) on all data: 0.0053914385
Train acc on all data:  0.9963530269876003
Test acc on all data:   1.0
Norm of the mean of gradients: 5.923336e-07
Norm of the params: 8.023068
     Influence (LOO): fixed 351 labels. Loss 0.00539. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034354646
Train loss (w/o reg) on all data: 0.0012970797
Test loss (w/o reg) on all data: 0.0028204261
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3235594e-08
Norm of the params: 6.539702
                Loss: fixed 365 labels. Loss 0.00282. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19056103
Train loss (w/o reg) on all data: 0.18343459
Test loss (w/o reg) on all data: 0.07298877
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.24225e-06
Norm of the params: 11.938541
              Random: fixed 126 labels. Loss 0.07299. Accuracy 0.996.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25219053
Train loss (w/o reg) on all data: 0.24545868
Test loss (w/o reg) on all data: 0.09946252
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.16032e-06
Norm of the params: 11.603324
Flipped loss: 0.09946. Accuracy: 0.993
### Flips: 410, rs: 39, checks: 205
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13137996
Train loss (w/o reg) on all data: 0.12344138
Test loss (w/o reg) on all data: 0.05036672
Train acc on all data:  0.9601264283977632
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3888277e-06
Norm of the params: 12.600459
     Influence (LOO): fixed 183 labels. Loss 0.05037. Accuracy 0.999.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09506197
Train loss (w/o reg) on all data: 0.08144054
Test loss (w/o reg) on all data: 0.04708044
Train acc on all data:  0.9698516897641624
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.004005e-06
Norm of the params: 16.505411
                Loss: fixed 205 labels. Loss 0.04708. Accuracy 0.994.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24484113
Train loss (w/o reg) on all data: 0.23819192
Test loss (w/o reg) on all data: 0.09472803
Train acc on all data:  0.9192803306588865
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.5675858e-05
Norm of the params: 11.531883
              Random: fixed  16 labels. Loss 0.09473. Accuracy 0.993.
### Flips: 410, rs: 39, checks: 410
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0513806
Train loss (w/o reg) on all data: 0.04563787
Test loss (w/o reg) on all data: 0.017156105
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3651702e-06
Norm of the params: 10.717027
     Influence (LOO): fixed 295 labels. Loss 0.01716. Accuracy 0.999.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034810498
Train loss (w/o reg) on all data: 0.001188501
Test loss (w/o reg) on all data: 0.0026644554
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4966246e-08
Norm of the params: 6.7713356
                Loss: fixed 348 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22992034
Train loss (w/o reg) on all data: 0.22316939
Test loss (w/o reg) on all data: 0.08719745
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.34485945e-05
Norm of the params: 11.619774
              Random: fixed  41 labels. Loss 0.08720. Accuracy 0.994.
### Flips: 410, rs: 39, checks: 615
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03383132
Train loss (w/o reg) on all data: 0.029458394
Test loss (w/o reg) on all data: 0.010642778
Train acc on all data:  0.9922197909068806
Test acc on all data:   1.0
Norm of the mean of gradients: 6.482902e-07
Norm of the params: 9.351928
     Influence (LOO): fixed 317 labels. Loss 0.01064. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.000960128
Test loss (w/o reg) on all data: 0.0026560833
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.250213e-08
Norm of the params: 6.092801
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21889001
Train loss (w/o reg) on all data: 0.21192282
Test loss (w/o reg) on all data: 0.0812991
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4803695e-05
Norm of the params: 11.8043995
              Random: fixed  63 labels. Loss 0.08130. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 820
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01976198
Train loss (w/o reg) on all data: 0.016339814
Test loss (w/o reg) on all data: 0.006809496
Train acc on all data:  0.9958667639192803
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7200304e-07
Norm of the params: 8.273046
     Influence (LOO): fixed 332 labels. Loss 0.00681. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.000960122
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1622027e-08
Norm of the params: 6.092811
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20482203
Train loss (w/o reg) on all data: 0.1978316
Test loss (w/o reg) on all data: 0.075669914
Train acc on all data:  0.936542669584245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.7960727e-06
Norm of the params: 11.824074
              Random: fixed  87 labels. Loss 0.07567. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 1025
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015699184
Train loss (w/o reg) on all data: 0.012807527
Test loss (w/o reg) on all data: 0.0058309855
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0460726e-07
Norm of the params: 7.6048107
     Influence (LOO): fixed 337 labels. Loss 0.00583. Accuracy 1.000.
Using normal model
LBFGS training took [18] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012006
Test loss (w/o reg) on all data: 0.0026560735
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3420107e-08
Norm of the params: 6.0928144
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260228
Train loss (w/o reg) on all data: 0.18546574
Test loss (w/o reg) on all data: 0.069709174
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.8099196e-06
Norm of the params: 11.946998
              Random: fixed 107 labels. Loss 0.06971. Accuracy 0.995.
### Flips: 410, rs: 39, checks: 1230
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009823111
Train loss (w/o reg) on all data: 0.0075752567
Test loss (w/o reg) on all data: 0.003752506
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3989966e-08
Norm of the params: 6.705005
     Influence (LOO): fixed 343 labels. Loss 0.00375. Accuracy 1.000.
Using normal model
LBFGS training took [16] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601227
Test loss (w/o reg) on all data: 0.0026560847
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.000708e-08
Norm of the params: 6.09281
                Loss: fixed 349 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153147
Train loss (w/o reg) on all data: 0.1742743
Test loss (w/o reg) on all data: 0.06500602
Train acc on all data:  0.9455385363481643
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5367712e-05
Norm of the params: 12.047554
              Random: fixed 124 labels. Loss 0.06501. Accuracy 0.997.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33491775
Train loss (w/o reg) on all data: 0.32845888
Test loss (w/o reg) on all data: 0.14969392
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3321522e-05
Norm of the params: 11.36562
Flipped loss: 0.14969. Accuracy: 0.990
### Flips: 615, rs: 0, checks: 205
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23026583
Train loss (w/o reg) on all data: 0.22162709
Test loss (w/o reg) on all data: 0.09343984
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.79997e-05
Norm of the params: 13.144382
     Influence (LOO): fixed 186 labels. Loss 0.09344. Accuracy 0.999.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18907633
Train loss (w/o reg) on all data: 0.17441344
Test loss (w/o reg) on all data: 0.10161169
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.1140179e-05
Norm of the params: 17.124777
                Loss: fixed 205 labels. Loss 0.10161. Accuracy 0.976.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32765397
Train loss (w/o reg) on all data: 0.321064
Test loss (w/o reg) on all data: 0.1456454
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7675155e-05
Norm of the params: 11.480408
              Random: fixed  17 labels. Loss 0.14565. Accuracy 0.988.
### Flips: 615, rs: 0, checks: 410
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14995094
Train loss (w/o reg) on all data: 0.14230259
Test loss (w/o reg) on all data: 0.05402923
Train acc on all data:  0.9521030877704838
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9822115e-06
Norm of the params: 12.367988
     Influence (LOO): fixed 326 labels. Loss 0.05403. Accuracy 1.000.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06475909
Train loss (w/o reg) on all data: 0.049815048
Test loss (w/o reg) on all data: 0.03440046
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.8864696e-06
Norm of the params: 17.288174
                Loss: fixed 410 labels. Loss 0.03440. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31388643
Train loss (w/o reg) on all data: 0.3072485
Test loss (w/o reg) on all data: 0.13717529
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.6184293e-05
Norm of the params: 11.522083
              Random: fixed  47 labels. Loss 0.13718. Accuracy 0.987.
### Flips: 615, rs: 0, checks: 615
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095694944
Train loss (w/o reg) on all data: 0.08895321
Test loss (w/o reg) on all data: 0.034324974
Train acc on all data:  0.9705810843666424
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2860366e-06
Norm of the params: 11.611831
     Influence (LOO): fixed 403 labels. Loss 0.03432. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004232459
Train loss (w/o reg) on all data: 0.0015869394
Test loss (w/o reg) on all data: 0.0030426993
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4585497e-08
Norm of the params: 7.2739525
                Loss: fixed 522 labels. Loss 0.00304. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30577654
Train loss (w/o reg) on all data: 0.29899248
Test loss (w/o reg) on all data: 0.13254113
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2300298e-05
Norm of the params: 11.648211
              Random: fixed  64 labels. Loss 0.13254. Accuracy 0.986.
### Flips: 615, rs: 0, checks: 820
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06683676
Train loss (w/o reg) on all data: 0.06115264
Test loss (w/o reg) on all data: 0.02254622
Train acc on all data:  0.9800632141988816
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5788166e-06
Norm of the params: 10.662191
     Influence (LOO): fixed 441 labels. Loss 0.02255. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003500654
Train loss (w/o reg) on all data: 0.0012505119
Test loss (w/o reg) on all data: 0.00283165
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2106212e-08
Norm of the params: 6.708416
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2922358
Train loss (w/o reg) on all data: 0.28541005
Test loss (w/o reg) on all data: 0.12293274
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.8829058e-05
Norm of the params: 11.683972
              Random: fixed  96 labels. Loss 0.12293. Accuracy 0.992.
### Flips: 615, rs: 0, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048788927
Train loss (w/o reg) on all data: 0.04350733
Test loss (w/o reg) on all data: 0.015524786
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7383587e-06
Norm of the params: 10.2777405
     Influence (LOO): fixed 467 labels. Loss 0.01552. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035006544
Train loss (w/o reg) on all data: 0.0012505181
Test loss (w/o reg) on all data: 0.0028316795
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0805595e-08
Norm of the params: 6.708407
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27961165
Train loss (w/o reg) on all data: 0.2727868
Test loss (w/o reg) on all data: 0.116040125
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.120869e-06
Norm of the params: 11.6832075
              Random: fixed 121 labels. Loss 0.11604. Accuracy 0.996.
### Flips: 615, rs: 0, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03529987
Train loss (w/o reg) on all data: 0.030778738
Test loss (w/o reg) on all data: 0.01063708
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 1.113986e-06
Norm of the params: 9.509085
     Influence (LOO): fixed 487 labels. Loss 0.01064. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035006544
Train loss (w/o reg) on all data: 0.0012505207
Test loss (w/o reg) on all data: 0.0028316483
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.792256e-08
Norm of the params: 6.708403
                Loss: fixed 523 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26684967
Train loss (w/o reg) on all data: 0.25988635
Test loss (w/o reg) on all data: 0.10798503
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.174175e-06
Norm of the params: 11.801124
              Random: fixed 149 labels. Loss 0.10799. Accuracy 0.996.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33779484
Train loss (w/o reg) on all data: 0.33175436
Test loss (w/o reg) on all data: 0.15055221
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.3948835e-06
Norm of the params: 10.991333
Flipped loss: 0.15055. Accuracy: 0.992
### Flips: 615, rs: 1, checks: 205
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23624156
Train loss (w/o reg) on all data: 0.22765568
Test loss (w/o reg) on all data: 0.09171024
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9401929e-05
Norm of the params: 13.104107
     Influence (LOO): fixed 187 labels. Loss 0.09171. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1961473
Train loss (w/o reg) on all data: 0.18377183
Test loss (w/o reg) on all data: 0.09547768
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.06559e-05
Norm of the params: 15.732423
                Loss: fixed 205 labels. Loss 0.09548. Accuracy 0.981.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32710168
Train loss (w/o reg) on all data: 0.32093954
Test loss (w/o reg) on all data: 0.14167744
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2243225e-05
Norm of the params: 11.101468
              Random: fixed  29 labels. Loss 0.14168. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 410
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15429603
Train loss (w/o reg) on all data: 0.14511403
Test loss (w/o reg) on all data: 0.058027677
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.0165182e-06
Norm of the params: 13.55138
     Influence (LOO): fixed 320 labels. Loss 0.05803. Accuracy 0.998.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07172456
Train loss (w/o reg) on all data: 0.057827704
Test loss (w/o reg) on all data: 0.036711615
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.55326e-06
Norm of the params: 16.671446
                Loss: fixed 410 labels. Loss 0.03671. Accuracy 0.994.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3154531
Train loss (w/o reg) on all data: 0.30930513
Test loss (w/o reg) on all data: 0.13228853
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.6554228e-05
Norm of the params: 11.088724
              Random: fixed  59 labels. Loss 0.13229. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102318965
Train loss (w/o reg) on all data: 0.094907455
Test loss (w/o reg) on all data: 0.035641808
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6875923e-06
Norm of the params: 12.174981
     Influence (LOO): fixed 401 labels. Loss 0.03564. Accuracy 1.000.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005629663
Train loss (w/o reg) on all data: 0.0021620085
Test loss (w/o reg) on all data: 0.0029759659
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.846861e-08
Norm of the params: 8.32785
                Loss: fixed 529 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.308523
Train loss (w/o reg) on all data: 0.30220073
Test loss (w/o reg) on all data: 0.12747045
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.2849903e-05
Norm of the params: 11.244779
              Random: fixed  75 labels. Loss 0.12747. Accuracy 0.991.
### Flips: 615, rs: 1, checks: 820
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06569608
Train loss (w/o reg) on all data: 0.059256054
Test loss (w/o reg) on all data: 0.023960086
Train acc on all data:  0.9810357403355215
Test acc on all data:   1.0
Norm of the mean of gradients: 7.395238e-06
Norm of the params: 11.349031
     Influence (LOO): fixed 454 labels. Loss 0.02396. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043270574
Train loss (w/o reg) on all data: 0.0015154037
Test loss (w/o reg) on all data: 0.0028312742
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.939558e-08
Norm of the params: 7.498872
                Loss: fixed 531 labels. Loss 0.00283. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29677805
Train loss (w/o reg) on all data: 0.29025328
Test loss (w/o reg) on all data: 0.12102327
Train acc on all data:  0.8954534403112083
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1405769e-05
Norm of the params: 11.423461
              Random: fixed 103 labels. Loss 0.12102. Accuracy 0.992.
### Flips: 615, rs: 1, checks: 1025
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049979895
Train loss (w/o reg) on all data: 0.04438502
Test loss (w/o reg) on all data: 0.016114576
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 8.060011e-07
Norm of the params: 10.578161
     Influence (LOO): fixed 476 labels. Loss 0.01611. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601218
Test loss (w/o reg) on all data: 0.0026560787
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7924191e-08
Norm of the params: 6.092812
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28332102
Train loss (w/o reg) on all data: 0.27655622
Test loss (w/o reg) on all data: 0.11356198
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.689818e-06
Norm of the params: 11.631674
              Random: fixed 131 labels. Loss 0.11356. Accuracy 0.994.
### Flips: 615, rs: 1, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034198876
Train loss (w/o reg) on all data: 0.029904915
Test loss (w/o reg) on all data: 0.010139569
Train acc on all data:  0.9919766593727206
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6277894e-06
Norm of the params: 9.267105
     Influence (LOO): fixed 499 labels. Loss 0.01014. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.00096011726
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5095855e-08
Norm of the params: 6.0928183
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2712382
Train loss (w/o reg) on all data: 0.2647423
Test loss (w/o reg) on all data: 0.105735
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.075837e-06
Norm of the params: 11.398157
              Random: fixed 156 labels. Loss 0.10573. Accuracy 0.994.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34383342
Train loss (w/o reg) on all data: 0.33754396
Test loss (w/o reg) on all data: 0.16605145
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.073671e-05
Norm of the params: 11.215576
Flipped loss: 0.16605. Accuracy: 0.984
### Flips: 615, rs: 2, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23863725
Train loss (w/o reg) on all data: 0.22936529
Test loss (w/o reg) on all data: 0.112439476
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.9870753e-05
Norm of the params: 13.617612
     Influence (LOO): fixed 181 labels. Loss 0.11244. Accuracy 0.993.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20126079
Train loss (w/o reg) on all data: 0.18885545
Test loss (w/o reg) on all data: 0.11992889
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.1272291e-05
Norm of the params: 15.751406
                Loss: fixed 205 labels. Loss 0.11993. Accuracy 0.975.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33002758
Train loss (w/o reg) on all data: 0.3235479
Test loss (w/o reg) on all data: 0.15410388
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0379885e-05
Norm of the params: 11.383921
              Random: fixed  32 labels. Loss 0.15410. Accuracy 0.988.
### Flips: 615, rs: 2, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16200015
Train loss (w/o reg) on all data: 0.15342832
Test loss (w/o reg) on all data: 0.07169006
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.243035e-06
Norm of the params: 13.09338
     Influence (LOO): fixed 311 labels. Loss 0.07169. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07361569
Train loss (w/o reg) on all data: 0.057393648
Test loss (w/o reg) on all data: 0.061632216
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.915609e-06
Norm of the params: 18.012243
                Loss: fixed 410 labels. Loss 0.06163. Accuracy 0.980.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32001725
Train loss (w/o reg) on all data: 0.31330281
Test loss (w/o reg) on all data: 0.14894757
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1958909e-05
Norm of the params: 11.588301
              Random: fixed  56 labels. Loss 0.14895. Accuracy 0.986.
### Flips: 615, rs: 2, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1114435
Train loss (w/o reg) on all data: 0.1035028
Test loss (w/o reg) on all data: 0.046971302
Train acc on all data:  0.9647459275468028
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2451126e-06
Norm of the params: 12.6021385
     Influence (LOO): fixed 391 labels. Loss 0.04697. Accuracy 0.997.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008384401
Train loss (w/o reg) on all data: 0.0039466335
Test loss (w/o reg) on all data: 0.006337915
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.1906856e-07
Norm of the params: 9.421006
                Loss: fixed 529 labels. Loss 0.00634. Accuracy 0.998.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31025225
Train loss (w/o reg) on all data: 0.30351382
Test loss (w/o reg) on all data: 0.14415284
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.138615e-06
Norm of the params: 11.6089735
              Random: fixed  76 labels. Loss 0.14415. Accuracy 0.986.
### Flips: 615, rs: 2, checks: 820
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07470516
Train loss (w/o reg) on all data: 0.06675705
Test loss (w/o reg) on all data: 0.029237203
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.4533333e-06
Norm of the params: 12.608023
     Influence (LOO): fixed 446 labels. Loss 0.02924. Accuracy 0.998.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0038359233
Train loss (w/o reg) on all data: 0.001430853
Test loss (w/o reg) on all data: 0.0032010612
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3445003e-07
Norm of the params: 6.935518
                Loss: fixed 537 labels. Loss 0.00320. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29917777
Train loss (w/o reg) on all data: 0.29231733
Test loss (w/o reg) on all data: 0.13698414
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.6051444e-05
Norm of the params: 11.713603
              Random: fixed 101 labels. Loss 0.13698. Accuracy 0.988.
### Flips: 615, rs: 2, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055170238
Train loss (w/o reg) on all data: 0.04859708
Test loss (w/o reg) on all data: 0.021265883
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3021681e-06
Norm of the params: 11.465738
     Influence (LOO): fixed 475 labels. Loss 0.02127. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003835923
Train loss (w/o reg) on all data: 0.0014308422
Test loss (w/o reg) on all data: 0.0032010681
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1243387e-07
Norm of the params: 6.935533
                Loss: fixed 537 labels. Loss 0.00320. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2849328
Train loss (w/o reg) on all data: 0.2778343
Test loss (w/o reg) on all data: 0.1272648
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.406823e-05
Norm of the params: 11.915124
              Random: fixed 128 labels. Loss 0.12726. Accuracy 0.990.
### Flips: 615, rs: 2, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038141742
Train loss (w/o reg) on all data: 0.03284953
Test loss (w/o reg) on all data: 0.01306539
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9564184e-07
Norm of the params: 10.288062
     Influence (LOO): fixed 498 labels. Loss 0.01307. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034790626
Train loss (w/o reg) on all data: 0.0012706107
Test loss (w/o reg) on all data: 0.0029040836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0205687e-08
Norm of the params: 6.6459794
                Loss: fixed 538 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2739839
Train loss (w/o reg) on all data: 0.2668872
Test loss (w/o reg) on all data: 0.11979465
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8870827e-05
Norm of the params: 11.913602
              Random: fixed 154 labels. Loss 0.11979. Accuracy 0.993.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33489716
Train loss (w/o reg) on all data: 0.32893822
Test loss (w/o reg) on all data: 0.1529348
Train acc on all data:  0.8769754437150499
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2002228e-05
Norm of the params: 10.91692
Flipped loss: 0.15293. Accuracy: 0.993
### Flips: 615, rs: 3, checks: 205
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22515427
Train loss (w/o reg) on all data: 0.2161961
Test loss (w/o reg) on all data: 0.09443521
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.774037e-06
Norm of the params: 13.38519
     Influence (LOO): fixed 187 labels. Loss 0.09444. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19170384
Train loss (w/o reg) on all data: 0.17735437
Test loss (w/o reg) on all data: 0.103986405
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4134246e-05
Norm of the params: 16.940767
                Loss: fixed 205 labels. Loss 0.10399. Accuracy 0.980.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.324837
Train loss (w/o reg) on all data: 0.31905094
Test loss (w/o reg) on all data: 0.14605233
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1319917e-05
Norm of the params: 10.757388
              Random: fixed  25 labels. Loss 0.14605. Accuracy 0.992.
### Flips: 615, rs: 3, checks: 410
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1490506
Train loss (w/o reg) on all data: 0.14103754
Test loss (w/o reg) on all data: 0.05878041
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2957448e-05
Norm of the params: 12.659429
     Influence (LOO): fixed 318 labels. Loss 0.05878. Accuracy 0.997.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06333841
Train loss (w/o reg) on all data: 0.04729468
Test loss (w/o reg) on all data: 0.031903546
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8032615e-06
Norm of the params: 17.912971
                Loss: fixed 410 labels. Loss 0.03190. Accuracy 0.993.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31573343
Train loss (w/o reg) on all data: 0.30975562
Test loss (w/o reg) on all data: 0.13817367
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.00077295e-05
Norm of the params: 10.934169
              Random: fixed  48 labels. Loss 0.13817. Accuracy 0.995.
### Flips: 615, rs: 3, checks: 615
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098555915
Train loss (w/o reg) on all data: 0.09134774
Test loss (w/o reg) on all data: 0.03579849
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7454942e-06
Norm of the params: 12.00681
     Influence (LOO): fixed 399 labels. Loss 0.03580. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0057460926
Train loss (w/o reg) on all data: 0.0022950734
Test loss (w/o reg) on all data: 0.002870312
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9184024e-07
Norm of the params: 8.307851
                Loss: fixed 520 labels. Loss 0.00287. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304534
Train loss (w/o reg) on all data: 0.2985625
Test loss (w/o reg) on all data: 0.1303585
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.740712e-05
Norm of the params: 10.928399
              Random: fixed  74 labels. Loss 0.13036. Accuracy 0.996.
### Flips: 615, rs: 3, checks: 820
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06852581
Train loss (w/o reg) on all data: 0.062357385
Test loss (w/o reg) on all data: 0.02242863
Train acc on all data:  0.9805494772672015
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3746856e-06
Norm of the params: 11.107138
     Influence (LOO): fixed 444 labels. Loss 0.02243. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043543344
Train loss (w/o reg) on all data: 0.0016682955
Test loss (w/o reg) on all data: 0.0027570808
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4467913e-08
Norm of the params: 7.329446
                Loss: fixed 522 labels. Loss 0.00276. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28819427
Train loss (w/o reg) on all data: 0.28186414
Test loss (w/o reg) on all data: 0.118418545
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.3215012e-05
Norm of the params: 11.251787
              Random: fixed 110 labels. Loss 0.11842. Accuracy 0.998.
### Flips: 615, rs: 3, checks: 1025
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04788706
Train loss (w/o reg) on all data: 0.04289694
Test loss (w/o reg) on all data: 0.015382959
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0671791e-06
Norm of the params: 9.990113
     Influence (LOO): fixed 474 labels. Loss 0.01538. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036663134
Train loss (w/o reg) on all data: 0.001383412
Test loss (w/o reg) on all data: 0.0026188763
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7092376e-08
Norm of the params: 6.7570724
                Loss: fixed 523 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2763325
Train loss (w/o reg) on all data: 0.26992327
Test loss (w/o reg) on all data: 0.11072159
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9184467e-05
Norm of the params: 11.321863
              Random: fixed 137 labels. Loss 0.11072. Accuracy 0.997.
### Flips: 615, rs: 3, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037334755
Train loss (w/o reg) on all data: 0.032194898
Test loss (w/o reg) on all data: 0.012156464
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 5.322692e-07
Norm of the params: 10.138892
     Influence (LOO): fixed 487 labels. Loss 0.01216. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036663138
Train loss (w/o reg) on all data: 0.0013834178
Test loss (w/o reg) on all data: 0.00261888
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8824034e-08
Norm of the params: 6.7570643
                Loss: fixed 523 labels. Loss 0.00262. Accuracy 1.000.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26106784
Train loss (w/o reg) on all data: 0.25441253
Test loss (w/o reg) on all data: 0.10230452
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8336334e-05
Norm of the params: 11.537174
              Random: fixed 168 labels. Loss 0.10230. Accuracy 0.998.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34223163
Train loss (w/o reg) on all data: 0.33608177
Test loss (w/o reg) on all data: 0.16295554
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.42523e-05
Norm of the params: 11.090401
Flipped loss: 0.16296. Accuracy: 0.990
### Flips: 615, rs: 4, checks: 205
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23796362
Train loss (w/o reg) on all data: 0.22848172
Test loss (w/o reg) on all data: 0.10246167
Train acc on all data:  0.9151470945781668
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.189408e-06
Norm of the params: 13.770911
     Influence (LOO): fixed 186 labels. Loss 0.10246. Accuracy 0.996.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19999856
Train loss (w/o reg) on all data: 0.1862052
Test loss (w/o reg) on all data: 0.11116765
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.005059e-06
Norm of the params: 16.609253
                Loss: fixed 205 labels. Loss 0.11117. Accuracy 0.978.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32656452
Train loss (w/o reg) on all data: 0.32046875
Test loss (w/o reg) on all data: 0.15003131
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3424138e-05
Norm of the params: 11.041516
              Random: fixed  36 labels. Loss 0.15003. Accuracy 0.991.
### Flips: 615, rs: 4, checks: 410
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16117036
Train loss (w/o reg) on all data: 0.15314548
Test loss (w/o reg) on all data: 0.059613343
Train acc on all data:  0.9457816678823243
Test acc on all data:   1.0
Norm of the mean of gradients: 9.12319e-06
Norm of the params: 12.668767
     Influence (LOO): fixed 322 labels. Loss 0.05961. Accuracy 1.000.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07683524
Train loss (w/o reg) on all data: 0.06136806
Test loss (w/o reg) on all data: 0.04689543
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.243024e-06
Norm of the params: 17.588167
                Loss: fixed 410 labels. Loss 0.04690. Accuracy 0.986.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32141474
Train loss (w/o reg) on all data: 0.3154795
Test loss (w/o reg) on all data: 0.14565665
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0951772e-05
Norm of the params: 10.895192
              Random: fixed  53 labels. Loss 0.14566. Accuracy 0.991.
### Flips: 615, rs: 4, checks: 615
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104244985
Train loss (w/o reg) on all data: 0.096792206
Test loss (w/o reg) on all data: 0.035303127
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 6.105279e-06
Norm of the params: 12.208831
     Influence (LOO): fixed 408 labels. Loss 0.03530. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007518418
Train loss (w/o reg) on all data: 0.003321599
Test loss (w/o reg) on all data: 0.0042445627
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1113193e-07
Norm of the params: 9.161681
                Loss: fixed 535 labels. Loss 0.00424. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30959988
Train loss (w/o reg) on all data: 0.3038691
Test loss (w/o reg) on all data: 0.1358982
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.299898e-06
Norm of the params: 10.705876
              Random: fixed  83 labels. Loss 0.13590. Accuracy 0.997.
### Flips: 615, rs: 4, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07496712
Train loss (w/o reg) on all data: 0.06824819
Test loss (w/o reg) on all data: 0.024407636
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5079563e-06
Norm of the params: 11.59218
     Influence (LOO): fixed 450 labels. Loss 0.02441. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058502564
Train loss (w/o reg) on all data: 0.002491742
Test loss (w/o reg) on all data: 0.0043949173
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5029346e-08
Norm of the params: 8.195748
                Loss: fixed 539 labels. Loss 0.00439. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29674613
Train loss (w/o reg) on all data: 0.29068992
Test loss (w/o reg) on all data: 0.13075805
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.945162e-06
Norm of the params: 11.005659
              Random: fixed 108 labels. Loss 0.13076. Accuracy 0.994.
### Flips: 615, rs: 4, checks: 1025
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055745218
Train loss (w/o reg) on all data: 0.049850576
Test loss (w/o reg) on all data: 0.018009275
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 4.425678e-06
Norm of the params: 10.857847
     Influence (LOO): fixed 479 labels. Loss 0.01801. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036378535
Train loss (w/o reg) on all data: 0.0013172977
Test loss (w/o reg) on all data: 0.0026034757
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6394048e-08
Norm of the params: 6.812571
                Loss: fixed 542 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28400415
Train loss (w/o reg) on all data: 0.27751032
Test loss (w/o reg) on all data: 0.12289199
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3606957e-05
Norm of the params: 11.3963585
              Random: fixed 135 labels. Loss 0.12289. Accuracy 0.993.
### Flips: 615, rs: 4, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04048094
Train loss (w/o reg) on all data: 0.035543893
Test loss (w/o reg) on all data: 0.012408923
Train acc on all data:  0.9897884755652808
Test acc on all data:   1.0
Norm of the mean of gradients: 9.228561e-07
Norm of the params: 9.93685
     Influence (LOO): fixed 501 labels. Loss 0.01241. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012664
Test loss (w/o reg) on all data: 0.0026561038
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.044586e-08
Norm of the params: 6.0928044
                Loss: fixed 543 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27263352
Train loss (w/o reg) on all data: 0.26631323
Test loss (w/o reg) on all data: 0.11524175
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.37664265e-05
Norm of the params: 11.243047
              Random: fixed 160 labels. Loss 0.11524. Accuracy 0.995.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34328127
Train loss (w/o reg) on all data: 0.3368946
Test loss (w/o reg) on all data: 0.15875483
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.547122e-06
Norm of the params: 11.301922
Flipped loss: 0.15875. Accuracy: 0.990
### Flips: 615, rs: 5, checks: 205
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24129716
Train loss (w/o reg) on all data: 0.23338965
Test loss (w/o reg) on all data: 0.10367965
Train acc on all data:  0.9134451738390469
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.980827e-06
Norm of the params: 12.57578
     Influence (LOO): fixed 179 labels. Loss 0.10368. Accuracy 0.995.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20435524
Train loss (w/o reg) on all data: 0.19248593
Test loss (w/o reg) on all data: 0.10943523
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.7392158e-05
Norm of the params: 15.407339
                Loss: fixed 205 labels. Loss 0.10944. Accuracy 0.978.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3311748
Train loss (w/o reg) on all data: 0.32479763
Test loss (w/o reg) on all data: 0.14637378
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4014706e-05
Norm of the params: 11.29351
              Random: fixed  30 labels. Loss 0.14637. Accuracy 0.993.
### Flips: 615, rs: 5, checks: 410
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15711989
Train loss (w/o reg) on all data: 0.14867303
Test loss (w/o reg) on all data: 0.0650885
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6763444e-06
Norm of the params: 12.997585
     Influence (LOO): fixed 314 labels. Loss 0.06509. Accuracy 0.998.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075067975
Train loss (w/o reg) on all data: 0.06087951
Test loss (w/o reg) on all data: 0.049556855
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.2293523e-06
Norm of the params: 16.845453
                Loss: fixed 410 labels. Loss 0.04956. Accuracy 0.987.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31790882
Train loss (w/o reg) on all data: 0.311736
Test loss (w/o reg) on all data: 0.1366065
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.3846274e-05
Norm of the params: 11.111109
              Random: fixed  63 labels. Loss 0.13661. Accuracy 0.992.
### Flips: 615, rs: 5, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1068005
Train loss (w/o reg) on all data: 0.098735385
Test loss (w/o reg) on all data: 0.042379357
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.1872163e-06
Norm of the params: 12.70048
     Influence (LOO): fixed 396 labels. Loss 0.04238. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042359037
Train loss (w/o reg) on all data: 0.0015771972
Test loss (w/o reg) on all data: 0.0031910567
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2077679e-07
Norm of the params: 7.2920594
                Loss: fixed 534 labels. Loss 0.00319. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30324617
Train loss (w/o reg) on all data: 0.29682654
Test loss (w/o reg) on all data: 0.12936178
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 7.1612844e-06
Norm of the params: 11.331035
              Random: fixed  95 labels. Loss 0.12936. Accuracy 0.992.
### Flips: 615, rs: 5, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07217111
Train loss (w/o reg) on all data: 0.06549305
Test loss (w/o reg) on all data: 0.026307143
Train acc on all data:  0.9783612934597617
Test acc on all data:   1.0
Norm of the mean of gradients: 7.382897e-06
Norm of the params: 11.556865
     Influence (LOO): fixed 447 labels. Loss 0.02631. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096011715
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2305944e-08
Norm of the params: 6.092819
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28821886
Train loss (w/o reg) on all data: 0.2820473
Test loss (w/o reg) on all data: 0.11930741
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.4774781e-05
Norm of the params: 11.109952
              Random: fixed 126 labels. Loss 0.11931. Accuracy 0.993.
### Flips: 615, rs: 5, checks: 1025
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05407033
Train loss (w/o reg) on all data: 0.048088733
Test loss (w/o reg) on all data: 0.019420205
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1001488e-06
Norm of the params: 10.93764
     Influence (LOO): fixed 474 labels. Loss 0.01942. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601176
Test loss (w/o reg) on all data: 0.0026560614
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.379369e-09
Norm of the params: 6.092819
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2726025
Train loss (w/o reg) on all data: 0.26599246
Test loss (w/o reg) on all data: 0.111865744
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1348676e-05
Norm of the params: 11.497865
              Random: fixed 156 labels. Loss 0.11187. Accuracy 0.996.
### Flips: 615, rs: 5, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044199664
Train loss (w/o reg) on all data: 0.03902321
Test loss (w/o reg) on all data: 0.015500589
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3357175e-06
Norm of the params: 10.174926
     Influence (LOO): fixed 488 labels. Loss 0.01550. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601105
Test loss (w/o reg) on all data: 0.0026560593
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6576993e-08
Norm of the params: 6.0928307
                Loss: fixed 536 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26445597
Train loss (w/o reg) on all data: 0.25730187
Test loss (w/o reg) on all data: 0.107214995
Train acc on all data:  0.9100413323608072
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2987606e-05
Norm of the params: 11.9616995
              Random: fixed 172 labels. Loss 0.10721. Accuracy 0.997.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33447415
Train loss (w/o reg) on all data: 0.3283726
Test loss (w/o reg) on all data: 0.15692225
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0088007e-05
Norm of the params: 11.04675
Flipped loss: 0.15692. Accuracy: 0.990
### Flips: 615, rs: 6, checks: 205
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22690855
Train loss (w/o reg) on all data: 0.21786338
Test loss (w/o reg) on all data: 0.10383255
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1576674e-05
Norm of the params: 13.450037
     Influence (LOO): fixed 187 labels. Loss 0.10383. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19240904
Train loss (w/o reg) on all data: 0.17878592
Test loss (w/o reg) on all data: 0.111384295
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.6323223e-05
Norm of the params: 16.506432
                Loss: fixed 205 labels. Loss 0.11138. Accuracy 0.970.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32380056
Train loss (w/o reg) on all data: 0.3174357
Test loss (w/o reg) on all data: 0.15087488
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.14966515e-05
Norm of the params: 11.2825985
              Random: fixed  26 labels. Loss 0.15087. Accuracy 0.989.
### Flips: 615, rs: 6, checks: 410
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15230246
Train loss (w/o reg) on all data: 0.14400843
Test loss (w/o reg) on all data: 0.06223128
Train acc on all data:  0.949914903963044
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9835513e-06
Norm of the params: 12.879469
     Influence (LOO): fixed 321 labels. Loss 0.06223. Accuracy 1.000.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0694585
Train loss (w/o reg) on all data: 0.053703908
Test loss (w/o reg) on all data: 0.04660283
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.7390495e-06
Norm of the params: 17.750828
                Loss: fixed 409 labels. Loss 0.04660. Accuracy 0.987.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31380144
Train loss (w/o reg) on all data: 0.30719924
Test loss (w/o reg) on all data: 0.14521669
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.4576262e-05
Norm of the params: 11.491033
              Random: fixed  48 labels. Loss 0.14522. Accuracy 0.991.
### Flips: 615, rs: 6, checks: 615
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10731936
Train loss (w/o reg) on all data: 0.09969661
Test loss (w/o reg) on all data: 0.04253197
Train acc on all data:  0.9659615852176027
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3217965e-05
Norm of the params: 12.347272
     Influence (LOO): fixed 391 labels. Loss 0.04253. Accuracy 1.000.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007021242
Train loss (w/o reg) on all data: 0.0031650781
Test loss (w/o reg) on all data: 0.0060908897
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7384764e-07
Norm of the params: 8.781986
                Loss: fixed 524 labels. Loss 0.00609. Accuracy 0.999.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30329275
Train loss (w/o reg) on all data: 0.29662195
Test loss (w/o reg) on all data: 0.13701041
Train acc on all data:  0.8901045465596887
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9966736e-05
Norm of the params: 11.550589
              Random: fixed  74 labels. Loss 0.13701. Accuracy 0.992.
### Flips: 615, rs: 6, checks: 820
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07273143
Train loss (w/o reg) on all data: 0.066261515
Test loss (w/o reg) on all data: 0.026726035
Train acc on all data:  0.9786044249939218
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6814043e-06
Norm of the params: 11.375338
     Influence (LOO): fixed 444 labels. Loss 0.02673. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003154451
Train loss (w/o reg) on all data: 0.0010668448
Test loss (w/o reg) on all data: 0.0025823025
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.539927e-08
Norm of the params: 6.461589
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29505143
Train loss (w/o reg) on all data: 0.28824303
Test loss (w/o reg) on all data: 0.13137908
Train acc on all data:  0.8944809141745684
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.731528e-05
Norm of the params: 11.669102
              Random: fixed  91 labels. Loss 0.13138. Accuracy 0.993.
### Flips: 615, rs: 6, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050894577
Train loss (w/o reg) on all data: 0.045142777
Test loss (w/o reg) on all data: 0.018754302
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1752096e-06
Norm of the params: 10.725484
     Influence (LOO): fixed 473 labels. Loss 0.01875. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668425
Test loss (w/o reg) on all data: 0.0025823326
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.255158e-08
Norm of the params: 6.4615917
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28402695
Train loss (w/o reg) on all data: 0.27743775
Test loss (w/o reg) on all data: 0.12749988
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.244803e-06
Norm of the params: 11.479725
              Random: fixed 114 labels. Loss 0.12750. Accuracy 0.990.
### Flips: 615, rs: 6, checks: 1230
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037529323
Train loss (w/o reg) on all data: 0.032470122
Test loss (w/o reg) on all data: 0.013567091
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0877222e-06
Norm of the params: 10.059026
     Influence (LOO): fixed 491 labels. Loss 0.01357. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544515
Train loss (w/o reg) on all data: 0.0010668356
Test loss (w/o reg) on all data: 0.0025823
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0715751e-08
Norm of the params: 6.461603
                Loss: fixed 531 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2717511
Train loss (w/o reg) on all data: 0.2651425
Test loss (w/o reg) on all data: 0.1202672
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5539397e-05
Norm of the params: 11.496608
              Random: fixed 142 labels. Loss 0.12027. Accuracy 0.990.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34101433
Train loss (w/o reg) on all data: 0.33395815
Test loss (w/o reg) on all data: 0.16277082
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.5482703e-06
Norm of the params: 11.879552
Flipped loss: 0.16277. Accuracy: 0.984
### Flips: 615, rs: 7, checks: 205
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23931043
Train loss (w/o reg) on all data: 0.23024881
Test loss (w/o reg) on all data: 0.10701552
Train acc on all data:  0.912229516168247
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4016269e-05
Norm of the params: 13.462259
     Influence (LOO): fixed 181 labels. Loss 0.10702. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20469093
Train loss (w/o reg) on all data: 0.19115552
Test loss (w/o reg) on all data: 0.112433665
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7298722e-05
Norm of the params: 16.453215
                Loss: fixed 205 labels. Loss 0.11243. Accuracy 0.974.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32826465
Train loss (w/o reg) on all data: 0.32111385
Test loss (w/o reg) on all data: 0.14948942
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7118966e-05
Norm of the params: 11.95894
              Random: fixed  31 labels. Loss 0.14949. Accuracy 0.990.
### Flips: 615, rs: 7, checks: 410
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1651482
Train loss (w/o reg) on all data: 0.15652089
Test loss (w/o reg) on all data: 0.06879225
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.853387e-06
Norm of the params: 13.135684
     Influence (LOO): fixed 312 labels. Loss 0.06879. Accuracy 0.998.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08153202
Train loss (w/o reg) on all data: 0.06590396
Test loss (w/o reg) on all data: 0.051708326
Train acc on all data:  0.973984925844882
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4097282e-06
Norm of the params: 17.6794
                Loss: fixed 410 labels. Loss 0.05171. Accuracy 0.989.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31398162
Train loss (w/o reg) on all data: 0.30709884
Test loss (w/o reg) on all data: 0.14089887
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.2296592e-05
Norm of the params: 11.732684
              Random: fixed  67 labels. Loss 0.14090. Accuracy 0.990.
### Flips: 615, rs: 7, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12183614
Train loss (w/o reg) on all data: 0.11319653
Test loss (w/o reg) on all data: 0.050882075
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.837237e-06
Norm of the params: 13.145046
     Influence (LOO): fixed 379 labels. Loss 0.05088. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007933978
Train loss (w/o reg) on all data: 0.0036394557
Test loss (w/o reg) on all data: 0.004757282
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3405237e-07
Norm of the params: 9.267711
                Loss: fixed 542 labels. Loss 0.00476. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3026889
Train loss (w/o reg) on all data: 0.29571235
Test loss (w/o reg) on all data: 0.13493761
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.08484e-06
Norm of the params: 11.812313
              Random: fixed  93 labels. Loss 0.13494. Accuracy 0.991.
### Flips: 615, rs: 7, checks: 820
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081994735
Train loss (w/o reg) on all data: 0.07422335
Test loss (w/o reg) on all data: 0.031623535
Train acc on all data:  0.973984925844882
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5349741e-06
Norm of the params: 12.467067
     Influence (LOO): fixed 442 labels. Loss 0.03162. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004919571
Train loss (w/o reg) on all data: 0.0020054753
Test loss (w/o reg) on all data: 0.0038157054
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4802796e-07
Norm of the params: 7.6342597
                Loss: fixed 546 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2925633
Train loss (w/o reg) on all data: 0.28568
Test loss (w/o reg) on all data: 0.12773244
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.7143767e-06
Norm of the params: 11.733123
              Random: fixed 115 labels. Loss 0.12773. Accuracy 0.991.
### Flips: 615, rs: 7, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06219444
Train loss (w/o reg) on all data: 0.05518613
Test loss (w/o reg) on all data: 0.024378406
Train acc on all data:  0.9807926088013615
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8710087e-06
Norm of the params: 11.839183
     Influence (LOO): fixed 470 labels. Loss 0.02438. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034880317
Train loss (w/o reg) on all data: 0.0012594495
Test loss (w/o reg) on all data: 0.0030900189
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8978616e-08
Norm of the params: 6.6762
                Loss: fixed 548 labels. Loss 0.00309. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27838698
Train loss (w/o reg) on all data: 0.2713498
Test loss (w/o reg) on all data: 0.12009712
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0333648e-05
Norm of the params: 11.863557
              Random: fixed 145 labels. Loss 0.12010. Accuracy 0.993.
### Flips: 615, rs: 7, checks: 1230
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045068245
Train loss (w/o reg) on all data: 0.039155945
Test loss (w/o reg) on all data: 0.01663598
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 7.388899e-07
Norm of the params: 10.874099
     Influence (LOO): fixed 498 labels. Loss 0.01664. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4041215e-08
Norm of the params: 6.092816
                Loss: fixed 549 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26885986
Train loss (w/o reg) on all data: 0.2617855
Test loss (w/o reg) on all data: 0.11269669
Train acc on all data:  0.9093119377583273
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.9420083e-05
Norm of the params: 11.89484
              Random: fixed 165 labels. Loss 0.11270. Accuracy 0.996.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429344
Train loss (w/o reg) on all data: 0.33689013
Test loss (w/o reg) on all data: 0.15165615
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.495094e-05
Norm of the params: 10.994786
Flipped loss: 0.15166. Accuracy: 0.990
### Flips: 615, rs: 8, checks: 205
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24131513
Train loss (w/o reg) on all data: 0.23281527
Test loss (w/o reg) on all data: 0.09981807
Train acc on all data:  0.911986384634087
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.3545557e-05
Norm of the params: 13.038301
     Influence (LOO): fixed 183 labels. Loss 0.09982. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20720322
Train loss (w/o reg) on all data: 0.19390602
Test loss (w/o reg) on all data: 0.09731652
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.1474e-06
Norm of the params: 16.30779
                Loss: fixed 205 labels. Loss 0.09732. Accuracy 0.983.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33048087
Train loss (w/o reg) on all data: 0.32423604
Test loss (w/o reg) on all data: 0.14352652
Train acc on all data:  0.8794067590566497
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.5228179e-05
Norm of the params: 11.175731
              Random: fixed  31 labels. Loss 0.14353. Accuracy 0.991.
### Flips: 615, rs: 8, checks: 410
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1633477
Train loss (w/o reg) on all data: 0.15482119
Test loss (w/o reg) on all data: 0.06333619
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7443885e-05
Norm of the params: 13.058732
     Influence (LOO): fixed 314 labels. Loss 0.06334. Accuracy 0.999.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07915159
Train loss (w/o reg) on all data: 0.063797824
Test loss (w/o reg) on all data: 0.037827466
Train acc on all data:  0.9732555312424022
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.876637e-06
Norm of the params: 17.523567
                Loss: fixed 410 labels. Loss 0.03783. Accuracy 0.993.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31940693
Train loss (w/o reg) on all data: 0.31329352
Test loss (w/o reg) on all data: 0.13560383
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.1906862e-05
Norm of the params: 11.057496
              Random: fixed  55 labels. Loss 0.13560. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 615
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107862376
Train loss (w/o reg) on all data: 0.09968971
Test loss (w/o reg) on all data: 0.039873738
Train acc on all data:  0.9642596644784829
Test acc on all data:   1.0
Norm of the mean of gradients: 7.3408805e-06
Norm of the params: 12.784889
     Influence (LOO): fixed 399 labels. Loss 0.03987. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060732057
Train loss (w/o reg) on all data: 0.0026994636
Test loss (w/o reg) on all data: 0.004156945
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7930765e-08
Norm of the params: 8.214308
                Loss: fixed 540 labels. Loss 0.00416. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30791476
Train loss (w/o reg) on all data: 0.3016696
Test loss (w/o reg) on all data: 0.12944888
Train acc on all data:  0.8888888888888888
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.8609807e-05
Norm of the params: 11.176012
              Random: fixed  79 labels. Loss 0.12945. Accuracy 0.993.
### Flips: 615, rs: 8, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0785246
Train loss (w/o reg) on all data: 0.071214214
Test loss (w/o reg) on all data: 0.027437948
Train acc on all data:  0.975443715049842
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1455849e-06
Norm of the params: 12.091637
     Influence (LOO): fixed 445 labels. Loss 0.02744. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034138595
Train loss (w/o reg) on all data: 0.0012052646
Test loss (w/o reg) on all data: 0.0026837494
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.032035e-08
Norm of the params: 6.646194
                Loss: fixed 545 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2976827
Train loss (w/o reg) on all data: 0.2915752
Test loss (w/o reg) on all data: 0.12314282
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.40597e-05
Norm of the params: 11.052156
              Random: fixed 102 labels. Loss 0.12314. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062050793
Train loss (w/o reg) on all data: 0.055550985
Test loss (w/o reg) on all data: 0.020519616
Train acc on all data:  0.9820082664721614
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7324206e-06
Norm of the params: 11.401587
     Influence (LOO): fixed 472 labels. Loss 0.02052. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034138598
Train loss (w/o reg) on all data: 0.001205258
Test loss (w/o reg) on all data: 0.0026837634
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.627949e-08
Norm of the params: 6.6462045
                Loss: fixed 545 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28218013
Train loss (w/o reg) on all data: 0.27579394
Test loss (w/o reg) on all data: 0.11471307
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.0646105e-05
Norm of the params: 11.301503
              Random: fixed 136 labels. Loss 0.11471. Accuracy 0.994.
### Flips: 615, rs: 8, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044938922
Train loss (w/o reg) on all data: 0.03963234
Test loss (w/o reg) on all data: 0.014521819
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7733847e-06
Norm of the params: 10.302021
     Influence (LOO): fixed 495 labels. Loss 0.01452. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096014637
Test loss (w/o reg) on all data: 0.0026561043
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.0661328e-08
Norm of the params: 6.092772
                Loss: fixed 546 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26967987
Train loss (w/o reg) on all data: 0.2633853
Test loss (w/o reg) on all data: 0.106730774
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.286567e-06
Norm of the params: 11.22014
              Random: fixed 162 labels. Loss 0.10673. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33740157
Train loss (w/o reg) on all data: 0.3308191
Test loss (w/o reg) on all data: 0.1615944
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.6202388e-06
Norm of the params: 11.473851
Flipped loss: 0.16159. Accuracy: 0.986
### Flips: 615, rs: 9, checks: 205
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23501389
Train loss (w/o reg) on all data: 0.22586681
Test loss (w/o reg) on all data: 0.10538794
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.9334646e-06
Norm of the params: 13.525587
     Influence (LOO): fixed 185 labels. Loss 0.10539. Accuracy 0.994.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19994026
Train loss (w/o reg) on all data: 0.18707229
Test loss (w/o reg) on all data: 0.10827941
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.49579e-06
Norm of the params: 16.042427
                Loss: fixed 205 labels. Loss 0.10828. Accuracy 0.980.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3241165
Train loss (w/o reg) on all data: 0.31767675
Test loss (w/o reg) on all data: 0.15299566
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.386769e-06
Norm of the params: 11.348787
              Random: fixed  33 labels. Loss 0.15300. Accuracy 0.986.
### Flips: 615, rs: 9, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16345523
Train loss (w/o reg) on all data: 0.15484715
Test loss (w/o reg) on all data: 0.06365666
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.513778e-06
Norm of the params: 13.121048
     Influence (LOO): fixed 310 labels. Loss 0.06366. Accuracy 0.999.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07323243
Train loss (w/o reg) on all data: 0.05885264
Test loss (w/o reg) on all data: 0.04756346
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7662866e-06
Norm of the params: 16.958649
                Loss: fixed 409 labels. Loss 0.04756. Accuracy 0.988.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31309694
Train loss (w/o reg) on all data: 0.30677673
Test loss (w/o reg) on all data: 0.143411
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2228867e-05
Norm of the params: 11.242973
              Random: fixed  63 labels. Loss 0.14341. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10974635
Train loss (w/o reg) on all data: 0.1023246
Test loss (w/o reg) on all data: 0.038767308
Train acc on all data:  0.9662047167517627
Test acc on all data:   1.0
Norm of the mean of gradients: 2.773878e-06
Norm of the params: 12.183393
     Influence (LOO): fixed 396 labels. Loss 0.03877. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.0014175078
Test loss (w/o reg) on all data: 0.002808931
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6258316e-07
Norm of the params: 7.087825
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30135506
Train loss (w/o reg) on all data: 0.29508474
Test loss (w/o reg) on all data: 0.13680075
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9149922e-05
Norm of the params: 11.198509
              Random: fixed  89 labels. Loss 0.13680. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 820
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066076204
Train loss (w/o reg) on all data: 0.05987195
Test loss (w/o reg) on all data: 0.02124793
Train acc on all data:  0.9815220034038414
Test acc on all data:   1.0
Norm of the mean of gradients: 3.1709083e-06
Norm of the params: 11.139347
     Influence (LOO): fixed 459 labels. Loss 0.02125. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.001417512
Test loss (w/o reg) on all data: 0.002808909
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8328684e-08
Norm of the params: 7.087819
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29093847
Train loss (w/o reg) on all data: 0.28450298
Test loss (w/o reg) on all data: 0.1305278
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.3712145e-06
Norm of the params: 11.345019
              Random: fixed 111 labels. Loss 0.13053. Accuracy 0.990.
### Flips: 615, rs: 9, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04971477
Train loss (w/o reg) on all data: 0.044257507
Test loss (w/o reg) on all data: 0.016225511
Train acc on all data:  0.987114028689521
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7538063e-06
Norm of the params: 10.447261
     Influence (LOO): fixed 482 labels. Loss 0.01623. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003929371
Train loss (w/o reg) on all data: 0.0014175329
Test loss (w/o reg) on all data: 0.0028089269
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.757004e-07
Norm of the params: 7.0877895
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27697882
Train loss (w/o reg) on all data: 0.2705613
Test loss (w/o reg) on all data: 0.121636674
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.036602e-06
Norm of the params: 11.329173
              Random: fixed 142 labels. Loss 0.12164. Accuracy 0.991.
### Flips: 615, rs: 9, checks: 1230
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032916274
Train loss (w/o reg) on all data: 0.028354097
Test loss (w/o reg) on all data: 0.010706902
Train acc on all data:  0.9924629224410406
Test acc on all data:   1.0
Norm of the mean of gradients: 5.823562e-07
Norm of the params: 9.552148
     Influence (LOO): fixed 504 labels. Loss 0.01071. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039293715
Train loss (w/o reg) on all data: 0.0014175077
Test loss (w/o reg) on all data: 0.0028089308
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9934074e-08
Norm of the params: 7.0878263
                Loss: fixed 533 labels. Loss 0.00281. Accuracy 1.000.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26461124
Train loss (w/o reg) on all data: 0.25806713
Test loss (w/o reg) on all data: 0.11546035
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.334003e-06
Norm of the params: 11.440388
              Random: fixed 166 labels. Loss 0.11546. Accuracy 0.991.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33747423
Train loss (w/o reg) on all data: 0.33033323
Test loss (w/o reg) on all data: 0.16476052
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.619424e-05
Norm of the params: 11.950719
Flipped loss: 0.16476. Accuracy: 0.988
### Flips: 615, rs: 10, checks: 205
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23304169
Train loss (w/o reg) on all data: 0.2233409
Test loss (w/o reg) on all data: 0.10336481
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3895398e-05
Norm of the params: 13.92896
     Influence (LOO): fixed 182 labels. Loss 0.10336. Accuracy 0.993.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19912578
Train loss (w/o reg) on all data: 0.185086
Test loss (w/o reg) on all data: 0.11177492
Train acc on all data:  0.9221979090688063
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.9347374e-06
Norm of the params: 16.75696
                Loss: fixed 205 labels. Loss 0.11177. Accuracy 0.975.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33000213
Train loss (w/o reg) on all data: 0.3227922
Test loss (w/o reg) on all data: 0.15842743
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3393626e-05
Norm of the params: 12.008265
              Random: fixed  20 labels. Loss 0.15843. Accuracy 0.987.
### Flips: 615, rs: 10, checks: 410
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16622515
Train loss (w/o reg) on all data: 0.15712999
Test loss (w/o reg) on all data: 0.06605921
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9348796e-06
Norm of the params: 13.487151
     Influence (LOO): fixed 308 labels. Loss 0.06606. Accuracy 0.999.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07984384
Train loss (w/o reg) on all data: 0.064516105
Test loss (w/o reg) on all data: 0.045853056
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.7719884e-06
Norm of the params: 17.508703
                Loss: fixed 410 labels. Loss 0.04585. Accuracy 0.990.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773818
Train loss (w/o reg) on all data: 0.31044716
Test loss (w/o reg) on all data: 0.15019618
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3172218e-05
Norm of the params: 12.0756035
              Random: fixed  50 labels. Loss 0.15020. Accuracy 0.986.
### Flips: 615, rs: 10, checks: 615
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11826869
Train loss (w/o reg) on all data: 0.110671155
Test loss (w/o reg) on all data: 0.045806296
Train acc on all data:  0.9613420860685631
Test acc on all data:   1.0
Norm of the mean of gradients: 7.770311e-06
Norm of the params: 12.32683
     Influence (LOO): fixed 388 labels. Loss 0.04581. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007558399
Train loss (w/o reg) on all data: 0.0033245042
Test loss (w/o reg) on all data: 0.0050483258
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.370461e-07
Norm of the params: 9.20206
                Loss: fixed 540 labels. Loss 0.00505. Accuracy 0.999.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30756018
Train loss (w/o reg) on all data: 0.29999128
Test loss (w/o reg) on all data: 0.14164647
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.5089893e-05
Norm of the params: 12.303564
              Random: fixed  75 labels. Loss 0.14165. Accuracy 0.988.
### Flips: 615, rs: 10, checks: 820
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081024565
Train loss (w/o reg) on all data: 0.07426411
Test loss (w/o reg) on all data: 0.027951015
Train acc on all data:  0.975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 3.365413e-06
Norm of the params: 11.627945
     Influence (LOO): fixed 448 labels. Loss 0.02795. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042529134
Train loss (w/o reg) on all data: 0.0015241543
Test loss (w/o reg) on all data: 0.0033314694
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5620884e-08
Norm of the params: 7.3875012
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29855153
Train loss (w/o reg) on all data: 0.2909677
Test loss (w/o reg) on all data: 0.13489684
Train acc on all data:  0.8915633357646486
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.6056565e-05
Norm of the params: 12.315705
              Random: fixed  98 labels. Loss 0.13490. Accuracy 0.988.
### Flips: 615, rs: 10, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062426295
Train loss (w/o reg) on all data: 0.05589888
Test loss (w/o reg) on all data: 0.021498417
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 7.311947e-07
Norm of the params: 11.425776
     Influence (LOO): fixed 475 labels. Loss 0.02150. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004252912
Train loss (w/o reg) on all data: 0.0015241919
Test loss (w/o reg) on all data: 0.0033314896
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3702349e-07
Norm of the params: 7.3874493
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2879135
Train loss (w/o reg) on all data: 0.28034687
Test loss (w/o reg) on all data: 0.12799105
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.743604e-06
Norm of the params: 12.301744
              Random: fixed 121 labels. Loss 0.12799. Accuracy 0.992.
### Flips: 615, rs: 10, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045580726
Train loss (w/o reg) on all data: 0.040113926
Test loss (w/o reg) on all data: 0.014228905
Train acc on all data:  0.9880865548261609
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1088715e-06
Norm of the params: 10.456387
     Influence (LOO): fixed 499 labels. Loss 0.01423. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004252913
Train loss (w/o reg) on all data: 0.0015241758
Test loss (w/o reg) on all data: 0.003331543
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6184397e-08
Norm of the params: 7.387472
                Loss: fixed 546 labels. Loss 0.00333. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27407956
Train loss (w/o reg) on all data: 0.2664422
Test loss (w/o reg) on all data: 0.1193324
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.2980977e-05
Norm of the params: 12.359081
              Random: fixed 152 labels. Loss 0.11933. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33602
Train loss (w/o reg) on all data: 0.32930556
Test loss (w/o reg) on all data: 0.16213647
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.781599e-05
Norm of the params: 11.588304
Flipped loss: 0.16214. Accuracy: 0.990
### Flips: 615, rs: 11, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23517461
Train loss (w/o reg) on all data: 0.22524333
Test loss (w/o reg) on all data: 0.107420675
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.8792754e-05
Norm of the params: 14.093456
     Influence (LOO): fixed 183 labels. Loss 0.10742. Accuracy 0.991.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19456926
Train loss (w/o reg) on all data: 0.17928629
Test loss (w/o reg) on all data: 0.11970432
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0106332e-05
Norm of the params: 17.483124
                Loss: fixed 205 labels. Loss 0.11970. Accuracy 0.980.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32856688
Train loss (w/o reg) on all data: 0.32213315
Test loss (w/o reg) on all data: 0.15728614
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.3980525e-06
Norm of the params: 11.343489
              Random: fixed  21 labels. Loss 0.15729. Accuracy 0.990.
### Flips: 615, rs: 11, checks: 410
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15761232
Train loss (w/o reg) on all data: 0.14950304
Test loss (w/o reg) on all data: 0.062773496
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6285065e-05
Norm of the params: 12.735217
     Influence (LOO): fixed 317 labels. Loss 0.06277. Accuracy 0.998.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07738518
Train loss (w/o reg) on all data: 0.061112314
Test loss (w/o reg) on all data: 0.04631072
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.2877737e-06
Norm of the params: 18.040434
                Loss: fixed 410 labels. Loss 0.04631. Accuracy 0.986.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32051104
Train loss (w/o reg) on all data: 0.31424394
Test loss (w/o reg) on all data: 0.15014438
Train acc on all data:  0.8830537320690494
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.168182e-06
Norm of the params: 11.195618
              Random: fixed  44 labels. Loss 0.15014. Accuracy 0.991.
### Flips: 615, rs: 11, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102725014
Train loss (w/o reg) on all data: 0.09536502
Test loss (w/o reg) on all data: 0.04037488
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4264487e-05
Norm of the params: 12.1326
     Influence (LOO): fixed 398 labels. Loss 0.04037. Accuracy 0.997.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006421295
Train loss (w/o reg) on all data: 0.0026698906
Test loss (w/o reg) on all data: 0.0037899665
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3327886e-07
Norm of the params: 8.661876
                Loss: fixed 528 labels. Loss 0.00379. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3121613
Train loss (w/o reg) on all data: 0.30570605
Test loss (w/o reg) on all data: 0.1439274
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9361745e-05
Norm of the params: 11.36243
              Random: fixed  64 labels. Loss 0.14393. Accuracy 0.991.
### Flips: 615, rs: 11, checks: 820
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07247735
Train loss (w/o reg) on all data: 0.06588779
Test loss (w/o reg) on all data: 0.026108567
Train acc on all data:  0.9778750303914417
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2003702e-06
Norm of the params: 11.480034
     Influence (LOO): fixed 443 labels. Loss 0.02611. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035537411
Train loss (w/o reg) on all data: 0.0013711051
Test loss (w/o reg) on all data: 0.0027915405
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.619278e-08
Norm of the params: 6.6070204
                Loss: fixed 533 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30157486
Train loss (w/o reg) on all data: 0.29525617
Test loss (w/o reg) on all data: 0.13680148
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.679104e-06
Norm of the params: 11.241619
              Random: fixed  89 labels. Loss 0.13680. Accuracy 0.988.
### Flips: 615, rs: 11, checks: 1025
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05300026
Train loss (w/o reg) on all data: 0.04721578
Test loss (w/o reg) on all data: 0.017531296
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9750275e-06
Norm of the params: 10.75591
     Influence (LOO): fixed 470 labels. Loss 0.01753. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601194
Test loss (w/o reg) on all data: 0.0026560745
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5814564e-08
Norm of the params: 6.0928164
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28842866
Train loss (w/o reg) on all data: 0.28237382
Test loss (w/o reg) on all data: 0.12936419
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.8460196e-05
Norm of the params: 11.0044155
              Random: fixed 119 labels. Loss 0.12936. Accuracy 0.988.
### Flips: 615, rs: 11, checks: 1230
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03718611
Train loss (w/o reg) on all data: 0.031989273
Test loss (w/o reg) on all data: 0.0128347315
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6013178e-06
Norm of the params: 10.194937
     Influence (LOO): fixed 490 labels. Loss 0.01283. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011977
Test loss (w/o reg) on all data: 0.0026560728
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9341009e-08
Norm of the params: 6.0928154
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26807418
Train loss (w/o reg) on all data: 0.26151428
Test loss (w/o reg) on all data: 0.11869936
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.422166e-06
Norm of the params: 11.454169
              Random: fixed 158 labels. Loss 0.11870. Accuracy 0.985.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33764744
Train loss (w/o reg) on all data: 0.3309865
Test loss (w/o reg) on all data: 0.15733463
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3786952e-05
Norm of the params: 11.542056
Flipped loss: 0.15733. Accuracy: 0.990
### Flips: 615, rs: 12, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23384628
Train loss (w/o reg) on all data: 0.22448976
Test loss (w/o reg) on all data: 0.102763064
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.5176275e-05
Norm of the params: 13.679558
     Influence (LOO): fixed 184 labels. Loss 0.10276. Accuracy 0.996.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19840373
Train loss (w/o reg) on all data: 0.18482576
Test loss (w/o reg) on all data: 0.10849266
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3840347e-05
Norm of the params: 16.479057
                Loss: fixed 205 labels. Loss 0.10849. Accuracy 0.975.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32816014
Train loss (w/o reg) on all data: 0.32142478
Test loss (w/o reg) on all data: 0.1531395
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2830679e-05
Norm of the params: 11.606345
              Random: fixed  24 labels. Loss 0.15314. Accuracy 0.987.
### Flips: 615, rs: 12, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1589643
Train loss (w/o reg) on all data: 0.15081458
Test loss (w/o reg) on all data: 0.06710129
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.0799102e-06
Norm of the params: 12.766933
     Influence (LOO): fixed 313 labels. Loss 0.06710. Accuracy 0.997.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070981674
Train loss (w/o reg) on all data: 0.05554677
Test loss (w/o reg) on all data: 0.0533823
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.93763e-07
Norm of the params: 17.569807
                Loss: fixed 410 labels. Loss 0.05338. Accuracy 0.987.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31358033
Train loss (w/o reg) on all data: 0.30696607
Test loss (w/o reg) on all data: 0.14360094
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.8038357e-06
Norm of the params: 11.50153
              Random: fixed  58 labels. Loss 0.14360. Accuracy 0.989.
### Flips: 615, rs: 12, checks: 615
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10186631
Train loss (w/o reg) on all data: 0.09457442
Test loss (w/o reg) on all data: 0.041372065
Train acc on all data:  0.9683929005592026
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.0026557e-06
Norm of the params: 12.076335
     Influence (LOO): fixed 402 labels. Loss 0.04137. Accuracy 0.999.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006988569
Train loss (w/o reg) on all data: 0.0028981694
Test loss (w/o reg) on all data: 0.0047643282
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.179355e-07
Norm of the params: 9.044777
                Loss: fixed 526 labels. Loss 0.00476. Accuracy 0.998.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30318323
Train loss (w/o reg) on all data: 0.29663742
Test loss (w/o reg) on all data: 0.13522732
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.305301e-06
Norm of the params: 11.441869
              Random: fixed  84 labels. Loss 0.13523. Accuracy 0.995.
### Flips: 615, rs: 12, checks: 820
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06749539
Train loss (w/o reg) on all data: 0.061264694
Test loss (w/o reg) on all data: 0.026820375
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5282736e-06
Norm of the params: 11.16306
     Influence (LOO): fixed 449 labels. Loss 0.02682. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004666795
Train loss (w/o reg) on all data: 0.0017106821
Test loss (w/o reg) on all data: 0.0038554717
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.8520577e-08
Norm of the params: 7.6891003
                Loss: fixed 531 labels. Loss 0.00386. Accuracy 0.998.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29374596
Train loss (w/o reg) on all data: 0.28709352
Test loss (w/o reg) on all data: 0.12754185
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8052506e-05
Norm of the params: 11.534679
              Random: fixed 107 labels. Loss 0.12754. Accuracy 0.993.
### Flips: 615, rs: 12, checks: 1025
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053672872
Train loss (w/o reg) on all data: 0.0479186
Test loss (w/o reg) on all data: 0.02143104
Train acc on all data:  0.9846827133479212
Test acc on all data:   1.0
Norm of the mean of gradients: 6.716256e-07
Norm of the params: 10.727791
     Influence (LOO): fixed 469 labels. Loss 0.02143. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003996824
Train loss (w/o reg) on all data: 0.001390273
Test loss (w/o reg) on all data: 0.0035521523
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.9618854e-08
Norm of the params: 7.2201815
                Loss: fixed 532 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27977067
Train loss (w/o reg) on all data: 0.27290675
Test loss (w/o reg) on all data: 0.119243726
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.3480245e-05
Norm of the params: 11.716579
              Random: fixed 137 labels. Loss 0.11924. Accuracy 0.994.
### Flips: 615, rs: 12, checks: 1230
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034621477
Train loss (w/o reg) on all data: 0.029400378
Test loss (w/o reg) on all data: 0.015622249
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.804935e-07
Norm of the params: 10.218707
     Influence (LOO): fixed 494 labels. Loss 0.01562. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039968244
Train loss (w/o reg) on all data: 0.0013902573
Test loss (w/o reg) on all data: 0.0035521109
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.374731e-08
Norm of the params: 7.2202034
                Loss: fixed 532 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26052397
Train loss (w/o reg) on all data: 0.25342807
Test loss (w/o reg) on all data: 0.10919017
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.46695e-06
Norm of the params: 11.912945
              Random: fixed 174 labels. Loss 0.10919. Accuracy 0.995.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33703798
Train loss (w/o reg) on all data: 0.33065543
Test loss (w/o reg) on all data: 0.15362696
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.157371e-05
Norm of the params: 11.298283
Flipped loss: 0.15363. Accuracy: 0.988
### Flips: 615, rs: 13, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23171887
Train loss (w/o reg) on all data: 0.22281282
Test loss (w/o reg) on all data: 0.100726135
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.495483e-06
Norm of the params: 13.346199
     Influence (LOO): fixed 188 labels. Loss 0.10073. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19851242
Train loss (w/o reg) on all data: 0.18513484
Test loss (w/o reg) on all data: 0.097457446
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.8934243e-06
Norm of the params: 16.356998
                Loss: fixed 205 labels. Loss 0.09746. Accuracy 0.984.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33004013
Train loss (w/o reg) on all data: 0.32381582
Test loss (w/o reg) on all data: 0.14760171
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.1151012e-05
Norm of the params: 11.157337
              Random: fixed  20 labels. Loss 0.14760. Accuracy 0.991.
### Flips: 615, rs: 13, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16479282
Train loss (w/o reg) on all data: 0.15547653
Test loss (w/o reg) on all data: 0.06469002
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1293074e-05
Norm of the params: 13.650124
     Influence (LOO): fixed 305 labels. Loss 0.06469. Accuracy 0.997.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073333964
Train loss (w/o reg) on all data: 0.0584549
Test loss (w/o reg) on all data: 0.04158291
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.9643523e-06
Norm of the params: 17.250542
                Loss: fixed 409 labels. Loss 0.04158. Accuracy 0.988.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32027397
Train loss (w/o reg) on all data: 0.31413358
Test loss (w/o reg) on all data: 0.13852265
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.1936011e-05
Norm of the params: 11.081854
              Random: fixed  45 labels. Loss 0.13852. Accuracy 0.993.
### Flips: 615, rs: 13, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10804022
Train loss (w/o reg) on all data: 0.09988662
Test loss (w/o reg) on all data: 0.04046205
Train acc on all data:  0.9640165329443229
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.167356e-06
Norm of the params: 12.769969
     Influence (LOO): fixed 394 labels. Loss 0.04046. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072600264
Train loss (w/o reg) on all data: 0.003297832
Test loss (w/o reg) on all data: 0.004537911
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.336496e-07
Norm of the params: 8.901904
                Loss: fixed 534 labels. Loss 0.00454. Accuracy 0.999.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31295934
Train loss (w/o reg) on all data: 0.30662596
Test loss (w/o reg) on all data: 0.13291644
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.4696102e-05
Norm of the params: 11.254679
              Random: fixed  64 labels. Loss 0.13292. Accuracy 0.992.
### Flips: 615, rs: 13, checks: 820
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07819278
Train loss (w/o reg) on all data: 0.070718184
Test loss (w/o reg) on all data: 0.027779756
Train acc on all data:  0.975929978118162
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8088026e-06
Norm of the params: 12.226689
     Influence (LOO): fixed 442 labels. Loss 0.02778. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003511866
Train loss (w/o reg) on all data: 0.0012997665
Test loss (w/o reg) on all data: 0.0039346856
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.3607392e-08
Norm of the params: 6.6514654
                Loss: fixed 539 labels. Loss 0.00393. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30012465
Train loss (w/o reg) on all data: 0.29375145
Test loss (w/o reg) on all data: 0.12834126
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.858791e-06
Norm of the params: 11.289981
              Random: fixed  92 labels. Loss 0.12834. Accuracy 0.991.
### Flips: 615, rs: 13, checks: 1025
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057019383
Train loss (w/o reg) on all data: 0.05076141
Test loss (w/o reg) on all data: 0.019070037
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6886931e-06
Norm of the params: 11.1874695
     Influence (LOO): fixed 473 labels. Loss 0.01907. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215597
Train loss (w/o reg) on all data: 0.001109751
Test loss (w/o reg) on all data: 0.0036361793
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.4872237e-08
Norm of the params: 6.183541
                Loss: fixed 540 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29107487
Train loss (w/o reg) on all data: 0.28464162
Test loss (w/o reg) on all data: 0.12194792
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.7663183e-06
Norm of the params: 11.343053
              Random: fixed 113 labels. Loss 0.12195. Accuracy 0.992.
### Flips: 615, rs: 13, checks: 1230
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04604314
Train loss (w/o reg) on all data: 0.04047502
Test loss (w/o reg) on all data: 0.014671665
Train acc on all data:  0.987357160223681
Test acc on all data:   1.0
Norm of the mean of gradients: 6.5778846e-07
Norm of the params: 10.55284
     Influence (LOO): fixed 489 labels. Loss 0.01467. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601296
Test loss (w/o reg) on all data: 0.0026560812
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2356279e-08
Norm of the params: 6.0927997
                Loss: fixed 541 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27669597
Train loss (w/o reg) on all data: 0.2701841
Test loss (w/o reg) on all data: 0.11262903
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8759762e-05
Norm of the params: 11.412145
              Random: fixed 144 labels. Loss 0.11263. Accuracy 0.995.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33768785
Train loss (w/o reg) on all data: 0.33079618
Test loss (w/o reg) on all data: 0.1632368
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.937084e-06
Norm of the params: 11.740246
Flipped loss: 0.16324. Accuracy: 0.987
### Flips: 615, rs: 14, checks: 205
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23496689
Train loss (w/o reg) on all data: 0.22568665
Test loss (w/o reg) on all data: 0.10445234
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.030714e-05
Norm of the params: 13.623684
     Influence (LOO): fixed 180 labels. Loss 0.10445. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19864033
Train loss (w/o reg) on all data: 0.18468784
Test loss (w/o reg) on all data: 0.11491594
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.6892654e-05
Norm of the params: 16.704786
                Loss: fixed 205 labels. Loss 0.11492. Accuracy 0.969.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3263919
Train loss (w/o reg) on all data: 0.31944168
Test loss (w/o reg) on all data: 0.15398547
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.5279027e-05
Norm of the params: 11.790016
              Random: fixed  26 labels. Loss 0.15399. Accuracy 0.989.
### Flips: 615, rs: 14, checks: 410
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16490752
Train loss (w/o reg) on all data: 0.15695468
Test loss (w/o reg) on all data: 0.06786306
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.7972677e-05
Norm of the params: 12.611767
     Influence (LOO): fixed 307 labels. Loss 0.06786. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07661757
Train loss (w/o reg) on all data: 0.06118278
Test loss (w/o reg) on all data: 0.04125119
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5460995e-06
Norm of the params: 17.569742
                Loss: fixed 410 labels. Loss 0.04125. Accuracy 0.987.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31877118
Train loss (w/o reg) on all data: 0.3117244
Test loss (w/o reg) on all data: 0.1500281
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.9702045e-05
Norm of the params: 11.87164
              Random: fixed  45 labels. Loss 0.15003. Accuracy 0.988.
### Flips: 615, rs: 14, checks: 615
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111607075
Train loss (w/o reg) on all data: 0.10454148
Test loss (w/o reg) on all data: 0.044830102
Train acc on all data:  0.962800875273523
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.041829e-06
Norm of the params: 11.887469
     Influence (LOO): fixed 390 labels. Loss 0.04483. Accuracy 0.996.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075130844
Train loss (w/o reg) on all data: 0.0034559944
Test loss (w/o reg) on all data: 0.0038887858
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2879254e-07
Norm of the params: 9.0078745
                Loss: fixed 530 labels. Loss 0.00389. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30722526
Train loss (w/o reg) on all data: 0.30026403
Test loss (w/o reg) on all data: 0.14206181
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.821843e-06
Norm of the params: 11.799351
              Random: fixed  73 labels. Loss 0.14206. Accuracy 0.989.
### Flips: 615, rs: 14, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07295436
Train loss (w/o reg) on all data: 0.06709942
Test loss (w/o reg) on all data: 0.02596923
Train acc on all data:  0.9776318988572817
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3901007e-06
Norm of the params: 10.821216
     Influence (LOO): fixed 449 labels. Loss 0.02597. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004610222
Train loss (w/o reg) on all data: 0.0017119988
Test loss (w/o reg) on all data: 0.003161586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8363746e-08
Norm of the params: 7.61344
                Loss: fixed 537 labels. Loss 0.00316. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299744
Train loss (w/o reg) on all data: 0.2930439
Test loss (w/o reg) on all data: 0.13566147
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.5807786e-05
Norm of the params: 11.57593
              Random: fixed  93 labels. Loss 0.13566. Accuracy 0.991.
### Flips: 615, rs: 14, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049576145
Train loss (w/o reg) on all data: 0.044578128
Test loss (w/o reg) on all data: 0.01697398
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 7.600601e-07
Norm of the params: 9.998015
     Influence (LOO): fixed 484 labels. Loss 0.01697. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.000960136
Test loss (w/o reg) on all data: 0.0026560922
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1061505e-08
Norm of the params: 6.092789
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2882123
Train loss (w/o reg) on all data: 0.28148308
Test loss (w/o reg) on all data: 0.12619701
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.526487e-06
Norm of the params: 11.6010475
              Random: fixed 119 labels. Loss 0.12620. Accuracy 0.988.
### Flips: 615, rs: 14, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037672527
Train loss (w/o reg) on all data: 0.033378318
Test loss (w/o reg) on all data: 0.012335105
Train acc on all data:  0.9902747386336008
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5337665e-06
Norm of the params: 9.26737
     Influence (LOO): fixed 501 labels. Loss 0.01234. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601188
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.332963e-09
Norm of the params: 6.0928173
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27610973
Train loss (w/o reg) on all data: 0.2694531
Test loss (w/o reg) on all data: 0.11940187
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.846718e-05
Norm of the params: 11.538303
              Random: fixed 144 labels. Loss 0.11940. Accuracy 0.989.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33303282
Train loss (w/o reg) on all data: 0.3262061
Test loss (w/o reg) on all data: 0.17194563
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3128691e-05
Norm of the params: 11.684788
Flipped loss: 0.17195. Accuracy: 0.983
### Flips: 615, rs: 15, checks: 205
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23158617
Train loss (w/o reg) on all data: 0.22247419
Test loss (w/o reg) on all data: 0.11106449
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.859771e-06
Norm of the params: 13.499615
     Influence (LOO): fixed 183 labels. Loss 0.11106. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1942893
Train loss (w/o reg) on all data: 0.18047835
Test loss (w/o reg) on all data: 0.122700945
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 5.021179e-06
Norm of the params: 16.61984
                Loss: fixed 205 labels. Loss 0.12270. Accuracy 0.973.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3190406
Train loss (w/o reg) on all data: 0.31212664
Test loss (w/o reg) on all data: 0.16004333
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.320637e-06
Norm of the params: 11.759227
              Random: fixed  34 labels. Loss 0.16004. Accuracy 0.986.
### Flips: 615, rs: 15, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1608911
Train loss (w/o reg) on all data: 0.15192126
Test loss (w/o reg) on all data: 0.072631516
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.151665e-06
Norm of the params: 13.393911
     Influence (LOO): fixed 307 labels. Loss 0.07263. Accuracy 0.995.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07213664
Train loss (w/o reg) on all data: 0.057067018
Test loss (w/o reg) on all data: 0.06338015
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.5394638e-06
Norm of the params: 17.360657
                Loss: fixed 408 labels. Loss 0.06338. Accuracy 0.982.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30555782
Train loss (w/o reg) on all data: 0.29843232
Test loss (w/o reg) on all data: 0.15294088
Train acc on all data:  0.8901045465596887
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.8669825e-06
Norm of the params: 11.937755
              Random: fixed  64 labels. Loss 0.15294. Accuracy 0.985.
### Flips: 615, rs: 15, checks: 615
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10314862
Train loss (w/o reg) on all data: 0.095165394
Test loss (w/o reg) on all data: 0.043404456
Train acc on all data:  0.9674203744225626
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.217125e-06
Norm of the params: 12.635838
     Influence (LOO): fixed 397 labels. Loss 0.04340. Accuracy 0.997.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00924946
Train loss (w/o reg) on all data: 0.0045381472
Test loss (w/o reg) on all data: 0.007514388
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.3218628e-07
Norm of the params: 9.707022
                Loss: fixed 521 labels. Loss 0.00751. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29069284
Train loss (w/o reg) on all data: 0.28324932
Test loss (w/o reg) on all data: 0.1445922
Train acc on all data:  0.8966690979820082
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3653266e-05
Norm of the params: 12.201245
              Random: fixed  97 labels. Loss 0.14459. Accuracy 0.986.
### Flips: 615, rs: 15, checks: 820
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07104369
Train loss (w/o reg) on all data: 0.064408064
Test loss (w/o reg) on all data: 0.028208338
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9153863e-06
Norm of the params: 11.520092
     Influence (LOO): fixed 445 labels. Loss 0.02821. Accuracy 0.998.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00734567
Train loss (w/o reg) on all data: 0.0032882649
Test loss (w/o reg) on all data: 0.006636322
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2304258e-07
Norm of the params: 9.008224
                Loss: fixed 524 labels. Loss 0.00664. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28077945
Train loss (w/o reg) on all data: 0.2733745
Test loss (w/o reg) on all data: 0.13017789
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4160134e-05
Norm of the params: 12.1696005
              Random: fixed 122 labels. Loss 0.13018. Accuracy 0.990.
### Flips: 615, rs: 15, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055733062
Train loss (w/o reg) on all data: 0.050058
Test loss (w/o reg) on all data: 0.023106586
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1713836e-06
Norm of the params: 10.653695
     Influence (LOO): fixed 468 labels. Loss 0.02311. Accuracy 0.998.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065517477
Train loss (w/o reg) on all data: 0.0028484508
Test loss (w/o reg) on all data: 0.005822948
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5745862e-07
Norm of the params: 8.606157
                Loss: fixed 526 labels. Loss 0.00582. Accuracy 0.999.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26775393
Train loss (w/o reg) on all data: 0.26018158
Test loss (w/o reg) on all data: 0.1249536
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.626674e-06
Norm of the params: 12.306373
              Random: fixed 149 labels. Loss 0.12495. Accuracy 0.989.
### Flips: 615, rs: 15, checks: 1230
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035287082
Train loss (w/o reg) on all data: 0.030619495
Test loss (w/o reg) on all data: 0.012412467
Train acc on all data:  0.9912472647702407
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4930298e-06
Norm of the params: 9.66187
     Influence (LOO): fixed 497 labels. Loss 0.01241. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005127207
Train loss (w/o reg) on all data: 0.0020589768
Test loss (w/o reg) on all data: 0.004966825
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.404236e-08
Norm of the params: 7.8335557
                Loss: fixed 529 labels. Loss 0.00497. Accuracy 0.999.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25403413
Train loss (w/o reg) on all data: 0.24645522
Test loss (w/o reg) on all data: 0.11869514
Train acc on all data:  0.9158764891806467
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1298447e-05
Norm of the params: 12.311711
              Random: fixed 176 labels. Loss 0.11870. Accuracy 0.987.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33379048
Train loss (w/o reg) on all data: 0.327521
Test loss (w/o reg) on all data: 0.1620687
Train acc on all data:  0.8735716022368101
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.3395679e-05
Norm of the params: 11.197754
Flipped loss: 0.16207. Accuracy: 0.983
### Flips: 615, rs: 16, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2265385
Train loss (w/o reg) on all data: 0.21731474
Test loss (w/o reg) on all data: 0.10056039
Train acc on all data:  0.9168490153172867
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.2666737e-06
Norm of the params: 13.582163
     Influence (LOO): fixed 184 labels. Loss 0.10056. Accuracy 0.993.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19096643
Train loss (w/o reg) on all data: 0.17701094
Test loss (w/o reg) on all data: 0.105903275
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.0263612e-06
Norm of the params: 16.706581
                Loss: fixed 205 labels. Loss 0.10590. Accuracy 0.977.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3203196
Train loss (w/o reg) on all data: 0.31396458
Test loss (w/o reg) on all data: 0.15590416
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.3975279e-05
Norm of the params: 11.27387
              Random: fixed  32 labels. Loss 0.15590. Accuracy 0.983.
### Flips: 615, rs: 16, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15033814
Train loss (w/o reg) on all data: 0.14187424
Test loss (w/o reg) on all data: 0.059691984
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2675351e-05
Norm of the params: 13.010693
     Influence (LOO): fixed 319 labels. Loss 0.05969. Accuracy 0.999.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071177475
Train loss (w/o reg) on all data: 0.05597136
Test loss (w/o reg) on all data: 0.04040988
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.9574664e-06
Norm of the params: 17.4391
                Loss: fixed 408 labels. Loss 0.04041. Accuracy 0.991.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3090745
Train loss (w/o reg) on all data: 0.30232018
Test loss (w/o reg) on all data: 0.1479294
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.02325685e-05
Norm of the params: 11.622659
              Random: fixed  55 labels. Loss 0.14793. Accuracy 0.983.
### Flips: 615, rs: 16, checks: 615
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10342184
Train loss (w/o reg) on all data: 0.09533429
Test loss (w/o reg) on all data: 0.039411932
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.765873e-05
Norm of the params: 12.718134
     Influence (LOO): fixed 392 labels. Loss 0.03941. Accuracy 0.999.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004425027
Train loss (w/o reg) on all data: 0.0019103814
Test loss (w/o reg) on all data: 0.0035283754
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.987114e-08
Norm of the params: 7.0917497
                Loss: fixed 527 labels. Loss 0.00353. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29168895
Train loss (w/o reg) on all data: 0.28431603
Test loss (w/o reg) on all data: 0.13664319
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.173649e-05
Norm of the params: 12.143236
              Random: fixed  89 labels. Loss 0.13664. Accuracy 0.986.
### Flips: 615, rs: 16, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07203813
Train loss (w/o reg) on all data: 0.06524244
Test loss (w/o reg) on all data: 0.02550885
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8778181e-06
Norm of the params: 11.65821
     Influence (LOO): fixed 439 labels. Loss 0.02551. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0032012095
Train loss (w/o reg) on all data: 0.001221371
Test loss (w/o reg) on all data: 0.0028971236
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5252055e-08
Norm of the params: 6.2925963
                Loss: fixed 531 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2801157
Train loss (w/o reg) on all data: 0.2726145
Test loss (w/o reg) on all data: 0.12887733
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.7569276e-05
Norm of the params: 12.248417
              Random: fixed 116 labels. Loss 0.12888. Accuracy 0.988.
### Flips: 615, rs: 16, checks: 1025
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056244288
Train loss (w/o reg) on all data: 0.04994909
Test loss (w/o reg) on all data: 0.020310257
Train acc on all data:  0.9832239241429613
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8664001e-06
Norm of the params: 11.220692
     Influence (LOO): fixed 463 labels. Loss 0.02031. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00320121
Train loss (w/o reg) on all data: 0.0012213698
Test loss (w/o reg) on all data: 0.0028971103
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5959507e-08
Norm of the params: 6.2925987
                Loss: fixed 531 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26809254
Train loss (w/o reg) on all data: 0.2605253
Test loss (w/o reg) on all data: 0.12066508
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.127087e-06
Norm of the params: 12.302233
              Random: fixed 141 labels. Loss 0.12067. Accuracy 0.987.
### Flips: 615, rs: 16, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039434772
Train loss (w/o reg) on all data: 0.033689264
Test loss (w/o reg) on all data: 0.013098655
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0347752e-06
Norm of the params: 10.719615
     Influence (LOO): fixed 487 labels. Loss 0.01310. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.0009601255
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4997984e-08
Norm of the params: 6.0928054
                Loss: fixed 532 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273895
Train loss (w/o reg) on all data: 0.24500026
Test loss (w/o reg) on all data: 0.11095574
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2555518e-05
Norm of the params: 12.440816
              Random: fixed 170 labels. Loss 0.11096. Accuracy 0.990.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33850026
Train loss (w/o reg) on all data: 0.3323153
Test loss (w/o reg) on all data: 0.15787145
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.3828742e-05
Norm of the params: 11.122024
Flipped loss: 0.15787. Accuracy: 0.990
### Flips: 615, rs: 17, checks: 205
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23184252
Train loss (w/o reg) on all data: 0.2231594
Test loss (w/o reg) on all data: 0.09515411
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.4269744e-06
Norm of the params: 13.178103
     Influence (LOO): fixed 186 labels. Loss 0.09515. Accuracy 0.999.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19894369
Train loss (w/o reg) on all data: 0.18567225
Test loss (w/o reg) on all data: 0.10754737
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.6904961e-05
Norm of the params: 16.291983
                Loss: fixed 205 labels. Loss 0.10755. Accuracy 0.982.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32893878
Train loss (w/o reg) on all data: 0.32264072
Test loss (w/o reg) on all data: 0.1501017
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.334263e-06
Norm of the params: 11.223253
              Random: fixed  25 labels. Loss 0.15010. Accuracy 0.991.
### Flips: 615, rs: 17, checks: 410
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15008165
Train loss (w/o reg) on all data: 0.14116906
Test loss (w/o reg) on all data: 0.05542405
Train acc on all data:  0.9506442985655239
Test acc on all data:   1.0
Norm of the mean of gradients: 1.560064e-05
Norm of the params: 13.351097
     Influence (LOO): fixed 323 labels. Loss 0.05542. Accuracy 1.000.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06915875
Train loss (w/o reg) on all data: 0.053279612
Test loss (w/o reg) on all data: 0.04277585
Train acc on all data:  0.9781181619256017
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4827729e-06
Norm of the params: 17.820852
                Loss: fixed 408 labels. Loss 0.04278. Accuracy 0.990.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3156176
Train loss (w/o reg) on all data: 0.3092732
Test loss (w/o reg) on all data: 0.14026362
Train acc on all data:  0.8862144420131292
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2096815e-05
Norm of the params: 11.264436
              Random: fixed  55 labels. Loss 0.14026. Accuracy 0.989.
### Flips: 615, rs: 17, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10109784
Train loss (w/o reg) on all data: 0.09318349
Test loss (w/o reg) on all data: 0.036329124
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 5.679918e-06
Norm of the params: 12.581216
     Influence (LOO): fixed 398 labels. Loss 0.03633. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003544506
Train loss (w/o reg) on all data: 0.0013002635
Test loss (w/o reg) on all data: 0.0036189742
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.9307256e-08
Norm of the params: 6.699616
                Loss: fixed 524 labels. Loss 0.00362. Accuracy 0.999.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3045128
Train loss (w/o reg) on all data: 0.29803002
Test loss (w/o reg) on all data: 0.13315937
Train acc on all data:  0.8910770726963287
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.72108e-06
Norm of the params: 11.386643
              Random: fixed  79 labels. Loss 0.13316. Accuracy 0.990.
### Flips: 615, rs: 17, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06993364
Train loss (w/o reg) on all data: 0.06359604
Test loss (w/o reg) on all data: 0.023197219
Train acc on all data:  0.9803063457330415
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5146888e-06
Norm of the params: 11.25842
     Influence (LOO): fixed 445 labels. Loss 0.02320. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009600708
Test loss (w/o reg) on all data: 0.0026560354
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.2932424e-08
Norm of the params: 6.092897
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29459712
Train loss (w/o reg) on all data: 0.28809708
Test loss (w/o reg) on all data: 0.12836885
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.628023e-06
Norm of the params: 11.401791
              Random: fixed  98 labels. Loss 0.12837. Accuracy 0.988.
### Flips: 615, rs: 17, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050272085
Train loss (w/o reg) on all data: 0.04464968
Test loss (w/o reg) on all data: 0.017095827
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 1.185375e-06
Norm of the params: 10.6041565
     Influence (LOO): fixed 469 labels. Loss 0.01710. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601228
Test loss (w/o reg) on all data: 0.0026560586
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0510473e-08
Norm of the params: 6.0928097
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27962744
Train loss (w/o reg) on all data: 0.27268052
Test loss (w/o reg) on all data: 0.121135876
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.4607642e-05
Norm of the params: 11.787204
              Random: fixed 127 labels. Loss 0.12114. Accuracy 0.988.
### Flips: 615, rs: 17, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037682302
Train loss (w/o reg) on all data: 0.03262742
Test loss (w/o reg) on all data: 0.012275431
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 8.549938e-07
Norm of the params: 10.054733
     Influence (LOO): fixed 487 labels. Loss 0.01228. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162403
Train loss (w/o reg) on all data: 0.00096012093
Test loss (w/o reg) on all data: 0.0026560603
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4435426e-08
Norm of the params: 6.0928144
                Loss: fixed 526 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27207914
Train loss (w/o reg) on all data: 0.26509464
Test loss (w/o reg) on all data: 0.11520998
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.318893e-06
Norm of the params: 11.81906
              Random: fixed 144 labels. Loss 0.11521. Accuracy 0.991.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33699888
Train loss (w/o reg) on all data: 0.33086208
Test loss (w/o reg) on all data: 0.16531323
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7585204e-05
Norm of the params: 11.078641
Flipped loss: 0.16531. Accuracy: 0.983
### Flips: 615, rs: 18, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23177528
Train loss (w/o reg) on all data: 0.22197114
Test loss (w/o reg) on all data: 0.10788709
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3026875e-05
Norm of the params: 14.002956
     Influence (LOO): fixed 184 labels. Loss 0.10789. Accuracy 0.988.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19540927
Train loss (w/o reg) on all data: 0.18231763
Test loss (w/o reg) on all data: 0.115252785
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.9677314e-05
Norm of the params: 16.18125
                Loss: fixed 205 labels. Loss 0.11525. Accuracy 0.971.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32644677
Train loss (w/o reg) on all data: 0.31995124
Test loss (w/o reg) on all data: 0.15865195
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.6941285e-06
Norm of the params: 11.397845
              Random: fixed  25 labels. Loss 0.15865. Accuracy 0.985.
### Flips: 615, rs: 18, checks: 410
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15930244
Train loss (w/o reg) on all data: 0.1508453
Test loss (w/o reg) on all data: 0.05798429
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.167615e-06
Norm of the params: 13.005495
     Influence (LOO): fixed 315 labels. Loss 0.05798. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072587565
Train loss (w/o reg) on all data: 0.056881245
Test loss (w/o reg) on all data: 0.05339148
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.6122806e-06
Norm of the params: 17.723612
                Loss: fixed 410 labels. Loss 0.05339. Accuracy 0.983.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31732208
Train loss (w/o reg) on all data: 0.3111863
Test loss (w/o reg) on all data: 0.15129551
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.3614728e-05
Norm of the params: 11.077681
              Random: fixed  51 labels. Loss 0.15130. Accuracy 0.984.
### Flips: 615, rs: 18, checks: 615
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10416862
Train loss (w/o reg) on all data: 0.09642792
Test loss (w/o reg) on all data: 0.03680615
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6508009e-06
Norm of the params: 12.442435
     Influence (LOO): fixed 402 labels. Loss 0.03681. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006302654
Train loss (w/o reg) on all data: 0.0025934363
Test loss (w/o reg) on all data: 0.006288752
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.468144e-08
Norm of the params: 8.613033
                Loss: fixed 530 labels. Loss 0.00629. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3044608
Train loss (w/o reg) on all data: 0.29826918
Test loss (w/o reg) on all data: 0.14034511
Train acc on all data:  0.8910770726963287
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.136659e-05
Norm of the params: 11.127993
              Random: fixed  80 labels. Loss 0.14035. Accuracy 0.988.
### Flips: 615, rs: 18, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06841693
Train loss (w/o reg) on all data: 0.06118841
Test loss (w/o reg) on all data: 0.025248712
Train acc on all data:  0.9790906880622416
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6613009e-06
Norm of the params: 12.023744
     Influence (LOO): fixed 451 labels. Loss 0.02525. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039045964
Train loss (w/o reg) on all data: 0.0014013131
Test loss (w/o reg) on all data: 0.0029887948
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.624075e-08
Norm of the params: 7.0757093
                Loss: fixed 535 labels. Loss 0.00299. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2901485
Train loss (w/o reg) on all data: 0.28365067
Test loss (w/o reg) on all data: 0.1307359
Train acc on all data:  0.8966690979820082
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7834012e-05
Norm of the params: 11.399841
              Random: fixed 109 labels. Loss 0.13074. Accuracy 0.990.
### Flips: 615, rs: 18, checks: 1025
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053217515
Train loss (w/o reg) on all data: 0.04713569
Test loss (w/o reg) on all data: 0.020082314
Train acc on all data:  0.9849258448820812
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0251804e-06
Norm of the params: 11.028894
     Influence (LOO): fixed 475 labels. Loss 0.02008. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717895
Train loss (w/o reg) on all data: 0.0010944224
Test loss (w/o reg) on all data: 0.0030825362
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.8351442e-08
Norm of the params: 6.4457226
                Loss: fixed 536 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2775714
Train loss (w/o reg) on all data: 0.27091026
Test loss (w/o reg) on all data: 0.121526495
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.9164303e-05
Norm of the params: 11.542229
              Random: fixed 133 labels. Loss 0.12153. Accuracy 0.991.
### Flips: 615, rs: 18, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035158604
Train loss (w/o reg) on all data: 0.029829757
Test loss (w/o reg) on all data: 0.012489094
Train acc on all data:  0.9905178701677607
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6001853e-06
Norm of the params: 10.323609
     Influence (LOO): fixed 498 labels. Loss 0.01249. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717885
Train loss (w/o reg) on all data: 0.0010944181
Test loss (w/o reg) on all data: 0.0030825175
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3772214e-08
Norm of the params: 6.445728
                Loss: fixed 536 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26766658
Train loss (w/o reg) on all data: 0.2610074
Test loss (w/o reg) on all data: 0.11423761
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 8.1416765e-06
Norm of the params: 11.540513
              Random: fixed 155 labels. Loss 0.11424. Accuracy 0.989.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33789074
Train loss (w/o reg) on all data: 0.3310374
Test loss (w/o reg) on all data: 0.16374926
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.0395076e-05
Norm of the params: 11.707567
Flipped loss: 0.16375. Accuracy: 0.986
### Flips: 615, rs: 19, checks: 205
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24009375
Train loss (w/o reg) on all data: 0.23097122
Test loss (w/o reg) on all data: 0.10152286
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6511621e-05
Norm of the params: 13.507435
     Influence (LOO): fixed 182 labels. Loss 0.10152. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19851954
Train loss (w/o reg) on all data: 0.18546797
Test loss (w/o reg) on all data: 0.108049296
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6087675e-05
Norm of the params: 16.15647
                Loss: fixed 205 labels. Loss 0.10805. Accuracy 0.974.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32709348
Train loss (w/o reg) on all data: 0.32050538
Test loss (w/o reg) on all data: 0.15349159
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1913681e-05
Norm of the params: 11.478752
              Random: fixed  30 labels. Loss 0.15349. Accuracy 0.989.
### Flips: 615, rs: 19, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165521
Train loss (w/o reg) on all data: 0.15649965
Test loss (w/o reg) on all data: 0.066020705
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.8548236e-06
Norm of the params: 13.432309
     Influence (LOO): fixed 311 labels. Loss 0.06602. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07692864
Train loss (w/o reg) on all data: 0.06067373
Test loss (w/o reg) on all data: 0.042467482
Train acc on all data:  0.9744711889132021
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9289564e-06
Norm of the params: 18.03048
                Loss: fixed 410 labels. Loss 0.04247. Accuracy 0.993.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3130329
Train loss (w/o reg) on all data: 0.30622718
Test loss (w/o reg) on all data: 0.14521937
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.1876955e-05
Norm of the params: 11.666799
              Random: fixed  61 labels. Loss 0.14522. Accuracy 0.988.
### Flips: 615, rs: 19, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1094842
Train loss (w/o reg) on all data: 0.101213574
Test loss (w/o reg) on all data: 0.04047939
Train acc on all data:  0.9657184536834428
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7518973e-06
Norm of the params: 12.861282
     Influence (LOO): fixed 400 labels. Loss 0.04048. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004550893
Train loss (w/o reg) on all data: 0.0017087144
Test loss (w/o reg) on all data: 0.0030844656
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.018748e-07
Norm of the params: 7.539468
                Loss: fixed 540 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2990414
Train loss (w/o reg) on all data: 0.29228336
Test loss (w/o reg) on all data: 0.13465007
Train acc on all data:  0.8913202042304887
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6205409e-05
Norm of the params: 11.625859
              Random: fixed  93 labels. Loss 0.13465. Accuracy 0.993.
### Flips: 615, rs: 19, checks: 820
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07200116
Train loss (w/o reg) on all data: 0.06462319
Test loss (w/o reg) on all data: 0.023639983
Train acc on all data:  0.9793338195964016
Test acc on all data:   1.0
Norm of the mean of gradients: 1.766647e-06
Norm of the params: 12.147403
     Influence (LOO): fixed 456 labels. Loss 0.02364. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004550892
Train loss (w/o reg) on all data: 0.0017086567
Test loss (w/o reg) on all data: 0.0030843525
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6990177e-07
Norm of the params: 7.5395436
                Loss: fixed 540 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2836757
Train loss (w/o reg) on all data: 0.2768698
Test loss (w/o reg) on all data: 0.12524322
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.8755469e-05
Norm of the params: 11.66696
              Random: fixed 128 labels. Loss 0.12524. Accuracy 0.994.
### Flips: 615, rs: 19, checks: 1025
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05287848
Train loss (w/o reg) on all data: 0.04635181
Test loss (w/o reg) on all data: 0.016395375
Train acc on all data:  0.9858983710187211
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0521982e-06
Norm of the params: 11.425123
     Influence (LOO): fixed 484 labels. Loss 0.01640. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003966168
Train loss (w/o reg) on all data: 0.0014478205
Test loss (w/o reg) on all data: 0.0030069938
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4099352e-08
Norm of the params: 7.096968
                Loss: fixed 541 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27163777
Train loss (w/o reg) on all data: 0.26465213
Test loss (w/o reg) on all data: 0.11822631
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.830137e-05
Norm of the params: 11.820008
              Random: fixed 151 labels. Loss 0.11823. Accuracy 0.993.
### Flips: 615, rs: 19, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040361747
Train loss (w/o reg) on all data: 0.03454447
Test loss (w/o reg) on all data: 0.012427992
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5551733e-06
Norm of the params: 10.786357
     Influence (LOO): fixed 499 labels. Loss 0.01243. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039661676
Train loss (w/o reg) on all data: 0.0014478319
Test loss (w/o reg) on all data: 0.0030069903
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1010742e-08
Norm of the params: 7.0969515
                Loss: fixed 541 labels. Loss 0.00301. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2598736
Train loss (w/o reg) on all data: 0.25272503
Test loss (w/o reg) on all data: 0.11227276
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.057868e-05
Norm of the params: 11.957056
              Random: fixed 173 labels. Loss 0.11227. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33736348
Train loss (w/o reg) on all data: 0.331085
Test loss (w/o reg) on all data: 0.15878484
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.028436e-05
Norm of the params: 11.205788
Flipped loss: 0.15878. Accuracy: 0.984
### Flips: 615, rs: 20, checks: 205
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23663756
Train loss (w/o reg) on all data: 0.22819246
Test loss (w/o reg) on all data: 0.102005556
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.5954654e-05
Norm of the params: 12.996226
     Influence (LOO): fixed 181 labels. Loss 0.10201. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19822785
Train loss (w/o reg) on all data: 0.18526284
Test loss (w/o reg) on all data: 0.10832438
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.862095e-06
Norm of the params: 16.102802
                Loss: fixed 205 labels. Loss 0.10832. Accuracy 0.974.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.324962
Train loss (w/o reg) on all data: 0.3185108
Test loss (w/o reg) on all data: 0.15047814
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3161711e-05
Norm of the params: 11.358871
              Random: fixed  28 labels. Loss 0.15048. Accuracy 0.988.
### Flips: 615, rs: 20, checks: 410
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15643792
Train loss (w/o reg) on all data: 0.14814459
Test loss (w/o reg) on all data: 0.05950923
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4709216e-05
Norm of the params: 12.878917
     Influence (LOO): fixed 317 labels. Loss 0.05951. Accuracy 0.999.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06992033
Train loss (w/o reg) on all data: 0.054677006
Test loss (w/o reg) on all data: 0.046462096
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.4455385e-06
Norm of the params: 17.460428
                Loss: fixed 410 labels. Loss 0.04646. Accuracy 0.984.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31310079
Train loss (w/o reg) on all data: 0.30665803
Test loss (w/o reg) on all data: 0.14336403
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2735583e-05
Norm of the params: 11.351433
              Random: fixed  57 labels. Loss 0.14336. Accuracy 0.991.
### Flips: 615, rs: 20, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10099279
Train loss (w/o reg) on all data: 0.093909234
Test loss (w/o reg) on all data: 0.03478339
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7148213e-06
Norm of the params: 11.902572
     Influence (LOO): fixed 404 labels. Loss 0.03478. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061530555
Train loss (w/o reg) on all data: 0.0024377867
Test loss (w/o reg) on all data: 0.0056000203
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.091973e-07
Norm of the params: 8.620057
                Loss: fixed 527 labels. Loss 0.00560. Accuracy 0.997.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3010496
Train loss (w/o reg) on all data: 0.2941693
Test loss (w/o reg) on all data: 0.13548614
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.158535e-05
Norm of the params: 11.730547
              Random: fixed  82 labels. Loss 0.13549. Accuracy 0.990.
### Flips: 615, rs: 20, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06822671
Train loss (w/o reg) on all data: 0.06200017
Test loss (w/o reg) on all data: 0.022290364
Train acc on all data:  0.9795769511305616
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4748998e-06
Norm of the params: 11.159336
     Influence (LOO): fixed 449 labels. Loss 0.02229. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036231081
Train loss (w/o reg) on all data: 0.0013771856
Test loss (w/o reg) on all data: 0.004197418
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.980948e-08
Norm of the params: 6.7021227
                Loss: fixed 531 labels. Loss 0.00420. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28659454
Train loss (w/o reg) on all data: 0.2797249
Test loss (w/o reg) on all data: 0.12711479
Train acc on all data:  0.8988572817894481
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.007399e-05
Norm of the params: 11.721468
              Random: fixed 113 labels. Loss 0.12711. Accuracy 0.986.
### Flips: 615, rs: 20, checks: 1025
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05600694
Train loss (w/o reg) on all data: 0.050180577
Test loss (w/o reg) on all data: 0.018284965
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 8.765503e-07
Norm of the params: 10.794779
     Influence (LOO): fixed 466 labels. Loss 0.01828. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0036231086
Train loss (w/o reg) on all data: 0.0013772056
Test loss (w/o reg) on all data: 0.0041974285
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.188417e-08
Norm of the params: 6.7020936
                Loss: fixed 531 labels. Loss 0.00420. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27099475
Train loss (w/o reg) on all data: 0.26393586
Test loss (w/o reg) on all data: 0.12052148
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.826878e-06
Norm of the params: 11.88183
              Random: fixed 145 labels. Loss 0.12052. Accuracy 0.986.
### Flips: 615, rs: 20, checks: 1230
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038868073
Train loss (w/o reg) on all data: 0.03403443
Test loss (w/o reg) on all data: 0.012677416
Train acc on all data:  0.9890590809628009
Test acc on all data:   1.0
Norm of the mean of gradients: 8.1011575e-07
Norm of the params: 9.832236
     Influence (LOO): fixed 488 labels. Loss 0.01268. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011854
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6161922e-08
Norm of the params: 6.0928173
                Loss: fixed 533 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25865862
Train loss (w/o reg) on all data: 0.25140905
Test loss (w/o reg) on all data: 0.114179894
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.505503e-06
Norm of the params: 12.041233
              Random: fixed 169 labels. Loss 0.11418. Accuracy 0.987.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32813367
Train loss (w/o reg) on all data: 0.32199347
Test loss (w/o reg) on all data: 0.15288842
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.761989e-06
Norm of the params: 11.081709
Flipped loss: 0.15289. Accuracy: 0.988
### Flips: 615, rs: 21, checks: 205
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22570615
Train loss (w/o reg) on all data: 0.2170507
Test loss (w/o reg) on all data: 0.100564405
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.4581712e-06
Norm of the params: 13.157089
     Influence (LOO): fixed 178 labels. Loss 0.10056. Accuracy 0.995.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1887469
Train loss (w/o reg) on all data: 0.1758182
Test loss (w/o reg) on all data: 0.106436625
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.423188e-06
Norm of the params: 16.080229
                Loss: fixed 205 labels. Loss 0.10644. Accuracy 0.972.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3152674
Train loss (w/o reg) on all data: 0.3090233
Test loss (w/o reg) on all data: 0.14434122
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5120339e-05
Norm of the params: 11.17507
              Random: fixed  31 labels. Loss 0.14434. Accuracy 0.992.
### Flips: 615, rs: 21, checks: 410
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15090969
Train loss (w/o reg) on all data: 0.14265186
Test loss (w/o reg) on all data: 0.061053157
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.226579e-06
Norm of the params: 12.851326
     Influence (LOO): fixed 308 labels. Loss 0.06105. Accuracy 0.999.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063055076
Train loss (w/o reg) on all data: 0.049941093
Test loss (w/o reg) on all data: 0.039272577
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3407662e-06
Norm of the params: 16.19505
                Loss: fixed 410 labels. Loss 0.03927. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3055839
Train loss (w/o reg) on all data: 0.29942465
Test loss (w/o reg) on all data: 0.13627239
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.268701e-05
Norm of the params: 11.098884
              Random: fixed  54 labels. Loss 0.13627. Accuracy 0.994.
### Flips: 615, rs: 21, checks: 615
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09621219
Train loss (w/o reg) on all data: 0.089266144
Test loss (w/o reg) on all data: 0.038663283
Train acc on all data:  0.9691222951616825
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.830602e-06
Norm of the params: 11.786473
     Influence (LOO): fixed 392 labels. Loss 0.03866. Accuracy 0.999.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003171789
Train loss (w/o reg) on all data: 0.0010943833
Test loss (w/o reg) on all data: 0.0030823783
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.054721e-08
Norm of the params: 6.445782
                Loss: fixed 518 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29230854
Train loss (w/o reg) on all data: 0.28555724
Test loss (w/o reg) on all data: 0.13041347
Train acc on all data:  0.8920495988329686
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.8970637e-06
Norm of the params: 11.620065
              Random: fixed  78 labels. Loss 0.13041. Accuracy 0.991.
### Flips: 615, rs: 21, checks: 820
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07249729
Train loss (w/o reg) on all data: 0.06634846
Test loss (w/o reg) on all data: 0.029141875
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.8148287e-06
Norm of the params: 11.089485
     Influence (LOO): fixed 425 labels. Loss 0.02914. Accuracy 0.998.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031717883
Train loss (w/o reg) on all data: 0.0010943837
Test loss (w/o reg) on all data: 0.003082516
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.9807975e-08
Norm of the params: 6.4457817
                Loss: fixed 518 labels. Loss 0.00308. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28374213
Train loss (w/o reg) on all data: 0.2768506
Test loss (w/o reg) on all data: 0.12387146
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.5248845e-06
Norm of the params: 11.740107
              Random: fixed  97 labels. Loss 0.12387. Accuracy 0.994.
### Flips: 615, rs: 21, checks: 1025
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049814068
Train loss (w/o reg) on all data: 0.043791194
Test loss (w/o reg) on all data: 0.021501344
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.496544e-07
Norm of the params: 10.975312
     Influence (LOO): fixed 456 labels. Loss 0.02150. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560682
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.332572e-09
Norm of the params: 6.09282
                Loss: fixed 519 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27124912
Train loss (w/o reg) on all data: 0.26451385
Test loss (w/o reg) on all data: 0.11582897
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1338326e-05
Norm of the params: 11.606265
              Random: fixed 127 labels. Loss 0.11583. Accuracy 0.996.
### Flips: 615, rs: 21, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04036959
Train loss (w/o reg) on all data: 0.03504178
Test loss (w/o reg) on all data: 0.015571613
Train acc on all data:  0.9880865548261609
Test acc on all data:   1.0
Norm of the mean of gradients: 4.174674e-06
Norm of the params: 10.322605
     Influence (LOO): fixed 470 labels. Loss 0.01557. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601278
Test loss (w/o reg) on all data: 0.0026561068
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1281534e-08
Norm of the params: 6.0928025
                Loss: fixed 519 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2612093
Train loss (w/o reg) on all data: 0.25419593
Test loss (w/o reg) on all data: 0.111396074
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.16109e-06
Norm of the params: 11.843459
              Random: fixed 146 labels. Loss 0.11140. Accuracy 0.995.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34830007
Train loss (w/o reg) on all data: 0.34258455
Test loss (w/o reg) on all data: 0.16307335
Train acc on all data:  0.8672501823486506
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.3957027e-06
Norm of the params: 10.691597
Flipped loss: 0.16307. Accuracy: 0.992
### Flips: 615, rs: 22, checks: 205
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24472338
Train loss (w/o reg) on all data: 0.23563115
Test loss (w/o reg) on all data: 0.11251522
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3929e-05
Norm of the params: 13.484977
     Influence (LOO): fixed 179 labels. Loss 0.11252. Accuracy 0.994.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20659833
Train loss (w/o reg) on all data: 0.19396701
Test loss (w/o reg) on all data: 0.110562496
Train acc on all data:  0.9183078045222466
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 5.359534e-06
Norm of the params: 15.894219
                Loss: fixed 205 labels. Loss 0.11056. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34042588
Train loss (w/o reg) on all data: 0.33472916
Test loss (w/o reg) on all data: 0.15669559
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.145279e-05
Norm of the params: 10.67399
              Random: fixed  20 labels. Loss 0.15670. Accuracy 0.992.
### Flips: 615, rs: 22, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16973019
Train loss (w/o reg) on all data: 0.16087063
Test loss (w/o reg) on all data: 0.074634545
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5754513e-05
Norm of the params: 13.311316
     Influence (LOO): fixed 309 labels. Loss 0.07463. Accuracy 0.997.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08499778
Train loss (w/o reg) on all data: 0.07008058
Test loss (w/o reg) on all data: 0.053962115
Train acc on all data:  0.9705810843666424
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.595313e-06
Norm of the params: 17.27264
                Loss: fixed 410 labels. Loss 0.05396. Accuracy 0.983.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32464787
Train loss (w/o reg) on all data: 0.31856647
Test loss (w/o reg) on all data: 0.14599022
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4906544e-05
Norm of the params: 11.02851
              Random: fixed  52 labels. Loss 0.14599. Accuracy 0.991.
### Flips: 615, rs: 22, checks: 615
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1162182
Train loss (w/o reg) on all data: 0.10786311
Test loss (w/o reg) on all data: 0.0484676
Train acc on all data:  0.9620714806710431
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.6576057e-06
Norm of the params: 12.9267845
     Influence (LOO): fixed 391 labels. Loss 0.04847. Accuracy 0.996.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004285237
Train loss (w/o reg) on all data: 0.0017259556
Test loss (w/o reg) on all data: 0.0029036985
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.309402e-07
Norm of the params: 7.154413
                Loss: fixed 548 labels. Loss 0.00290. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31290615
Train loss (w/o reg) on all data: 0.30682418
Test loss (w/o reg) on all data: 0.13729654
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.671747e-06
Norm of the params: 11.029022
              Random: fixed  81 labels. Loss 0.13730. Accuracy 0.994.
### Flips: 615, rs: 22, checks: 820
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08076888
Train loss (w/o reg) on all data: 0.07224257
Test loss (w/o reg) on all data: 0.03219718
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.194788e-06
Norm of the params: 13.058568
     Influence (LOO): fixed 443 labels. Loss 0.03220. Accuracy 0.998.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668467
Test loss (w/o reg) on all data: 0.002582321
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3871086e-08
Norm of the params: 6.461585
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29985943
Train loss (w/o reg) on all data: 0.2935113
Test loss (w/o reg) on all data: 0.12710999
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.007323e-05
Norm of the params: 11.267782
              Random: fixed 111 labels. Loss 0.12711. Accuracy 0.995.
### Flips: 615, rs: 22, checks: 1025
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059002876
Train loss (w/o reg) on all data: 0.051846217
Test loss (w/o reg) on all data: 0.02209233
Train acc on all data:  0.9822513980063214
Test acc on all data:   1.0
Norm of the mean of gradients: 5.793596e-06
Norm of the params: 11.963828
     Influence (LOO): fixed 478 labels. Loss 0.02209. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544515
Train loss (w/o reg) on all data: 0.0010668571
Test loss (w/o reg) on all data: 0.0025823426
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6544514e-08
Norm of the params: 6.46157
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2860242
Train loss (w/o reg) on all data: 0.27941144
Test loss (w/o reg) on all data: 0.12049603
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8126573e-05
Norm of the params: 11.500245
              Random: fixed 133 labels. Loss 0.12050. Accuracy 0.993.
### Flips: 615, rs: 22, checks: 1230
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042441443
Train loss (w/o reg) on all data: 0.03654678
Test loss (w/o reg) on all data: 0.015866883
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2460292e-07
Norm of the params: 10.857864
     Influence (LOO): fixed 501 labels. Loss 0.01587. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031544506
Train loss (w/o reg) on all data: 0.0010668562
Test loss (w/o reg) on all data: 0.0025823696
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.723729e-08
Norm of the params: 6.4615703
                Loss: fixed 550 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27359876
Train loss (w/o reg) on all data: 0.2667887
Test loss (w/o reg) on all data: 0.116318636
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.308702e-06
Norm of the params: 11.670542
              Random: fixed 157 labels. Loss 0.11632. Accuracy 0.991.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33449236
Train loss (w/o reg) on all data: 0.32767954
Test loss (w/o reg) on all data: 0.16622442
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.8816936e-05
Norm of the params: 11.672879
Flipped loss: 0.16622. Accuracy: 0.984
### Flips: 615, rs: 23, checks: 205
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23304361
Train loss (w/o reg) on all data: 0.22432026
Test loss (w/o reg) on all data: 0.10543284
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5957075e-05
Norm of the params: 13.208596
     Influence (LOO): fixed 184 labels. Loss 0.10543. Accuracy 0.994.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19705974
Train loss (w/o reg) on all data: 0.18254352
Test loss (w/o reg) on all data: 0.11919922
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.007745e-06
Norm of the params: 17.038908
                Loss: fixed 205 labels. Loss 0.11920. Accuracy 0.971.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3248893
Train loss (w/o reg) on all data: 0.31804237
Test loss (w/o reg) on all data: 0.15832733
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.9201514e-05
Norm of the params: 11.702082
              Random: fixed  22 labels. Loss 0.15833. Accuracy 0.983.
### Flips: 615, rs: 23, checks: 410
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15805618
Train loss (w/o reg) on all data: 0.14934757
Test loss (w/o reg) on all data: 0.070215665
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2082028e-06
Norm of the params: 13.197431
     Influence (LOO): fixed 313 labels. Loss 0.07022. Accuracy 0.997.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0718504
Train loss (w/o reg) on all data: 0.057234082
Test loss (w/o reg) on all data: 0.054548286
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.5751702e-06
Norm of the params: 17.097551
                Loss: fixed 410 labels. Loss 0.05455. Accuracy 0.986.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3133005
Train loss (w/o reg) on all data: 0.3061373
Test loss (w/o reg) on all data: 0.15138859
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.497787e-05
Norm of the params: 11.969283
              Random: fixed  48 labels. Loss 0.15139. Accuracy 0.984.
### Flips: 615, rs: 23, checks: 615
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10597261
Train loss (w/o reg) on all data: 0.09845821
Test loss (w/o reg) on all data: 0.042539626
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.815984e-06
Norm of the params: 12.259202
     Influence (LOO): fixed 396 labels. Loss 0.04254. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007562927
Train loss (w/o reg) on all data: 0.003350122
Test loss (w/o reg) on all data: 0.0066681644
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.450428e-08
Norm of the params: 9.179112
                Loss: fixed 527 labels. Loss 0.00667. Accuracy 0.999.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30089608
Train loss (w/o reg) on all data: 0.29339868
Test loss (w/o reg) on all data: 0.14356759
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.8976428e-05
Norm of the params: 12.245333
              Random: fixed  75 labels. Loss 0.14357. Accuracy 0.985.
### Flips: 615, rs: 23, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074959345
Train loss (w/o reg) on all data: 0.068097085
Test loss (w/o reg) on all data: 0.029230705
Train acc on all data:  0.9771456357889619
Test acc on all data:   1.0
Norm of the mean of gradients: 1.034209e-06
Norm of the params: 11.715173
     Influence (LOO): fixed 442 labels. Loss 0.02923. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0050335885
Train loss (w/o reg) on all data: 0.0020179162
Test loss (w/o reg) on all data: 0.005045139
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.9267316e-08
Norm of the params: 7.766174
                Loss: fixed 531 labels. Loss 0.00505. Accuracy 0.999.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28923166
Train loss (w/o reg) on all data: 0.28161436
Test loss (w/o reg) on all data: 0.13393787
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.1806714e-05
Norm of the params: 12.3428545
              Random: fixed 100 labels. Loss 0.13394. Accuracy 0.987.
### Flips: 615, rs: 23, checks: 1025
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056557126
Train loss (w/o reg) on all data: 0.050288692
Test loss (w/o reg) on all data: 0.021369496
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 4.026845e-06
Norm of the params: 11.196816
     Influence (LOO): fixed 467 labels. Loss 0.02137. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037321304
Train loss (w/o reg) on all data: 0.0014723375
Test loss (w/o reg) on all data: 0.0043936367
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.523526e-08
Norm of the params: 6.722786
                Loss: fixed 534 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27805912
Train loss (w/o reg) on all data: 0.2701811
Test loss (w/o reg) on all data: 0.124162346
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.5663685e-05
Norm of the params: 12.552307
              Random: fixed 123 labels. Loss 0.12416. Accuracy 0.990.
### Flips: 615, rs: 23, checks: 1230
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044374656
Train loss (w/o reg) on all data: 0.03879361
Test loss (w/o reg) on all data: 0.017203324
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 8.5483816e-07
Norm of the params: 10.565082
     Influence (LOO): fixed 484 labels. Loss 0.01720. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00373213
Train loss (w/o reg) on all data: 0.001472328
Test loss (w/o reg) on all data: 0.004393555
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.3221926e-08
Norm of the params: 6.7228
                Loss: fixed 534 labels. Loss 0.00439. Accuracy 0.999.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26799458
Train loss (w/o reg) on all data: 0.25990906
Test loss (w/o reg) on all data: 0.1198547
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.915979e-06
Norm of the params: 12.716544
              Random: fixed 144 labels. Loss 0.11985. Accuracy 0.990.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34350508
Train loss (w/o reg) on all data: 0.33756185
Test loss (w/o reg) on all data: 0.15485395
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.310422e-06
Norm of the params: 10.902502
Flipped loss: 0.15485. Accuracy: 0.993
### Flips: 615, rs: 24, checks: 205
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23964088
Train loss (w/o reg) on all data: 0.23112594
Test loss (w/o reg) on all data: 0.09928719
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.180483e-05
Norm of the params: 13.049862
     Influence (LOO): fixed 184 labels. Loss 0.09929. Accuracy 0.994.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2050611
Train loss (w/o reg) on all data: 0.19233419
Test loss (w/o reg) on all data: 0.099109285
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.7672223e-06
Norm of the params: 15.9542465
                Loss: fixed 205 labels. Loss 0.09911. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33183908
Train loss (w/o reg) on all data: 0.3258968
Test loss (w/o reg) on all data: 0.1474474
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6049392e-06
Norm of the params: 10.901646
              Random: fixed  28 labels. Loss 0.14745. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 410
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15936464
Train loss (w/o reg) on all data: 0.15116024
Test loss (w/o reg) on all data: 0.05920245
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.0936612e-06
Norm of the params: 12.809689
     Influence (LOO): fixed 319 labels. Loss 0.05920. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07844353
Train loss (w/o reg) on all data: 0.063706115
Test loss (w/o reg) on all data: 0.046230152
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1543938e-06
Norm of the params: 17.168234
                Loss: fixed 410 labels. Loss 0.04623. Accuracy 0.987.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31712553
Train loss (w/o reg) on all data: 0.31109792
Test loss (w/o reg) on all data: 0.13754219
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.128784e-05
Norm of the params: 10.979612
              Random: fixed  62 labels. Loss 0.13754. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11013162
Train loss (w/o reg) on all data: 0.102582835
Test loss (w/o reg) on all data: 0.03804397
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9402196e-06
Norm of the params: 12.287217
     Influence (LOO): fixed 397 labels. Loss 0.03804. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004512337
Train loss (w/o reg) on all data: 0.001929713
Test loss (w/o reg) on all data: 0.0026895409
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9059416e-08
Norm of the params: 7.186966
                Loss: fixed 537 labels. Loss 0.00269. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30763632
Train loss (w/o reg) on all data: 0.3015217
Test loss (w/o reg) on all data: 0.13112651
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0663959e-05
Norm of the params: 11.058597
              Random: fixed  82 labels. Loss 0.13113. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07678442
Train loss (w/o reg) on all data: 0.07025465
Test loss (w/o reg) on all data: 0.02546889
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5790833e-06
Norm of the params: 11.427831
     Influence (LOO): fixed 446 labels. Loss 0.02547. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601083
Test loss (w/o reg) on all data: 0.0026560419
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.214615e-08
Norm of the params: 6.092834
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29782924
Train loss (w/o reg) on all data: 0.29160425
Test loss (w/o reg) on all data: 0.12570229
Train acc on all data:  0.8956965718453683
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.7944406e-05
Norm of the params: 11.15795
              Random: fixed 104 labels. Loss 0.12570. Accuracy 0.992.
### Flips: 615, rs: 24, checks: 1025
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057124723
Train loss (w/o reg) on all data: 0.051236942
Test loss (w/o reg) on all data: 0.018321974
Train acc on all data:  0.9841964502796012
Test acc on all data:   1.0
Norm of the mean of gradients: 2.543382e-06
Norm of the params: 10.851525
     Influence (LOO): fixed 474 labels. Loss 0.01832. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601403
Test loss (w/o reg) on all data: 0.0026560964
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.315938e-08
Norm of the params: 6.0927815
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28699797
Train loss (w/o reg) on all data: 0.28091654
Test loss (w/o reg) on all data: 0.11695548
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8726692e-05
Norm of the params: 11.028551
              Random: fixed 128 labels. Loss 0.11696. Accuracy 0.993.
### Flips: 615, rs: 24, checks: 1230
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040982693
Train loss (w/o reg) on all data: 0.035949677
Test loss (w/o reg) on all data: 0.012365654
Train acc on all data:  0.9890590809628009
Test acc on all data:   1.0
Norm of the mean of gradients: 8.140554e-07
Norm of the params: 10.03296
     Influence (LOO): fixed 494 labels. Loss 0.01237. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096011907
Test loss (w/o reg) on all data: 0.0026560668
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4696654e-08
Norm of the params: 6.092817
                Loss: fixed 539 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26809508
Train loss (w/o reg) on all data: 0.26175076
Test loss (w/o reg) on all data: 0.10410239
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.696736e-06
Norm of the params: 11.26438
              Random: fixed 165 labels. Loss 0.10410. Accuracy 0.995.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3374462
Train loss (w/o reg) on all data: 0.33024743
Test loss (w/o reg) on all data: 0.1726599
Train acc on all data:  0.8704108922927304
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 8.826423e-06
Norm of the params: 11.9989805
Flipped loss: 0.17266. Accuracy: 0.980
### Flips: 615, rs: 25, checks: 205
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24410853
Train loss (w/o reg) on all data: 0.2348575
Test loss (w/o reg) on all data: 0.11151279
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.7738803e-05
Norm of the params: 13.602225
     Influence (LOO): fixed 173 labels. Loss 0.11151. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19791517
Train loss (w/o reg) on all data: 0.18336777
Test loss (w/o reg) on all data: 0.12238329
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.5888456e-06
Norm of the params: 17.057192
                Loss: fixed 205 labels. Loss 0.12238. Accuracy 0.973.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32722765
Train loss (w/o reg) on all data: 0.31982347
Test loss (w/o reg) on all data: 0.163581
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.990115e-05
Norm of the params: 12.168956
              Random: fixed  24 labels. Loss 0.16358. Accuracy 0.979.
### Flips: 615, rs: 25, checks: 410
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16610572
Train loss (w/o reg) on all data: 0.15722147
Test loss (w/o reg) on all data: 0.06498446
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.019918e-05
Norm of the params: 13.329854
     Influence (LOO): fixed 318 labels. Loss 0.06498. Accuracy 0.996.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08113088
Train loss (w/o reg) on all data: 0.066183984
Test loss (w/o reg) on all data: 0.053410023
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.5841446e-06
Norm of the params: 17.289822
                Loss: fixed 409 labels. Loss 0.05341. Accuracy 0.985.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773043
Train loss (w/o reg) on all data: 0.3103808
Test loss (w/o reg) on all data: 0.15838447
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.497414e-06
Norm of the params: 12.124052
              Random: fixed  48 labels. Loss 0.15838. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10694414
Train loss (w/o reg) on all data: 0.098321676
Test loss (w/o reg) on all data: 0.040912136
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.9650506e-06
Norm of the params: 13.131992
     Influence (LOO): fixed 410 labels. Loss 0.04091. Accuracy 0.997.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010279266
Train loss (w/o reg) on all data: 0.0053503932
Test loss (w/o reg) on all data: 0.006815066
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8057216e-07
Norm of the params: 9.928618
                Loss: fixed 536 labels. Loss 0.00682. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30574247
Train loss (w/o reg) on all data: 0.298355
Test loss (w/o reg) on all data: 0.15067512
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.833638e-05
Norm of the params: 12.155201
              Random: fixed  76 labels. Loss 0.15068. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 820
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0746945
Train loss (w/o reg) on all data: 0.06775248
Test loss (w/o reg) on all data: 0.024804158
Train acc on all data:  0.9778750303914417
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6392272e-06
Norm of the params: 11.783054
     Influence (LOO): fixed 459 labels. Loss 0.02480. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054062456
Train loss (w/o reg) on all data: 0.0023650185
Test loss (w/o reg) on all data: 0.0044897567
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5708786e-07
Norm of the params: 7.799009
                Loss: fixed 546 labels. Loss 0.00449. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2930343
Train loss (w/o reg) on all data: 0.2855317
Test loss (w/o reg) on all data: 0.14191075
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.7243261e-05
Norm of the params: 12.249566
              Random: fixed 105 labels. Loss 0.14191. Accuracy 0.983.
### Flips: 615, rs: 25, checks: 1025
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05758189
Train loss (w/o reg) on all data: 0.051548224
Test loss (w/o reg) on all data: 0.019154498
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3200664e-06
Norm of the params: 10.985143
     Influence (LOO): fixed 482 labels. Loss 0.01915. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005072645
Train loss (w/o reg) on all data: 0.0021577294
Test loss (w/o reg) on all data: 0.0037462148
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.727201e-08
Norm of the params: 7.635333
                Loss: fixed 547 labels. Loss 0.00375. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28180227
Train loss (w/o reg) on all data: 0.27465186
Test loss (w/o reg) on all data: 0.1334773
Train acc on all data:  0.9008023340627279
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.778902e-05
Norm of the params: 11.958592
              Random: fixed 132 labels. Loss 0.13348. Accuracy 0.985.
### Flips: 615, rs: 25, checks: 1230
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04401715
Train loss (w/o reg) on all data: 0.03898533
Test loss (w/o reg) on all data: 0.014802405
Train acc on all data:  0.9878434232920009
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.8587707e-06
Norm of the params: 10.03177
     Influence (LOO): fixed 500 labels. Loss 0.01480. Accuracy 0.999.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042684786
Train loss (w/o reg) on all data: 0.0016213274
Test loss (w/o reg) on all data: 0.0033914463
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4327173e-08
Norm of the params: 7.2761955
                Loss: fixed 548 labels. Loss 0.00339. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2654189
Train loss (w/o reg) on all data: 0.25821024
Test loss (w/o reg) on all data: 0.12419733
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.534405e-06
Norm of the params: 12.007196
              Random: fixed 168 labels. Loss 0.12420. Accuracy 0.984.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3420739
Train loss (w/o reg) on all data: 0.33652934
Test loss (w/o reg) on all data: 0.16394147
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7776343e-05
Norm of the params: 10.530475
Flipped loss: 0.16394. Accuracy: 0.992
### Flips: 615, rs: 26, checks: 205
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230582
Train loss (w/o reg) on all data: 0.22132571
Test loss (w/o reg) on all data: 0.10262725
Train acc on all data:  0.9163627522489667
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.395923e-06
Norm of the params: 13.6060915
     Influence (LOO): fixed 184 labels. Loss 0.10263. Accuracy 0.996.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2028164
Train loss (w/o reg) on all data: 0.19004263
Test loss (w/o reg) on all data: 0.10991554
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.747256e-06
Norm of the params: 15.983594
                Loss: fixed 205 labels. Loss 0.10992. Accuracy 0.982.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3289035
Train loss (w/o reg) on all data: 0.32321098
Test loss (w/o reg) on all data: 0.15637563
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.865065e-06
Norm of the params: 10.670052
              Random: fixed  31 labels. Loss 0.15638. Accuracy 0.990.
### Flips: 615, rs: 26, checks: 410
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15682615
Train loss (w/o reg) on all data: 0.14817248
Test loss (w/o reg) on all data: 0.06773063
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.565291e-06
Norm of the params: 13.155735
     Influence (LOO): fixed 309 labels. Loss 0.06773. Accuracy 0.997.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07630946
Train loss (w/o reg) on all data: 0.061634794
Test loss (w/o reg) on all data: 0.04144365
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.183965e-06
Norm of the params: 17.131647
                Loss: fixed 410 labels. Loss 0.04144. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31668663
Train loss (w/o reg) on all data: 0.31078956
Test loss (w/o reg) on all data: 0.14957517
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4667332e-05
Norm of the params: 10.860095
              Random: fixed  57 labels. Loss 0.14958. Accuracy 0.990.
### Flips: 615, rs: 26, checks: 615
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10867928
Train loss (w/o reg) on all data: 0.10149782
Test loss (w/o reg) on all data: 0.041675188
Train acc on all data:  0.9642596644784829
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1077205e-06
Norm of the params: 11.984535
     Influence (LOO): fixed 388 labels. Loss 0.04168. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044889892
Train loss (w/o reg) on all data: 0.0018073801
Test loss (w/o reg) on all data: 0.003168288
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.242713e-08
Norm of the params: 7.3234
                Loss: fixed 531 labels. Loss 0.00317. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30701855
Train loss (w/o reg) on all data: 0.30096075
Test loss (w/o reg) on all data: 0.13996048
Train acc on all data:  0.8908339411621687
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4121226e-05
Norm of the params: 11.007094
              Random: fixed  80 labels. Loss 0.13996. Accuracy 0.991.
### Flips: 615, rs: 26, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07856505
Train loss (w/o reg) on all data: 0.07198203
Test loss (w/o reg) on all data: 0.027088258
Train acc on all data:  0.975200583515682
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7414445e-06
Norm of the params: 11.474338
     Influence (LOO): fixed 433 labels. Loss 0.02709. Accuracy 0.999.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003915035
Train loss (w/o reg) on all data: 0.0014900179
Test loss (w/o reg) on all data: 0.0026008862
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.890076e-08
Norm of the params: 6.9642186
                Loss: fixed 533 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29423222
Train loss (w/o reg) on all data: 0.2880535
Test loss (w/o reg) on all data: 0.13062143
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.028102e-06
Norm of the params: 11.116381
              Random: fixed 109 labels. Loss 0.13062. Accuracy 0.993.
### Flips: 615, rs: 26, checks: 1025
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05813677
Train loss (w/o reg) on all data: 0.05215794
Test loss (w/o reg) on all data: 0.018404575
Train acc on all data:  0.9827376610746413
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0393638e-05
Norm of the params: 10.935107
     Influence (LOO): fixed 464 labels. Loss 0.01840. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039150342
Train loss (w/o reg) on all data: 0.0014900722
Test loss (w/o reg) on all data: 0.00260092
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2692027e-07
Norm of the params: 6.9641395
                Loss: fixed 533 labels. Loss 0.00260. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27654946
Train loss (w/o reg) on all data: 0.2702394
Test loss (w/o reg) on all data: 0.11959025
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.061335e-06
Norm of the params: 11.233931
              Random: fixed 146 labels. Loss 0.11959. Accuracy 0.995.
### Flips: 615, rs: 26, checks: 1230
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042827565
Train loss (w/o reg) on all data: 0.037374597
Test loss (w/o reg) on all data: 0.013507748
Train acc on all data:  0.9878434232920009
Test acc on all data:   1.0
Norm of the mean of gradients: 5.3483507e-07
Norm of the params: 10.443148
     Influence (LOO): fixed 484 labels. Loss 0.01351. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162391
Train loss (w/o reg) on all data: 0.0009601214
Test loss (w/o reg) on all data: 0.0026560386
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.837901e-08
Norm of the params: 6.092812
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26361465
Train loss (w/o reg) on all data: 0.25720263
Test loss (w/o reg) on all data: 0.110975094
Train acc on all data:  0.912472647702407
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.292315e-05
Norm of the params: 11.324339
              Random: fixed 173 labels. Loss 0.11098. Accuracy 0.997.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33904606
Train loss (w/o reg) on all data: 0.33314422
Test loss (w/o reg) on all data: 0.1523206
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.6084265e-05
Norm of the params: 10.864465
Flipped loss: 0.15232. Accuracy: 0.996
### Flips: 615, rs: 27, checks: 205
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23309882
Train loss (w/o reg) on all data: 0.22545068
Test loss (w/o reg) on all data: 0.09895419
Train acc on all data:  0.9153902261123268
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.556562e-05
Norm of the params: 12.367809
     Influence (LOO): fixed 186 labels. Loss 0.09895. Accuracy 0.998.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19669339
Train loss (w/o reg) on all data: 0.18520807
Test loss (w/o reg) on all data: 0.09285875
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2228354e-05
Norm of the params: 15.156073
                Loss: fixed 205 labels. Loss 0.09286. Accuracy 0.984.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33236364
Train loss (w/o reg) on all data: 0.3265715
Test loss (w/o reg) on all data: 0.14742965
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.986047e-05
Norm of the params: 10.763023
              Random: fixed  17 labels. Loss 0.14743. Accuracy 0.995.
### Flips: 615, rs: 27, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16188107
Train loss (w/o reg) on all data: 0.15398614
Test loss (w/o reg) on all data: 0.06538111
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.7918162e-06
Norm of the params: 12.565769
     Influence (LOO): fixed 305 labels. Loss 0.06538. Accuracy 0.997.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07183351
Train loss (w/o reg) on all data: 0.05814444
Test loss (w/o reg) on all data: 0.03848881
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.620238e-06
Norm of the params: 16.546345
                Loss: fixed 410 labels. Loss 0.03849. Accuracy 0.990.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32042918
Train loss (w/o reg) on all data: 0.31479135
Test loss (w/o reg) on all data: 0.13973927
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.579158e-05
Norm of the params: 10.618698
              Random: fixed  45 labels. Loss 0.13974. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 615
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11544999
Train loss (w/o reg) on all data: 0.108468086
Test loss (w/o reg) on all data: 0.04372365
Train acc on all data:  0.9645027960126429
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0761177e-06
Norm of the params: 11.816852
     Influence (LOO): fixed 385 labels. Loss 0.04372. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004655451
Train loss (w/o reg) on all data: 0.0018194286
Test loss (w/o reg) on all data: 0.004318791
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2744547e-07
Norm of the params: 7.531298
                Loss: fixed 527 labels. Loss 0.00432. Accuracy 0.999.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30767837
Train loss (w/o reg) on all data: 0.30197006
Test loss (w/o reg) on all data: 0.13255456
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.8127954e-05
Norm of the params: 10.684869
              Random: fixed  72 labels. Loss 0.13255. Accuracy 0.996.
### Flips: 615, rs: 27, checks: 820
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07466452
Train loss (w/o reg) on all data: 0.06862524
Test loss (w/o reg) on all data: 0.025274456
Train acc on all data:  0.9786044249939218
Test acc on all data:   1.0
Norm of the mean of gradients: 5.1202883e-06
Norm of the params: 10.990248
     Influence (LOO): fixed 443 labels. Loss 0.02527. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601143
Test loss (w/o reg) on all data: 0.0026560414
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.220807e-08
Norm of the params: 6.092824
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29992244
Train loss (w/o reg) on all data: 0.29405773
Test loss (w/o reg) on all data: 0.12595084
Train acc on all data:  0.8939946511062484
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.932541e-06
Norm of the params: 10.830236
              Random: fixed  93 labels. Loss 0.12595. Accuracy 0.995.
### Flips: 615, rs: 27, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052711032
Train loss (w/o reg) on all data: 0.04736392
Test loss (w/o reg) on all data: 0.016569728
Train acc on all data:  0.9854121079504011
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2754382e-06
Norm of the params: 10.341291
     Influence (LOO): fixed 472 labels. Loss 0.01657. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.00096012774
Test loss (w/o reg) on all data: 0.002656064
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7531064e-08
Norm of the params: 6.092803
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2853513
Train loss (w/o reg) on all data: 0.2796994
Test loss (w/o reg) on all data: 0.11817575
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5637964e-05
Norm of the params: 10.631955
              Random: fixed 125 labels. Loss 0.11818. Accuracy 0.997.
### Flips: 615, rs: 27, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037454188
Train loss (w/o reg) on all data: 0.03282105
Test loss (w/o reg) on all data: 0.011748282
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1176867e-06
Norm of the params: 9.626146
     Influence (LOO): fixed 491 labels. Loss 0.01175. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096013374
Test loss (w/o reg) on all data: 0.0026560908
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6601452e-08
Norm of the params: 6.092792
                Loss: fixed 531 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26942316
Train loss (w/o reg) on all data: 0.26360798
Test loss (w/o reg) on all data: 0.10843262
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1579136e-05
Norm of the params: 10.784424
              Random: fixed 155 labels. Loss 0.10843. Accuracy 0.997.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34065416
Train loss (w/o reg) on all data: 0.33475122
Test loss (w/o reg) on all data: 0.16300923
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.5204276e-05
Norm of the params: 10.865504
Flipped loss: 0.16301. Accuracy: 0.992
### Flips: 615, rs: 28, checks: 205
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23170124
Train loss (w/o reg) on all data: 0.22234137
Test loss (w/o reg) on all data: 0.10124367
Train acc on all data:  0.9170921468514467
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.4433e-06
Norm of the params: 13.682012
     Influence (LOO): fixed 187 labels. Loss 0.10124. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20273668
Train loss (w/o reg) on all data: 0.19045678
Test loss (w/o reg) on all data: 0.109407574
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.588072e-06
Norm of the params: 15.671569
                Loss: fixed 205 labels. Loss 0.10941. Accuracy 0.980.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3314765
Train loss (w/o reg) on all data: 0.32556224
Test loss (w/o reg) on all data: 0.15584344
Train acc on all data:  0.8764891806467299
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.3126755e-06
Norm of the params: 10.875907
              Random: fixed  23 labels. Loss 0.15584. Accuracy 0.993.
### Flips: 615, rs: 28, checks: 410
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1596428
Train loss (w/o reg) on all data: 0.15064175
Test loss (w/o reg) on all data: 0.0665607
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.4205626e-06
Norm of the params: 13.417184
     Influence (LOO): fixed 312 labels. Loss 0.06656. Accuracy 0.997.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07001768
Train loss (w/o reg) on all data: 0.055660732
Test loss (w/o reg) on all data: 0.036086097
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.009093e-06
Norm of the params: 16.945173
                Loss: fixed 410 labels. Loss 0.03609. Accuracy 0.994.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31698895
Train loss (w/o reg) on all data: 0.31055325
Test loss (w/o reg) on all data: 0.1495413
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.7024958e-05
Norm of the params: 11.3452015
              Random: fixed  54 labels. Loss 0.14954. Accuracy 0.992.
### Flips: 615, rs: 28, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107840136
Train loss (w/o reg) on all data: 0.100036964
Test loss (w/o reg) on all data: 0.039928354
Train acc on all data:  0.9652321906151228
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5572658e-05
Norm of the params: 12.492535
     Influence (LOO): fixed 394 labels. Loss 0.03993. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048884796
Train loss (w/o reg) on all data: 0.0018235355
Test loss (w/o reg) on all data: 0.002892901
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8354125e-08
Norm of the params: 7.82936
                Loss: fixed 534 labels. Loss 0.00289. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30495495
Train loss (w/o reg) on all data: 0.29827103
Test loss (w/o reg) on all data: 0.14025728
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.0807143e-05
Norm of the params: 11.56194
              Random: fixed  79 labels. Loss 0.14026. Accuracy 0.991.
### Flips: 615, rs: 28, checks: 820
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076688804
Train loss (w/o reg) on all data: 0.069791675
Test loss (w/o reg) on all data: 0.027796058
Train acc on all data:  0.9769025042548019
Test acc on all data:   1.0
Norm of the mean of gradients: 9.346197e-06
Norm of the params: 11.744897
     Influence (LOO): fixed 442 labels. Loss 0.02780. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048884796
Train loss (w/o reg) on all data: 0.0018235357
Test loss (w/o reg) on all data: 0.0028928842
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.309701e-08
Norm of the params: 7.82936
                Loss: fixed 534 labels. Loss 0.00289. Accuracy 1.000.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2891905
Train loss (w/o reg) on all data: 0.28218696
Test loss (w/o reg) on all data: 0.13045426
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.372448e-05
Norm of the params: 11.835147
              Random: fixed 111 labels. Loss 0.13045. Accuracy 0.993.
### Flips: 615, rs: 28, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053657927
Train loss (w/o reg) on all data: 0.047711764
Test loss (w/o reg) on all data: 0.019024275
Train acc on all data:  0.9851689764162412
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5363886e-06
Norm of the params: 10.905194
     Influence (LOO): fixed 476 labels. Loss 0.01902. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004186409
Train loss (w/o reg) on all data: 0.0015017479
Test loss (w/o reg) on all data: 0.0025759176
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.83909e-08
Norm of the params: 7.3275657
                Loss: fixed 535 labels. Loss 0.00258. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27825755
Train loss (w/o reg) on all data: 0.27151677
Test loss (w/o reg) on all data: 0.12306196
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.936349e-06
Norm of the params: 11.611018
              Random: fixed 136 labels. Loss 0.12306. Accuracy 0.995.
### Flips: 615, rs: 28, checks: 1230
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038151804
Train loss (w/o reg) on all data: 0.03282166
Test loss (w/o reg) on all data: 0.013176966
Train acc on all data:  0.9900316070994408
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8800723e-06
Norm of the params: 10.324867
     Influence (LOO): fixed 496 labels. Loss 0.01318. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601243
Test loss (w/o reg) on all data: 0.0026560808
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3665953e-08
Norm of the params: 6.092808
                Loss: fixed 537 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26174888
Train loss (w/o reg) on all data: 0.25460336
Test loss (w/o reg) on all data: 0.114005916
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.2988134e-06
Norm of the params: 11.954506
              Random: fixed 170 labels. Loss 0.11401. Accuracy 0.994.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33137086
Train loss (w/o reg) on all data: 0.32503012
Test loss (w/o reg) on all data: 0.17008096
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.4484767e-05
Norm of the params: 11.261206
Flipped loss: 0.17008. Accuracy: 0.977
### Flips: 615, rs: 29, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22648358
Train loss (w/o reg) on all data: 0.21702822
Test loss (w/o reg) on all data: 0.10705842
Train acc on all data:  0.9175784099197666
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.156893e-05
Norm of the params: 13.751637
     Influence (LOO): fixed 180 labels. Loss 0.10706. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18777129
Train loss (w/o reg) on all data: 0.17404923
Test loss (w/o reg) on all data: 0.12973794
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.6272661e-05
Norm of the params: 16.566269
                Loss: fixed 205 labels. Loss 0.12974. Accuracy 0.967.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32194254
Train loss (w/o reg) on all data: 0.3157973
Test loss (w/o reg) on all data: 0.16019844
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.2850195e-05
Norm of the params: 11.086236
              Random: fixed  27 labels. Loss 0.16020. Accuracy 0.981.
### Flips: 615, rs: 29, checks: 410
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15303966
Train loss (w/o reg) on all data: 0.1445914
Test loss (w/o reg) on all data: 0.06212258
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.660586e-06
Norm of the params: 12.998661
     Influence (LOO): fixed 316 labels. Loss 0.06212. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06548906
Train loss (w/o reg) on all data: 0.0502714
Test loss (w/o reg) on all data: 0.057722807
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5080363e-06
Norm of the params: 17.445723
                Loss: fixed 406 labels. Loss 0.05772. Accuracy 0.986.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30521852
Train loss (w/o reg) on all data: 0.29903087
Test loss (w/o reg) on all data: 0.14700377
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.430321e-06
Norm of the params: 11.124433
              Random: fixed  65 labels. Loss 0.14700. Accuracy 0.984.
### Flips: 615, rs: 29, checks: 615
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10186991
Train loss (w/o reg) on all data: 0.09413775
Test loss (w/o reg) on all data: 0.037653115
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9594156e-06
Norm of the params: 12.435561
     Influence (LOO): fixed 397 labels. Loss 0.03765. Accuracy 0.999.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007974518
Train loss (w/o reg) on all data: 0.0035049596
Test loss (w/o reg) on all data: 0.00668143
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8592523e-07
Norm of the params: 9.454691
                Loss: fixed 518 labels. Loss 0.00668. Accuracy 0.999.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2936456
Train loss (w/o reg) on all data: 0.28738064
Test loss (w/o reg) on all data: 0.1396198
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.9845787e-06
Norm of the params: 11.193714
              Random: fixed  90 labels. Loss 0.13962. Accuracy 0.983.
### Flips: 615, rs: 29, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068439744
Train loss (w/o reg) on all data: 0.06100138
Test loss (w/o reg) on all data: 0.026129335
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.196069e-06
Norm of the params: 12.197023
     Influence (LOO): fixed 442 labels. Loss 0.02613. Accuracy 0.999.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006737929
Train loss (w/o reg) on all data: 0.0027958604
Test loss (w/o reg) on all data: 0.003910338
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5424703e-07
Norm of the params: 8.879267
                Loss: fixed 521 labels. Loss 0.00391. Accuracy 1.000.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27872813
Train loss (w/o reg) on all data: 0.27201116
Test loss (w/o reg) on all data: 0.13129434
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.0914878e-05
Norm of the params: 11.590482
              Random: fixed 119 labels. Loss 0.13129. Accuracy 0.984.
### Flips: 615, rs: 29, checks: 1025
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048157547
Train loss (w/o reg) on all data: 0.041697614
Test loss (w/o reg) on all data: 0.019397298
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.3255609e-06
Norm of the params: 11.36656
     Influence (LOO): fixed 468 labels. Loss 0.01940. Accuracy 0.999.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0067379284
Train loss (w/o reg) on all data: 0.0027959174
Test loss (w/o reg) on all data: 0.0039101923
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0862203e-07
Norm of the params: 8.879202
                Loss: fixed 521 labels. Loss 0.00391. Accuracy 1.000.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26699057
Train loss (w/o reg) on all data: 0.26045513
Test loss (w/o reg) on all data: 0.124101356
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.426042e-05
Norm of the params: 11.432784
              Random: fixed 146 labels. Loss 0.12410. Accuracy 0.987.
### Flips: 615, rs: 29, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036014013
Train loss (w/o reg) on all data: 0.030300695
Test loss (w/o reg) on all data: 0.012889677
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5670015e-06
Norm of the params: 10.689543
     Influence (LOO): fixed 485 labels. Loss 0.01289. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006166627
Train loss (w/o reg) on all data: 0.0025381192
Test loss (w/o reg) on all data: 0.0033893264
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7577476e-07
Norm of the params: 8.518812
                Loss: fixed 523 labels. Loss 0.00339. Accuracy 1.000.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25796074
Train loss (w/o reg) on all data: 0.25169644
Test loss (w/o reg) on all data: 0.11552467
Train acc on all data:  0.913202042304887
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.7451265e-06
Norm of the params: 11.1931305
              Random: fixed 168 labels. Loss 0.11552. Accuracy 0.986.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3341535
Train loss (w/o reg) on all data: 0.32748958
Test loss (w/o reg) on all data: 0.14738075
Train acc on all data:  0.8735716022368101
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.9775498e-05
Norm of the params: 11.54463
Flipped loss: 0.14738. Accuracy: 0.992
### Flips: 615, rs: 30, checks: 205
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22871265
Train loss (w/o reg) on all data: 0.2200012
Test loss (w/o reg) on all data: 0.098149635
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.742248e-06
Norm of the params: 13.199579
     Influence (LOO): fixed 187 labels. Loss 0.09815. Accuracy 0.998.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19240992
Train loss (w/o reg) on all data: 0.17868839
Test loss (w/o reg) on all data: 0.0960358
Train acc on all data:  0.925844882081206
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6471784e-05
Norm of the params: 16.565945
                Loss: fixed 205 labels. Loss 0.09604. Accuracy 0.984.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32158995
Train loss (w/o reg) on all data: 0.31520224
Test loss (w/o reg) on all data: 0.13953799
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.9575083e-05
Norm of the params: 11.302837
              Random: fixed  29 labels. Loss 0.13954. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14872989
Train loss (w/o reg) on all data: 0.1400593
Test loss (w/o reg) on all data: 0.062226217
Train acc on all data:  0.949671772428884
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.6291639e-06
Norm of the params: 13.168583
     Influence (LOO): fixed 318 labels. Loss 0.06223. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06395259
Train loss (w/o reg) on all data: 0.049548108
Test loss (w/o reg) on all data: 0.031849757
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9509318e-06
Norm of the params: 16.9732
                Loss: fixed 410 labels. Loss 0.03185. Accuracy 0.993.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3122914
Train loss (w/o reg) on all data: 0.30587342
Test loss (w/o reg) on all data: 0.13440548
Train acc on all data:  0.8862144420131292
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9545309e-05
Norm of the params: 11.329592
              Random: fixed  52 labels. Loss 0.13441. Accuracy 0.993.
### Flips: 615, rs: 30, checks: 615
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1016195
Train loss (w/o reg) on all data: 0.093853615
Test loss (w/o reg) on all data: 0.039224617
Train acc on all data:  0.9676635059567226
Test acc on all data:   1.0
Norm of the mean of gradients: 7.648312e-06
Norm of the params: 12.462648
     Influence (LOO): fixed 397 labels. Loss 0.03922. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054401653
Train loss (w/o reg) on all data: 0.002207693
Test loss (w/o reg) on all data: 0.003296337
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4647668e-08
Norm of the params: 8.040488
                Loss: fixed 525 labels. Loss 0.00330. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30279467
Train loss (w/o reg) on all data: 0.29627174
Test loss (w/o reg) on all data: 0.12734051
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.540942e-06
Norm of the params: 11.42183
              Random: fixed  74 labels. Loss 0.12734. Accuracy 0.994.
### Flips: 615, rs: 30, checks: 820
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07342417
Train loss (w/o reg) on all data: 0.06692782
Test loss (w/o reg) on all data: 0.025299514
Train acc on all data:  0.9773887673231219
Test acc on all data:   1.0
Norm of the mean of gradients: 5.9073195e-06
Norm of the params: 11.398553
     Influence (LOO): fixed 438 labels. Loss 0.02530. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023472
Train loss (w/o reg) on all data: 0.0014920566
Test loss (w/o reg) on all data: 0.0029244528
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9707264e-08
Norm of the params: 7.115357
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2910179
Train loss (w/o reg) on all data: 0.2846675
Test loss (w/o reg) on all data: 0.11942661
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1466659e-05
Norm of the params: 11.269778
              Random: fixed 102 labels. Loss 0.11943. Accuracy 0.996.
### Flips: 615, rs: 30, checks: 1025
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05358411
Train loss (w/o reg) on all data: 0.047569342
Test loss (w/o reg) on all data: 0.019113561
Train acc on all data:  0.9844395818137612
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4861604e-06
Norm of the params: 10.967925
     Influence (LOO): fixed 465 labels. Loss 0.01911. Accuracy 1.000.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023473
Train loss (w/o reg) on all data: 0.0014920764
Test loss (w/o reg) on all data: 0.0029244632
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4710714e-08
Norm of the params: 7.11533
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2764175
Train loss (w/o reg) on all data: 0.27008432
Test loss (w/o reg) on all data: 0.110867545
Train acc on all data:  0.9049355701434476
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.786596e-06
Norm of the params: 11.254475
              Random: fixed 131 labels. Loss 0.11087. Accuracy 0.996.
### Flips: 615, rs: 30, checks: 1230
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039958403
Train loss (w/o reg) on all data: 0.03516559
Test loss (w/o reg) on all data: 0.013129422
Train acc on all data:  0.9893022124969608
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7666738e-06
Norm of the params: 9.790621
     Influence (LOO): fixed 485 labels. Loss 0.01313. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004023473
Train loss (w/o reg) on all data: 0.0014920873
Test loss (w/o reg) on all data: 0.002924485
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.799324e-08
Norm of the params: 7.1153154
                Loss: fixed 528 labels. Loss 0.00292. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26488477
Train loss (w/o reg) on all data: 0.25864527
Test loss (w/o reg) on all data: 0.10418423
Train acc on all data:  0.9100413323608072
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.769684e-06
Norm of the params: 11.170938
              Random: fixed 155 labels. Loss 0.10418. Accuracy 0.997.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33346385
Train loss (w/o reg) on all data: 0.32700348
Test loss (w/o reg) on all data: 0.15941758
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.983167e-05
Norm of the params: 11.366955
Flipped loss: 0.15942. Accuracy: 0.986
### Flips: 615, rs: 31, checks: 205
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22763075
Train loss (w/o reg) on all data: 0.21885602
Test loss (w/o reg) on all data: 0.10026579
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.774634e-06
Norm of the params: 13.247441
     Influence (LOO): fixed 187 labels. Loss 0.10027. Accuracy 0.998.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19070303
Train loss (w/o reg) on all data: 0.1778714
Test loss (w/o reg) on all data: 0.11134828
Train acc on all data:  0.9273036712861659
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.2237575e-06
Norm of the params: 16.019753
                Loss: fixed 205 labels. Loss 0.11135. Accuracy 0.976.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31693023
Train loss (w/o reg) on all data: 0.30995157
Test loss (w/o reg) on all data: 0.15000544
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.9090796e-05
Norm of the params: 11.8141165
              Random: fixed  35 labels. Loss 0.15001. Accuracy 0.989.
### Flips: 615, rs: 31, checks: 410
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15593675
Train loss (w/o reg) on all data: 0.14784306
Test loss (w/o reg) on all data: 0.060595915
Train acc on all data:  0.9469973255531242
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4284032e-05
Norm of the params: 12.722955
     Influence (LOO): fixed 312 labels. Loss 0.06060. Accuracy 1.000.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06630491
Train loss (w/o reg) on all data: 0.05125526
Test loss (w/o reg) on all data: 0.049519908
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.0293628e-05
Norm of the params: 17.349146
                Loss: fixed 410 labels. Loss 0.04952. Accuracy 0.985.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30804017
Train loss (w/o reg) on all data: 0.30113685
Test loss (w/o reg) on all data: 0.14248312
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.3121307e-05
Norm of the params: 11.750172
              Random: fixed  56 labels. Loss 0.14248. Accuracy 0.990.
### Flips: 615, rs: 31, checks: 615
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10340326
Train loss (w/o reg) on all data: 0.09614614
Test loss (w/o reg) on all data: 0.03590283
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5936063e-06
Norm of the params: 12.047512
     Influence (LOO): fixed 396 labels. Loss 0.03590. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0043785744
Train loss (w/o reg) on all data: 0.0018253187
Test loss (w/o reg) on all data: 0.0033195189
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 6.455238e-08
Norm of the params: 7.1459856
                Loss: fixed 527 labels. Loss 0.00332. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29875833
Train loss (w/o reg) on all data: 0.291731
Test loss (w/o reg) on all data: 0.1350679
Train acc on all data:  0.8935083880379285
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.624344e-06
Norm of the params: 11.855225
              Random: fixed  77 labels. Loss 0.13507. Accuracy 0.991.
### Flips: 615, rs: 31, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0697444
Train loss (w/o reg) on all data: 0.06365159
Test loss (w/o reg) on all data: 0.02349662
Train acc on all data:  0.9800632141988816
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2895516e-06
Norm of the params: 11.038847
     Influence (LOO): fixed 448 labels. Loss 0.02350. Accuracy 1.000.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034206016
Train loss (w/o reg) on all data: 0.0011897179
Test loss (w/o reg) on all data: 0.0025928807
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8479048e-08
Norm of the params: 6.6796465
                Loss: fixed 529 labels. Loss 0.00259. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28612047
Train loss (w/o reg) on all data: 0.27902263
Test loss (w/o reg) on all data: 0.1275911
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6442891e-05
Norm of the params: 11.914562
              Random: fixed 105 labels. Loss 0.12759. Accuracy 0.988.
### Flips: 615, rs: 31, checks: 1025
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051282324
Train loss (w/o reg) on all data: 0.04594933
Test loss (w/o reg) on all data: 0.017693454
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3186533e-06
Norm of the params: 10.327627
     Influence (LOO): fixed 471 labels. Loss 0.01769. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.0009601321
Test loss (w/o reg) on all data: 0.0026560961
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7009401e-08
Norm of the params: 6.0927944
                Loss: fixed 530 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27439567
Train loss (w/o reg) on all data: 0.2672381
Test loss (w/o reg) on all data: 0.12029329
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.536312e-05
Norm of the params: 11.964584
              Random: fixed 132 labels. Loss 0.12029. Accuracy 0.990.
### Flips: 615, rs: 31, checks: 1230
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034539204
Train loss (w/o reg) on all data: 0.029831475
Test loss (w/o reg) on all data: 0.012435629
Train acc on all data:  0.9910041332360807
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0891246e-06
Norm of the params: 9.703327
     Influence (LOO): fixed 493 labels. Loss 0.01244. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00281624
Train loss (w/o reg) on all data: 0.0009601388
Test loss (w/o reg) on all data: 0.0026561052
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 6.8997636e-08
Norm of the params: 6.0927844
                Loss: fixed 530 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26038182
Train loss (w/o reg) on all data: 0.25339264
Test loss (w/o reg) on all data: 0.10799842
Train acc on all data:  0.911986384634087
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3213195e-05
Norm of the params: 11.823023
              Random: fixed 162 labels. Loss 0.10800. Accuracy 0.993.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34022105
Train loss (w/o reg) on all data: 0.33326095
Test loss (w/o reg) on all data: 0.16066901
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5698128e-05
Norm of the params: 11.798395
Flipped loss: 0.16067. Accuracy: 0.986
### Flips: 615, rs: 32, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23067464
Train loss (w/o reg) on all data: 0.2213696
Test loss (w/o reg) on all data: 0.09881987
Train acc on all data:  0.9185509360564065
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.114046e-05
Norm of the params: 13.641885
     Influence (LOO): fixed 185 labels. Loss 0.09882. Accuracy 0.993.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19932576
Train loss (w/o reg) on all data: 0.1857332
Test loss (w/o reg) on all data: 0.10950132
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.008644e-06
Norm of the params: 16.487906
                Loss: fixed 205 labels. Loss 0.10950. Accuracy 0.978.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317062
Train loss (w/o reg) on all data: 0.3249858
Test loss (w/o reg) on all data: 0.15347359
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.204085e-05
Norm of the params: 11.593445
              Random: fixed  25 labels. Loss 0.15347. Accuracy 0.988.
### Flips: 615, rs: 32, checks: 410
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15209916
Train loss (w/o reg) on all data: 0.14309175
Test loss (w/o reg) on all data: 0.062557824
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4786312e-06
Norm of the params: 13.421928
     Influence (LOO): fixed 316 labels. Loss 0.06256. Accuracy 0.997.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069976136
Train loss (w/o reg) on all data: 0.054323684
Test loss (w/o reg) on all data: 0.05538289
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7338614e-06
Norm of the params: 17.693195
                Loss: fixed 410 labels. Loss 0.05538. Accuracy 0.983.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3219869
Train loss (w/o reg) on all data: 0.31494677
Test loss (w/o reg) on all data: 0.14313318
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4975792e-05
Norm of the params: 11.866035
              Random: fixed  47 labels. Loss 0.14313. Accuracy 0.990.
### Flips: 615, rs: 32, checks: 615
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10247628
Train loss (w/o reg) on all data: 0.09469337
Test loss (w/o reg) on all data: 0.039233148
Train acc on all data:  0.9671772428884027
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2597205e-06
Norm of the params: 12.476302
     Influence (LOO): fixed 395 labels. Loss 0.03923. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059745936
Train loss (w/o reg) on all data: 0.0026378846
Test loss (w/o reg) on all data: 0.0050558923
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8932099e-07
Norm of the params: 8.169099
                Loss: fixed 524 labels. Loss 0.00506. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30787855
Train loss (w/o reg) on all data: 0.3006042
Test loss (w/o reg) on all data: 0.13376516
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.495743e-05
Norm of the params: 12.061798
              Random: fixed  81 labels. Loss 0.13377. Accuracy 0.988.
### Flips: 615, rs: 32, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072511636
Train loss (w/o reg) on all data: 0.06517815
Test loss (w/o reg) on all data: 0.026876051
Train acc on all data:  0.9781181619256017
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5733275e-06
Norm of the params: 12.110728
     Influence (LOO): fixed 441 labels. Loss 0.02688. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035330732
Train loss (w/o reg) on all data: 0.001273004
Test loss (w/o reg) on all data: 0.0034286093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.206071e-08
Norm of the params: 6.723198
                Loss: fixed 529 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29909405
Train loss (w/o reg) on all data: 0.29187325
Test loss (w/o reg) on all data: 0.12785384
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2840813e-05
Norm of the params: 12.017319
              Random: fixed 101 labels. Loss 0.12785. Accuracy 0.992.
### Flips: 615, rs: 32, checks: 1025
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056311987
Train loss (w/o reg) on all data: 0.050122414
Test loss (w/o reg) on all data: 0.020198248
Train acc on all data:  0.9837101872112813
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1183073e-06
Norm of the params: 11.126161
     Influence (LOO): fixed 464 labels. Loss 0.02020. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035330732
Train loss (w/o reg) on all data: 0.0012730471
Test loss (w/o reg) on all data: 0.0034286643
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4369539e-07
Norm of the params: 6.723134
                Loss: fixed 529 labels. Loss 0.00343. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28272462
Train loss (w/o reg) on all data: 0.27514946
Test loss (w/o reg) on all data: 0.11790861
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.5937475e-06
Norm of the params: 12.308656
              Random: fixed 133 labels. Loss 0.11791. Accuracy 0.991.
### Flips: 615, rs: 32, checks: 1230
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04433215
Train loss (w/o reg) on all data: 0.0389276
Test loss (w/o reg) on all data: 0.016705386
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2907657e-06
Norm of the params: 10.396684
     Influence (LOO): fixed 480 labels. Loss 0.01671. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030362247
Train loss (w/o reg) on all data: 0.0011011972
Test loss (w/o reg) on all data: 0.002976784
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.147066e-08
Norm of the params: 6.2209764
                Loss: fixed 530 labels. Loss 0.00298. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2729268
Train loss (w/o reg) on all data: 0.26534382
Test loss (w/o reg) on all data: 0.11282508
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.0251314e-06
Norm of the params: 12.315028
              Random: fixed 151 labels. Loss 0.11283. Accuracy 0.988.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34124428
Train loss (w/o reg) on all data: 0.33573514
Test loss (w/o reg) on all data: 0.15919712
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1785592e-05
Norm of the params: 10.496809
Flipped loss: 0.15920. Accuracy: 0.988
### Flips: 615, rs: 33, checks: 205
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23077345
Train loss (w/o reg) on all data: 0.22190227
Test loss (w/o reg) on all data: 0.10005572
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.3801713e-05
Norm of the params: 13.320048
     Influence (LOO): fixed 186 labels. Loss 0.10006. Accuracy 0.998.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2029578
Train loss (w/o reg) on all data: 0.1908476
Test loss (w/o reg) on all data: 0.10440285
Train acc on all data:  0.9243860928762461
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.7796352e-05
Norm of the params: 15.5629
                Loss: fixed 205 labels. Loss 0.10440. Accuracy 0.975.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3321297
Train loss (w/o reg) on all data: 0.32654056
Test loss (w/o reg) on all data: 0.15214941
Train acc on all data:  0.8769754437150499
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3363275e-05
Norm of the params: 10.572732
              Random: fixed  22 labels. Loss 0.15215. Accuracy 0.993.
### Flips: 615, rs: 33, checks: 410
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15896045
Train loss (w/o reg) on all data: 0.15100569
Test loss (w/o reg) on all data: 0.06276689
Train acc on all data:  0.9469973255531242
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5178409e-05
Norm of the params: 12.613298
     Influence (LOO): fixed 315 labels. Loss 0.06277. Accuracy 1.000.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07596011
Train loss (w/o reg) on all data: 0.062150933
Test loss (w/o reg) on all data: 0.04800365
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6540702e-06
Norm of the params: 16.618769
                Loss: fixed 410 labels. Loss 0.04800. Accuracy 0.988.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32307366
Train loss (w/o reg) on all data: 0.31741846
Test loss (w/o reg) on all data: 0.14572956
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.2559016e-06
Norm of the params: 10.635038
              Random: fixed  42 labels. Loss 0.14573. Accuracy 0.991.
### Flips: 615, rs: 33, checks: 615
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10918095
Train loss (w/o reg) on all data: 0.1020771
Test loss (w/o reg) on all data: 0.039988518
Train acc on all data:  0.9664478482859227
Test acc on all data:   1.0
Norm of the mean of gradients: 7.217681e-06
Norm of the params: 11.919609
     Influence (LOO): fixed 396 labels. Loss 0.03999. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060113138
Train loss (w/o reg) on all data: 0.0023209166
Test loss (w/o reg) on all data: 0.0032206296
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6323548e-07
Norm of the params: 8.591155
                Loss: fixed 531 labels. Loss 0.00322. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3120011
Train loss (w/o reg) on all data: 0.30617243
Test loss (w/o reg) on all data: 0.139163
Train acc on all data:  0.888402625820569
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4025549e-05
Norm of the params: 10.796926
              Random: fixed  67 labels. Loss 0.13916. Accuracy 0.990.
### Flips: 615, rs: 33, checks: 820
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0707934
Train loss (w/o reg) on all data: 0.06437952
Test loss (w/o reg) on all data: 0.025248332
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8636675e-06
Norm of the params: 11.325969
     Influence (LOO): fixed 450 labels. Loss 0.02525. Accuracy 0.999.
Using normal model
LBFGS training took [22] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091934
Train loss (w/o reg) on all data: 0.001179342
Test loss (w/o reg) on all data: 0.0029090338
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.544357e-08
Norm of the params: 6.6781
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3012923
Train loss (w/o reg) on all data: 0.2954728
Test loss (w/o reg) on all data: 0.1327475
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0426905e-05
Norm of the params: 10.78842
              Random: fixed  89 labels. Loss 0.13275. Accuracy 0.991.
### Flips: 615, rs: 33, checks: 1025
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048325393
Train loss (w/o reg) on all data: 0.042723868
Test loss (w/o reg) on all data: 0.017945362
Train acc on all data:  0.9863846340870411
Test acc on all data:   1.0
Norm of the mean of gradients: 8.8589917e-07
Norm of the params: 10.584446
     Influence (LOO): fixed 479 labels. Loss 0.01795. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091924
Train loss (w/o reg) on all data: 0.0011793463
Test loss (w/o reg) on all data: 0.0029090699
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.810395e-08
Norm of the params: 6.678093
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28911313
Train loss (w/o reg) on all data: 0.28300172
Test loss (w/o reg) on all data: 0.12482204
Train acc on all data:  0.9008023340627279
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.196231e-06
Norm of the params: 11.055682
              Random: fixed 116 labels. Loss 0.12482. Accuracy 0.993.
### Flips: 615, rs: 33, checks: 1230
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036664575
Train loss (w/o reg) on all data: 0.03183069
Test loss (w/o reg) on all data: 0.014186179
Train acc on all data:  0.9895453440311208
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1333592e-06
Norm of the params: 9.832482
     Influence (LOO): fixed 492 labels. Loss 0.01419. Accuracy 1.000.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0034091931
Train loss (w/o reg) on all data: 0.0011793356
Test loss (w/o reg) on all data: 0.0029090606
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.908396e-08
Norm of the params: 6.67811
                Loss: fixed 534 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277223
Train loss (w/o reg) on all data: 0.27105448
Test loss (w/o reg) on all data: 0.11796891
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6824013e-05
Norm of the params: 11.107223
              Random: fixed 141 labels. Loss 0.11797. Accuracy 0.995.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33680874
Train loss (w/o reg) on all data: 0.3299984
Test loss (w/o reg) on all data: 0.15702745
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.429987e-05
Norm of the params: 11.670769
Flipped loss: 0.15703. Accuracy: 0.987
### Flips: 615, rs: 34, checks: 205
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2363902
Train loss (w/o reg) on all data: 0.22613141
Test loss (w/o reg) on all data: 0.10203187
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.62474e-05
Norm of the params: 14.323958
     Influence (LOO): fixed 182 labels. Loss 0.10203. Accuracy 0.996.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19699025
Train loss (w/o reg) on all data: 0.1837189
Test loss (w/o reg) on all data: 0.11081733
Train acc on all data:  0.9287624604911257
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.188554e-06
Norm of the params: 16.29193
                Loss: fixed 204 labels. Loss 0.11082. Accuracy 0.971.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32257366
Train loss (w/o reg) on all data: 0.3155659
Test loss (w/o reg) on all data: 0.14688602
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.857647e-05
Norm of the params: 11.8387165
              Random: fixed  32 labels. Loss 0.14689. Accuracy 0.990.
### Flips: 615, rs: 34, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16484042
Train loss (w/o reg) on all data: 0.1563394
Test loss (w/o reg) on all data: 0.06442793
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2562513e-05
Norm of the params: 13.039182
     Influence (LOO): fixed 309 labels. Loss 0.06443. Accuracy 0.998.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06945739
Train loss (w/o reg) on all data: 0.053482
Test loss (w/o reg) on all data: 0.046314154
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.8925936e-06
Norm of the params: 17.874783
                Loss: fixed 408 labels. Loss 0.04631. Accuracy 0.982.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31721294
Train loss (w/o reg) on all data: 0.31024247
Test loss (w/o reg) on all data: 0.14166856
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.2466387e-05
Norm of the params: 11.80718
              Random: fixed  47 labels. Loss 0.14167. Accuracy 0.993.
### Flips: 615, rs: 34, checks: 615
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10384309
Train loss (w/o reg) on all data: 0.095983826
Test loss (w/o reg) on all data: 0.03840869
Train acc on all data:  0.9666909798200827
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7882502e-06
Norm of the params: 12.53736
     Influence (LOO): fixed 398 labels. Loss 0.03841. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00800826
Train loss (w/o reg) on all data: 0.003643776
Test loss (w/o reg) on all data: 0.004313317
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 7.252483e-08
Norm of the params: 9.342895
                Loss: fixed 523 labels. Loss 0.00431. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3040853
Train loss (w/o reg) on all data: 0.2974529
Test loss (w/o reg) on all data: 0.1326876
Train acc on all data:  0.8932652565037685
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.756528e-06
Norm of the params: 11.517294
              Random: fixed  79 labels. Loss 0.13269. Accuracy 0.991.
### Flips: 615, rs: 34, checks: 820
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07454037
Train loss (w/o reg) on all data: 0.06774673
Test loss (w/o reg) on all data: 0.027940381
Train acc on all data:  0.9766593727206418
Test acc on all data:   1.0
Norm of the mean of gradients: 2.107581e-06
Norm of the params: 11.656447
     Influence (LOO): fixed 440 labels. Loss 0.02794. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046856613
Train loss (w/o reg) on all data: 0.0019460734
Test loss (w/o reg) on all data: 0.0036926472
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0415361e-07
Norm of the params: 7.402146
                Loss: fixed 531 labels. Loss 0.00369. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29379395
Train loss (w/o reg) on all data: 0.28692728
Test loss (w/o reg) on all data: 0.12699941
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.007104e-06
Norm of the params: 11.718917
              Random: fixed 101 labels. Loss 0.12700. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1025
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046472844
Train loss (w/o reg) on all data: 0.04041531
Test loss (w/o reg) on all data: 0.018526992
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2896085e-06
Norm of the params: 11.006848
     Influence (LOO): fixed 479 labels. Loss 0.01853. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601162
Test loss (w/o reg) on all data: 0.002656058
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.272939e-08
Norm of the params: 6.092821
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2812355
Train loss (w/o reg) on all data: 0.27426597
Test loss (w/o reg) on all data: 0.11834966
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.725781e-06
Norm of the params: 11.806362
              Random: fixed 130 labels. Loss 0.11835. Accuracy 0.992.
### Flips: 615, rs: 34, checks: 1230
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03794877
Train loss (w/o reg) on all data: 0.03272954
Test loss (w/o reg) on all data: 0.014474973
Train acc on all data:  0.9893022124969608
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7612654e-06
Norm of the params: 10.216876
     Influence (LOO): fixed 491 labels. Loss 0.01447. Accuracy 0.999.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.00096010725
Test loss (w/o reg) on all data: 0.0026560395
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.85647e-08
Norm of the params: 6.092836
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269514
Train loss (w/o reg) on all data: 0.2625712
Test loss (w/o reg) on all data: 0.109781355
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.6721996e-06
Norm of the params: 11.783732
              Random: fixed 155 labels. Loss 0.10978. Accuracy 0.994.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33802563
Train loss (w/o reg) on all data: 0.33137766
Test loss (w/o reg) on all data: 0.16098398
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.547989e-06
Norm of the params: 11.530817
Flipped loss: 0.16098. Accuracy: 0.988
### Flips: 615, rs: 35, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22866301
Train loss (w/o reg) on all data: 0.21967553
Test loss (w/o reg) on all data: 0.10107044
Train acc on all data:  0.9187940675905665
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.8364208e-06
Norm of the params: 13.407073
     Influence (LOO): fixed 191 labels. Loss 0.10107. Accuracy 0.998.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2018789
Train loss (w/o reg) on all data: 0.1896433
Test loss (w/o reg) on all data: 0.10729536
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0793939e-05
Norm of the params: 15.643277
                Loss: fixed 205 labels. Loss 0.10730. Accuracy 0.975.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32753205
Train loss (w/o reg) on all data: 0.32083187
Test loss (w/o reg) on all data: 0.15246461
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.8364719e-05
Norm of the params: 11.576006
              Random: fixed  26 labels. Loss 0.15246. Accuracy 0.990.
### Flips: 615, rs: 35, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16367073
Train loss (w/o reg) on all data: 0.15490967
Test loss (w/o reg) on all data: 0.06542116
Train acc on all data:  0.9452954048140044
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.51255e-06
Norm of the params: 13.237115
     Influence (LOO): fixed 307 labels. Loss 0.06542. Accuracy 0.998.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07269869
Train loss (w/o reg) on all data: 0.05871153
Test loss (w/o reg) on all data: 0.04274699
Train acc on all data:  0.9766593727206418
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.8886935e-07
Norm of the params: 16.725529
                Loss: fixed 409 labels. Loss 0.04275. Accuracy 0.984.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3146333
Train loss (w/o reg) on all data: 0.3074358
Test loss (w/o reg) on all data: 0.1432246
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.8566355e-05
Norm of the params: 11.997914
              Random: fixed  54 labels. Loss 0.14322. Accuracy 0.992.
### Flips: 615, rs: 35, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11295895
Train loss (w/o reg) on all data: 0.10449494
Test loss (w/o reg) on all data: 0.042155366
Train acc on all data:  0.9640165329443229
Test acc on all data:   1.0
Norm of the mean of gradients: 6.6607327e-06
Norm of the params: 13.010775
     Influence (LOO): fixed 392 labels. Loss 0.04216. Accuracy 1.000.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007421744
Train loss (w/o reg) on all data: 0.0033416764
Test loss (w/o reg) on all data: 0.00568283
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7504335e-07
Norm of the params: 9.033347
                Loss: fixed 531 labels. Loss 0.00568. Accuracy 0.998.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3050007
Train loss (w/o reg) on all data: 0.29773173
Test loss (w/o reg) on all data: 0.13706034
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0154898e-05
Norm of the params: 12.057338
              Random: fixed  76 labels. Loss 0.13706. Accuracy 0.992.
### Flips: 615, rs: 35, checks: 820
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08219425
Train loss (w/o reg) on all data: 0.0750938
Test loss (w/o reg) on all data: 0.029278075
Train acc on all data:  0.975443715049842
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4929706e-06
Norm of the params: 11.916751
     Influence (LOO): fixed 439 labels. Loss 0.02928. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601019
Test loss (w/o reg) on all data: 0.002656029
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2583293e-08
Norm of the params: 6.092845
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2896842
Train loss (w/o reg) on all data: 0.2824227
Test loss (w/o reg) on all data: 0.12708035
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.016216e-05
Norm of the params: 12.051144
              Random: fixed 110 labels. Loss 0.12708. Accuracy 0.995.
### Flips: 615, rs: 35, checks: 1025
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059749566
Train loss (w/o reg) on all data: 0.053347856
Test loss (w/o reg) on all data: 0.020441012
Train acc on all data:  0.9834670556771213
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4927055e-06
Norm of the params: 11.31522
     Influence (LOO): fixed 472 labels. Loss 0.02044. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601208
Test loss (w/o reg) on all data: 0.0026560717
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8166594e-08
Norm of the params: 6.092814
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27730495
Train loss (w/o reg) on all data: 0.27014208
Test loss (w/o reg) on all data: 0.11956835
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.679631e-06
Norm of the params: 11.96901
              Random: fixed 137 labels. Loss 0.11957. Accuracy 0.995.
### Flips: 615, rs: 35, checks: 1230
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045337215
Train loss (w/o reg) on all data: 0.039256062
Test loss (w/o reg) on all data: 0.014544065
Train acc on all data:  0.987600291757841
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3933061e-06
Norm of the params: 11.028284
     Influence (LOO): fixed 489 labels. Loss 0.01454. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096011587
Test loss (w/o reg) on all data: 0.0026560659
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5299795e-08
Norm of the params: 6.0928206
                Loss: fixed 540 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26384264
Train loss (w/o reg) on all data: 0.2564896
Test loss (w/o reg) on all data: 0.111659735
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4936997e-05
Norm of the params: 12.12686
              Random: fixed 166 labels. Loss 0.11166. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3310782
Train loss (w/o reg) on all data: 0.32421505
Test loss (w/o reg) on all data: 0.1552266
Train acc on all data:  0.8764891806467299
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.711864e-06
Norm of the params: 11.715935
Flipped loss: 0.15523. Accuracy: 0.986
### Flips: 615, rs: 36, checks: 205
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23037685
Train loss (w/o reg) on all data: 0.22171396
Test loss (w/o reg) on all data: 0.101366304
Train acc on all data:  0.9173352783856066
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.559296e-05
Norm of the params: 13.162751
     Influence (LOO): fixed 181 labels. Loss 0.10137. Accuracy 0.994.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19060647
Train loss (w/o reg) on all data: 0.17662482
Test loss (w/o reg) on all data: 0.101166986
Train acc on all data:  0.9285193289569658
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.85042e-06
Norm of the params: 16.72223
                Loss: fixed 205 labels. Loss 0.10117. Accuracy 0.983.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32124603
Train loss (w/o reg) on all data: 0.31400254
Test loss (w/o reg) on all data: 0.14943415
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1687985e-05
Norm of the params: 12.03617
              Random: fixed  19 labels. Loss 0.14943. Accuracy 0.985.
### Flips: 615, rs: 36, checks: 410
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14971237
Train loss (w/o reg) on all data: 0.14137508
Test loss (w/o reg) on all data: 0.059115004
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.868923e-06
Norm of the params: 12.913008
     Influence (LOO): fixed 324 labels. Loss 0.05912. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06204537
Train loss (w/o reg) on all data: 0.047318958
Test loss (w/o reg) on all data: 0.038229827
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1665967e-06
Norm of the params: 17.161825
                Loss: fixed 410 labels. Loss 0.03823. Accuracy 0.989.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3099796
Train loss (w/o reg) on all data: 0.30275232
Test loss (w/o reg) on all data: 0.1429506
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9233514e-05
Norm of the params: 12.022717
              Random: fixed  44 labels. Loss 0.14295. Accuracy 0.984.
### Flips: 615, rs: 36, checks: 615
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09678482
Train loss (w/o reg) on all data: 0.0888459
Test loss (w/o reg) on all data: 0.036313184
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4853687e-06
Norm of the params: 12.600733
     Influence (LOO): fixed 400 labels. Loss 0.03631. Accuracy 1.000.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005756858
Train loss (w/o reg) on all data: 0.0023774419
Test loss (w/o reg) on all data: 0.006291648
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.4291734e-08
Norm of the params: 8.221211
                Loss: fixed 521 labels. Loss 0.00629. Accuracy 0.998.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298131
Train loss (w/o reg) on all data: 0.29100963
Test loss (w/o reg) on all data: 0.13621691
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.790357e-06
Norm of the params: 11.934284
              Random: fixed  71 labels. Loss 0.13622. Accuracy 0.987.
### Flips: 615, rs: 36, checks: 820
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06719948
Train loss (w/o reg) on all data: 0.06030535
Test loss (w/o reg) on all data: 0.02526421
Train acc on all data:  0.9793338195964016
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7195127e-06
Norm of the params: 11.742348
     Influence (LOO): fixed 441 labels. Loss 0.02526. Accuracy 1.000.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003936438
Train loss (w/o reg) on all data: 0.0015328514
Test loss (w/o reg) on all data: 0.004602556
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.5487484e-08
Norm of the params: 6.933378
                Loss: fixed 525 labels. Loss 0.00460. Accuracy 0.999.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2780371
Train loss (w/o reg) on all data: 0.27036703
Test loss (w/o reg) on all data: 0.1253601
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.186017e-06
Norm of the params: 12.385542
              Random: fixed 110 labels. Loss 0.12536. Accuracy 0.985.
### Flips: 615, rs: 36, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05176158
Train loss (w/o reg) on all data: 0.046017785
Test loss (w/o reg) on all data: 0.018440364
Train acc on all data:  0.9856552394845611
Test acc on all data:   1.0
Norm of the mean of gradients: 1.465654e-06
Norm of the params: 10.718016
     Influence (LOO): fixed 468 labels. Loss 0.01844. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0030215606
Train loss (w/o reg) on all data: 0.0011097605
Test loss (w/o reg) on all data: 0.0036361483
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4355729e-08
Norm of the params: 6.1835265
                Loss: fixed 526 labels. Loss 0.00364. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2683644
Train loss (w/o reg) on all data: 0.2605983
Test loss (w/o reg) on all data: 0.11830799
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.5009694e-06
Norm of the params: 12.462832
              Random: fixed 131 labels. Loss 0.11831. Accuracy 0.987.
### Flips: 615, rs: 36, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037248917
Train loss (w/o reg) on all data: 0.032069225
Test loss (w/o reg) on all data: 0.014857658
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.0952464e-07
Norm of the params: 10.178105
     Influence (LOO): fixed 487 labels. Loss 0.01486. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011267
Test loss (w/o reg) on all data: 0.002656063
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5637454e-08
Norm of the params: 6.092826
                Loss: fixed 527 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2555217
Train loss (w/o reg) on all data: 0.24794476
Test loss (w/o reg) on all data: 0.10623782
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.917766e-06
Norm of the params: 12.310116
              Random: fixed 165 labels. Loss 0.10624. Accuracy 0.991.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3315459
Train loss (w/o reg) on all data: 0.32488835
Test loss (w/o reg) on all data: 0.15236342
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.16748115e-05
Norm of the params: 11.539096
Flipped loss: 0.15236. Accuracy: 0.990
### Flips: 615, rs: 37, checks: 205
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22440653
Train loss (w/o reg) on all data: 0.21525165
Test loss (w/o reg) on all data: 0.09514582
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.41997725e-05
Norm of the params: 13.53135
     Influence (LOO): fixed 185 labels. Loss 0.09515. Accuracy 0.994.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18960133
Train loss (w/o reg) on all data: 0.17547862
Test loss (w/o reg) on all data: 0.093106374
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.1186116e-06
Norm of the params: 16.806374
                Loss: fixed 204 labels. Loss 0.09311. Accuracy 0.987.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3198232
Train loss (w/o reg) on all data: 0.3133331
Test loss (w/o reg) on all data: 0.14257082
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5275882e-05
Norm of the params: 11.393074
              Random: fixed  31 labels. Loss 0.14257. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 410
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641666
Train loss (w/o reg) on all data: 0.13724805
Test loss (w/o reg) on all data: 0.05871808
Train acc on all data:  0.949428640894724
Test acc on all data:   1.0
Norm of the mean of gradients: 3.9336846e-06
Norm of the params: 13.541503
     Influence (LOO): fixed 316 labels. Loss 0.05872. Accuracy 1.000.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06963726
Train loss (w/o reg) on all data: 0.05518586
Test loss (w/o reg) on all data: 0.035425294
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7392291e-06
Norm of the params: 17.000828
                Loss: fixed 409 labels. Loss 0.03543. Accuracy 0.990.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30821475
Train loss (w/o reg) on all data: 0.30197498
Test loss (w/o reg) on all data: 0.13412493
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.369216e-06
Norm of the params: 11.171177
              Random: fixed  58 labels. Loss 0.13412. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 615
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09791408
Train loss (w/o reg) on all data: 0.08989611
Test loss (w/o reg) on all data: 0.036044933
Train acc on all data:  0.9664478482859227
Test acc on all data:   1.0
Norm of the mean of gradients: 4.6591254e-06
Norm of the params: 12.663308
     Influence (LOO): fixed 388 labels. Loss 0.03604. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071039693
Train loss (w/o reg) on all data: 0.0031159767
Test loss (w/o reg) on all data: 0.005011896
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4665069e-07
Norm of the params: 8.930838
                Loss: fixed 520 labels. Loss 0.00501. Accuracy 0.998.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29980767
Train loss (w/o reg) on all data: 0.29362983
Test loss (w/o reg) on all data: 0.12933938
Train acc on all data:  0.8935083880379285
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.335945e-06
Norm of the params: 11.115608
              Random: fixed  81 labels. Loss 0.12934. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 820
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0752281
Train loss (w/o reg) on all data: 0.06771588
Test loss (w/o reg) on all data: 0.031038348
Train acc on all data:  0.9747143204473621
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2595427e-06
Norm of the params: 12.257421
     Influence (LOO): fixed 423 labels. Loss 0.03104. Accuracy 0.999.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047900416
Train loss (w/o reg) on all data: 0.0018329821
Test loss (w/o reg) on all data: 0.004735605
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.293879e-08
Norm of the params: 7.6903305
                Loss: fixed 524 labels. Loss 0.00474. Accuracy 0.999.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28732002
Train loss (w/o reg) on all data: 0.28071752
Test loss (w/o reg) on all data: 0.12298041
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.376573e-05
Norm of the params: 11.491302
              Random: fixed 107 labels. Loss 0.12298. Accuracy 0.996.
### Flips: 615, rs: 37, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05662559
Train loss (w/o reg) on all data: 0.050332133
Test loss (w/o reg) on all data: 0.021711662
Train acc on all data:  0.9824945295404814
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6108053e-06
Norm of the params: 11.219143
     Influence (LOO): fixed 454 labels. Loss 0.02171. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039342325
Train loss (w/o reg) on all data: 0.0013878134
Test loss (w/o reg) on all data: 0.003309214
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.014524e-08
Norm of the params: 7.136412
                Loss: fixed 525 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27147925
Train loss (w/o reg) on all data: 0.26475054
Test loss (w/o reg) on all data: 0.11409855
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0563235e-05
Norm of the params: 11.60062
              Random: fixed 139 labels. Loss 0.11410. Accuracy 0.995.
### Flips: 615, rs: 37, checks: 1230
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436895
Train loss (w/o reg) on all data: 0.03802124
Test loss (w/o reg) on all data: 0.016841643
Train acc on all data:  0.986627765621201
Test acc on all data:   1.0
Norm of the mean of gradients: 8.886676e-07
Norm of the params: 10.647308
     Influence (LOO): fixed 472 labels. Loss 0.01684. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003934232
Train loss (w/o reg) on all data: 0.0013878149
Test loss (w/o reg) on all data: 0.0033092317
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.036173e-08
Norm of the params: 7.13641
                Loss: fixed 525 labels. Loss 0.00331. Accuracy 1.000.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26021677
Train loss (w/o reg) on all data: 0.25328043
Test loss (w/o reg) on all data: 0.10876453
Train acc on all data:  0.913202042304887
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.6703866e-05
Norm of the params: 11.778247
              Random: fixed 161 labels. Loss 0.10876. Accuracy 0.996.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33792454
Train loss (w/o reg) on all data: 0.33187932
Test loss (w/o reg) on all data: 0.15768756
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2113606e-05
Norm of the params: 10.995649
Flipped loss: 0.15769. Accuracy: 0.987
### Flips: 615, rs: 38, checks: 205
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22731505
Train loss (w/o reg) on all data: 0.21809201
Test loss (w/o reg) on all data: 0.098463416
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7593109e-05
Norm of the params: 13.581635
     Influence (LOO): fixed 190 labels. Loss 0.09846. Accuracy 0.995.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19766153
Train loss (w/o reg) on all data: 0.18446875
Test loss (w/o reg) on all data: 0.105159104
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.0643894e-06
Norm of the params: 16.243635
                Loss: fixed 205 labels. Loss 0.10516. Accuracy 0.981.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3261277
Train loss (w/o reg) on all data: 0.31988913
Test loss (w/o reg) on all data: 0.15041065
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4545386e-05
Norm of the params: 11.170107
              Random: fixed  28 labels. Loss 0.15041. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 410
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1557804
Train loss (w/o reg) on all data: 0.1473676
Test loss (w/o reg) on all data: 0.05975484
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.016158e-05
Norm of the params: 12.971362
     Influence (LOO): fixed 316 labels. Loss 0.05975. Accuracy 0.999.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07042335
Train loss (w/o reg) on all data: 0.055282474
Test loss (w/o reg) on all data: 0.042794876
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.219924e-06
Norm of the params: 17.401653
                Loss: fixed 410 labels. Loss 0.04279. Accuracy 0.990.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31355077
Train loss (w/o reg) on all data: 0.30716184
Test loss (w/o reg) on all data: 0.1433201
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.654687e-06
Norm of the params: 11.303914
              Random: fixed  57 labels. Loss 0.14332. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 615
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10710622
Train loss (w/o reg) on all data: 0.09992296
Test loss (w/o reg) on all data: 0.03875171
Train acc on all data:  0.9662047167517627
Test acc on all data:   1.0
Norm of the mean of gradients: 8.699108e-06
Norm of the params: 11.986045
     Influence (LOO): fixed 394 labels. Loss 0.03875. Accuracy 1.000.
Using normal model
LBFGS training took [24] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004774265
Train loss (w/o reg) on all data: 0.0021003091
Test loss (w/o reg) on all data: 0.0036115774
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.7402214e-08
Norm of the params: 7.312942
                Loss: fixed 531 labels. Loss 0.00361. Accuracy 1.000.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30451182
Train loss (w/o reg) on all data: 0.29796883
Test loss (w/o reg) on all data: 0.136986
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5971238e-05
Norm of the params: 11.439383
              Random: fixed  79 labels. Loss 0.13699. Accuracy 0.990.
### Flips: 615, rs: 38, checks: 820
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0705756
Train loss (w/o reg) on all data: 0.064221494
Test loss (w/o reg) on all data: 0.023174709
Train acc on all data:  0.9798200826647216
Test acc on all data:   1.0
Norm of the mean of gradients: 6.3589655e-06
Norm of the params: 11.27307
     Influence (LOO): fixed 447 labels. Loss 0.02317. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.002816239
Train loss (w/o reg) on all data: 0.00096009194
Test loss (w/o reg) on all data: 0.0026560093
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.250145e-08
Norm of the params: 6.09286
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29248247
Train loss (w/o reg) on all data: 0.28596985
Test loss (w/o reg) on all data: 0.12668909
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.580476e-05
Norm of the params: 11.412799
              Random: fixed 108 labels. Loss 0.12669. Accuracy 0.991.
### Flips: 615, rs: 38, checks: 1025
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049706187
Train loss (w/o reg) on all data: 0.044131745
Test loss (w/o reg) on all data: 0.015750732
Train acc on all data:  0.9861415025528811
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7881354e-06
Norm of the params: 10.558826
     Influence (LOO): fixed 477 labels. Loss 0.01575. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.00096012565
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.25368835e-08
Norm of the params: 6.092806
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28300563
Train loss (w/o reg) on all data: 0.27651718
Test loss (w/o reg) on all data: 0.11998736
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.8557492e-06
Norm of the params: 11.391625
              Random: fixed 127 labels. Loss 0.11999. Accuracy 0.994.
### Flips: 615, rs: 38, checks: 1230
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040227324
Train loss (w/o reg) on all data: 0.035340555
Test loss (w/o reg) on all data: 0.013029549
Train acc on all data:  0.9888159494286409
Test acc on all data:   1.0
Norm of the mean of gradients: 6.987989e-07
Norm of the params: 9.88612
     Influence (LOO): fixed 488 labels. Loss 0.01303. Accuracy 1.000.
Using normal model
LBFGS training took [19] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096012454
Test loss (w/o reg) on all data: 0.002656078
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3399855e-08
Norm of the params: 6.0928073
                Loss: fixed 534 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27260602
Train loss (w/o reg) on all data: 0.26589218
Test loss (w/o reg) on all data: 0.11540272
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.4319936e-05
Norm of the params: 11.587777
              Random: fixed 146 labels. Loss 0.11540. Accuracy 0.994.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3382498
Train loss (w/o reg) on all data: 0.33193418
Test loss (w/o reg) on all data: 0.15439202
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.935133e-05
Norm of the params: 11.238867
Flipped loss: 0.15439. Accuracy: 0.991
### Flips: 615, rs: 39, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23289034
Train loss (w/o reg) on all data: 0.22339012
Test loss (w/o reg) on all data: 0.10124865
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.3219622e-06
Norm of the params: 13.784206
     Influence (LOO): fixed 188 labels. Loss 0.10125. Accuracy 0.996.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20079133
Train loss (w/o reg) on all data: 0.18836714
Test loss (w/o reg) on all data: 0.09782306
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.518764e-06
Norm of the params: 15.763364
                Loss: fixed 205 labels. Loss 0.09782. Accuracy 0.983.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32549948
Train loss (w/o reg) on all data: 0.3188265
Test loss (w/o reg) on all data: 0.1487796
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.8918741e-05
Norm of the params: 11.552483
              Random: fixed  28 labels. Loss 0.14878. Accuracy 0.993.
### Flips: 615, rs: 39, checks: 410
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161173
Train loss (w/o reg) on all data: 0.1522802
Test loss (w/o reg) on all data: 0.06289141
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.376806e-05
Norm of the params: 13.336266
     Influence (LOO): fixed 313 labels. Loss 0.06289. Accuracy 0.998.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07047078
Train loss (w/o reg) on all data: 0.055104848
Test loss (w/o reg) on all data: 0.037714433
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.3703556e-06
Norm of the params: 17.530506
                Loss: fixed 410 labels. Loss 0.03771. Accuracy 0.994.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31587112
Train loss (w/o reg) on all data: 0.3092113
Test loss (w/o reg) on all data: 0.14348611
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.869868e-06
Norm of the params: 11.541062
              Random: fixed  49 labels. Loss 0.14349. Accuracy 0.994.
### Flips: 615, rs: 39, checks: 615
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10874131
Train loss (w/o reg) on all data: 0.10057397
Test loss (w/o reg) on all data: 0.04044075
Train acc on all data:  0.9654753221492828
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.9467922e-06
Norm of the params: 12.78072
     Influence (LOO): fixed 392 labels. Loss 0.04044. Accuracy 0.999.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006084604
Train loss (w/o reg) on all data: 0.0025540963
Test loss (w/o reg) on all data: 0.0037928496
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.13881136e-07
Norm of the params: 8.402986
                Loss: fixed 530 labels. Loss 0.00379. Accuracy 1.000.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30291355
Train loss (w/o reg) on all data: 0.2959126
Test loss (w/o reg) on all data: 0.13516098
Train acc on all data:  0.8915633357646486
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.6579565e-05
Norm of the params: 11.832968
              Random: fixed  78 labels. Loss 0.13516. Accuracy 0.995.
### Flips: 615, rs: 39, checks: 820
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07467677
Train loss (w/o reg) on all data: 0.067811824
Test loss (w/o reg) on all data: 0.027678354
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6479909e-06
Norm of the params: 11.71746
     Influence (LOO): fixed 441 labels. Loss 0.02768. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040627676
Train loss (w/o reg) on all data: 0.0015014237
Test loss (w/o reg) on all data: 0.0027393624
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.877678e-08
Norm of the params: 7.157295
                Loss: fixed 533 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2891919
Train loss (w/o reg) on all data: 0.28198475
Test loss (w/o reg) on all data: 0.12626575
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.959596e-06
Norm of the params: 12.005949
              Random: fixed 109 labels. Loss 0.12627. Accuracy 0.996.
### Flips: 615, rs: 39, checks: 1025
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050050143
Train loss (w/o reg) on all data: 0.04390363
Test loss (w/o reg) on all data: 0.018773478
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9037053e-06
Norm of the params: 11.087392
     Influence (LOO): fixed 475 labels. Loss 0.01877. Accuracy 0.999.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040627685
Train loss (w/o reg) on all data: 0.0015014269
Test loss (w/o reg) on all data: 0.0027393734
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.845095e-08
Norm of the params: 7.157292
                Loss: fixed 533 labels. Loss 0.00274. Accuracy 1.000.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27715674
Train loss (w/o reg) on all data: 0.27017125
Test loss (w/o reg) on all data: 0.11716279
Train acc on all data:  0.9044493070751276
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.0487896e-05
Norm of the params: 11.819888
              Random: fixed 134 labels. Loss 0.11716. Accuracy 0.995.
### Flips: 615, rs: 39, checks: 1230
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04101617
Train loss (w/o reg) on all data: 0.035490092
Test loss (w/o reg) on all data: 0.015831323
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1034676e-06
Norm of the params: 10.512922
     Influence (LOO): fixed 488 labels. Loss 0.01583. Accuracy 0.999.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162396
Train loss (w/o reg) on all data: 0.0009601235
Test loss (w/o reg) on all data: 0.0026560738
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9384998e-08
Norm of the params: 6.092809
                Loss: fixed 535 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26409692
Train loss (w/o reg) on all data: 0.25648677
Test loss (w/o reg) on all data: 0.110725515
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5205918e-05
Norm of the params: 12.337043
              Random: fixed 159 labels. Loss 0.11073. Accuracy 0.994.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40600786
Train loss (w/o reg) on all data: 0.40027133
Test loss (w/o reg) on all data: 0.21944065
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.232459e-05
Norm of the params: 10.711226
Flipped loss: 0.21944. Accuracy: 0.980
### Flips: 820, rs: 0, checks: 205
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3160299
Train loss (w/o reg) on all data: 0.30736127
Test loss (w/o reg) on all data: 0.16193926
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4591597e-05
Norm of the params: 13.167092
     Influence (LOO): fixed 174 labels. Loss 0.16194. Accuracy 0.989.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27687058
Train loss (w/o reg) on all data: 0.26477826
Test loss (w/o reg) on all data: 0.16600016
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 3.0650248e-05
Norm of the params: 15.551409
                Loss: fixed 205 labels. Loss 0.16600. Accuracy 0.955.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39528322
Train loss (w/o reg) on all data: 0.3894024
Test loss (w/o reg) on all data: 0.2077941
Train acc on all data:  0.837588135181133
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1549339e-05
Norm of the params: 10.845129
              Random: fixed  32 labels. Loss 0.20779. Accuracy 0.985.
### Flips: 820, rs: 0, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25355983
Train loss (w/o reg) on all data: 0.24501087
Test loss (w/o reg) on all data: 0.1208446
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.597312e-06
Norm of the params: 13.075894
     Influence (LOO): fixed 306 labels. Loss 0.12084. Accuracy 0.994.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16387196
Train loss (w/o reg) on all data: 0.14778036
Test loss (w/o reg) on all data: 0.104439944
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.4456862e-05
Norm of the params: 17.939674
                Loss: fixed 410 labels. Loss 0.10444. Accuracy 0.970.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38112554
Train loss (w/o reg) on all data: 0.37484932
Test loss (w/o reg) on all data: 0.19735385
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4105664e-05
Norm of the params: 11.203761
              Random: fixed  72 labels. Loss 0.19735. Accuracy 0.990.
### Flips: 820, rs: 0, checks: 615
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20244044
Train loss (w/o reg) on all data: 0.19424385
Test loss (w/o reg) on all data: 0.088955864
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.5533125e-06
Norm of the params: 12.803589
     Influence (LOO): fixed 404 labels. Loss 0.08896. Accuracy 0.997.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05764088
Train loss (w/o reg) on all data: 0.044018794
Test loss (w/o reg) on all data: 0.033691727
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.375677e-06
Norm of the params: 16.50581
                Loss: fixed 612 labels. Loss 0.03369. Accuracy 0.991.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3667985
Train loss (w/o reg) on all data: 0.36061427
Test loss (w/o reg) on all data: 0.184655
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2489503e-05
Norm of the params: 11.121362
              Random: fixed 109 labels. Loss 0.18465. Accuracy 0.989.
### Flips: 820, rs: 0, checks: 820
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15035342
Train loss (w/o reg) on all data: 0.14202462
Test loss (w/o reg) on all data: 0.062639214
Train acc on all data:  0.9472404570872842
Test acc on all data:   1.0
Norm of the mean of gradients: 1.013804e-05
Norm of the params: 12.906425
     Influence (LOO): fixed 494 labels. Loss 0.06264. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009755777
Train loss (w/o reg) on all data: 0.0046707024
Test loss (w/o reg) on all data: 0.0050170436
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0925494e-07
Norm of the params: 10.084716
                Loss: fixed 701 labels. Loss 0.00502. Accuracy 0.999.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35267326
Train loss (w/o reg) on all data: 0.34640518
Test loss (w/o reg) on all data: 0.17657709
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.237754e-06
Norm of the params: 11.196495
              Random: fixed 145 labels. Loss 0.17658. Accuracy 0.988.
### Flips: 820, rs: 0, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11514884
Train loss (w/o reg) on all data: 0.107607625
Test loss (w/o reg) on all data: 0.043171693
Train acc on all data:  0.9613420860685631
Test acc on all data:   1.0
Norm of the mean of gradients: 2.791561e-06
Norm of the params: 12.2810545
     Influence (LOO): fixed 555 labels. Loss 0.04317. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053525446
Train loss (w/o reg) on all data: 0.0020424896
Test loss (w/o reg) on all data: 0.0030898203
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7789376e-07
Norm of the params: 8.136405
                Loss: fixed 709 labels. Loss 0.00309. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34018812
Train loss (w/o reg) on all data: 0.33378178
Test loss (w/o reg) on all data: 0.16650438
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.233939e-05
Norm of the params: 11.31931
              Random: fixed 176 labels. Loss 0.16650. Accuracy 0.988.
### Flips: 820, rs: 0, checks: 1230
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08618472
Train loss (w/o reg) on all data: 0.07898814
Test loss (w/o reg) on all data: 0.030672658
Train acc on all data:  0.9717967420374423
Test acc on all data:   1.0
Norm of the mean of gradients: 1.52059e-06
Norm of the params: 11.997145
     Influence (LOO): fixed 600 labels. Loss 0.03067. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005013216
Train loss (w/o reg) on all data: 0.0018859539
Test loss (w/o reg) on all data: 0.0030347372
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0521402e-07
Norm of the params: 7.9085546
                Loss: fixed 710 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32770294
Train loss (w/o reg) on all data: 0.32137987
Test loss (w/o reg) on all data: 0.15730357
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.907243e-06
Norm of the params: 11.245494
              Random: fixed 207 labels. Loss 0.15730. Accuracy 0.988.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4068962
Train loss (w/o reg) on all data: 0.40183434
Test loss (w/o reg) on all data: 0.20702499
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.8737002e-05
Norm of the params: 10.0616665
Flipped loss: 0.20702. Accuracy: 0.984
### Flips: 820, rs: 1, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31298578
Train loss (w/o reg) on all data: 0.30351385
Test loss (w/o reg) on all data: 0.1479477
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2447207e-05
Norm of the params: 13.763663
     Influence (LOO): fixed 180 labels. Loss 0.14795. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27821118
Train loss (w/o reg) on all data: 0.26640788
Test loss (w/o reg) on all data: 0.16039073
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 6.5894023e-06
Norm of the params: 15.364443
                Loss: fixed 205 labels. Loss 0.16039. Accuracy 0.961.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39587194
Train loss (w/o reg) on all data: 0.3906438
Test loss (w/o reg) on all data: 0.19558837
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.137473e-05
Norm of the params: 10.225599
              Random: fixed  33 labels. Loss 0.19559. Accuracy 0.986.
### Flips: 820, rs: 1, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24522725
Train loss (w/o reg) on all data: 0.23563907
Test loss (w/o reg) on all data: 0.112351894
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.2204289e-05
Norm of the params: 13.8478775
     Influence (LOO): fixed 313 labels. Loss 0.11235. Accuracy 0.995.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16488369
Train loss (w/o reg) on all data: 0.14828016
Test loss (w/o reg) on all data: 0.10253033
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.253811e-06
Norm of the params: 18.222805
                Loss: fixed 410 labels. Loss 0.10253. Accuracy 0.971.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837116
Train loss (w/o reg) on all data: 0.3783271
Test loss (w/o reg) on all data: 0.18545075
Train acc on all data:  0.849015317286652
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6023974e-05
Norm of the params: 10.377384
              Random: fixed  67 labels. Loss 0.18545. Accuracy 0.988.
### Flips: 820, rs: 1, checks: 615
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19406822
Train loss (w/o reg) on all data: 0.1848102
Test loss (w/o reg) on all data: 0.08447777
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.346965e-06
Norm of the params: 13.60736
     Influence (LOO): fixed 404 labels. Loss 0.08448. Accuracy 0.999.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05389574
Train loss (w/o reg) on all data: 0.04065775
Test loss (w/o reg) on all data: 0.026326116
Train acc on all data:  0.9863846340870411
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.3518679e-06
Norm of the params: 16.271442
                Loss: fixed 611 labels. Loss 0.02633. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37271476
Train loss (w/o reg) on all data: 0.36730108
Test loss (w/o reg) on all data: 0.17571537
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1197976e-05
Norm of the params: 10.405459
              Random: fixed  95 labels. Loss 0.17572. Accuracy 0.987.
### Flips: 820, rs: 1, checks: 820
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15596004
Train loss (w/o reg) on all data: 0.14749262
Test loss (w/o reg) on all data: 0.06571739
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.357662e-06
Norm of the params: 13.013393
     Influence (LOO): fixed 471 labels. Loss 0.06572. Accuracy 0.998.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059883306
Train loss (w/o reg) on all data: 0.0024990812
Test loss (w/o reg) on all data: 0.0032114615
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 4.9321855e-08
Norm of the params: 8.353741
                Loss: fixed 697 labels. Loss 0.00321. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.359559
Train loss (w/o reg) on all data: 0.35416228
Test loss (w/o reg) on all data: 0.16334307
Train acc on all data:  0.862387551665451
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4204658e-05
Norm of the params: 10.389145
              Random: fixed 132 labels. Loss 0.16334. Accuracy 0.989.
### Flips: 820, rs: 1, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11733628
Train loss (w/o reg) on all data: 0.109087974
Test loss (w/o reg) on all data: 0.046609547
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1231386e-05
Norm of the params: 12.843913
     Influence (LOO): fixed 535 labels. Loss 0.04661. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601144
Test loss (w/o reg) on all data: 0.0026560563
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.065159e-09
Norm of the params: 6.092825
                Loss: fixed 702 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429243
Train loss (w/o reg) on all data: 0.33730564
Test loss (w/o reg) on all data: 0.1519371
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.122864e-06
Norm of the params: 10.600625
              Random: fixed 170 labels. Loss 0.15194. Accuracy 0.993.
### Flips: 820, rs: 1, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091706626
Train loss (w/o reg) on all data: 0.08361958
Test loss (w/o reg) on all data: 0.035983216
Train acc on all data:  0.9691222951616825
Test acc on all data:   1.0
Norm of the mean of gradients: 6.258441e-06
Norm of the params: 12.717738
     Influence (LOO): fixed 575 labels. Loss 0.03598. Accuracy 1.000.
Using normal model
LBFGS training took [21] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162394
Train loss (w/o reg) on all data: 0.00096011657
Test loss (w/o reg) on all data: 0.0026560542
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.137097e-09
Norm of the params: 6.09282
                Loss: fixed 702 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32701093
Train loss (w/o reg) on all data: 0.32112107
Test loss (w/o reg) on all data: 0.142996
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.1239113e-05
Norm of the params: 10.853454
              Random: fixed 203 labels. Loss 0.14300. Accuracy 0.993.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40377468
Train loss (w/o reg) on all data: 0.39771327
Test loss (w/o reg) on all data: 0.2123148
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.479648e-06
Norm of the params: 11.010372
Flipped loss: 0.21231. Accuracy: 0.974
### Flips: 820, rs: 2, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32084596
Train loss (w/o reg) on all data: 0.31246978
Test loss (w/o reg) on all data: 0.16011336
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.050765e-06
Norm of the params: 12.943096
     Influence (LOO): fixed 174 labels. Loss 0.16011. Accuracy 0.988.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27892807
Train loss (w/o reg) on all data: 0.2661743
Test loss (w/o reg) on all data: 0.16111298
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.8943423e-05
Norm of the params: 15.971086
                Loss: fixed 205 labels. Loss 0.16111. Accuracy 0.960.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39090094
Train loss (w/o reg) on all data: 0.384745
Test loss (w/o reg) on all data: 0.19707137
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.1731402e-05
Norm of the params: 11.09589
              Random: fixed  38 labels. Loss 0.19707. Accuracy 0.983.
### Flips: 820, rs: 2, checks: 410
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24845451
Train loss (w/o reg) on all data: 0.23856767
Test loss (w/o reg) on all data: 0.114765204
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.548345e-06
Norm of the params: 14.061894
     Influence (LOO): fixed 313 labels. Loss 0.11477. Accuracy 0.990.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1655327
Train loss (w/o reg) on all data: 0.14791615
Test loss (w/o reg) on all data: 0.10330393
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.6631783e-06
Norm of the params: 18.770475
                Loss: fixed 409 labels. Loss 0.10330. Accuracy 0.973.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37915784
Train loss (w/o reg) on all data: 0.37282506
Test loss (w/o reg) on all data: 0.18705358
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.0925141e-05
Norm of the params: 11.254141
              Random: fixed  69 labels. Loss 0.18705. Accuracy 0.982.
### Flips: 820, rs: 2, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19052514
Train loss (w/o reg) on all data: 0.18113646
Test loss (w/o reg) on all data: 0.08255093
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5896192e-05
Norm of the params: 13.703059
     Influence (LOO): fixed 424 labels. Loss 0.08255. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056671433
Train loss (w/o reg) on all data: 0.04169573
Test loss (w/o reg) on all data: 0.04595096
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5187763e-06
Norm of the params: 17.306477
                Loss: fixed 610 labels. Loss 0.04595. Accuracy 0.987.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3690058
Train loss (w/o reg) on all data: 0.36259666
Test loss (w/o reg) on all data: 0.17714344
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 7.139526e-06
Norm of the params: 11.32178
              Random: fixed  95 labels. Loss 0.17714. Accuracy 0.984.
### Flips: 820, rs: 2, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15143937
Train loss (w/o reg) on all data: 0.14213532
Test loss (w/o reg) on all data: 0.06167197
Train acc on all data:  0.9465110624848043
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2313517e-06
Norm of the params: 13.6411495
     Influence (LOO): fixed 494 labels. Loss 0.06167. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010922138
Train loss (w/o reg) on all data: 0.0055148415
Test loss (w/o reg) on all data: 0.0055578067
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.5370178e-07
Norm of the params: 10.399323
                Loss: fixed 703 labels. Loss 0.00556. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35786238
Train loss (w/o reg) on all data: 0.35137588
Test loss (w/o reg) on all data: 0.16614023
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 9.071875e-06
Norm of the params: 11.389923
              Random: fixed 126 labels. Loss 0.16614. Accuracy 0.985.
### Flips: 820, rs: 2, checks: 1025
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11889634
Train loss (w/o reg) on all data: 0.11094549
Test loss (w/o reg) on all data: 0.047054745
Train acc on all data:  0.9593970337952833
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1388669e-05
Norm of the params: 12.610195
     Influence (LOO): fixed 546 labels. Loss 0.04705. Accuracy 0.999.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007561993
Train loss (w/o reg) on all data: 0.0035282576
Test loss (w/o reg) on all data: 0.0053211837
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8354207e-07
Norm of the params: 8.98191
                Loss: fixed 708 labels. Loss 0.00532. Accuracy 0.999.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3377408
Train loss (w/o reg) on all data: 0.33116516
Test loss (w/o reg) on all data: 0.15330425
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.80863e-06
Norm of the params: 11.467902
              Random: fixed 176 labels. Loss 0.15330. Accuracy 0.986.
### Flips: 820, rs: 2, checks: 1230
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08624299
Train loss (w/o reg) on all data: 0.07865367
Test loss (w/o reg) on all data: 0.032884993
Train acc on all data:  0.9710673474349624
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4465415e-06
Norm of the params: 12.320161
     Influence (LOO): fixed 597 labels. Loss 0.03288. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004987283
Train loss (w/o reg) on all data: 0.0019895006
Test loss (w/o reg) on all data: 0.0044290177
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.516147e-08
Norm of the params: 7.7431035
                Loss: fixed 712 labels. Loss 0.00443. Accuracy 0.999.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3226419
Train loss (w/o reg) on all data: 0.31611526
Test loss (w/o reg) on all data: 0.14109322
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.2703539e-05
Norm of the params: 11.425097
              Random: fixed 216 labels. Loss 0.14109. Accuracy 0.989.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39547467
Train loss (w/o reg) on all data: 0.3894768
Test loss (w/o reg) on all data: 0.21622339
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.5179677e-05
Norm of the params: 10.952496
Flipped loss: 0.21622. Accuracy: 0.967
### Flips: 820, rs: 3, checks: 205
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30666322
Train loss (w/o reg) on all data: 0.29662225
Test loss (w/o reg) on all data: 0.15994549
Train acc on all data:  0.8760029175784099
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.968826e-06
Norm of the params: 14.171081
     Influence (LOO): fixed 172 labels. Loss 0.15995. Accuracy 0.982.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26980907
Train loss (w/o reg) on all data: 0.25644565
Test loss (w/o reg) on all data: 0.17433058
Train acc on all data:  0.8898614150255288
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 2.0900783e-05
Norm of the params: 16.348352
                Loss: fixed 205 labels. Loss 0.17433. Accuracy 0.948.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3826764
Train loss (w/o reg) on all data: 0.37673834
Test loss (w/o reg) on all data: 0.20230147
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3279998e-05
Norm of the params: 10.8977585
              Random: fixed  39 labels. Loss 0.20230. Accuracy 0.972.
### Flips: 820, rs: 3, checks: 410
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24151605
Train loss (w/o reg) on all data: 0.23158479
Test loss (w/o reg) on all data: 0.11149426
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.21971e-06
Norm of the params: 14.093448
     Influence (LOO): fixed 306 labels. Loss 0.11149. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15651119
Train loss (w/o reg) on all data: 0.13938096
Test loss (w/o reg) on all data: 0.11798046
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.7905625e-06
Norm of the params: 18.50958
                Loss: fixed 408 labels. Loss 0.11798. Accuracy 0.962.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37135822
Train loss (w/o reg) on all data: 0.36535335
Test loss (w/o reg) on all data: 0.19211352
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.4244545e-06
Norm of the params: 10.958888
              Random: fixed  72 labels. Loss 0.19211. Accuracy 0.973.
### Flips: 820, rs: 3, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19542456
Train loss (w/o reg) on all data: 0.18581374
Test loss (w/o reg) on all data: 0.08585613
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 5.097253e-06
Norm of the params: 13.864215
     Influence (LOO): fixed 401 labels. Loss 0.08586. Accuracy 0.993.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05505872
Train loss (w/o reg) on all data: 0.04062756
Test loss (w/o reg) on all data: 0.038122594
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.4362633e-06
Norm of the params: 16.988914
                Loss: fixed 604 labels. Loss 0.03812. Accuracy 0.990.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35973257
Train loss (w/o reg) on all data: 0.3536493
Test loss (w/o reg) on all data: 0.17818664
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.691164e-06
Norm of the params: 11.030212
              Random: fixed 108 labels. Loss 0.17819. Accuracy 0.979.
### Flips: 820, rs: 3, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14589286
Train loss (w/o reg) on all data: 0.13708809
Test loss (w/o reg) on all data: 0.0581295
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.9190974e-06
Norm of the params: 13.270099
     Influence (LOO): fixed 492 labels. Loss 0.05813. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011796025
Train loss (w/o reg) on all data: 0.0060103904
Test loss (w/o reg) on all data: 0.0055677462
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7969043e-07
Norm of the params: 10.756983
                Loss: fixed 688 labels. Loss 0.00557. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34866604
Train loss (w/o reg) on all data: 0.3424888
Test loss (w/o reg) on all data: 0.16850263
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.3248379e-05
Norm of the params: 11.115079
              Random: fixed 139 labels. Loss 0.16850. Accuracy 0.982.
### Flips: 820, rs: 3, checks: 1025
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101985745
Train loss (w/o reg) on all data: 0.09394647
Test loss (w/o reg) on all data: 0.03961206
Train acc on all data:  0.9654753221492828
Test acc on all data:   1.0
Norm of the mean of gradients: 1.590579e-06
Norm of the params: 12.6801195
     Influence (LOO): fixed 563 labels. Loss 0.03961. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006424996
Train loss (w/o reg) on all data: 0.0029633406
Test loss (w/o reg) on all data: 0.0037591283
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.12445484e-07
Norm of the params: 8.320643
                Loss: fixed 697 labels. Loss 0.00376. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3329884
Train loss (w/o reg) on all data: 0.3269058
Test loss (w/o reg) on all data: 0.15668449
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.7215747e-05
Norm of the params: 11.029622
              Random: fixed 179 labels. Loss 0.15668. Accuracy 0.983.
### Flips: 820, rs: 3, checks: 1230
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07677103
Train loss (w/o reg) on all data: 0.06919082
Test loss (w/o reg) on all data: 0.031843647
Train acc on all data:  0.9742280573790421
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.186706e-07
Norm of the params: 12.312764
     Influence (LOO): fixed 600 labels. Loss 0.03184. Accuracy 0.996.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037254915
Train loss (w/o reg) on all data: 0.0015213402
Test loss (w/o reg) on all data: 0.0031758223
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7457535e-08
Norm of the params: 6.639505
                Loss: fixed 701 labels. Loss 0.00318. Accuracy 1.000.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31693068
Train loss (w/o reg) on all data: 0.31098118
Test loss (w/o reg) on all data: 0.14113791
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.4919294e-06
Norm of the params: 10.908241
              Random: fixed 217 labels. Loss 0.14114. Accuracy 0.991.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40062317
Train loss (w/o reg) on all data: 0.39492163
Test loss (w/o reg) on all data: 0.20725346
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.4718596e-05
Norm of the params: 10.678511
Flipped loss: 0.20725. Accuracy: 0.983
### Flips: 820, rs: 4, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3071152
Train loss (w/o reg) on all data: 0.2976237
Test loss (w/o reg) on all data: 0.15022217
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.4190743e-05
Norm of the params: 13.777879
     Influence (LOO): fixed 180 labels. Loss 0.15022. Accuracy 0.989.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26853335
Train loss (w/o reg) on all data: 0.25516537
Test loss (w/o reg) on all data: 0.1597979
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 3.7819234e-05
Norm of the params: 16.351135
                Loss: fixed 205 labels. Loss 0.15980. Accuracy 0.963.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38600054
Train loss (w/o reg) on all data: 0.3799893
Test loss (w/o reg) on all data: 0.1960122
Train acc on all data:  0.8407488451252128
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.45431e-05
Norm of the params: 10.9647255
              Random: fixed  38 labels. Loss 0.19601. Accuracy 0.985.
### Flips: 820, rs: 4, checks: 410
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24402542
Train loss (w/o reg) on all data: 0.2338842
Test loss (w/o reg) on all data: 0.10942137
Train acc on all data:  0.9042061755409677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.0296105e-06
Norm of the params: 14.241643
     Influence (LOO): fixed 310 labels. Loss 0.10942. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15477799
Train loss (w/o reg) on all data: 0.13704821
Test loss (w/o reg) on all data: 0.10703342
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 8.9547275e-06
Norm of the params: 18.830708
                Loss: fixed 410 labels. Loss 0.10703. Accuracy 0.964.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3722072
Train loss (w/o reg) on all data: 0.36593258
Test loss (w/o reg) on all data: 0.18359178
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3615543e-05
Norm of the params: 11.202328
              Random: fixed  74 labels. Loss 0.18359. Accuracy 0.985.
### Flips: 820, rs: 4, checks: 615
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18384677
Train loss (w/o reg) on all data: 0.17355405
Test loss (w/o reg) on all data: 0.07645049
Train acc on all data:  0.9319231704352055
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0206184e-05
Norm of the params: 14.347626
     Influence (LOO): fixed 425 labels. Loss 0.07645. Accuracy 0.997.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053422004
Train loss (w/o reg) on all data: 0.0396661
Test loss (w/o reg) on all data: 0.03471791
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.9363304e-06
Norm of the params: 16.586681
                Loss: fixed 611 labels. Loss 0.03472. Accuracy 0.990.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35845098
Train loss (w/o reg) on all data: 0.35184315
Test loss (w/o reg) on all data: 0.17640181
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.490547e-05
Norm of the params: 11.495932
              Random: fixed 107 labels. Loss 0.17640. Accuracy 0.986.
### Flips: 820, rs: 4, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14597164
Train loss (w/o reg) on all data: 0.13716027
Test loss (w/o reg) on all data: 0.059062444
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.814286e-06
Norm of the params: 13.275064
     Influence (LOO): fixed 491 labels. Loss 0.05906. Accuracy 0.997.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00803331
Train loss (w/o reg) on all data: 0.003714861
Test loss (w/o reg) on all data: 0.0044467235
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.733622e-07
Norm of the params: 9.293491
                Loss: fixed 695 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34912062
Train loss (w/o reg) on all data: 0.3427177
Test loss (w/o reg) on all data: 0.16817431
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1596535e-05
Norm of the params: 11.316272
              Random: fixed 136 labels. Loss 0.16817. Accuracy 0.984.
### Flips: 820, rs: 4, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1180354
Train loss (w/o reg) on all data: 0.10969751
Test loss (w/o reg) on all data: 0.04622851
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2414553e-06
Norm of the params: 12.913468
     Influence (LOO): fixed 537 labels. Loss 0.04623. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005765235
Train loss (w/o reg) on all data: 0.002331878
Test loss (w/o reg) on all data: 0.0033435381
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2052342e-07
Norm of the params: 8.286564
                Loss: fixed 699 labels. Loss 0.00334. Accuracy 1.000.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33746216
Train loss (w/o reg) on all data: 0.33098623
Test loss (w/o reg) on all data: 0.15866399
Train acc on all data:  0.8711402868952103
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8893297e-05
Norm of the params: 11.3806095
              Random: fixed 164 labels. Loss 0.15866. Accuracy 0.990.
### Flips: 820, rs: 4, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09210799
Train loss (w/o reg) on all data: 0.084265135
Test loss (w/o reg) on all data: 0.034830432
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6395547e-06
Norm of the params: 12.524258
     Influence (LOO): fixed 578 labels. Loss 0.03483. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044076974
Train loss (w/o reg) on all data: 0.0016621319
Test loss (w/o reg) on all data: 0.0027852869
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2445445e-08
Norm of the params: 7.410217
                Loss: fixed 702 labels. Loss 0.00279. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3234703
Train loss (w/o reg) on all data: 0.31697482
Test loss (w/o reg) on all data: 0.1485822
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.5220208e-05
Norm of the params: 11.397779
              Random: fixed 198 labels. Loss 0.14858. Accuracy 0.987.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40283075
Train loss (w/o reg) on all data: 0.3973336
Test loss (w/o reg) on all data: 0.21768238
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4874573e-05
Norm of the params: 10.485377
Flipped loss: 0.21768. Accuracy: 0.982
### Flips: 820, rs: 5, checks: 205
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31298128
Train loss (w/o reg) on all data: 0.30361295
Test loss (w/o reg) on all data: 0.165068
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1602189e-05
Norm of the params: 13.688186
     Influence (LOO): fixed 177 labels. Loss 0.16507. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27250773
Train loss (w/o reg) on all data: 0.26083994
Test loss (w/o reg) on all data: 0.16695766
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 8.9541e-06
Norm of the params: 15.275987
                Loss: fixed 205 labels. Loss 0.16696. Accuracy 0.958.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39274737
Train loss (w/o reg) on all data: 0.38721737
Test loss (w/o reg) on all data: 0.208296
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.3291504e-05
Norm of the params: 10.51666
              Random: fixed  29 labels. Loss 0.20830. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 410
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24366759
Train loss (w/o reg) on all data: 0.23472592
Test loss (w/o reg) on all data: 0.12283702
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.9369705e-06
Norm of the params: 13.372854
     Influence (LOO): fixed 312 labels. Loss 0.12284. Accuracy 0.990.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16022988
Train loss (w/o reg) on all data: 0.14448778
Test loss (w/o reg) on all data: 0.11097278
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.511495e-06
Norm of the params: 17.743782
                Loss: fixed 410 labels. Loss 0.11097. Accuracy 0.965.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3816245
Train loss (w/o reg) on all data: 0.37601015
Test loss (w/o reg) on all data: 0.19786528
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.5058317e-05
Norm of the params: 10.596542
              Random: fixed  63 labels. Loss 0.19787. Accuracy 0.984.
### Flips: 820, rs: 5, checks: 615
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19672033
Train loss (w/o reg) on all data: 0.18736489
Test loss (w/o reg) on all data: 0.093048565
Train acc on all data:  0.9241429613420861
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.0612655e-05
Norm of the params: 13.678772
     Influence (LOO): fixed 401 labels. Loss 0.09305. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058917485
Train loss (w/o reg) on all data: 0.045118466
Test loss (w/o reg) on all data: 0.043258756
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.142894e-06
Norm of the params: 16.61266
                Loss: fixed 611 labels. Loss 0.04326. Accuracy 0.985.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36670145
Train loss (w/o reg) on all data: 0.3611922
Test loss (w/o reg) on all data: 0.1857218
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.844872e-06
Norm of the params: 10.496922
              Random: fixed 103 labels. Loss 0.18572. Accuracy 0.987.
### Flips: 820, rs: 5, checks: 820
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15179071
Train loss (w/o reg) on all data: 0.1430861
Test loss (w/o reg) on all data: 0.06899704
Train acc on all data:  0.9433503525407245
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.143958e-06
Norm of the params: 13.194399
     Influence (LOO): fixed 479 labels. Loss 0.06900. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01137839
Train loss (w/o reg) on all data: 0.0057227234
Test loss (w/o reg) on all data: 0.0073380945
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8588355e-07
Norm of the params: 10.635475
                Loss: fixed 703 labels. Loss 0.00734. Accuracy 0.997.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35023072
Train loss (w/o reg) on all data: 0.34447363
Test loss (w/o reg) on all data: 0.1716865
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.399902e-06
Norm of the params: 10.730405
              Random: fixed 144 labels. Loss 0.17169. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 1025
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1171061
Train loss (w/o reg) on all data: 0.10833089
Test loss (w/o reg) on all data: 0.05393817
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.144965e-06
Norm of the params: 13.247805
     Influence (LOO): fixed 535 labels. Loss 0.05394. Accuracy 0.994.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007552716
Train loss (w/o reg) on all data: 0.0034258973
Test loss (w/o reg) on all data: 0.0040151994
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7041074e-07
Norm of the params: 9.084953
                Loss: fixed 712 labels. Loss 0.00402. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33346853
Train loss (w/o reg) on all data: 0.32792068
Test loss (w/o reg) on all data: 0.1600009
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.462499e-06
Norm of the params: 10.533607
              Random: fixed 188 labels. Loss 0.16000. Accuracy 0.986.
### Flips: 820, rs: 5, checks: 1230
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09890899
Train loss (w/o reg) on all data: 0.090729326
Test loss (w/o reg) on all data: 0.04152264
Train acc on all data:  0.9649890590809628
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.098639e-06
Norm of the params: 12.790362
     Influence (LOO): fixed 571 labels. Loss 0.04152. Accuracy 0.997.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006218318
Train loss (w/o reg) on all data: 0.0027032557
Test loss (w/o reg) on all data: 0.0035239568
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 9.492201e-08
Norm of the params: 8.384583
                Loss: fixed 714 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31938043
Train loss (w/o reg) on all data: 0.3138508
Test loss (w/o reg) on all data: 0.14978549
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.62224e-06
Norm of the params: 10.516327
              Random: fixed 224 labels. Loss 0.14979. Accuracy 0.988.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40066463
Train loss (w/o reg) on all data: 0.39573327
Test loss (w/o reg) on all data: 0.20891973
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.9194866e-06
Norm of the params: 9.93112
Flipped loss: 0.20892. Accuracy: 0.977
### Flips: 820, rs: 6, checks: 205
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31082827
Train loss (w/o reg) on all data: 0.30209342
Test loss (w/o reg) on all data: 0.15191998
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.0465188e-05
Norm of the params: 13.21731
     Influence (LOO): fixed 176 labels. Loss 0.15192. Accuracy 0.990.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27045095
Train loss (w/o reg) on all data: 0.2588534
Test loss (w/o reg) on all data: 0.15505315
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.3902463e-05
Norm of the params: 15.229942
                Loss: fixed 205 labels. Loss 0.15505. Accuracy 0.957.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39303714
Train loss (w/o reg) on all data: 0.38802767
Test loss (w/o reg) on all data: 0.20002481
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1415402e-05
Norm of the params: 10.009472
              Random: fixed  24 labels. Loss 0.20002. Accuracy 0.979.
### Flips: 820, rs: 6, checks: 410
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24380346
Train loss (w/o reg) on all data: 0.23435257
Test loss (w/o reg) on all data: 0.112794675
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7056453e-05
Norm of the params: 13.748368
     Influence (LOO): fixed 303 labels. Loss 0.11279. Accuracy 0.991.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15589684
Train loss (w/o reg) on all data: 0.14068532
Test loss (w/o reg) on all data: 0.09569844
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.1826321e-05
Norm of the params: 17.442202
                Loss: fixed 410 labels. Loss 0.09570. Accuracy 0.963.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3772526
Train loss (w/o reg) on all data: 0.3717451
Test loss (w/o reg) on all data: 0.18888666
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4274529e-05
Norm of the params: 10.495226
              Random: fixed  65 labels. Loss 0.18889. Accuracy 0.980.
### Flips: 820, rs: 6, checks: 615
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19461046
Train loss (w/o reg) on all data: 0.18505271
Test loss (w/o reg) on all data: 0.08552018
Train acc on all data:  0.9282761974228058
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.090438e-06
Norm of the params: 13.825887
     Influence (LOO): fixed 396 labels. Loss 0.08552. Accuracy 0.993.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052614607
Train loss (w/o reg) on all data: 0.03841206
Test loss (w/o reg) on all data: 0.03824488
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.7777994e-06
Norm of the params: 16.853813
                Loss: fixed 609 labels. Loss 0.03824. Accuracy 0.990.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3625948
Train loss (w/o reg) on all data: 0.35699674
Test loss (w/o reg) on all data: 0.17569025
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.180845e-05
Norm of the params: 10.581173
              Random: fixed 102 labels. Loss 0.17569. Accuracy 0.982.
### Flips: 820, rs: 6, checks: 820
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14737524
Train loss (w/o reg) on all data: 0.13840972
Test loss (w/o reg) on all data: 0.061770763
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.933084e-06
Norm of the params: 13.390684
     Influence (LOO): fixed 477 labels. Loss 0.06177. Accuracy 0.993.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0126221385
Train loss (w/o reg) on all data: 0.0063436953
Test loss (w/o reg) on all data: 0.007410857
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1685448e-06
Norm of the params: 11.205751
                Loss: fixed 684 labels. Loss 0.00741. Accuracy 0.999.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34988168
Train loss (w/o reg) on all data: 0.34424424
Test loss (w/o reg) on all data: 0.16605113
Train acc on all data:  0.8653051300753708
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.8904906e-06
Norm of the params: 10.618333
              Random: fixed 135 labels. Loss 0.16605. Accuracy 0.981.
### Flips: 820, rs: 6, checks: 1025
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11177419
Train loss (w/o reg) on all data: 0.10384595
Test loss (w/o reg) on all data: 0.044159945
Train acc on all data:  0.9618283491368831
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.284358e-06
Norm of the params: 12.5922575
     Influence (LOO): fixed 539 labels. Loss 0.04416. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062779365
Train loss (w/o reg) on all data: 0.0027718486
Test loss (w/o reg) on all data: 0.0043557244
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.760104e-08
Norm of the params: 8.373874
                Loss: fixed 695 labels. Loss 0.00436. Accuracy 0.999.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33124712
Train loss (w/o reg) on all data: 0.32541576
Test loss (w/o reg) on all data: 0.15373977
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6678638e-05
Norm of the params: 10.799412
              Random: fixed 181 labels. Loss 0.15374. Accuracy 0.982.
### Flips: 820, rs: 6, checks: 1230
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08907828
Train loss (w/o reg) on all data: 0.08136062
Test loss (w/o reg) on all data: 0.033070218
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.6074424e-06
Norm of the params: 12.423889
     Influence (LOO): fixed 577 labels. Loss 0.03307. Accuracy 0.998.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035644767
Train loss (w/o reg) on all data: 0.0012908224
Test loss (w/o reg) on all data: 0.0030733468
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.294879e-08
Norm of the params: 6.7433734
                Loss: fixed 699 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31853455
Train loss (w/o reg) on all data: 0.31279954
Test loss (w/o reg) on all data: 0.14319892
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.543816e-06
Norm of the params: 10.709819
              Random: fixed 215 labels. Loss 0.14320. Accuracy 0.984.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4057077
Train loss (w/o reg) on all data: 0.40012714
Test loss (w/o reg) on all data: 0.21718258
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.1911193e-05
Norm of the params: 10.564601
Flipped loss: 0.21718. Accuracy: 0.983
### Flips: 820, rs: 7, checks: 205
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31922662
Train loss (w/o reg) on all data: 0.31030393
Test loss (w/o reg) on all data: 0.16014625
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.254879e-05
Norm of the params: 13.358669
     Influence (LOO): fixed 175 labels. Loss 0.16015. Accuracy 0.991.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28111792
Train loss (w/o reg) on all data: 0.2699791
Test loss (w/o reg) on all data: 0.17310768
Train acc on all data:  0.8830537320690494
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.1522698e-05
Norm of the params: 14.925707
                Loss: fixed 205 labels. Loss 0.17311. Accuracy 0.948.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39205942
Train loss (w/o reg) on all data: 0.38622722
Test loss (w/o reg) on all data: 0.20610523
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6895427e-05
Norm of the params: 10.80017
              Random: fixed  39 labels. Loss 0.20611. Accuracy 0.982.
### Flips: 820, rs: 7, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24802917
Train loss (w/o reg) on all data: 0.2383144
Test loss (w/o reg) on all data: 0.11506785
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7789867e-05
Norm of the params: 13.938986
     Influence (LOO): fixed 315 labels. Loss 0.11507. Accuracy 0.996.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16224931
Train loss (w/o reg) on all data: 0.14730117
Test loss (w/o reg) on all data: 0.10719899
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.6398222e-06
Norm of the params: 17.29054
                Loss: fixed 410 labels. Loss 0.10720. Accuracy 0.966.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37799385
Train loss (w/o reg) on all data: 0.37205288
Test loss (w/o reg) on all data: 0.1930658
Train acc on all data:  0.8509603695599319
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.566893e-06
Norm of the params: 10.900429
              Random: fixed  79 labels. Loss 0.19307. Accuracy 0.987.
### Flips: 820, rs: 7, checks: 615
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19542293
Train loss (w/o reg) on all data: 0.18541934
Test loss (w/o reg) on all data: 0.08580918
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 5.064979e-06
Norm of the params: 14.144677
     Influence (LOO): fixed 409 labels. Loss 0.08581. Accuracy 0.996.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051732577
Train loss (w/o reg) on all data: 0.037747335
Test loss (w/o reg) on all data: 0.032145426
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.489738e-06
Norm of the params: 16.724379
                Loss: fixed 612 labels. Loss 0.03215. Accuracy 0.988.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36389238
Train loss (w/o reg) on all data: 0.35763636
Test loss (w/o reg) on all data: 0.18020335
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.9280922e-05
Norm of the params: 11.185726
              Random: fixed 119 labels. Loss 0.18020. Accuracy 0.982.
### Flips: 820, rs: 7, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15553553
Train loss (w/o reg) on all data: 0.1469419
Test loss (w/o reg) on all data: 0.06280113
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.71583e-06
Norm of the params: 13.110018
     Influence (LOO): fixed 482 labels. Loss 0.06280. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071083764
Train loss (w/o reg) on all data: 0.0031571395
Test loss (w/o reg) on all data: 0.0058017196
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9860404e-07
Norm of the params: 8.889586
                Loss: fixed 694 labels. Loss 0.00580. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35331076
Train loss (w/o reg) on all data: 0.34694552
Test loss (w/o reg) on all data: 0.17010939
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8661692e-05
Norm of the params: 11.282952
              Random: fixed 145 labels. Loss 0.17011. Accuracy 0.984.
### Flips: 820, rs: 7, checks: 1025
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12103907
Train loss (w/o reg) on all data: 0.11328291
Test loss (w/o reg) on all data: 0.048815444
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.6823666e-06
Norm of the params: 12.454846
     Influence (LOO): fixed 535 labels. Loss 0.04882. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060831597
Train loss (w/o reg) on all data: 0.0026623493
Test loss (w/o reg) on all data: 0.0043506045
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6936214e-07
Norm of the params: 8.271409
                Loss: fixed 697 labels. Loss 0.00435. Accuracy 0.999.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34142983
Train loss (w/o reg) on all data: 0.33500275
Test loss (w/o reg) on all data: 0.1600448
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.3524068e-05
Norm of the params: 11.337617
              Random: fixed 175 labels. Loss 0.16004. Accuracy 0.984.
### Flips: 820, rs: 7, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091631934
Train loss (w/o reg) on all data: 0.08411045
Test loss (w/o reg) on all data: 0.037731823
Train acc on all data:  0.9698516897641624
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4767635e-06
Norm of the params: 12.264972
     Influence (LOO): fixed 580 labels. Loss 0.03773. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042159455
Train loss (w/o reg) on all data: 0.0017915873
Test loss (w/o reg) on all data: 0.0041373884
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.719793e-08
Norm of the params: 6.9632735
                Loss: fixed 701 labels. Loss 0.00414. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3225378
Train loss (w/o reg) on all data: 0.31578913
Test loss (w/o reg) on all data: 0.14779674
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1705004e-05
Norm of the params: 11.617799
              Random: fixed 214 labels. Loss 0.14780. Accuracy 0.990.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40358672
Train loss (w/o reg) on all data: 0.39744598
Test loss (w/o reg) on all data: 0.22059558
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.502441e-06
Norm of the params: 11.082173
Flipped loss: 0.22060. Accuracy: 0.980
### Flips: 820, rs: 8, checks: 205
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3151097
Train loss (w/o reg) on all data: 0.30542403
Test loss (w/o reg) on all data: 0.16335246
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.799215e-06
Norm of the params: 13.918087
     Influence (LOO): fixed 176 labels. Loss 0.16335. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2802477
Train loss (w/o reg) on all data: 0.26717013
Test loss (w/o reg) on all data: 0.17344317
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.8258663e-05
Norm of the params: 16.172546
                Loss: fixed 204 labels. Loss 0.17344. Accuracy 0.956.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39221576
Train loss (w/o reg) on all data: 0.38630378
Test loss (w/o reg) on all data: 0.20799251
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.8171333e-05
Norm of the params: 10.873788
              Random: fixed  37 labels. Loss 0.20799. Accuracy 0.981.
### Flips: 820, rs: 8, checks: 410
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25146347
Train loss (w/o reg) on all data: 0.24136893
Test loss (w/o reg) on all data: 0.11760668
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.064654e-05
Norm of the params: 14.208818
     Influence (LOO): fixed 311 labels. Loss 0.11761. Accuracy 0.994.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16803831
Train loss (w/o reg) on all data: 0.15112503
Test loss (w/o reg) on all data: 0.11346898
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.9333767e-06
Norm of the params: 18.391994
                Loss: fixed 407 labels. Loss 0.11347. Accuracy 0.966.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3805587
Train loss (w/o reg) on all data: 0.37477592
Test loss (w/o reg) on all data: 0.19720781
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.4244065e-05
Norm of the params: 10.754319
              Random: fixed  73 labels. Loss 0.19721. Accuracy 0.981.
### Flips: 820, rs: 8, checks: 615
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19920146
Train loss (w/o reg) on all data: 0.19043271
Test loss (w/o reg) on all data: 0.08685453
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 9.889092e-06
Norm of the params: 13.242926
     Influence (LOO): fixed 411 labels. Loss 0.08685. Accuracy 0.995.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054959223
Train loss (w/o reg) on all data: 0.039468415
Test loss (w/o reg) on all data: 0.04258871
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3497057e-06
Norm of the params: 17.601595
                Loss: fixed 609 labels. Loss 0.04259. Accuracy 0.985.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36588857
Train loss (w/o reg) on all data: 0.35996175
Test loss (w/o reg) on all data: 0.18507689
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.2790555e-06
Norm of the params: 10.887435
              Random: fixed 110 labels. Loss 0.18508. Accuracy 0.983.
### Flips: 820, rs: 8, checks: 820
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14961837
Train loss (w/o reg) on all data: 0.14086097
Test loss (w/o reg) on all data: 0.062308203
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.436126e-06
Norm of the params: 13.234345
     Influence (LOO): fixed 497 labels. Loss 0.06231. Accuracy 0.998.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008994998
Train loss (w/o reg) on all data: 0.0043590795
Test loss (w/o reg) on all data: 0.0057333405
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.3604674e-07
Norm of the params: 9.62904
                Loss: fixed 698 labels. Loss 0.00573. Accuracy 0.998.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3572019
Train loss (w/o reg) on all data: 0.35127193
Test loss (w/o reg) on all data: 0.17825113
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0698149e-05
Norm of the params: 10.8903475
              Random: fixed 132 labels. Loss 0.17825. Accuracy 0.983.
### Flips: 820, rs: 8, checks: 1025
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113952115
Train loss (w/o reg) on all data: 0.10565576
Test loss (w/o reg) on all data: 0.047429476
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.894593e-06
Norm of the params: 12.881269
     Influence (LOO): fixed 551 labels. Loss 0.04743. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0047376426
Train loss (w/o reg) on all data: 0.0017720186
Test loss (w/o reg) on all data: 0.0035018178
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 5.39433e-08
Norm of the params: 7.70146
                Loss: fixed 708 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34337
Train loss (w/o reg) on all data: 0.33697027
Test loss (w/o reg) on all data: 0.16952336
Train acc on all data:  0.8694383661560905
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.579765e-06
Norm of the params: 11.313472
              Random: fixed 162 labels. Loss 0.16952. Accuracy 0.987.
### Flips: 820, rs: 8, checks: 1230
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089364015
Train loss (w/o reg) on all data: 0.0817556
Test loss (w/o reg) on all data: 0.035154745
Train acc on all data:  0.9700948212983224
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.3839636e-06
Norm of the params: 12.3356495
     Influence (LOO): fixed 589 labels. Loss 0.03515. Accuracy 0.999.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.003925835
Train loss (w/o reg) on all data: 0.0013753938
Test loss (w/o reg) on all data: 0.0029414597
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.1506823e-08
Norm of the params: 7.142046
                Loss: fixed 710 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317929
Train loss (w/o reg) on all data: 0.325514
Test loss (w/o reg) on all data: 0.15701437
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.20471e-06
Norm of the params: 11.206167
              Random: fixed 195 labels. Loss 0.15701. Accuracy 0.988.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40194875
Train loss (w/o reg) on all data: 0.3957744
Test loss (w/o reg) on all data: 0.22473592
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7319042e-05
Norm of the params: 11.112474
Flipped loss: 0.22474. Accuracy: 0.964
### Flips: 820, rs: 9, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.314321
Train loss (w/o reg) on all data: 0.3042322
Test loss (w/o reg) on all data: 0.16057771
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.9899776e-06
Norm of the params: 14.204789
     Influence (LOO): fixed 179 labels. Loss 0.16058. Accuracy 0.986.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27513847
Train loss (w/o reg) on all data: 0.2613194
Test loss (w/o reg) on all data: 0.18559788
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.3960925e-05
Norm of the params: 16.624722
                Loss: fixed 204 labels. Loss 0.18560. Accuracy 0.947.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3906826
Train loss (w/o reg) on all data: 0.38425606
Test loss (w/o reg) on all data: 0.21552362
Train acc on all data:  0.8417213712618526
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.156169e-06
Norm of the params: 11.337137
              Random: fixed  31 labels. Loss 0.21552. Accuracy 0.966.
### Flips: 820, rs: 9, checks: 410
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2500288
Train loss (w/o reg) on all data: 0.2397409
Test loss (w/o reg) on all data: 0.12101247
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2935194e-05
Norm of the params: 14.344269
     Influence (LOO): fixed 307 labels. Loss 0.12101. Accuracy 0.990.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15969454
Train loss (w/o reg) on all data: 0.14168361
Test loss (w/o reg) on all data: 0.1412848
Train acc on all data:  0.9380014587892049
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.5348762e-05
Norm of the params: 18.979427
                Loss: fixed 408 labels. Loss 0.14128. Accuracy 0.949.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38114372
Train loss (w/o reg) on all data: 0.37469083
Test loss (w/o reg) on all data: 0.2059156
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.9704506e-05
Norm of the params: 11.360352
              Random: fixed  60 labels. Loss 0.20592. Accuracy 0.972.
### Flips: 820, rs: 9, checks: 615
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18628712
Train loss (w/o reg) on all data: 0.17749181
Test loss (w/o reg) on all data: 0.08373767
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.4176993e-06
Norm of the params: 13.262961
     Influence (LOO): fixed 426 labels. Loss 0.08374. Accuracy 0.995.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056962196
Train loss (w/o reg) on all data: 0.04223141
Test loss (w/o reg) on all data: 0.052030265
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.7807444e-06
Norm of the params: 17.164373
                Loss: fixed 608 labels. Loss 0.05203. Accuracy 0.981.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3683066
Train loss (w/o reg) on all data: 0.36140275
Test loss (w/o reg) on all data: 0.19625403
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.3946463e-05
Norm of the params: 11.750614
              Random: fixed  92 labels. Loss 0.19625. Accuracy 0.971.
### Flips: 820, rs: 9, checks: 820
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14259668
Train loss (w/o reg) on all data: 0.13433944
Test loss (w/o reg) on all data: 0.059153218
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.263793e-06
Norm of the params: 12.850869
     Influence (LOO): fixed 497 labels. Loss 0.05915. Accuracy 0.999.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013429468
Train loss (w/o reg) on all data: 0.0069728987
Test loss (w/o reg) on all data: 0.0073900335
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.931056e-07
Norm of the params: 11.363599
                Loss: fixed 688 labels. Loss 0.00739. Accuracy 0.997.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35526595
Train loss (w/o reg) on all data: 0.34845605
Test loss (w/o reg) on all data: 0.18455312
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.3833367e-05
Norm of the params: 11.670369
              Random: fixed 128 labels. Loss 0.18455. Accuracy 0.975.
### Flips: 820, rs: 9, checks: 1025
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10955067
Train loss (w/o reg) on all data: 0.10115839
Test loss (w/o reg) on all data: 0.045001373
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.23412e-06
Norm of the params: 12.955526
     Influence (LOO): fixed 547 labels. Loss 0.04500. Accuracy 0.997.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006392462
Train loss (w/o reg) on all data: 0.0026117235
Test loss (w/o reg) on all data: 0.0041308207
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.07228985e-07
Norm of the params: 8.695675
                Loss: fixed 701 labels. Loss 0.00413. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34455538
Train loss (w/o reg) on all data: 0.3375512
Test loss (w/o reg) on all data: 0.17465724
Train acc on all data:  0.8691952346219305
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.123155e-05
Norm of the params: 11.835677
              Random: fixed 156 labels. Loss 0.17466. Accuracy 0.979.
### Flips: 820, rs: 9, checks: 1230
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08458304
Train loss (w/o reg) on all data: 0.076620296
Test loss (w/o reg) on all data: 0.033433396
Train acc on all data:  0.9708242159008024
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.468992e-06
Norm of the params: 12.619621
     Influence (LOO): fixed 587 labels. Loss 0.03343. Accuracy 0.999.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051352354
Train loss (w/o reg) on all data: 0.0020358835
Test loss (w/o reg) on all data: 0.0039673233
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.3189456e-08
Norm of the params: 7.8731856
                Loss: fixed 703 labels. Loss 0.00397. Accuracy 0.999.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32675463
Train loss (w/o reg) on all data: 0.31922117
Test loss (w/o reg) on all data: 0.16208364
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.6149228e-05
Norm of the params: 12.274739
              Random: fixed 196 labels. Loss 0.16208. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40327826
Train loss (w/o reg) on all data: 0.39841482
Test loss (w/o reg) on all data: 0.2099033
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.95379e-05
Norm of the params: 9.862493
Flipped loss: 0.20990. Accuracy: 0.984
### Flips: 820, rs: 10, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3089944
Train loss (w/o reg) on all data: 0.29937997
Test loss (w/o reg) on all data: 0.15079077
Train acc on all data:  0.8745441283734501
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1060129e-05
Norm of the params: 13.866817
     Influence (LOO): fixed 181 labels. Loss 0.15079. Accuracy 0.991.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27979904
Train loss (w/o reg) on all data: 0.2691553
Test loss (w/o reg) on all data: 0.15244983
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 9.270462e-06
Norm of the params: 14.590226
                Loss: fixed 205 labels. Loss 0.15245. Accuracy 0.969.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38854736
Train loss (w/o reg) on all data: 0.38344845
Test loss (w/o reg) on all data: 0.19880939
Train acc on all data:  0.8424507658643327
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.8284023e-05
Norm of the params: 10.098433
              Random: fixed  40 labels. Loss 0.19881. Accuracy 0.986.
### Flips: 820, rs: 10, checks: 410
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24366407
Train loss (w/o reg) on all data: 0.23463443
Test loss (w/o reg) on all data: 0.11205339
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.2876375e-05
Norm of the params: 13.438483
     Influence (LOO): fixed 310 labels. Loss 0.11205. Accuracy 0.996.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1609138
Train loss (w/o reg) on all data: 0.14579809
Test loss (w/o reg) on all data: 0.09460941
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2271367e-05
Norm of the params: 17.387186
                Loss: fixed 409 labels. Loss 0.09461. Accuracy 0.978.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3784065
Train loss (w/o reg) on all data: 0.37354997
Test loss (w/o reg) on all data: 0.18998711
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.697397e-05
Norm of the params: 9.855482
              Random: fixed  72 labels. Loss 0.18999. Accuracy 0.987.
### Flips: 820, rs: 10, checks: 615
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19193417
Train loss (w/o reg) on all data: 0.18311502
Test loss (w/o reg) on all data: 0.084417075
Train acc on all data:  0.9299781181619255
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.5622872e-06
Norm of the params: 13.280929
     Influence (LOO): fixed 408 labels. Loss 0.08442. Accuracy 0.997.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055613913
Train loss (w/o reg) on all data: 0.041677494
Test loss (w/o reg) on all data: 0.027188722
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.187571e-06
Norm of the params: 16.695162
                Loss: fixed 608 labels. Loss 0.02719. Accuracy 0.994.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36233425
Train loss (w/o reg) on all data: 0.35719693
Test loss (w/o reg) on all data: 0.17577565
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.1371313e-05
Norm of the params: 10.136397
              Random: fixed 115 labels. Loss 0.17578. Accuracy 0.988.
### Flips: 820, rs: 10, checks: 820
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14648157
Train loss (w/o reg) on all data: 0.13729225
Test loss (w/o reg) on all data: 0.06385453
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.2886155e-05
Norm of the params: 13.556788
     Influence (LOO): fixed 487 labels. Loss 0.06385. Accuracy 0.998.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009901248
Train loss (w/o reg) on all data: 0.004786962
Test loss (w/o reg) on all data: 0.0058881524
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 2.419992e-07
Norm of the params: 10.11364
                Loss: fixed 689 labels. Loss 0.00589. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34763193
Train loss (w/o reg) on all data: 0.34257764
Test loss (w/o reg) on all data: 0.16416171
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2566427e-05
Norm of the params: 10.054162
              Random: fixed 153 labels. Loss 0.16416. Accuracy 0.991.
### Flips: 820, rs: 10, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11118457
Train loss (w/o reg) on all data: 0.10311911
Test loss (w/o reg) on all data: 0.04702886
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1980998e-06
Norm of the params: 12.700751
     Influence (LOO): fixed 548 labels. Loss 0.04703. Accuracy 0.998.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042363405
Train loss (w/o reg) on all data: 0.0015978612
Test loss (w/o reg) on all data: 0.0032735553
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0518281e-07
Norm of the params: 7.2642684
                Loss: fixed 697 labels. Loss 0.00327. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33045426
Train loss (w/o reg) on all data: 0.32530123
Test loss (w/o reg) on all data: 0.15203181
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6869447e-06
Norm of the params: 10.1518755
              Random: fixed 195 labels. Loss 0.15203. Accuracy 0.992.
### Flips: 820, rs: 10, checks: 1230
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08595286
Train loss (w/o reg) on all data: 0.07901958
Test loss (w/o reg) on all data: 0.034780394
Train acc on all data:  0.9732555312424022
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3726885e-06
Norm of the params: 11.775642
     Influence (LOO): fixed 589 labels. Loss 0.03478. Accuracy 1.000.
Using normal model
LBFGS training took [20] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0028162398
Train loss (w/o reg) on all data: 0.0009601155
Test loss (w/o reg) on all data: 0.002656061
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.719952e-08
Norm of the params: 6.092822
                Loss: fixed 699 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31663153
Train loss (w/o reg) on all data: 0.31170037
Test loss (w/o reg) on all data: 0.14236355
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.244501e-06
Norm of the params: 9.930924
              Random: fixed 229 labels. Loss 0.14236. Accuracy 0.991.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39581588
Train loss (w/o reg) on all data: 0.3902057
Test loss (w/o reg) on all data: 0.20401286
Train acc on all data:  0.8334548991004134
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.9101425e-05
Norm of the params: 10.592606
Flipped loss: 0.20401. Accuracy: 0.979
### Flips: 820, rs: 11, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30653277
Train loss (w/o reg) on all data: 0.2970353
Test loss (w/o reg) on all data: 0.15193345
Train acc on all data:  0.87527352297593
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1980075e-05
Norm of the params: 13.782218
     Influence (LOO): fixed 176 labels. Loss 0.15193. Accuracy 0.985.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27308515
Train loss (w/o reg) on all data: 0.26069963
Test loss (w/o reg) on all data: 0.15752651
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8418653e-05
Norm of the params: 15.73882
                Loss: fixed 205 labels. Loss 0.15753. Accuracy 0.958.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3794957
Train loss (w/o reg) on all data: 0.37394032
Test loss (w/o reg) on all data: 0.19141747
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.944945e-06
Norm of the params: 10.540762
              Random: fixed  47 labels. Loss 0.19142. Accuracy 0.978.
### Flips: 820, rs: 11, checks: 410
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24303164
Train loss (w/o reg) on all data: 0.23370521
Test loss (w/o reg) on all data: 0.11453313
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.693346e-06
Norm of the params: 13.657545
     Influence (LOO): fixed 304 labels. Loss 0.11453. Accuracy 0.989.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15311474
Train loss (w/o reg) on all data: 0.13639505
Test loss (w/o reg) on all data: 0.10024234
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.850584e-06
Norm of the params: 18.28643
                Loss: fixed 410 labels. Loss 0.10024. Accuracy 0.970.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36391255
Train loss (w/o reg) on all data: 0.35854936
Test loss (w/o reg) on all data: 0.1733361
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.80106e-05
Norm of the params: 10.356824
              Random: fixed  97 labels. Loss 0.17334. Accuracy 0.981.
### Flips: 820, rs: 11, checks: 615
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139817
Train loss (w/o reg) on all data: 0.181575
Test loss (w/o reg) on all data: 0.08469519
Train acc on all data:  0.9290055920252857
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7407124e-06
Norm of the params: 14.0165415
     Influence (LOO): fixed 404 labels. Loss 0.08470. Accuracy 0.994.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049843647
Train loss (w/o reg) on all data: 0.03587167
Test loss (w/o reg) on all data: 0.03201806
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.0814915e-06
Norm of the params: 16.716444
                Loss: fixed 608 labels. Loss 0.03202. Accuracy 0.990.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35308763
Train loss (w/o reg) on all data: 0.34772706
Test loss (w/o reg) on all data: 0.16462642
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.9180415e-06
Norm of the params: 10.354289
              Random: fixed 125 labels. Loss 0.16463. Accuracy 0.987.
### Flips: 820, rs: 11, checks: 820
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14118321
Train loss (w/o reg) on all data: 0.13187845
Test loss (w/o reg) on all data: 0.05978727
Train acc on all data:  0.949914903963044
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.5902744e-06
Norm of the params: 13.641668
     Influence (LOO): fixed 491 labels. Loss 0.05979. Accuracy 0.997.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010316942
Train loss (w/o reg) on all data: 0.0051793056
Test loss (w/o reg) on all data: 0.00964232
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3317022e-07
Norm of the params: 10.136702
                Loss: fixed 690 labels. Loss 0.00964. Accuracy 0.997.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34032932
Train loss (w/o reg) on all data: 0.3346264
Test loss (w/o reg) on all data: 0.15734345
Train acc on all data:  0.8694383661560905
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.311695e-05
Norm of the params: 10.679802
              Random: fixed 155 labels. Loss 0.15734. Accuracy 0.988.
### Flips: 820, rs: 11, checks: 1025
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10711108
Train loss (w/o reg) on all data: 0.09839075
Test loss (w/o reg) on all data: 0.045403782
Train acc on all data:  0.963287138341843
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.909336e-06
Norm of the params: 13.20631
     Influence (LOO): fixed 550 labels. Loss 0.04540. Accuracy 0.996.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006481714
Train loss (w/o reg) on all data: 0.0029573233
Test loss (w/o reg) on all data: 0.0053311135
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.205901e-08
Norm of the params: 8.395702
                Loss: fixed 697 labels. Loss 0.00533. Accuracy 1.000.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32582793
Train loss (w/o reg) on all data: 0.32003412
Test loss (w/o reg) on all data: 0.14772493
Train acc on all data:  0.8784342329200098
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.2573234e-05
Norm of the params: 10.764594
              Random: fixed 193 labels. Loss 0.14772. Accuracy 0.987.
### Flips: 820, rs: 11, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08646194
Train loss (w/o reg) on all data: 0.07875227
Test loss (w/o reg) on all data: 0.03551251
Train acc on all data:  0.9710673474349624
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.7503003e-06
Norm of the params: 12.417463
     Influence (LOO): fixed 584 labels. Loss 0.03551. Accuracy 0.998.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005520237
Train loss (w/o reg) on all data: 0.0025212727
Test loss (w/o reg) on all data: 0.005076599
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.363872e-07
Norm of the params: 7.7446294
                Loss: fixed 699 labels. Loss 0.00508. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3151799
Train loss (w/o reg) on all data: 0.30921158
Test loss (w/o reg) on all data: 0.14026706
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.957205e-05
Norm of the params: 10.925499
              Random: fixed 218 labels. Loss 0.14027. Accuracy 0.987.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39839792
Train loss (w/o reg) on all data: 0.39282867
Test loss (w/o reg) on all data: 0.21276613
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.031935e-05
Norm of the params: 10.553899
Flipped loss: 0.21277. Accuracy: 0.972
### Flips: 820, rs: 12, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30769378
Train loss (w/o reg) on all data: 0.2985249
Test loss (w/o reg) on all data: 0.1518517
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4142736e-05
Norm of the params: 13.54171
     Influence (LOO): fixed 182 labels. Loss 0.15185. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26911724
Train loss (w/o reg) on all data: 0.2562726
Test loss (w/o reg) on all data: 0.1664324
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 9.502076e-06
Norm of the params: 16.027866
                Loss: fixed 205 labels. Loss 0.16643. Accuracy 0.957.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37981725
Train loss (w/o reg) on all data: 0.3738523
Test loss (w/o reg) on all data: 0.19552742
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.9303245e-05
Norm of the params: 10.92238
              Random: fixed  48 labels. Loss 0.19553. Accuracy 0.977.
### Flips: 820, rs: 12, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23988882
Train loss (w/o reg) on all data: 0.23056513
Test loss (w/o reg) on all data: 0.10659062
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.566684e-06
Norm of the params: 13.655542
     Influence (LOO): fixed 323 labels. Loss 0.10659. Accuracy 0.996.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15116565
Train loss (w/o reg) on all data: 0.13399076
Test loss (w/o reg) on all data: 0.114914276
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.4096367e-05
Norm of the params: 18.533691
                Loss: fixed 410 labels. Loss 0.11491. Accuracy 0.959.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36563453
Train loss (w/o reg) on all data: 0.35950413
Test loss (w/o reg) on all data: 0.18312895
Train acc on all data:  0.8533916849015317
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.7256094e-05
Norm of the params: 11.07285
              Random: fixed  86 labels. Loss 0.18313. Accuracy 0.979.
### Flips: 820, rs: 12, checks: 615
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19033824
Train loss (w/o reg) on all data: 0.18116589
Test loss (w/o reg) on all data: 0.08342897
Train acc on all data:  0.9304643812302456
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.7542676e-06
Norm of the params: 13.544257
     Influence (LOO): fixed 410 labels. Loss 0.08343. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056055725
Train loss (w/o reg) on all data: 0.041776083
Test loss (w/o reg) on all data: 0.032218296
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.6449595e-06
Norm of the params: 16.899492
                Loss: fixed 608 labels. Loss 0.03222. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35351202
Train loss (w/o reg) on all data: 0.34738642
Test loss (w/o reg) on all data: 0.17190753
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.18878925e-05
Norm of the params: 11.068514
              Random: fixed 122 labels. Loss 0.17191. Accuracy 0.984.
### Flips: 820, rs: 12, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1461666
Train loss (w/o reg) on all data: 0.13743094
Test loss (w/o reg) on all data: 0.0591241
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.2411946e-06
Norm of the params: 13.217914
     Influence (LOO): fixed 485 labels. Loss 0.05912. Accuracy 0.999.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009301083
Train loss (w/o reg) on all data: 0.0042178323
Test loss (w/o reg) on all data: 0.0037023292
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 8.693346e-08
Norm of the params: 10.082907
                Loss: fixed 689 labels. Loss 0.00370. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34348026
Train loss (w/o reg) on all data: 0.3376855
Test loss (w/o reg) on all data: 0.16142552
Train acc on all data:  0.8687089715536105
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.914502e-05
Norm of the params: 10.76547
              Random: fixed 153 labels. Loss 0.16143. Accuracy 0.988.
### Flips: 820, rs: 12, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11290177
Train loss (w/o reg) on all data: 0.10488237
Test loss (w/o reg) on all data: 0.04371785
Train acc on all data:  0.962557743739363
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.7981e-06
Norm of the params: 12.66444
     Influence (LOO): fixed 542 labels. Loss 0.04372. Accuracy 0.999.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00575954
Train loss (w/o reg) on all data: 0.0022757319
Test loss (w/o reg) on all data: 0.002969289
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.305587e-07
Norm of the params: 8.347224
                Loss: fixed 694 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32903123
Train loss (w/o reg) on all data: 0.3231969
Test loss (w/o reg) on all data: 0.14985187
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3269606e-05
Norm of the params: 10.802165
              Random: fixed 190 labels. Loss 0.14985. Accuracy 0.991.
### Flips: 820, rs: 12, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08695716
Train loss (w/o reg) on all data: 0.079971366
Test loss (w/o reg) on all data: 0.030613327
Train acc on all data:  0.9720398735716023
Test acc on all data:   1.0
Norm of the mean of gradients: 1.454951e-06
Norm of the params: 11.820146
     Influence (LOO): fixed 586 labels. Loss 0.03061. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00575954
Train loss (w/o reg) on all data: 0.0022757365
Test loss (w/o reg) on all data: 0.0029693365
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.7595736e-08
Norm of the params: 8.347219
                Loss: fixed 694 labels. Loss 0.00297. Accuracy 1.000.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31672582
Train loss (w/o reg) on all data: 0.3109467
Test loss (w/o reg) on all data: 0.13955481
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.59895e-05
Norm of the params: 10.750929
              Random: fixed 220 labels. Loss 0.13955. Accuracy 0.991.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39620453
Train loss (w/o reg) on all data: 0.38987115
Test loss (w/o reg) on all data: 0.21348038
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2966498e-05
Norm of the params: 11.254675
Flipped loss: 0.21348. Accuracy: 0.983
### Flips: 820, rs: 13, checks: 205
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3037733
Train loss (w/o reg) on all data: 0.2947479
Test loss (w/o reg) on all data: 0.14948228
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.657806e-06
Norm of the params: 13.43535
     Influence (LOO): fixed 182 labels. Loss 0.14948. Accuracy 0.994.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271639
Train loss (w/o reg) on all data: 0.25702044
Test loss (w/o reg) on all data: 0.15495987
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.75322e-05
Norm of the params: 17.098864
                Loss: fixed 205 labels. Loss 0.15496. Accuracy 0.974.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3892532
Train loss (w/o reg) on all data: 0.38297486
Test loss (w/o reg) on all data: 0.20466247
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.970028e-06
Norm of the params: 11.20566
              Random: fixed  26 labels. Loss 0.20466. Accuracy 0.986.
### Flips: 820, rs: 13, checks: 410
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24688366
Train loss (w/o reg) on all data: 0.23669982
Test loss (w/o reg) on all data: 0.114412196
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6295516e-05
Norm of the params: 14.271537
     Influence (LOO): fixed 297 labels. Loss 0.11441. Accuracy 0.993.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15602274
Train loss (w/o reg) on all data: 0.13701868
Test loss (w/o reg) on all data: 0.09292133
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 6.3205994e-06
Norm of the params: 19.495668
                Loss: fixed 410 labels. Loss 0.09292. Accuracy 0.975.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38006178
Train loss (w/o reg) on all data: 0.37348765
Test loss (w/o reg) on all data: 0.19543436
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.7888977e-05
Norm of the params: 11.46659
              Random: fixed  51 labels. Loss 0.19543. Accuracy 0.987.
### Flips: 820, rs: 13, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1909256
Train loss (w/o reg) on all data: 0.1803523
Test loss (w/o reg) on all data: 0.0830293
Train acc on all data:  0.9285193289569658
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7221268e-05
Norm of the params: 14.541865
     Influence (LOO): fixed 404 labels. Loss 0.08303. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053244084
Train loss (w/o reg) on all data: 0.03887173
Test loss (w/o reg) on all data: 0.0251789
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.8413134e-06
Norm of the params: 16.954264
                Loss: fixed 610 labels. Loss 0.02518. Accuracy 0.995.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36461115
Train loss (w/o reg) on all data: 0.35791814
Test loss (w/o reg) on all data: 0.18064402
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 9.431373e-06
Norm of the params: 11.569788
              Random: fixed  93 labels. Loss 0.18064. Accuracy 0.991.
### Flips: 820, rs: 13, checks: 820
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14607169
Train loss (w/o reg) on all data: 0.13627537
Test loss (w/o reg) on all data: 0.06081617
Train acc on all data:  0.9467541940189642
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7485372e-05
Norm of the params: 13.9973755
     Influence (LOO): fixed 482 labels. Loss 0.06082. Accuracy 1.000.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009924587
Train loss (w/o reg) on all data: 0.004449251
Test loss (w/o reg) on all data: 0.0049971086
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.04609086e-07
Norm of the params: 10.464545
                Loss: fixed 685 labels. Loss 0.00500. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3508461
Train loss (w/o reg) on all data: 0.34365833
Test loss (w/o reg) on all data: 0.17077959
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.4851814e-05
Norm of the params: 11.989828
              Random: fixed 126 labels. Loss 0.17078. Accuracy 0.994.
### Flips: 820, rs: 13, checks: 1025
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111130595
Train loss (w/o reg) on all data: 0.10203248
Test loss (w/o reg) on all data: 0.044517733
Train acc on all data:  0.9618283491368831
Test acc on all data:   1.0
Norm of the mean of gradients: 5.192778e-06
Norm of the params: 13.489337
     Influence (LOO): fixed 540 labels. Loss 0.04452. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008299515
Train loss (w/o reg) on all data: 0.003527726
Test loss (w/o reg) on all data: 0.004300069
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2507403e-07
Norm of the params: 9.769124
                Loss: fixed 688 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33994585
Train loss (w/o reg) on all data: 0.3328933
Test loss (w/o reg) on all data: 0.15830824
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.085516e-05
Norm of the params: 11.876487
              Random: fixed 157 labels. Loss 0.15831. Accuracy 0.995.
### Flips: 820, rs: 13, checks: 1230
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08634068
Train loss (w/o reg) on all data: 0.078489766
Test loss (w/o reg) on all data: 0.03391133
Train acc on all data:  0.9710673474349624
Test acc on all data:   1.0
Norm of the mean of gradients: 9.096534e-06
Norm of the params: 12.530692
     Influence (LOO): fixed 579 labels. Loss 0.03391. Accuracy 1.000.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007068956
Train loss (w/o reg) on all data: 0.0029162858
Test loss (w/o reg) on all data: 0.0041153124
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7562425e-08
Norm of the params: 9.113363
                Loss: fixed 690 labels. Loss 0.00412. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32517257
Train loss (w/o reg) on all data: 0.31815964
Test loss (w/o reg) on all data: 0.14532539
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.5376914e-05
Norm of the params: 11.843076
              Random: fixed 196 labels. Loss 0.14533. Accuracy 0.996.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38489366
Train loss (w/o reg) on all data: 0.37824976
Test loss (w/o reg) on all data: 0.2014847
Train acc on all data:  0.8431801604668125
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.0966144e-05
Norm of the params: 11.527256
Flipped loss: 0.20148. Accuracy: 0.978
### Flips: 820, rs: 14, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29520467
Train loss (w/o reg) on all data: 0.28513277
Test loss (w/o reg) on all data: 0.14583282
Train acc on all data:  0.8813518113299295
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3370298e-05
Norm of the params: 14.192883
     Influence (LOO): fixed 175 labels. Loss 0.14583. Accuracy 0.983.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25231227
Train loss (w/o reg) on all data: 0.23786736
Test loss (w/o reg) on all data: 0.15373261
Train acc on all data:  0.8956965718453683
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.4881007e-05
Norm of the params: 16.997004
                Loss: fixed 205 labels. Loss 0.15373. Accuracy 0.964.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37407202
Train loss (w/o reg) on all data: 0.36714786
Test loss (w/o reg) on all data: 0.18832248
Train acc on all data:  0.849744711889132
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0995754e-05
Norm of the params: 11.767879
              Random: fixed  33 labels. Loss 0.18832. Accuracy 0.980.
### Flips: 820, rs: 14, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22696038
Train loss (w/o reg) on all data: 0.21564603
Test loss (w/o reg) on all data: 0.104522325
Train acc on all data:  0.912958910770727
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5493808e-05
Norm of the params: 15.042836
     Influence (LOO): fixed 314 labels. Loss 0.10452. Accuracy 0.990.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13952388
Train loss (w/o reg) on all data: 0.12065108
Test loss (w/o reg) on all data: 0.10488976
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.3545048e-06
Norm of the params: 19.428228
                Loss: fixed 409 labels. Loss 0.10489. Accuracy 0.970.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3635575
Train loss (w/o reg) on all data: 0.35674837
Test loss (w/o reg) on all data: 0.17928648
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1283334e-05
Norm of the params: 11.669721
              Random: fixed  65 labels. Loss 0.17929. Accuracy 0.983.
### Flips: 820, rs: 14, checks: 615
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17556466
Train loss (w/o reg) on all data: 0.16562726
Test loss (w/o reg) on all data: 0.07447321
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.7935453e-06
Norm of the params: 14.097808
     Influence (LOO): fixed 418 labels. Loss 0.07447. Accuracy 0.998.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04896045
Train loss (w/o reg) on all data: 0.03416935
Test loss (w/o reg) on all data: 0.03673592
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3732913e-06
Norm of the params: 17.199476
                Loss: fixed 594 labels. Loss 0.03674. Accuracy 0.990.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35233724
Train loss (w/o reg) on all data: 0.34548336
Test loss (w/o reg) on all data: 0.16688848
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4100962e-05
Norm of the params: 11.708023
              Random: fixed  97 labels. Loss 0.16689. Accuracy 0.987.
### Flips: 820, rs: 14, checks: 820
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13920476
Train loss (w/o reg) on all data: 0.13000998
Test loss (w/o reg) on all data: 0.054438006
Train acc on all data:  0.9516168247021639
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.023718e-06
Norm of the params: 13.560807
     Influence (LOO): fixed 483 labels. Loss 0.05444. Accuracy 0.999.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014919021
Train loss (w/o reg) on all data: 0.008046971
Test loss (w/o reg) on all data: 0.0072621224
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.695585e-07
Norm of the params: 11.723523
                Loss: fixed 664 labels. Loss 0.00726. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3397808
Train loss (w/o reg) on all data: 0.33288142
Test loss (w/o reg) on all data: 0.15924712
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.51311e-05
Norm of the params: 11.746831
              Random: fixed 125 labels. Loss 0.15925. Accuracy 0.985.
### Flips: 820, rs: 14, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109740615
Train loss (w/o reg) on all data: 0.1013946
Test loss (w/o reg) on all data: 0.041269153
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6333329e-06
Norm of the params: 12.919761
     Influence (LOO): fixed 530 labels. Loss 0.04127. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210889
Train loss (w/o reg) on all data: 0.0041923467
Test loss (w/o reg) on all data: 0.004328898
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5472236e-07
Norm of the params: 10.018526
                Loss: fixed 675 labels. Loss 0.00433. Accuracy 1.000.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32938743
Train loss (w/o reg) on all data: 0.32264525
Test loss (w/o reg) on all data: 0.15003702
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.8529688e-05
Norm of the params: 11.612224
              Random: fixed 158 labels. Loss 0.15004. Accuracy 0.984.
### Flips: 820, rs: 14, checks: 1230
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08419254
Train loss (w/o reg) on all data: 0.076971345
Test loss (w/o reg) on all data: 0.029721316
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 3.798708e-06
Norm of the params: 12.017645
     Influence (LOO): fixed 573 labels. Loss 0.02972. Accuracy 1.000.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008543421
Train loss (w/o reg) on all data: 0.0038215616
Test loss (w/o reg) on all data: 0.0042636604
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 5.102838e-07
Norm of the params: 9.717879
                Loss: fixed 676 labels. Loss 0.00426. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.315972
Train loss (w/o reg) on all data: 0.3089824
Test loss (w/o reg) on all data: 0.1413605
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.052047e-05
Norm of the params: 11.82336
              Random: fixed 189 labels. Loss 0.14136. Accuracy 0.985.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4086317
Train loss (w/o reg) on all data: 0.40299487
Test loss (w/o reg) on all data: 0.21800274
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.8629129e-05
Norm of the params: 10.617758
Flipped loss: 0.21800. Accuracy: 0.972
### Flips: 820, rs: 15, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3166966
Train loss (w/o reg) on all data: 0.3076983
Test loss (w/o reg) on all data: 0.15859921
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.6642742e-05
Norm of the params: 13.415148
     Influence (LOO): fixed 181 labels. Loss 0.15860. Accuracy 0.983.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28489572
Train loss (w/o reg) on all data: 0.2730727
Test loss (w/o reg) on all data: 0.17492707
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 8.411095e-06
Norm of the params: 15.37727
                Loss: fixed 205 labels. Loss 0.17493. Accuracy 0.952.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39570478
Train loss (w/o reg) on all data: 0.3895575
Test loss (w/o reg) on all data: 0.20935696
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.1193011e-05
Norm of the params: 11.088082
              Random: fixed  33 labels. Loss 0.20936. Accuracy 0.972.
### Flips: 820, rs: 15, checks: 410
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2538557
Train loss (w/o reg) on all data: 0.2437093
Test loss (w/o reg) on all data: 0.12282362
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.566862e-06
Norm of the params: 14.245296
     Influence (LOO): fixed 306 labels. Loss 0.12282. Accuracy 0.988.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17000234
Train loss (w/o reg) on all data: 0.15217371
Test loss (w/o reg) on all data: 0.121921785
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 5.889243e-06
Norm of the params: 18.883131
                Loss: fixed 409 labels. Loss 0.12192. Accuracy 0.959.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3831382
Train loss (w/o reg) on all data: 0.37688637
Test loss (w/o reg) on all data: 0.19618234
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.1068266e-05
Norm of the params: 11.181998
              Random: fixed  70 labels. Loss 0.19618. Accuracy 0.977.
### Flips: 820, rs: 15, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1997238
Train loss (w/o reg) on all data: 0.18952309
Test loss (w/o reg) on all data: 0.09276759
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.301906e-06
Norm of the params: 14.283349
     Influence (LOO): fixed 407 labels. Loss 0.09277. Accuracy 0.990.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06413019
Train loss (w/o reg) on all data: 0.047536876
Test loss (w/o reg) on all data: 0.051169902
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2088148e-06
Norm of the params: 18.217197
                Loss: fixed 612 labels. Loss 0.05117. Accuracy 0.986.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36794966
Train loss (w/o reg) on all data: 0.36168164
Test loss (w/o reg) on all data: 0.184653
Train acc on all data:  0.8563092633114515
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.739472e-06
Norm of the params: 11.1964445
              Random: fixed 111 labels. Loss 0.18465. Accuracy 0.979.
### Flips: 820, rs: 15, checks: 820
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15929842
Train loss (w/o reg) on all data: 0.1496104
Test loss (w/o reg) on all data: 0.07246247
Train acc on all data:  0.9411621687332847
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.113113e-06
Norm of the params: 13.919779
     Influence (LOO): fixed 481 labels. Loss 0.07246. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014033744
Train loss (w/o reg) on all data: 0.0072819623
Test loss (w/o reg) on all data: 0.0077537764
Train acc on all data:  0.9980549477267202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.97506e-07
Norm of the params: 11.620482
                Loss: fixed 713 labels. Loss 0.00775. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35167426
Train loss (w/o reg) on all data: 0.34507084
Test loss (w/o reg) on all data: 0.1748453
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 5.7257097e-05
Norm of the params: 11.4921
              Random: fixed 149 labels. Loss 0.17485. Accuracy 0.974.
### Flips: 820, rs: 15, checks: 1025
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124143384
Train loss (w/o reg) on all data: 0.11499177
Test loss (w/o reg) on all data: 0.056740623
Train acc on all data:  0.9547775346462436
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1280842e-06
Norm of the params: 13.528944
     Influence (LOO): fixed 540 labels. Loss 0.05674. Accuracy 0.995.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0058773886
Train loss (w/o reg) on all data: 0.0024470356
Test loss (w/o reg) on all data: 0.0040706126
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.3432786e-08
Norm of the params: 8.282938
                Loss: fixed 724 labels. Loss 0.00407. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3376129
Train loss (w/o reg) on all data: 0.3308974
Test loss (w/o reg) on all data: 0.1640193
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2476027e-05
Norm of the params: 11.589236
              Random: fixed 188 labels. Loss 0.16402. Accuracy 0.978.
### Flips: 820, rs: 15, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10106511
Train loss (w/o reg) on all data: 0.09287681
Test loss (w/o reg) on all data: 0.04253073
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.9785732e-06
Norm of the params: 12.797111
     Influence (LOO): fixed 586 labels. Loss 0.04253. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0049343193
Train loss (w/o reg) on all data: 0.0020112402
Test loss (w/o reg) on all data: 0.003815509
Train acc on all data:  0.99975686846584
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.471277e-08
Norm of the params: 7.646018
                Loss: fixed 727 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3232857
Train loss (w/o reg) on all data: 0.31618384
Test loss (w/o reg) on all data: 0.15475431
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.2966338e-05
Norm of the params: 11.917931
              Random: fixed 223 labels. Loss 0.15475. Accuracy 0.981.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40370485
Train loss (w/o reg) on all data: 0.39753932
Test loss (w/o reg) on all data: 0.21818413
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0744721e-05
Norm of the params: 11.104532
Flipped loss: 0.21818. Accuracy: 0.977
### Flips: 820, rs: 16, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31096232
Train loss (w/o reg) on all data: 0.3015457
Test loss (w/o reg) on all data: 0.1549431
Train acc on all data:  0.8721128130318502
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2538488e-05
Norm of the params: 13.723428
     Influence (LOO): fixed 179 labels. Loss 0.15494. Accuracy 0.984.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2810119
Train loss (w/o reg) on all data: 0.26837188
Test loss (w/o reg) on all data: 0.16929302
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 6.0640414e-06
Norm of the params: 15.899697
                Loss: fixed 205 labels. Loss 0.16929. Accuracy 0.960.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39100355
Train loss (w/o reg) on all data: 0.38481745
Test loss (w/o reg) on all data: 0.20359096
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.8841137e-05
Norm of the params: 11.123033
              Random: fixed  41 labels. Loss 0.20359. Accuracy 0.980.
### Flips: 820, rs: 16, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25174055
Train loss (w/o reg) on all data: 0.24177961
Test loss (w/o reg) on all data: 0.11793797
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8827453e-05
Norm of the params: 14.114476
     Influence (LOO): fixed 306 labels. Loss 0.11794. Accuracy 0.990.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16939515
Train loss (w/o reg) on all data: 0.15130617
Test loss (w/o reg) on all data: 0.122081414
Train acc on all data:  0.9345976173109652
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.07692795e-05
Norm of the params: 19.020502
                Loss: fixed 410 labels. Loss 0.12208. Accuracy 0.962.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37769446
Train loss (w/o reg) on all data: 0.37126386
Test loss (w/o reg) on all data: 0.19457227
Train acc on all data:  0.8473133965475322
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.4042343e-05
Norm of the params: 11.340711
              Random: fixed  76 labels. Loss 0.19457. Accuracy 0.981.
### Flips: 820, rs: 16, checks: 615
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1977697
Train loss (w/o reg) on all data: 0.18809378
Test loss (w/o reg) on all data: 0.082883984
Train acc on all data:  0.9270605397520059
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 6.780339e-06
Norm of the params: 13.911086
     Influence (LOO): fixed 413 labels. Loss 0.08288. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062436916
Train loss (w/o reg) on all data: 0.04632123
Test loss (w/o reg) on all data: 0.046391968
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.8031819e-06
Norm of the params: 17.9531
                Loss: fixed 611 labels. Loss 0.04639. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36501053
Train loss (w/o reg) on all data: 0.35844162
Test loss (w/o reg) on all data: 0.18589783
Train acc on all data:  0.8541210795040116
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7815752e-05
Norm of the params: 11.462036
              Random: fixed 108 labels. Loss 0.18590. Accuracy 0.983.
### Flips: 820, rs: 16, checks: 820
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15341401
Train loss (w/o reg) on all data: 0.14422835
Test loss (w/o reg) on all data: 0.061733313
Train acc on all data:  0.9448091417456844
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4083928e-05
Norm of the params: 13.554079
     Influence (LOO): fixed 487 labels. Loss 0.06173. Accuracy 0.998.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013755372
Train loss (w/o reg) on all data: 0.0073871776
Test loss (w/o reg) on all data: 0.010088389
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.708563e-07
Norm of the params: 11.285561
                Loss: fixed 703 labels. Loss 0.01009. Accuracy 0.997.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35188258
Train loss (w/o reg) on all data: 0.34530923
Test loss (w/o reg) on all data: 0.1754073
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.969725e-06
Norm of the params: 11.465899
              Random: fixed 144 labels. Loss 0.17541. Accuracy 0.984.
### Flips: 820, rs: 16, checks: 1025
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12358876
Train loss (w/o reg) on all data: 0.11538543
Test loss (w/o reg) on all data: 0.04786191
Train acc on all data:  0.9572088499878434
Test acc on all data:   1.0
Norm of the mean of gradients: 5.2752703e-06
Norm of the params: 12.808851
     Influence (LOO): fixed 541 labels. Loss 0.04786. Accuracy 1.000.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009828361
Train loss (w/o reg) on all data: 0.0047478606
Test loss (w/o reg) on all data: 0.009353004
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.7931517e-07
Norm of the params: 10.080178
                Loss: fixed 711 labels. Loss 0.00935. Accuracy 0.997.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34046385
Train loss (w/o reg) on all data: 0.3334524
Test loss (w/o reg) on all data: 0.16756004
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.2220602e-05
Norm of the params: 11.841824
              Random: fixed 171 labels. Loss 0.16756. Accuracy 0.985.
### Flips: 820, rs: 16, checks: 1230
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09859353
Train loss (w/o reg) on all data: 0.09101435
Test loss (w/o reg) on all data: 0.039227087
Train acc on all data:  0.9664478482859227
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4782681e-05
Norm of the params: 12.311934
     Influence (LOO): fixed 581 labels. Loss 0.03923. Accuracy 0.999.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007964648
Train loss (w/o reg) on all data: 0.0036103677
Test loss (w/o reg) on all data: 0.007916094
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5992082e-07
Norm of the params: 9.331967
                Loss: fixed 715 labels. Loss 0.00792. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32518297
Train loss (w/o reg) on all data: 0.31812903
Test loss (w/o reg) on all data: 0.15481225
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.693214e-06
Norm of the params: 11.877672
              Random: fixed 210 labels. Loss 0.15481. Accuracy 0.983.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4003284
Train loss (w/o reg) on all data: 0.3933856
Test loss (w/o reg) on all data: 0.21347332
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.78025e-05
Norm of the params: 11.783719
Flipped loss: 0.21347. Accuracy: 0.983
### Flips: 820, rs: 17, checks: 205
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3140061
Train loss (w/o reg) on all data: 0.30461958
Test loss (w/o reg) on all data: 0.15585688
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0308263e-05
Norm of the params: 13.701477
     Influence (LOO): fixed 174 labels. Loss 0.15586. Accuracy 0.988.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2724942
Train loss (w/o reg) on all data: 0.2594823
Test loss (w/o reg) on all data: 0.1669848
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.88128e-05
Norm of the params: 16.131903
                Loss: fixed 205 labels. Loss 0.16698. Accuracy 0.957.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39094353
Train loss (w/o reg) on all data: 0.3838932
Test loss (w/o reg) on all data: 0.2042321
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.6223248e-05
Norm of the params: 11.874623
              Random: fixed  28 labels. Loss 0.20423. Accuracy 0.985.
### Flips: 820, rs: 17, checks: 410
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244139
Train loss (w/o reg) on all data: 0.23425469
Test loss (w/o reg) on all data: 0.110555045
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.2367902e-05
Norm of the params: 14.060097
     Influence (LOO): fixed 313 labels. Loss 0.11056. Accuracy 0.996.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16047199
Train loss (w/o reg) on all data: 0.14201176
Test loss (w/o reg) on all data: 0.121132575
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.083738e-06
Norm of the params: 19.214697
                Loss: fixed 410 labels. Loss 0.12113. Accuracy 0.963.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3773895
Train loss (w/o reg) on all data: 0.37023228
Test loss (w/o reg) on all data: 0.19411635
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.712558e-05
Norm of the params: 11.964278
              Random: fixed  70 labels. Loss 0.19412. Accuracy 0.984.
### Flips: 820, rs: 17, checks: 615
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18821758
Train loss (w/o reg) on all data: 0.17838566
Test loss (w/o reg) on all data: 0.08167932
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8361964e-06
Norm of the params: 14.022783
     Influence (LOO): fixed 412 labels. Loss 0.08168. Accuracy 0.999.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056861755
Train loss (w/o reg) on all data: 0.042305738
Test loss (w/o reg) on all data: 0.030788299
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.6504152e-06
Norm of the params: 17.062248
                Loss: fixed 612 labels. Loss 0.03079. Accuracy 0.993.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3656819
Train loss (w/o reg) on all data: 0.35859513
Test loss (w/o reg) on all data: 0.18164372
Train acc on all data:  0.8553367371748116
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.01831965e-05
Norm of the params: 11.905259
              Random: fixed 103 labels. Loss 0.18164. Accuracy 0.988.
### Flips: 820, rs: 17, checks: 820
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14507186
Train loss (w/o reg) on all data: 0.13544163
Test loss (w/o reg) on all data: 0.061509497
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2814553e-06
Norm of the params: 13.878209
     Influence (LOO): fixed 485 labels. Loss 0.06151. Accuracy 0.999.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009373032
Train loss (w/o reg) on all data: 0.004163579
Test loss (w/o reg) on all data: 0.0071607865
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.4662468e-07
Norm of the params: 10.207304
                Loss: fixed 696 labels. Loss 0.00716. Accuracy 0.998.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35487685
Train loss (w/o reg) on all data: 0.34808937
Test loss (w/o reg) on all data: 0.17458887
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.411807e-06
Norm of the params: 11.651153
              Random: fixed 138 labels. Loss 0.17459. Accuracy 0.986.
### Flips: 820, rs: 17, checks: 1025
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11862477
Train loss (w/o reg) on all data: 0.10927238
Test loss (w/o reg) on all data: 0.044484723
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 3.395782e-06
Norm of the params: 13.67654
     Influence (LOO): fixed 534 labels. Loss 0.04448. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046436796
Train loss (w/o reg) on all data: 0.001658912
Test loss (w/o reg) on all data: 0.0036220425
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.0054954e-08
Norm of the params: 7.7262764
                Loss: fixed 702 labels. Loss 0.00362. Accuracy 0.999.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34361112
Train loss (w/o reg) on all data: 0.33652586
Test loss (w/o reg) on all data: 0.16523543
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0138626e-05
Norm of the params: 11.903993
              Random: fixed 163 labels. Loss 0.16524. Accuracy 0.987.
### Flips: 820, rs: 17, checks: 1230
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09457334
Train loss (w/o reg) on all data: 0.08654329
Test loss (w/o reg) on all data: 0.032671038
Train acc on all data:  0.9679066374908826
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8680724e-06
Norm of the params: 12.672844
     Influence (LOO): fixed 573 labels. Loss 0.03267. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0039968244
Train loss (w/o reg) on all data: 0.0013902466
Test loss (w/o reg) on all data: 0.0035520387
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.1981398e-07
Norm of the params: 7.220218
                Loss: fixed 703 labels. Loss 0.00355. Accuracy 0.998.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33111286
Train loss (w/o reg) on all data: 0.32389
Test loss (w/o reg) on all data: 0.15920462
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.2091393e-06
Norm of the params: 12.019027
              Random: fixed 191 labels. Loss 0.15920. Accuracy 0.987.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40709037
Train loss (w/o reg) on all data: 0.4021575
Test loss (w/o reg) on all data: 0.20983988
Train acc on all data:  0.8312667152929735
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.7627002e-05
Norm of the params: 9.932651
Flipped loss: 0.20984. Accuracy: 0.978
### Flips: 820, rs: 18, checks: 205
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31851783
Train loss (w/o reg) on all data: 0.3100884
Test loss (w/o reg) on all data: 0.15152231
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.4439195e-05
Norm of the params: 12.984166
     Influence (LOO): fixed 180 labels. Loss 0.15152. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28406474
Train loss (w/o reg) on all data: 0.2723182
Test loss (w/o reg) on all data: 0.15308516
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 6.566428e-06
Norm of the params: 15.32744
                Loss: fixed 205 labels. Loss 0.15309. Accuracy 0.964.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3957457
Train loss (w/o reg) on all data: 0.39091766
Test loss (w/o reg) on all data: 0.1981284
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.85851e-06
Norm of the params: 9.826517
              Random: fixed  35 labels. Loss 0.19813. Accuracy 0.987.
### Flips: 820, rs: 18, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24612553
Train loss (w/o reg) on all data: 0.23752144
Test loss (w/o reg) on all data: 0.10668474
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.3987789e-05
Norm of the params: 13.117994
     Influence (LOO): fixed 324 labels. Loss 0.10668. Accuracy 0.997.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16276881
Train loss (w/o reg) on all data: 0.14616871
Test loss (w/o reg) on all data: 0.09642361
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.0369585e-06
Norm of the params: 18.220919
                Loss: fixed 410 labels. Loss 0.09642. Accuracy 0.966.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37947187
Train loss (w/o reg) on all data: 0.37407318
Test loss (w/o reg) on all data: 0.1873726
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.6937447e-05
Norm of the params: 10.391052
              Random: fixed  76 labels. Loss 0.18737. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 615
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19068262
Train loss (w/o reg) on all data: 0.18287617
Test loss (w/o reg) on all data: 0.0771142
Train acc on all data:  0.9321663019693655
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.1075939e-05
Norm of the params: 12.495161
     Influence (LOO): fixed 430 labels. Loss 0.07711. Accuracy 0.997.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04858551
Train loss (w/o reg) on all data: 0.033948276
Test loss (w/o reg) on all data: 0.028417286
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4728739e-06
Norm of the params: 17.109783
                Loss: fixed 614 labels. Loss 0.02842. Accuracy 0.990.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36323223
Train loss (w/o reg) on all data: 0.3575245
Test loss (w/o reg) on all data: 0.1751098
Train acc on all data:  0.8570386579139314
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.10907e-06
Norm of the params: 10.684309
              Random: fixed 117 labels. Loss 0.17511. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 820
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13840455
Train loss (w/o reg) on all data: 0.13042991
Test loss (w/o reg) on all data: 0.055892583
Train acc on all data:  0.9518599562363238
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8094213e-05
Norm of the params: 12.629052
     Influence (LOO): fixed 511 labels. Loss 0.05589. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076252194
Train loss (w/o reg) on all data: 0.0035293798
Test loss (w/o reg) on all data: 0.004946348
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.7825076e-07
Norm of the params: 9.050789
                Loss: fixed 698 labels. Loss 0.00495. Accuracy 0.999.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35262343
Train loss (w/o reg) on all data: 0.34677872
Test loss (w/o reg) on all data: 0.16768155
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.212843e-05
Norm of the params: 10.811773
              Random: fixed 146 labels. Loss 0.16768. Accuracy 0.985.
### Flips: 820, rs: 18, checks: 1025
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10583195
Train loss (w/o reg) on all data: 0.09801071
Test loss (w/o reg) on all data: 0.041535947
Train acc on all data:  0.9645027960126429
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5023189e-06
Norm of the params: 12.506987
     Influence (LOO): fixed 562 labels. Loss 0.04154. Accuracy 0.998.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0040249005
Train loss (w/o reg) on all data: 0.0015036239
Test loss (w/o reg) on all data: 0.0028595545
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.267171e-08
Norm of the params: 7.1010933
                Loss: fixed 705 labels. Loss 0.00286. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33839244
Train loss (w/o reg) on all data: 0.33236724
Test loss (w/o reg) on all data: 0.15681157
Train acc on all data:  0.8725990761001702
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.949262e-05
Norm of the params: 10.977423
              Random: fixed 180 labels. Loss 0.15681. Accuracy 0.990.
### Flips: 820, rs: 18, checks: 1230
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08516659
Train loss (w/o reg) on all data: 0.07804776
Test loss (w/o reg) on all data: 0.033906568
Train acc on all data:  0.9725261366399222
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4979198e-06
Norm of the params: 11.932164
     Influence (LOO): fixed 593 labels. Loss 0.03391. Accuracy 0.998.
Using normal model
LBFGS training took [23] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0031704062
Train loss (w/o reg) on all data: 0.0011096192
Test loss (w/o reg) on all data: 0.0029596975
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6374468e-07
Norm of the params: 6.4199486
                Loss: fixed 707 labels. Loss 0.00296. Accuracy 1.000.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32615113
Train loss (w/o reg) on all data: 0.3197925
Test loss (w/o reg) on all data: 0.14430253
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0114969e-05
Norm of the params: 11.277088
              Random: fixed 208 labels. Loss 0.14430. Accuracy 0.995.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3987244
Train loss (w/o reg) on all data: 0.3931953
Test loss (w/o reg) on all data: 0.198781
Train acc on all data:  0.8315098468271335
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.2394856e-05
Norm of the params: 10.515813
Flipped loss: 0.19878. Accuracy: 0.984
### Flips: 820, rs: 19, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30697417
Train loss (w/o reg) on all data: 0.29826954
Test loss (w/o reg) on all data: 0.14127968
Train acc on all data:  0.87478725990761
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.77799e-05
Norm of the params: 13.194413
     Influence (LOO): fixed 182 labels. Loss 0.14128. Accuracy 0.993.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27225837
Train loss (w/o reg) on all data: 0.26037034
Test loss (w/o reg) on all data: 0.14142732
Train acc on all data:  0.8813518113299295
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.18027e-06
Norm of the params: 15.419489
                Loss: fixed 205 labels. Loss 0.14143. Accuracy 0.974.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3863889
Train loss (w/o reg) on all data: 0.3809333
Test loss (w/o reg) on all data: 0.18620764
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.0055368e-05
Norm of the params: 10.445644
              Random: fixed  39 labels. Loss 0.18621. Accuracy 0.989.
### Flips: 820, rs: 19, checks: 410
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23926887
Train loss (w/o reg) on all data: 0.2302318
Test loss (w/o reg) on all data: 0.104216
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.264667e-06
Norm of the params: 13.444008
     Influence (LOO): fixed 313 labels. Loss 0.10422. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15880395
Train loss (w/o reg) on all data: 0.1424218
Test loss (w/o reg) on all data: 0.07743271
Train acc on all data:  0.937515195720885
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.3207136e-05
Norm of the params: 18.100916
                Loss: fixed 409 labels. Loss 0.07743. Accuracy 0.986.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37424237
Train loss (w/o reg) on all data: 0.36863136
Test loss (w/o reg) on all data: 0.17723921
Train acc on all data:  0.849744711889132
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1079e-05
Norm of the params: 10.5934
              Random: fixed  76 labels. Loss 0.17724. Accuracy 0.990.
### Flips: 820, rs: 19, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153083
Train loss (w/o reg) on all data: 0.1721623
Test loss (w/o reg) on all data: 0.08082975
Train acc on all data:  0.9311937758327256
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.8864785e-05
Norm of the params: 13.688346
     Influence (LOO): fixed 410 labels. Loss 0.08083. Accuracy 0.997.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05351682
Train loss (w/o reg) on all data: 0.039064776
Test loss (w/o reg) on all data: 0.022542404
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6072213e-06
Norm of the params: 17.001205
                Loss: fixed 608 labels. Loss 0.02254. Accuracy 0.997.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35963497
Train loss (w/o reg) on all data: 0.35377106
Test loss (w/o reg) on all data: 0.16747703
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.028198e-05
Norm of the params: 10.829505
              Random: fixed 112 labels. Loss 0.16748. Accuracy 0.988.
### Flips: 820, rs: 19, checks: 820
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14199889
Train loss (w/o reg) on all data: 0.13319057
Test loss (w/o reg) on all data: 0.06090121
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6232944e-06
Norm of the params: 13.27277
     Influence (LOO): fixed 483 labels. Loss 0.06090. Accuracy 0.998.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009597626
Train loss (w/o reg) on all data: 0.004801608
Test loss (w/o reg) on all data: 0.0051226374
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 8.866196e-07
Norm of the params: 9.793893
                Loss: fixed 689 labels. Loss 0.00512. Accuracy 1.000.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34784237
Train loss (w/o reg) on all data: 0.3416063
Test loss (w/o reg) on all data: 0.15646689
Train acc on all data:  0.8679795769511306
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.3373186e-05
Norm of the params: 11.167872
              Random: fixed 146 labels. Loss 0.15647. Accuracy 0.992.
### Flips: 820, rs: 19, checks: 1025
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11080356
Train loss (w/o reg) on all data: 0.10218636
Test loss (w/o reg) on all data: 0.046913538
Train acc on all data:  0.9579382445903234
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.1655252e-06
Norm of the params: 13.127986
     Influence (LOO): fixed 531 labels. Loss 0.04691. Accuracy 0.998.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044300994
Train loss (w/o reg) on all data: 0.0016926451
Test loss (w/o reg) on all data: 0.0029435281
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 1.438946e-07
Norm of the params: 7.3992624
                Loss: fixed 698 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33190995
Train loss (w/o reg) on all data: 0.32581717
Test loss (w/o reg) on all data: 0.14506352
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.6436946e-06
Norm of the params: 11.038836
              Random: fixed 186 labels. Loss 0.14506. Accuracy 0.992.
### Flips: 820, rs: 19, checks: 1230
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08893815
Train loss (w/o reg) on all data: 0.08080271
Test loss (w/o reg) on all data: 0.03693852
Train acc on all data:  0.9683929005592026
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6150786e-06
Norm of the params: 12.755734
     Influence (LOO): fixed 570 labels. Loss 0.03694. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0044300985
Train loss (w/o reg) on all data: 0.0016926894
Test loss (w/o reg) on all data: 0.002943509
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 7.697274e-08
Norm of the params: 7.399201
                Loss: fixed 698 labels. Loss 0.00294. Accuracy 1.000.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3170183
Train loss (w/o reg) on all data: 0.31108856
Test loss (w/o reg) on all data: 0.13595918
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.0594694e-05
Norm of the params: 10.890115
              Random: fixed 223 labels. Loss 0.13596. Accuracy 0.996.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3960762
Train loss (w/o reg) on all data: 0.38936645
Test loss (w/o reg) on all data: 0.21868789
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.237827e-05
Norm of the params: 11.584258
Flipped loss: 0.21869. Accuracy: 0.978
### Flips: 820, rs: 20, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304693
Train loss (w/o reg) on all data: 0.29413554
Test loss (w/o reg) on all data: 0.15899144
Train acc on all data:  0.8789204959883297
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.42816825e-05
Norm of the params: 14.530991
     Influence (LOO): fixed 179 labels. Loss 0.15899. Accuracy 0.989.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26954278
Train loss (w/o reg) on all data: 0.257026
Test loss (w/o reg) on all data: 0.15925196
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.6009577e-05
Norm of the params: 15.822008
                Loss: fixed 205 labels. Loss 0.15925. Accuracy 0.962.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38291818
Train loss (w/o reg) on all data: 0.37628832
Test loss (w/o reg) on all data: 0.2053163
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4355973e-05
Norm of the params: 11.515089
              Random: fixed  37 labels. Loss 0.20532. Accuracy 0.980.
### Flips: 820, rs: 20, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2391315
Train loss (w/o reg) on all data: 0.2275008
Test loss (w/o reg) on all data: 0.117848806
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.080605e-06
Norm of the params: 15.251686
     Influence (LOO): fixed 306 labels. Loss 0.11785. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15554647
Train loss (w/o reg) on all data: 0.1386938
Test loss (w/o reg) on all data: 0.091385506
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.7259941e-06
Norm of the params: 18.35902
                Loss: fixed 408 labels. Loss 0.09139. Accuracy 0.981.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36463824
Train loss (w/o reg) on all data: 0.35742176
Test loss (w/o reg) on all data: 0.18905605
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0896441e-05
Norm of the params: 12.013719
              Random: fixed  85 labels. Loss 0.18906. Accuracy 0.983.
### Flips: 820, rs: 20, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19249097
Train loss (w/o reg) on all data: 0.18153521
Test loss (w/o reg) on all data: 0.08795564
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.1498765e-06
Norm of the params: 14.80253
     Influence (LOO): fixed 403 labels. Loss 0.08796. Accuracy 0.993.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052923024
Train loss (w/o reg) on all data: 0.03754389
Test loss (w/o reg) on all data: 0.034912657
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4705182e-06
Norm of the params: 17.538038
                Loss: fixed 598 labels. Loss 0.03491. Accuracy 0.990.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3508947
Train loss (w/o reg) on all data: 0.3434673
Test loss (w/o reg) on all data: 0.17811266
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.0758943e-05
Norm of the params: 12.188021
              Random: fixed 119 labels. Loss 0.17811. Accuracy 0.984.
### Flips: 820, rs: 20, checks: 820
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14469771
Train loss (w/o reg) on all data: 0.13490975
Test loss (w/o reg) on all data: 0.059475757
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.5764077e-06
Norm of the params: 13.991395
     Influence (LOO): fixed 491 labels. Loss 0.05948. Accuracy 0.999.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015073219
Train loss (w/o reg) on all data: 0.007826192
Test loss (w/o reg) on all data: 0.0074850256
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.3420285e-07
Norm of the params: 12.0391245
                Loss: fixed 681 labels. Loss 0.00749. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33519292
Train loss (w/o reg) on all data: 0.327704
Test loss (w/o reg) on all data: 0.16518381
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.194401e-06
Norm of the params: 12.23838
              Random: fixed 162 labels. Loss 0.16518. Accuracy 0.987.
### Flips: 820, rs: 20, checks: 1025
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10918409
Train loss (w/o reg) on all data: 0.100209564
Test loss (w/o reg) on all data: 0.044223882
Train acc on all data:  0.962314612205203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0057819e-06
Norm of the params: 13.397404
     Influence (LOO): fixed 550 labels. Loss 0.04422. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009005335
Train loss (w/o reg) on all data: 0.004173178
Test loss (w/o reg) on all data: 0.0043025473
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.6274682e-07
Norm of the params: 9.830725
                Loss: fixed 694 labels. Loss 0.00430. Accuracy 1.000.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31851667
Train loss (w/o reg) on all data: 0.3106415
Test loss (w/o reg) on all data: 0.15372442
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.8675699e-05
Norm of the params: 12.550028
              Random: fixed 200 labels. Loss 0.15372. Accuracy 0.989.
### Flips: 820, rs: 20, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08585262
Train loss (w/o reg) on all data: 0.07787539
Test loss (w/o reg) on all data: 0.032687128
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3533768e-06
Norm of the params: 12.631099
     Influence (LOO): fixed 590 labels. Loss 0.03269. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007165039
Train loss (w/o reg) on all data: 0.0033274035
Test loss (w/o reg) on all data: 0.0038524552
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3243742e-07
Norm of the params: 8.760862
                Loss: fixed 699 labels. Loss 0.00385. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30167592
Train loss (w/o reg) on all data: 0.29388058
Test loss (w/o reg) on all data: 0.14382912
Train acc on all data:  0.8896182834913688
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.0434192e-05
Norm of the params: 12.486251
              Random: fixed 237 labels. Loss 0.14383. Accuracy 0.991.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40806612
Train loss (w/o reg) on all data: 0.40293348
Test loss (w/o reg) on all data: 0.21502207
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.1463572e-05
Norm of the params: 10.131776
Flipped loss: 0.21502. Accuracy: 0.983
### Flips: 820, rs: 21, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3161656
Train loss (w/o reg) on all data: 0.30682838
Test loss (w/o reg) on all data: 0.1637345
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2788572e-05
Norm of the params: 13.665452
     Influence (LOO): fixed 181 labels. Loss 0.16373. Accuracy 0.990.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28455442
Train loss (w/o reg) on all data: 0.27350977
Test loss (w/o reg) on all data: 0.16237472
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.455272e-05
Norm of the params: 14.862461
                Loss: fixed 205 labels. Loss 0.16237. Accuracy 0.966.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39692885
Train loss (w/o reg) on all data: 0.39164615
Test loss (w/o reg) on all data: 0.20597443
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4937257e-05
Norm of the params: 10.278816
              Random: fixed  34 labels. Loss 0.20597. Accuracy 0.982.
### Flips: 820, rs: 21, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25121504
Train loss (w/o reg) on all data: 0.24243507
Test loss (w/o reg) on all data: 0.12280471
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.118132e-05
Norm of the params: 13.2514
     Influence (LOO): fixed 312 labels. Loss 0.12280. Accuracy 0.989.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16967145
Train loss (w/o reg) on all data: 0.15452999
Test loss (w/o reg) on all data: 0.10442414
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.30816e-06
Norm of the params: 17.401983
                Loss: fixed 409 labels. Loss 0.10442. Accuracy 0.977.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38440183
Train loss (w/o reg) on all data: 0.37907732
Test loss (w/o reg) on all data: 0.19446969
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9358877e-05
Norm of the params: 10.319421
              Random: fixed  70 labels. Loss 0.19447. Accuracy 0.984.
### Flips: 820, rs: 21, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19601037
Train loss (w/o reg) on all data: 0.18690312
Test loss (w/o reg) on all data: 0.086469606
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.00069e-06
Norm of the params: 13.496104
     Influence (LOO): fixed 413 labels. Loss 0.08647. Accuracy 0.997.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055923093
Train loss (w/o reg) on all data: 0.040837657
Test loss (w/o reg) on all data: 0.03769253
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.6040892e-06
Norm of the params: 17.369764
                Loss: fixed 614 labels. Loss 0.03769. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3731064
Train loss (w/o reg) on all data: 0.3678078
Test loss (w/o reg) on all data: 0.1842077
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.7122813e-05
Norm of the params: 10.294265
              Random: fixed 103 labels. Loss 0.18421. Accuracy 0.988.
### Flips: 820, rs: 21, checks: 820
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15262173
Train loss (w/o reg) on all data: 0.14339375
Test loss (w/o reg) on all data: 0.06459592
Train acc on all data:  0.9448091417456844
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0625472e-05
Norm of the params: 13.585265
     Influence (LOO): fixed 489 labels. Loss 0.06460. Accuracy 1.000.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009746602
Train loss (w/o reg) on all data: 0.0046751387
Test loss (w/o reg) on all data: 0.0071503026
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.892681e-07
Norm of the params: 10.071209
                Loss: fixed 705 labels. Loss 0.00715. Accuracy 0.998.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35918656
Train loss (w/o reg) on all data: 0.3535941
Test loss (w/o reg) on all data: 0.1741392
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.702476e-05
Norm of the params: 10.575877
              Random: fixed 138 labels. Loss 0.17414. Accuracy 0.992.
### Flips: 820, rs: 21, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11596914
Train loss (w/o reg) on all data: 0.107854314
Test loss (w/o reg) on all data: 0.04661283
Train acc on all data:  0.9589107707269633
Test acc on all data:   1.0
Norm of the mean of gradients: 4.4069816e-06
Norm of the params: 12.739562
     Influence (LOO): fixed 549 labels. Loss 0.04661. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071888017
Train loss (w/o reg) on all data: 0.0029360652
Test loss (w/o reg) on all data: 0.0064634546
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.4719267e-07
Norm of the params: 9.222512
                Loss: fixed 709 labels. Loss 0.00646. Accuracy 0.998.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34712288
Train loss (w/o reg) on all data: 0.34132767
Test loss (w/o reg) on all data: 0.1641331
Train acc on all data:  0.8704108922927304
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.4940677e-05
Norm of the params: 10.765893
              Random: fixed 171 labels. Loss 0.16413. Accuracy 0.990.
### Flips: 820, rs: 21, checks: 1230
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089412205
Train loss (w/o reg) on all data: 0.08217169
Test loss (w/o reg) on all data: 0.033316404
Train acc on all data:  0.9688791636275225
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4449354e-06
Norm of the params: 12.033715
     Influence (LOO): fixed 591 labels. Loss 0.03332. Accuracy 1.000.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0042282767
Train loss (w/o reg) on all data: 0.001648526
Test loss (w/o reg) on all data: 0.0060863458
Train acc on all data:  1.0
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.068129e-08
Norm of the params: 7.1829667
                Loss: fixed 712 labels. Loss 0.00609. Accuracy 0.998.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3317332
Train loss (w/o reg) on all data: 0.3257602
Test loss (w/o reg) on all data: 0.15483682
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.5822237e-06
Norm of the params: 10.929777
              Random: fixed 206 labels. Loss 0.15484. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40882102
Train loss (w/o reg) on all data: 0.40371263
Test loss (w/o reg) on all data: 0.21515015
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.028748e-05
Norm of the params: 10.107817
Flipped loss: 0.21515. Accuracy: 0.988
### Flips: 820, rs: 22, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31848028
Train loss (w/o reg) on all data: 0.30892673
Test loss (w/o reg) on all data: 0.15393114
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7179855e-05
Norm of the params: 13.822844
     Influence (LOO): fixed 179 labels. Loss 0.15393. Accuracy 0.995.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28595802
Train loss (w/o reg) on all data: 0.27324995
Test loss (w/o reg) on all data: 0.15792552
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6317948e-05
Norm of the params: 15.942433
                Loss: fixed 205 labels. Loss 0.15793. Accuracy 0.974.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39438882
Train loss (w/o reg) on all data: 0.38899064
Test loss (w/o reg) on all data: 0.20314558
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.6952373e-05
Norm of the params: 10.390554
              Random: fixed  39 labels. Loss 0.20315. Accuracy 0.988.
### Flips: 820, rs: 22, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25061753
Train loss (w/o reg) on all data: 0.24023767
Test loss (w/o reg) on all data: 0.11378243
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.9404888e-05
Norm of the params: 14.408241
     Influence (LOO): fixed 310 labels. Loss 0.11378. Accuracy 0.996.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17374195
Train loss (w/o reg) on all data: 0.15740424
Test loss (w/o reg) on all data: 0.094919704
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.6003718e-06
Norm of the params: 18.07634
                Loss: fixed 409 labels. Loss 0.09492. Accuracy 0.983.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3823142
Train loss (w/o reg) on all data: 0.37698448
Test loss (w/o reg) on all data: 0.19235563
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.302038e-06
Norm of the params: 10.324462
              Random: fixed  77 labels. Loss 0.19236. Accuracy 0.988.
### Flips: 820, rs: 22, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2056405
Train loss (w/o reg) on all data: 0.19613725
Test loss (w/o reg) on all data: 0.088695355
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.922412e-06
Norm of the params: 13.786408
     Influence (LOO): fixed 398 labels. Loss 0.08870. Accuracy 0.997.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05620432
Train loss (w/o reg) on all data: 0.041212734
Test loss (w/o reg) on all data: 0.026379695
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2981532e-06
Norm of the params: 17.31565
                Loss: fixed 614 labels. Loss 0.02638. Accuracy 0.997.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3690309
Train loss (w/o reg) on all data: 0.36367846
Test loss (w/o reg) on all data: 0.17987455
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.4228303e-05
Norm of the params: 10.346428
              Random: fixed 115 labels. Loss 0.17987. Accuracy 0.993.
### Flips: 820, rs: 22, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16162123
Train loss (w/o reg) on all data: 0.15287033
Test loss (w/o reg) on all data: 0.064601004
Train acc on all data:  0.9433503525407245
Test acc on all data:   1.0
Norm of the mean of gradients: 3.593942e-06
Norm of the params: 13.229435
     Influence (LOO): fixed 481 labels. Loss 0.06460. Accuracy 1.000.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008078334
Train loss (w/o reg) on all data: 0.003479631
Test loss (w/o reg) on all data: 0.004082309
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9869854e-07
Norm of the params: 9.59031
                Loss: fixed 702 labels. Loss 0.00408. Accuracy 1.000.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35752746
Train loss (w/o reg) on all data: 0.3518238
Test loss (w/o reg) on all data: 0.1729713
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1777818e-05
Norm of the params: 10.680499
              Random: fixed 142 labels. Loss 0.17297. Accuracy 0.992.
### Flips: 820, rs: 22, checks: 1025
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12010915
Train loss (w/o reg) on all data: 0.11192329
Test loss (w/o reg) on all data: 0.04477156
Train acc on all data:  0.9610989545344031
Test acc on all data:   1.0
Norm of the mean of gradients: 3.0027118e-06
Norm of the params: 12.795199
     Influence (LOO): fixed 550 labels. Loss 0.04477. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073363446
Train loss (w/o reg) on all data: 0.0031567249
Test loss (w/o reg) on all data: 0.004159177
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.4289952e-07
Norm of the params: 9.142888
                Loss: fixed 705 labels. Loss 0.00416. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34582207
Train loss (w/o reg) on all data: 0.34015822
Test loss (w/o reg) on all data: 0.1619849
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.779286e-06
Norm of the params: 10.643156
              Random: fixed 172 labels. Loss 0.16198. Accuracy 0.990.
### Flips: 820, rs: 22, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08944832
Train loss (w/o reg) on all data: 0.08195691
Test loss (w/o reg) on all data: 0.032528054
Train acc on all data:  0.9717967420374423
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4421214e-06
Norm of the params: 12.240434
     Influence (LOO): fixed 596 labels. Loss 0.03253. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060321176
Train loss (w/o reg) on all data: 0.002503368
Test loss (w/o reg) on all data: 0.003496953
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1307717e-07
Norm of the params: 8.400892
                Loss: fixed 707 labels. Loss 0.00350. Accuracy 1.000.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33094794
Train loss (w/o reg) on all data: 0.32513234
Test loss (w/o reg) on all data: 0.15391326
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.44191e-06
Norm of the params: 10.784809
              Random: fixed 204 labels. Loss 0.15391. Accuracy 0.993.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.404835
Train loss (w/o reg) on all data: 0.399268
Test loss (w/o reg) on all data: 0.23043117
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.442941e-05
Norm of the params: 10.55176
Flipped loss: 0.23043. Accuracy: 0.967
### Flips: 820, rs: 23, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30941674
Train loss (w/o reg) on all data: 0.2996175
Test loss (w/o reg) on all data: 0.16409509
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.643552e-05
Norm of the params: 13.999463
     Influence (LOO): fixed 179 labels. Loss 0.16410. Accuracy 0.980.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28020388
Train loss (w/o reg) on all data: 0.26824403
Test loss (w/o reg) on all data: 0.18467982
Train acc on all data:  0.8847556528081693
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 3.6486457e-05
Norm of the params: 15.466003
                Loss: fixed 205 labels. Loss 0.18468. Accuracy 0.945.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39851224
Train loss (w/o reg) on all data: 0.39301595
Test loss (w/o reg) on all data: 0.2240893
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.7320686e-05
Norm of the params: 10.484559
              Random: fixed  22 labels. Loss 0.22409. Accuracy 0.975.
### Flips: 820, rs: 23, checks: 410
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24668188
Train loss (w/o reg) on all data: 0.23721108
Test loss (w/o reg) on all data: 0.12042475
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.067874e-06
Norm of the params: 13.762849
     Influence (LOO): fixed 308 labels. Loss 0.12042. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16535074
Train loss (w/o reg) on all data: 0.14863425
Test loss (w/o reg) on all data: 0.12637584
Train acc on all data:  0.9343544857768052
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 8.414921e-06
Norm of the params: 18.284685
                Loss: fixed 409 labels. Loss 0.12638. Accuracy 0.959.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3876579
Train loss (w/o reg) on all data: 0.38207352
Test loss (w/o reg) on all data: 0.21485211
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.1362458e-05
Norm of the params: 10.56825
              Random: fixed  54 labels. Loss 0.21485. Accuracy 0.972.
### Flips: 820, rs: 23, checks: 615
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19354099
Train loss (w/o reg) on all data: 0.18478066
Test loss (w/o reg) on all data: 0.084806375
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.7878736e-06
Norm of the params: 13.236565
     Influence (LOO): fixed 415 labels. Loss 0.08481. Accuracy 0.997.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059843507
Train loss (w/o reg) on all data: 0.044817403
Test loss (w/o reg) on all data: 0.04391458
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4048684e-06
Norm of the params: 17.335571
                Loss: fixed 612 labels. Loss 0.04391. Accuracy 0.985.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37670383
Train loss (w/o reg) on all data: 0.37116972
Test loss (w/o reg) on all data: 0.1990628
Train acc on all data:  0.8509603695599319
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.670062e-05
Norm of the params: 10.520567
              Random: fixed  86 labels. Loss 0.19906. Accuracy 0.980.
### Flips: 820, rs: 23, checks: 820
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1469966
Train loss (w/o reg) on all data: 0.13889697
Test loss (w/o reg) on all data: 0.06214892
Train acc on all data:  0.9482129832239241
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.907432e-06
Norm of the params: 12.72763
     Influence (LOO): fixed 495 labels. Loss 0.06215. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01046702
Train loss (w/o reg) on all data: 0.005063226
Test loss (w/o reg) on all data: 0.0070790383
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2607482e-07
Norm of the params: 10.395955
                Loss: fixed 698 labels. Loss 0.00708. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36329493
Train loss (w/o reg) on all data: 0.3577856
Test loss (w/o reg) on all data: 0.18948093
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5299145e-05
Norm of the params: 10.49697
              Random: fixed 123 labels. Loss 0.18948. Accuracy 0.981.
### Flips: 820, rs: 23, checks: 1025
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11438866
Train loss (w/o reg) on all data: 0.106826216
Test loss (w/o reg) on all data: 0.04621392
Train acc on all data:  0.9610989545344031
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.71005e-06
Norm of the params: 12.298329
     Influence (LOO): fixed 553 labels. Loss 0.04621. Accuracy 0.999.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061152494
Train loss (w/o reg) on all data: 0.0025894449
Test loss (w/o reg) on all data: 0.0049929204
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.1278096e-07
Norm of the params: 8.397386
                Loss: fixed 707 labels. Loss 0.00499. Accuracy 0.999.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3423691
Train loss (w/o reg) on all data: 0.33657742
Test loss (w/o reg) on all data: 0.1730625
Train acc on all data:  0.8708971553610503
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.1016792e-05
Norm of the params: 10.7626095
              Random: fixed 171 labels. Loss 0.17306. Accuracy 0.980.
### Flips: 820, rs: 23, checks: 1230
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090897456
Train loss (w/o reg) on all data: 0.08330656
Test loss (w/o reg) on all data: 0.03617253
Train acc on all data:  0.9696085582300025
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6051646e-06
Norm of the params: 12.321442
     Influence (LOO): fixed 589 labels. Loss 0.03617. Accuracy 0.998.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0050132778
Train loss (w/o reg) on all data: 0.0020257325
Test loss (w/o reg) on all data: 0.0036778967
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1579236e-08
Norm of the params: 7.7298713
                Loss: fixed 710 labels. Loss 0.00368. Accuracy 1.000.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32730234
Train loss (w/o reg) on all data: 0.32134256
Test loss (w/o reg) on all data: 0.16218418
Train acc on all data:  0.8784342329200098
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 3.634907e-05
Norm of the params: 10.917683
              Random: fixed 205 labels. Loss 0.16218. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40812284
Train loss (w/o reg) on all data: 0.4021516
Test loss (w/o reg) on all data: 0.21406123
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.6350432e-05
Norm of the params: 10.928152
Flipped loss: 0.21406. Accuracy: 0.977
### Flips: 820, rs: 24, checks: 205
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31500956
Train loss (w/o reg) on all data: 0.30570972
Test loss (w/o reg) on all data: 0.15335493
Train acc on all data:  0.8706540238268904
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.201269e-06
Norm of the params: 13.638078
     Influence (LOO): fixed 181 labels. Loss 0.15335. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28479534
Train loss (w/o reg) on all data: 0.27154028
Test loss (w/o reg) on all data: 0.15428005
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.2487834e-06
Norm of the params: 16.281925
                Loss: fixed 205 labels. Loss 0.15428. Accuracy 0.968.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39499182
Train loss (w/o reg) on all data: 0.3888694
Test loss (w/o reg) on all data: 0.20119214
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.6753909e-05
Norm of the params: 11.065623
              Random: fixed  42 labels. Loss 0.20119. Accuracy 0.983.
### Flips: 820, rs: 24, checks: 410
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25638667
Train loss (w/o reg) on all data: 0.24700972
Test loss (w/o reg) on all data: 0.11944778
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.3019767e-06
Norm of the params: 13.694486
     Influence (LOO): fixed 300 labels. Loss 0.11945. Accuracy 0.994.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17294961
Train loss (w/o reg) on all data: 0.15533015
Test loss (w/o reg) on all data: 0.096537516
Train acc on all data:  0.9336250911743253
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 5.4349284e-06
Norm of the params: 18.772032
                Loss: fixed 409 labels. Loss 0.09654. Accuracy 0.979.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38286045
Train loss (w/o reg) on all data: 0.37664452
Test loss (w/o reg) on all data: 0.19045778
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.179276e-05
Norm of the params: 11.149829
              Random: fixed  78 labels. Loss 0.19046. Accuracy 0.984.
### Flips: 820, rs: 24, checks: 615
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20129548
Train loss (w/o reg) on all data: 0.1921891
Test loss (w/o reg) on all data: 0.09053839
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.102819e-05
Norm of the params: 13.495473
     Influence (LOO): fixed 403 labels. Loss 0.09054. Accuracy 0.996.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06478131
Train loss (w/o reg) on all data: 0.049421772
Test loss (w/o reg) on all data: 0.034722924
Train acc on all data:  0.9812788718696815
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.957724e-07
Norm of the params: 17.526854
                Loss: fixed 610 labels. Loss 0.03472. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3644059
Train loss (w/o reg) on all data: 0.35779676
Test loss (w/o reg) on all data: 0.17448226
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.815806e-05
Norm of the params: 11.497074
              Random: fixed 128 labels. Loss 0.17448. Accuracy 0.987.
### Flips: 820, rs: 24, checks: 820
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14759058
Train loss (w/o reg) on all data: 0.13848776
Test loss (w/o reg) on all data: 0.061504796
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.222867e-06
Norm of the params: 13.492831
     Influence (LOO): fixed 494 labels. Loss 0.06150. Accuracy 0.998.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216228
Train loss (w/o reg) on all data: 0.004936608
Test loss (w/o reg) on all data: 0.006258105
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6361827e-07
Norm of the params: 10.275816
                Loss: fixed 706 labels. Loss 0.00626. Accuracy 0.999.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35168418
Train loss (w/o reg) on all data: 0.34517834
Test loss (w/o reg) on all data: 0.16385312
Train acc on all data:  0.8670070508144906
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1432681e-05
Norm of the params: 11.406876
              Random: fixed 160 labels. Loss 0.16385. Accuracy 0.987.
### Flips: 820, rs: 24, checks: 1025
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11321423
Train loss (w/o reg) on all data: 0.105542906
Test loss (w/o reg) on all data: 0.046214897
Train acc on all data:  0.9591539022611233
Test acc on all data:   1.0
Norm of the mean of gradients: 5.7419197e-06
Norm of the params: 12.386547
     Influence (LOO): fixed 552 labels. Loss 0.04621. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069558164
Train loss (w/o reg) on all data: 0.0029998051
Test loss (w/o reg) on all data: 0.0051861373
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.7840454e-08
Norm of the params: 8.894956
                Loss: fixed 712 labels. Loss 0.00519. Accuracy 0.999.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33952418
Train loss (w/o reg) on all data: 0.33278397
Test loss (w/o reg) on all data: 0.1552445
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.8064633e-05
Norm of the params: 11.610519
              Random: fixed 190 labels. Loss 0.15524. Accuracy 0.989.
### Flips: 820, rs: 24, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09033142
Train loss (w/o reg) on all data: 0.08340322
Test loss (w/o reg) on all data: 0.03563217
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1624912e-06
Norm of the params: 11.771321
     Influence (LOO): fixed 590 labels. Loss 0.03563. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046372367
Train loss (w/o reg) on all data: 0.0018720011
Test loss (w/o reg) on all data: 0.0037419964
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.5660735e-08
Norm of the params: 7.4367137
                Loss: fixed 716 labels. Loss 0.00374. Accuracy 0.999.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32152346
Train loss (w/o reg) on all data: 0.31445852
Test loss (w/o reg) on all data: 0.14490315
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.977871e-06
Norm of the params: 11.886909
              Random: fixed 230 labels. Loss 0.14490. Accuracy 0.991.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39951438
Train loss (w/o reg) on all data: 0.39360023
Test loss (w/o reg) on all data: 0.2165771
Train acc on all data:  0.8358862144420132
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.3128263e-05
Norm of the params: 10.875791
Flipped loss: 0.21658. Accuracy: 0.978
### Flips: 820, rs: 25, checks: 205
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30434743
Train loss (w/o reg) on all data: 0.29399583
Test loss (w/o reg) on all data: 0.16067055
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0248975e-05
Norm of the params: 14.388616
     Influence (LOO): fixed 179 labels. Loss 0.16067. Accuracy 0.980.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27249727
Train loss (w/o reg) on all data: 0.2589495
Test loss (w/o reg) on all data: 0.16654982
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 6.2180975e-06
Norm of the params: 16.46073
                Loss: fixed 205 labels. Loss 0.16655. Accuracy 0.957.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38949096
Train loss (w/o reg) on all data: 0.383599
Test loss (w/o reg) on all data: 0.20311731
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.2570412e-05
Norm of the params: 10.855357
              Random: fixed  28 labels. Loss 0.20312. Accuracy 0.983.
### Flips: 820, rs: 25, checks: 410
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23956095
Train loss (w/o reg) on all data: 0.22901288
Test loss (w/o reg) on all data: 0.119981565
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.8604315e-06
Norm of the params: 14.524508
     Influence (LOO): fixed 309 labels. Loss 0.11998. Accuracy 0.986.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15554778
Train loss (w/o reg) on all data: 0.13824943
Test loss (w/o reg) on all data: 0.11412059
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.1437296e-06
Norm of the params: 18.60019
                Loss: fixed 410 labels. Loss 0.11412. Accuracy 0.967.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37493825
Train loss (w/o reg) on all data: 0.3686128
Test loss (w/o reg) on all data: 0.19275726
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.4683533e-05
Norm of the params: 11.247625
              Random: fixed  63 labels. Loss 0.19276. Accuracy 0.983.
### Flips: 820, rs: 25, checks: 615
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19300485
Train loss (w/o reg) on all data: 0.18295515
Test loss (w/o reg) on all data: 0.0904776
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.7145277e-06
Norm of the params: 14.177239
     Influence (LOO): fixed 404 labels. Loss 0.09048. Accuracy 0.993.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04921452
Train loss (w/o reg) on all data: 0.03502768
Test loss (w/o reg) on all data: 0.038042612
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.3378412e-06
Norm of the params: 16.84449
                Loss: fixed 610 labels. Loss 0.03804. Accuracy 0.987.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3592024
Train loss (w/o reg) on all data: 0.35278392
Test loss (w/o reg) on all data: 0.17964299
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.0787054e-05
Norm of the params: 11.330045
              Random: fixed 101 labels. Loss 0.17964. Accuracy 0.982.
### Flips: 820, rs: 25, checks: 820
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15311716
Train loss (w/o reg) on all data: 0.1444616
Test loss (w/o reg) on all data: 0.06354454
Train acc on all data:  0.9455385363481643
Test acc on all data:   1.0
Norm of the mean of gradients: 9.47813e-06
Norm of the params: 13.15717
     Influence (LOO): fixed 475 labels. Loss 0.06354. Accuracy 1.000.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013925375
Train loss (w/o reg) on all data: 0.0075450735
Test loss (w/o reg) on all data: 0.0081391
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.435223e-07
Norm of the params: 11.296285
                Loss: fixed 684 labels. Loss 0.00814. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34732145
Train loss (w/o reg) on all data: 0.34099102
Test loss (w/o reg) on all data: 0.1670797
Train acc on all data:  0.8674933138828106
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6686954e-05
Norm of the params: 11.252056
              Random: fixed 136 labels. Loss 0.16708. Accuracy 0.988.
### Flips: 820, rs: 25, checks: 1025
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12140301
Train loss (w/o reg) on all data: 0.11412243
Test loss (w/o reg) on all data: 0.048094314
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.785021e-06
Norm of the params: 12.066966
     Influence (LOO): fixed 530 labels. Loss 0.04809. Accuracy 0.999.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006364169
Train loss (w/o reg) on all data: 0.0027683747
Test loss (w/o reg) on all data: 0.0035371957
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5822663e-07
Norm of the params: 8.480324
                Loss: fixed 697 labels. Loss 0.00354. Accuracy 1.000.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33337945
Train loss (w/o reg) on all data: 0.32680923
Test loss (w/o reg) on all data: 0.15949029
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.252865e-06
Norm of the params: 11.463173
              Random: fixed 168 labels. Loss 0.15949. Accuracy 0.987.
### Flips: 820, rs: 25, checks: 1230
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08926481
Train loss (w/o reg) on all data: 0.08215667
Test loss (w/o reg) on all data: 0.034396175
Train acc on all data:  0.9696085582300025
Test acc on all data:   1.0
Norm of the mean of gradients: 4.760308e-06
Norm of the params: 11.923204
     Influence (LOO): fixed 578 labels. Loss 0.03440. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051237084
Train loss (w/o reg) on all data: 0.0020533672
Test loss (w/o reg) on all data: 0.0029096403
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.7340084e-07
Norm of the params: 7.8362503
                Loss: fixed 699 labels. Loss 0.00291. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32125223
Train loss (w/o reg) on all data: 0.31491706
Test loss (w/o reg) on all data: 0.14889954
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.0250772e-05
Norm of the params: 11.2562685
              Random: fixed 201 labels. Loss 0.14890. Accuracy 0.989.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40178454
Train loss (w/o reg) on all data: 0.39647862
Test loss (w/o reg) on all data: 0.22214775
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1887894e-05
Norm of the params: 10.301378
Flipped loss: 0.22215. Accuracy: 0.985
### Flips: 820, rs: 26, checks: 205
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3129613
Train loss (w/o reg) on all data: 0.30497098
Test loss (w/o reg) on all data: 0.16841783
Train acc on all data:  0.8733284707026502
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.5008966e-05
Norm of the params: 12.64146
     Influence (LOO): fixed 174 labels. Loss 0.16842. Accuracy 0.985.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27405253
Train loss (w/o reg) on all data: 0.26168644
Test loss (w/o reg) on all data: 0.17006429
Train acc on all data:  0.8806224167274496
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 9.553782e-06
Norm of the params: 15.726463
                Loss: fixed 205 labels. Loss 0.17006. Accuracy 0.967.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3934887
Train loss (w/o reg) on all data: 0.3880133
Test loss (w/o reg) on all data: 0.21338046
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1487165e-05
Norm of the params: 10.464616
              Random: fixed  28 labels. Loss 0.21338. Accuracy 0.985.
### Flips: 820, rs: 26, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24193788
Train loss (w/o reg) on all data: 0.23240761
Test loss (w/o reg) on all data: 0.12448818
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.4719095e-06
Norm of the params: 13.805984
     Influence (LOO): fixed 313 labels. Loss 0.12449. Accuracy 0.988.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16151184
Train loss (w/o reg) on all data: 0.14307162
Test loss (w/o reg) on all data: 0.11502418
Train acc on all data:  0.937272064186725
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.1379225e-06
Norm of the params: 19.204279
                Loss: fixed 409 labels. Loss 0.11502. Accuracy 0.971.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38269144
Train loss (w/o reg) on all data: 0.3770046
Test loss (w/o reg) on all data: 0.2045759
Train acc on all data:  0.8426938973984925
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.094964e-05
Norm of the params: 10.664756
              Random: fixed  56 labels. Loss 0.20458. Accuracy 0.985.
### Flips: 820, rs: 26, checks: 615
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18717685
Train loss (w/o reg) on all data: 0.17717497
Test loss (w/o reg) on all data: 0.090864144
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.804132e-06
Norm of the params: 14.143467
     Influence (LOO): fixed 412 labels. Loss 0.09086. Accuracy 0.998.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058711205
Train loss (w/o reg) on all data: 0.04368315
Test loss (w/o reg) on all data: 0.038043685
Train acc on all data:  0.9827376610746413
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.4210374e-06
Norm of the params: 17.336697
                Loss: fixed 610 labels. Loss 0.03804. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36716622
Train loss (w/o reg) on all data: 0.36087748
Test loss (w/o reg) on all data: 0.1905178
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.374329e-06
Norm of the params: 11.214937
              Random: fixed  96 labels. Loss 0.19052. Accuracy 0.983.
### Flips: 820, rs: 26, checks: 820
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14286955
Train loss (w/o reg) on all data: 0.13263223
Test loss (w/o reg) on all data: 0.06715658
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.652125e-06
Norm of the params: 14.308966
     Influence (LOO): fixed 490 labels. Loss 0.06716. Accuracy 0.997.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010635036
Train loss (w/o reg) on all data: 0.0052241627
Test loss (w/o reg) on all data: 0.0068047615
Train acc on all data:  0.9987843423292001
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1834049e-07
Norm of the params: 10.402762
                Loss: fixed 705 labels. Loss 0.00680. Accuracy 0.999.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3568755
Train loss (w/o reg) on all data: 0.35070926
Test loss (w/o reg) on all data: 0.1778588
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.6815484e-06
Norm of the params: 11.105171
              Random: fixed 130 labels. Loss 0.17786. Accuracy 0.992.
### Flips: 820, rs: 26, checks: 1025
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11837121
Train loss (w/o reg) on all data: 0.109152064
Test loss (w/o reg) on all data: 0.050152592
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.856977e-06
Norm of the params: 13.578769
     Influence (LOO): fixed 540 labels. Loss 0.05015. Accuracy 0.998.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006487879
Train loss (w/o reg) on all data: 0.0027159867
Test loss (w/o reg) on all data: 0.0039318088
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7503754e-07
Norm of the params: 8.685496
                Loss: fixed 712 labels. Loss 0.00393. Accuracy 1.000.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34202275
Train loss (w/o reg) on all data: 0.33546537
Test loss (w/o reg) on all data: 0.16541462
Train acc on all data:  0.8687089715536105
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.8996561e-05
Norm of the params: 11.451975
              Random: fixed 168 labels. Loss 0.16541. Accuracy 0.990.
### Flips: 820, rs: 26, checks: 1230
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09359809
Train loss (w/o reg) on all data: 0.08540433
Test loss (w/o reg) on all data: 0.037452385
Train acc on all data:  0.9666909798200827
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.6168691e-06
Norm of the params: 12.801374
     Influence (LOO): fixed 583 labels. Loss 0.03745. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048846463
Train loss (w/o reg) on all data: 0.0019182592
Test loss (w/o reg) on all data: 0.0027682541
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 5.8954917e-08
Norm of the params: 7.7024503
                Loss: fixed 716 labels. Loss 0.00277. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32850176
Train loss (w/o reg) on all data: 0.32163683
Test loss (w/o reg) on all data: 0.1553723
Train acc on all data:  0.87551665451009
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5618643e-05
Norm of the params: 11.717461
              Random: fixed 202 labels. Loss 0.15537. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4012774
Train loss (w/o reg) on all data: 0.3945159
Test loss (w/o reg) on all data: 0.21040726
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.4482566e-05
Norm of the params: 11.628846
Flipped loss: 0.21041. Accuracy: 0.984
### Flips: 820, rs: 27, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30880222
Train loss (w/o reg) on all data: 0.29853344
Test loss (w/o reg) on all data: 0.15444082
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.5773845e-05
Norm of the params: 14.330923
     Influence (LOO): fixed 181 labels. Loss 0.15444. Accuracy 0.981.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27723
Train loss (w/o reg) on all data: 0.26289386
Test loss (w/o reg) on all data: 0.16106226
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.4592782e-05
Norm of the params: 16.93289
                Loss: fixed 205 labels. Loss 0.16106. Accuracy 0.957.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38811508
Train loss (w/o reg) on all data: 0.38116705
Test loss (w/o reg) on all data: 0.19608364
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.758501e-06
Norm of the params: 11.788152
              Random: fixed  41 labels. Loss 0.19608. Accuracy 0.986.
### Flips: 820, rs: 27, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24740286
Train loss (w/o reg) on all data: 0.23627807
Test loss (w/o reg) on all data: 0.12407482
Train acc on all data:  0.9027473863360078
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.1672246e-05
Norm of the params: 14.916291
     Influence (LOO): fixed 303 labels. Loss 0.12407. Accuracy 0.982.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16921683
Train loss (w/o reg) on all data: 0.1509436
Test loss (w/o reg) on all data: 0.101901926
Train acc on all data:  0.9336250911743253
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.8251295e-06
Norm of the params: 19.117125
                Loss: fixed 408 labels. Loss 0.10190. Accuracy 0.976.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37646446
Train loss (w/o reg) on all data: 0.3695307
Test loss (w/o reg) on all data: 0.18476728
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.6344573e-05
Norm of the params: 11.776038
              Random: fixed  78 labels. Loss 0.18477. Accuracy 0.988.
### Flips: 820, rs: 27, checks: 615
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19810499
Train loss (w/o reg) on all data: 0.18794286
Test loss (w/o reg) on all data: 0.090590954
Train acc on all data:  0.9263311451495259
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.6301943e-06
Norm of the params: 14.256316
     Influence (LOO): fixed 402 labels. Loss 0.09059. Accuracy 0.993.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06298703
Train loss (w/o reg) on all data: 0.0470177
Test loss (w/o reg) on all data: 0.042005714
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.0953985e-06
Norm of the params: 17.871391
                Loss: fixed 609 labels. Loss 0.04201. Accuracy 0.987.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36548352
Train loss (w/o reg) on all data: 0.35856044
Test loss (w/o reg) on all data: 0.17291479
Train acc on all data:  0.8558230002431315
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.825841e-05
Norm of the params: 11.766983
              Random: fixed 109 labels. Loss 0.17291. Accuracy 0.990.
### Flips: 820, rs: 27, checks: 820
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15348163
Train loss (w/o reg) on all data: 0.14383522
Test loss (w/o reg) on all data: 0.06778071
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.999204e-06
Norm of the params: 13.889862
     Influence (LOO): fixed 488 labels. Loss 0.06778. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014305104
Train loss (w/o reg) on all data: 0.0077220243
Test loss (w/o reg) on all data: 0.007855795
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5570453e-06
Norm of the params: 11.474388
                Loss: fixed 699 labels. Loss 0.00786. Accuracy 0.998.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3534809
Train loss (w/o reg) on all data: 0.34678924
Test loss (w/o reg) on all data: 0.16041243
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.588455e-05
Norm of the params: 11.568632
              Random: fixed 144 labels. Loss 0.16041. Accuracy 0.991.
### Flips: 820, rs: 27, checks: 1025
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12591386
Train loss (w/o reg) on all data: 0.11687316
Test loss (w/o reg) on all data: 0.05318265
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.5288891e-06
Norm of the params: 13.4467125
     Influence (LOO): fixed 536 labels. Loss 0.05318. Accuracy 0.997.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00599562
Train loss (w/o reg) on all data: 0.0026142604
Test loss (w/o reg) on all data: 0.0036741132
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 2.421847e-07
Norm of the params: 8.223575
                Loss: fixed 713 labels. Loss 0.00367. Accuracy 1.000.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33822238
Train loss (w/o reg) on all data: 0.3314202
Test loss (w/o reg) on all data: 0.14978208
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.254426e-05
Norm of the params: 11.663759
              Random: fixed 185 labels. Loss 0.14978. Accuracy 0.993.
### Flips: 820, rs: 27, checks: 1230
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093911946
Train loss (w/o reg) on all data: 0.08534586
Test loss (w/o reg) on all data: 0.038615663
Train acc on all data:  0.9686360320933625
Test acc on all data:   1.0
Norm of the mean of gradients: 6.0303973e-06
Norm of the params: 13.088995
     Influence (LOO): fixed 588 labels. Loss 0.03862. Accuracy 1.000.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056729317
Train loss (w/o reg) on all data: 0.0023279488
Test loss (w/o reg) on all data: 0.0030670967
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 8.9121635e-08
Norm of the params: 8.179221
                Loss: fixed 714 labels. Loss 0.00307. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3226881
Train loss (w/o reg) on all data: 0.31534347
Test loss (w/o reg) on all data: 0.13814974
Train acc on all data:  0.8828106005348894
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.4823312e-05
Norm of the params: 12.119925
              Random: fixed 220 labels. Loss 0.13815. Accuracy 0.996.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41015872
Train loss (w/o reg) on all data: 0.4050196
Test loss (w/o reg) on all data: 0.21331325
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.878812e-05
Norm of the params: 10.1381445
Flipped loss: 0.21331. Accuracy: 0.991
### Flips: 820, rs: 28, checks: 205
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3185342
Train loss (w/o reg) on all data: 0.3088869
Test loss (w/o reg) on all data: 0.15322185
Train acc on all data:  0.8677364454169706
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.883918e-05
Norm of the params: 13.890509
     Influence (LOO): fixed 183 labels. Loss 0.15322. Accuracy 0.994.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28498328
Train loss (w/o reg) on all data: 0.27359334
Test loss (w/o reg) on all data: 0.15888946
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0423579e-05
Norm of the params: 15.093
                Loss: fixed 205 labels. Loss 0.15889. Accuracy 0.977.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39915714
Train loss (w/o reg) on all data: 0.39387587
Test loss (w/o reg) on all data: 0.2003805
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.5286914e-05
Norm of the params: 10.277419
              Random: fixed  37 labels. Loss 0.20038. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25850642
Train loss (w/o reg) on all data: 0.24833225
Test loss (w/o reg) on all data: 0.11082469
Train acc on all data:  0.8983710187211281
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2110611e-05
Norm of the params: 14.264767
     Influence (LOO): fixed 309 labels. Loss 0.11082. Accuracy 1.000.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1695473
Train loss (w/o reg) on all data: 0.15296207
Test loss (w/o reg) on all data: 0.09703516
Train acc on all data:  0.9314369073668854
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.0624915e-05
Norm of the params: 18.212759
                Loss: fixed 410 labels. Loss 0.09704. Accuracy 0.978.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38815337
Train loss (w/o reg) on all data: 0.3828852
Test loss (w/o reg) on all data: 0.19152528
Train acc on all data:  0.8441526866034524
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8833045e-05
Norm of the params: 10.26467
              Random: fixed  71 labels. Loss 0.19153. Accuracy 0.994.
### Flips: 820, rs: 28, checks: 615
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20553829
Train loss (w/o reg) on all data: 0.19527887
Test loss (w/o reg) on all data: 0.08689349
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5004497e-05
Norm of the params: 14.324399
     Influence (LOO): fixed 409 labels. Loss 0.08689. Accuracy 0.998.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06139622
Train loss (w/o reg) on all data: 0.046636567
Test loss (w/o reg) on all data: 0.038570683
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2116772e-06
Norm of the params: 17.181183
                Loss: fixed 614 labels. Loss 0.03857. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37539926
Train loss (w/o reg) on all data: 0.36986274
Test loss (w/o reg) on all data: 0.18132977
Train acc on all data:  0.8529054218332117
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.8176184e-05
Norm of the params: 10.52285
              Random: fixed 104 labels. Loss 0.18133. Accuracy 0.993.
### Flips: 820, rs: 28, checks: 820
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1595667
Train loss (w/o reg) on all data: 0.15021068
Test loss (w/o reg) on all data: 0.06392373
Train acc on all data:  0.9428640894724045
Test acc on all data:   1.0
Norm of the mean of gradients: 7.940434e-06
Norm of the params: 13.679199
     Influence (LOO): fixed 490 labels. Loss 0.06392. Accuracy 1.000.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010017311
Train loss (w/o reg) on all data: 0.004844656
Test loss (w/o reg) on all data: 0.00433789
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 5.5359965e-07
Norm of the params: 10.17119
                Loss: fixed 714 labels. Loss 0.00434. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35922113
Train loss (w/o reg) on all data: 0.35353643
Test loss (w/o reg) on all data: 0.170945
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.5939824e-06
Norm of the params: 10.662732
              Random: fixed 143 labels. Loss 0.17095. Accuracy 0.991.
### Flips: 820, rs: 28, checks: 1025
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12057697
Train loss (w/o reg) on all data: 0.11194238
Test loss (w/o reg) on all data: 0.04482821
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7549246e-06
Norm of the params: 13.141226
     Influence (LOO): fixed 556 labels. Loss 0.04483. Accuracy 0.999.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0062414315
Train loss (w/o reg) on all data: 0.0025345336
Test loss (w/o reg) on all data: 0.0032421327
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.1728104e-08
Norm of the params: 8.610339
                Loss: fixed 720 labels. Loss 0.00324. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34552658
Train loss (w/o reg) on all data: 0.34011152
Test loss (w/o reg) on all data: 0.16162916
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.6692316e-05
Norm of the params: 10.406785
              Random: fixed 178 labels. Loss 0.16163. Accuracy 0.991.
### Flips: 820, rs: 28, checks: 1230
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095703825
Train loss (w/o reg) on all data: 0.087829754
Test loss (w/o reg) on all data: 0.034603294
Train acc on all data:  0.9681497690250426
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.9857232e-06
Norm of the params: 12.549161
     Influence (LOO): fixed 596 labels. Loss 0.03460. Accuracy 0.999.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054001887
Train loss (w/o reg) on all data: 0.002115111
Test loss (w/o reg) on all data: 0.003028613
Train acc on all data:  0.99951373693168
Test acc on all data:   1.0
Norm of the mean of gradients: 9.9454574e-08
Norm of the params: 8.105649
                Loss: fixed 722 labels. Loss 0.00303. Accuracy 1.000.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32869655
Train loss (w/o reg) on all data: 0.32304138
Test loss (w/o reg) on all data: 0.15113811
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.10727005e-05
Norm of the params: 10.635021
              Random: fixed 218 labels. Loss 0.15114. Accuracy 0.995.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3975348
Train loss (w/o reg) on all data: 0.39112112
Test loss (w/o reg) on all data: 0.20938776
Train acc on all data:  0.8349136883053732
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.774204e-05
Norm of the params: 11.325796
Flipped loss: 0.20939. Accuracy: 0.978
### Flips: 820, rs: 29, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31186792
Train loss (w/o reg) on all data: 0.3018098
Test loss (w/o reg) on all data: 0.15233491
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2441081e-05
Norm of the params: 14.18318
     Influence (LOO): fixed 176 labels. Loss 0.15233. Accuracy 0.988.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27041197
Train loss (w/o reg) on all data: 0.25752717
Test loss (w/o reg) on all data: 0.16517514
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 3.2936146e-05
Norm of the params: 16.052906
                Loss: fixed 205 labels. Loss 0.16518. Accuracy 0.956.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38578114
Train loss (w/o reg) on all data: 0.37928966
Test loss (w/o reg) on all data: 0.19627799
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.7865025e-05
Norm of the params: 11.394279
              Random: fixed  37 labels. Loss 0.19628. Accuracy 0.981.
### Flips: 820, rs: 29, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24011052
Train loss (w/o reg) on all data: 0.22961873
Test loss (w/o reg) on all data: 0.10705007
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.0126983e-05
Norm of the params: 14.485712
     Influence (LOO): fixed 314 labels. Loss 0.10705. Accuracy 0.993.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15534903
Train loss (w/o reg) on all data: 0.13901897
Test loss (w/o reg) on all data: 0.102521405
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.9365992e-06
Norm of the params: 18.072113
                Loss: fixed 410 labels. Loss 0.10252. Accuracy 0.972.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3736567
Train loss (w/o reg) on all data: 0.36714694
Test loss (w/o reg) on all data: 0.18614104
Train acc on all data:  0.8524191587648918
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.3528424e-05
Norm of the params: 11.410294
              Random: fixed  71 labels. Loss 0.18614. Accuracy 0.983.
### Flips: 820, rs: 29, checks: 615
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920077
Train loss (w/o reg) on all data: 0.1797639
Test loss (w/o reg) on all data: 0.08199776
Train acc on all data:  0.9307075127644056
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.650693e-06
Norm of the params: 13.738181
     Influence (LOO): fixed 413 labels. Loss 0.08200. Accuracy 0.995.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04935535
Train loss (w/o reg) on all data: 0.035127796
Test loss (w/o reg) on all data: 0.03656313
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0555314e-06
Norm of the params: 16.868643
                Loss: fixed 610 labels. Loss 0.03656. Accuracy 0.988.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36532518
Train loss (w/o reg) on all data: 0.35889837
Test loss (w/o reg) on all data: 0.17882636
Train acc on all data:  0.8582543155847313
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.3651943e-05
Norm of the params: 11.33739
              Random: fixed  93 labels. Loss 0.17883. Accuracy 0.983.
### Flips: 820, rs: 29, checks: 820
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15137997
Train loss (w/o reg) on all data: 0.14274651
Test loss (w/o reg) on all data: 0.062686026
Train acc on all data:  0.9467541940189642
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3619107e-06
Norm of the params: 13.140368
     Influence (LOO): fixed 480 labels. Loss 0.06269. Accuracy 0.998.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012912847
Train loss (w/o reg) on all data: 0.006492001
Test loss (w/o reg) on all data: 0.007989967
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3749316e-07
Norm of the params: 11.332119
                Loss: fixed 684 labels. Loss 0.00799. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35327652
Train loss (w/o reg) on all data: 0.34687567
Test loss (w/o reg) on all data: 0.16722581
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.8842895e-06
Norm of the params: 11.3144655
              Random: fixed 128 labels. Loss 0.16723. Accuracy 0.986.
### Flips: 820, rs: 29, checks: 1025
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112078
Train loss (w/o reg) on all data: 0.10414526
Test loss (w/o reg) on all data: 0.04525147
Train acc on all data:  0.9615852176027231
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.6291602e-06
Norm of the params: 12.595826
     Influence (LOO): fixed 541 labels. Loss 0.04525. Accuracy 0.995.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006314952
Train loss (w/o reg) on all data: 0.0025974729
Test loss (w/o reg) on all data: 0.0041692834
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.564339e-08
Norm of the params: 8.622621
                Loss: fixed 693 labels. Loss 0.00417. Accuracy 0.999.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33853397
Train loss (w/o reg) on all data: 0.33185804
Test loss (w/o reg) on all data: 0.15587555
Train acc on all data:  0.8728422076343302
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.283057e-06
Norm of the params: 11.555026
              Random: fixed 165 labels. Loss 0.15588. Accuracy 0.992.
### Flips: 820, rs: 29, checks: 1230
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08374707
Train loss (w/o reg) on all data: 0.0762561
Test loss (w/o reg) on all data: 0.030081559
Train acc on all data:  0.9727692681740822
Test acc on all data:   1.0
Norm of the mean of gradients: 6.186051e-06
Norm of the params: 12.240073
     Influence (LOO): fixed 587 labels. Loss 0.03008. Accuracy 1.000.
Using normal model
LBFGS training took [27] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0048052403
Train loss (w/o reg) on all data: 0.0019451012
Test loss (w/o reg) on all data: 0.0033373372
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2305057e-08
Norm of the params: 7.563253
                Loss: fixed 696 labels. Loss 0.00334. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32615462
Train loss (w/o reg) on all data: 0.3193653
Test loss (w/o reg) on all data: 0.14740777
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.2225136e-05
Norm of the params: 11.6527605
              Random: fixed 192 labels. Loss 0.14741. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40062472
Train loss (w/o reg) on all data: 0.39484265
Test loss (w/o reg) on all data: 0.22289282
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.2286677e-05
Norm of the params: 10.753663
Flipped loss: 0.22289. Accuracy: 0.970
### Flips: 820, rs: 30, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30905086
Train loss (w/o reg) on all data: 0.3004042
Test loss (w/o reg) on all data: 0.15997007
Train acc on all data:  0.8740578653051301
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.2226357e-05
Norm of the params: 13.150421
     Influence (LOO): fixed 181 labels. Loss 0.15997. Accuracy 0.982.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27370247
Train loss (w/o reg) on all data: 0.26057187
Test loss (w/o reg) on all data: 0.18078558
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 9.756128e-06
Norm of the params: 16.205309
                Loss: fixed 205 labels. Loss 0.18079. Accuracy 0.941.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38958836
Train loss (w/o reg) on all data: 0.38387185
Test loss (w/o reg) on all data: 0.21145636
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.868787e-06
Norm of the params: 10.692511
              Random: fixed  35 labels. Loss 0.21146. Accuracy 0.976.
### Flips: 820, rs: 30, checks: 410
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24633947
Train loss (w/o reg) on all data: 0.23671599
Test loss (w/o reg) on all data: 0.122435026
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3266144e-05
Norm of the params: 13.873347
     Influence (LOO): fixed 306 labels. Loss 0.12244. Accuracy 0.990.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1605055
Train loss (w/o reg) on all data: 0.14392942
Test loss (w/o reg) on all data: 0.120308965
Train acc on all data:  0.9380014587892049
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.9061783e-06
Norm of the params: 18.207731
                Loss: fixed 410 labels. Loss 0.12031. Accuracy 0.965.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3790398
Train loss (w/o reg) on all data: 0.37319285
Test loss (w/o reg) on all data: 0.2003182
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8807977e-05
Norm of the params: 10.813834
              Random: fixed  67 labels. Loss 0.20032. Accuracy 0.979.
### Flips: 820, rs: 30, checks: 615
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1998456
Train loss (w/o reg) on all data: 0.19057171
Test loss (w/o reg) on all data: 0.093933836
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.2597636e-06
Norm of the params: 13.619022
     Influence (LOO): fixed 396 labels. Loss 0.09393. Accuracy 0.997.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052682854
Train loss (w/o reg) on all data: 0.03931253
Test loss (w/o reg) on all data: 0.05783333
Train acc on all data:  0.9851689764162412
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.6237036e-06
Norm of the params: 16.352568
                Loss: fixed 611 labels. Loss 0.05783. Accuracy 0.984.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36623615
Train loss (w/o reg) on all data: 0.3605043
Test loss (w/o reg) on all data: 0.18863142
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.900097e-05
Norm of the params: 10.7068615
              Random: fixed 102 labels. Loss 0.18863. Accuracy 0.984.
### Flips: 820, rs: 30, checks: 820
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15381937
Train loss (w/o reg) on all data: 0.14487784
Test loss (w/o reg) on all data: 0.06599025
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1792981e-06
Norm of the params: 13.372761
     Influence (LOO): fixed 481 labels. Loss 0.06599. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0109256385
Train loss (w/o reg) on all data: 0.005325126
Test loss (w/o reg) on all data: 0.0078931
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.825486e-08
Norm of the params: 10.583489
                Loss: fixed 693 labels. Loss 0.00789. Accuracy 0.998.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35149887
Train loss (w/o reg) on all data: 0.34535787
Test loss (w/o reg) on all data: 0.17520817
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.178015e-06
Norm of the params: 11.082416
              Random: fixed 139 labels. Loss 0.17521. Accuracy 0.990.
### Flips: 820, rs: 30, checks: 1025
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11419422
Train loss (w/o reg) on all data: 0.10568968
Test loss (w/o reg) on all data: 0.046467815
Train acc on all data:  0.9606126914660832
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0862716e-06
Norm of the params: 13.041888
     Influence (LOO): fixed 545 labels. Loss 0.04647. Accuracy 0.996.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0054602213
Train loss (w/o reg) on all data: 0.002143149
Test loss (w/o reg) on all data: 0.004622577
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1436259e-07
Norm of the params: 8.145026
                Loss: fixed 703 labels. Loss 0.00462. Accuracy 0.999.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3406042
Train loss (w/o reg) on all data: 0.3345423
Test loss (w/o reg) on all data: 0.16479583
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.4865055e-05
Norm of the params: 11.010794
              Random: fixed 167 labels. Loss 0.16480. Accuracy 0.988.
### Flips: 820, rs: 30, checks: 1230
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088264346
Train loss (w/o reg) on all data: 0.08082959
Test loss (w/o reg) on all data: 0.03390305
Train acc on all data:  0.9715536105032823
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.6539954e-06
Norm of the params: 12.194063
     Influence (LOO): fixed 588 labels. Loss 0.03390. Accuracy 0.998.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0041835243
Train loss (w/o reg) on all data: 0.0015734275
Test loss (w/o reg) on all data: 0.0037405386
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.7916565e-08
Norm of the params: 7.225091
                Loss: fixed 706 labels. Loss 0.00374. Accuracy 0.999.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32629347
Train loss (w/o reg) on all data: 0.32021075
Test loss (w/o reg) on all data: 0.15456054
Train acc on all data:  0.8786773644541697
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.316518e-05
Norm of the params: 11.029697
              Random: fixed 202 labels. Loss 0.15456. Accuracy 0.989.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40637156
Train loss (w/o reg) on all data: 0.40027153
Test loss (w/o reg) on all data: 0.21978049
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.3970536e-05
Norm of the params: 11.045387
Flipped loss: 0.21978. Accuracy: 0.977
### Flips: 820, rs: 31, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31967643
Train loss (w/o reg) on all data: 0.310302
Test loss (w/o reg) on all data: 0.15931721
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.9643017e-05
Norm of the params: 13.692663
     Influence (LOO): fixed 181 labels. Loss 0.15932. Accuracy 0.993.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28342927
Train loss (w/o reg) on all data: 0.27030626
Test loss (w/o reg) on all data: 0.17452402
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.6144542e-05
Norm of the params: 16.20062
                Loss: fixed 205 labels. Loss 0.17452. Accuracy 0.954.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3954835
Train loss (w/o reg) on all data: 0.38953242
Test loss (w/o reg) on all data: 0.20555207
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 8.404702e-06
Norm of the params: 10.909698
              Random: fixed  38 labels. Loss 0.20555. Accuracy 0.981.
### Flips: 820, rs: 31, checks: 410
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253603
Train loss (w/o reg) on all data: 0.24298534
Test loss (w/o reg) on all data: 0.11966473
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3302216e-05
Norm of the params: 14.572359
     Influence (LOO): fixed 313 labels. Loss 0.11966. Accuracy 0.993.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16550305
Train loss (w/o reg) on all data: 0.14684461
Test loss (w/o reg) on all data: 0.11627027
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.56589e-06
Norm of the params: 19.31758
                Loss: fixed 410 labels. Loss 0.11627. Accuracy 0.966.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38518715
Train loss (w/o reg) on all data: 0.37917536
Test loss (w/o reg) on all data: 0.19517204
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.9107294e-05
Norm of the params: 10.965217
              Random: fixed  71 labels. Loss 0.19517. Accuracy 0.984.
### Flips: 820, rs: 31, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1992456
Train loss (w/o reg) on all data: 0.18988344
Test loss (w/o reg) on all data: 0.0883834
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.0398114e-06
Norm of the params: 13.68369
     Influence (LOO): fixed 419 labels. Loss 0.08838. Accuracy 0.996.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061048355
Train loss (w/o reg) on all data: 0.044810127
Test loss (w/o reg) on all data: 0.039620634
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.326517e-06
Norm of the params: 18.021225
                Loss: fixed 611 labels. Loss 0.03962. Accuracy 0.991.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37254542
Train loss (w/o reg) on all data: 0.36630562
Test loss (w/o reg) on all data: 0.1861288
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.2381703e-05
Norm of the params: 11.171203
              Random: fixed 104 labels. Loss 0.18613. Accuracy 0.983.
### Flips: 820, rs: 31, checks: 820
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16123523
Train loss (w/o reg) on all data: 0.15193248
Test loss (w/o reg) on all data: 0.067231305
Train acc on all data:  0.9423778264040846
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.163836e-05
Norm of the params: 13.640199
     Influence (LOO): fixed 486 labels. Loss 0.06723. Accuracy 0.998.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011113446
Train loss (w/o reg) on all data: 0.0054620993
Test loss (w/o reg) on all data: 0.005556151
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.16586556e-07
Norm of the params: 10.6314125
                Loss: fixed 709 labels. Loss 0.00556. Accuracy 0.999.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.362927
Train loss (w/o reg) on all data: 0.35662758
Test loss (w/o reg) on all data: 0.17962363
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 6.76105e-06
Norm of the params: 11.224441
              Random: fixed 129 labels. Loss 0.17962. Accuracy 0.983.
### Flips: 820, rs: 31, checks: 1025
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12108483
Train loss (w/o reg) on all data: 0.11305589
Test loss (w/o reg) on all data: 0.049946073
Train acc on all data:  0.9581813761244834
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.4854066e-06
Norm of the params: 12.671967
     Influence (LOO): fixed 552 labels. Loss 0.04995. Accuracy 0.999.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006175466
Train loss (w/o reg) on all data: 0.002668193
Test loss (w/o reg) on all data: 0.004866314
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.6692595e-08
Norm of the params: 8.375289
                Loss: fixed 719 labels. Loss 0.00487. Accuracy 0.998.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34673473
Train loss (w/o reg) on all data: 0.3402372
Test loss (w/o reg) on all data: 0.16856325
Train acc on all data:  0.8672501823486506
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.0392019e-05
Norm of the params: 11.3995905
              Random: fixed 167 labels. Loss 0.16856. Accuracy 0.985.
### Flips: 820, rs: 31, checks: 1230
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09022911
Train loss (w/o reg) on all data: 0.082980506
Test loss (w/o reg) on all data: 0.035303365
Train acc on all data:  0.9700948212983224
Test acc on all data:   1.0
Norm of the mean of gradients: 7.570454e-06
Norm of the params: 12.040437
     Influence (LOO): fixed 601 labels. Loss 0.03530. Accuracy 1.000.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0035895973
Train loss (w/o reg) on all data: 0.0012758318
Test loss (w/o reg) on all data: 0.0026635204
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0276072e-08
Norm of the params: 6.802596
                Loss: fixed 723 labels. Loss 0.00266. Accuracy 1.000.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32798448
Train loss (w/o reg) on all data: 0.3211203
Test loss (w/o reg) on all data: 0.15513453
Train acc on all data:  0.8781911013858498
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.207818e-06
Norm of the params: 11.716814
              Random: fixed 211 labels. Loss 0.15513. Accuracy 0.987.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40261558
Train loss (w/o reg) on all data: 0.39770135
Test loss (w/o reg) on all data: 0.21935385
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.1149884e-05
Norm of the params: 9.913851
Flipped loss: 0.21935. Accuracy: 0.977
### Flips: 820, rs: 32, checks: 205
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31395695
Train loss (w/o reg) on all data: 0.30465052
Test loss (w/o reg) on all data: 0.1557097
Train acc on all data:  0.8762460491125699
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.15184e-05
Norm of the params: 13.642888
     Influence (LOO): fixed 178 labels. Loss 0.15571. Accuracy 0.988.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2762543
Train loss (w/o reg) on all data: 0.26504436
Test loss (w/o reg) on all data: 0.17548336
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.502933e-06
Norm of the params: 14.973261
                Loss: fixed 205 labels. Loss 0.17548. Accuracy 0.954.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38658413
Train loss (w/o reg) on all data: 0.3815102
Test loss (w/o reg) on all data: 0.20374785
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.757585e-05
Norm of the params: 10.073674
              Random: fixed  45 labels. Loss 0.20375. Accuracy 0.979.
### Flips: 820, rs: 32, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25205013
Train loss (w/o reg) on all data: 0.24223965
Test loss (w/o reg) on all data: 0.1175274
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.7573064e-06
Norm of the params: 14.007483
     Influence (LOO): fixed 305 labels. Loss 0.11753. Accuracy 0.999.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15474333
Train loss (w/o reg) on all data: 0.1392087
Test loss (w/o reg) on all data: 0.121727705
Train acc on all data:  0.9409190371991247
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.0636868e-05
Norm of the params: 17.626474
                Loss: fixed 410 labels. Loss 0.12173. Accuracy 0.961.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37222937
Train loss (w/o reg) on all data: 0.36673445
Test loss (w/o reg) on all data: 0.1903659
Train acc on all data:  0.8516897641624118
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.2856498e-05
Norm of the params: 10.483243
              Random: fixed  83 labels. Loss 0.19037. Accuracy 0.982.
### Flips: 820, rs: 32, checks: 615
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19828226
Train loss (w/o reg) on all data: 0.18865483
Test loss (w/o reg) on all data: 0.08847158
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.12097e-06
Norm of the params: 13.876189
     Influence (LOO): fixed 403 labels. Loss 0.08847. Accuracy 0.997.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05069957
Train loss (w/o reg) on all data: 0.036537524
Test loss (w/o reg) on all data: 0.045833744
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3143382e-06
Norm of the params: 16.829762
                Loss: fixed 613 labels. Loss 0.04583. Accuracy 0.988.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35704654
Train loss (w/o reg) on all data: 0.3513715
Test loss (w/o reg) on all data: 0.1780255
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.1406593e-05
Norm of the params: 10.653696
              Random: fixed 122 labels. Loss 0.17803. Accuracy 0.985.
### Flips: 820, rs: 32, checks: 820
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15265894
Train loss (w/o reg) on all data: 0.14398636
Test loss (w/o reg) on all data: 0.062784076
Train acc on all data:  0.9479698516897641
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.629816e-06
Norm of the params: 13.170104
     Influence (LOO): fixed 486 labels. Loss 0.06278. Accuracy 0.998.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0079300525
Train loss (w/o reg) on all data: 0.0033574759
Test loss (w/o reg) on all data: 0.005694498
Train acc on all data:  0.99951373693168
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.589344e-08
Norm of the params: 9.563029
                Loss: fixed 695 labels. Loss 0.00569. Accuracy 0.999.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34206703
Train loss (w/o reg) on all data: 0.3360243
Test loss (w/o reg) on all data: 0.16878225
Train acc on all data:  0.8689521030877705
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.014422e-06
Norm of the params: 10.993383
              Random: fixed 157 labels. Loss 0.16878. Accuracy 0.982.
### Flips: 820, rs: 32, checks: 1025
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12141559
Train loss (w/o reg) on all data: 0.11376064
Test loss (w/o reg) on all data: 0.045623414
Train acc on all data:  0.9598832968636032
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.9106183e-06
Norm of the params: 12.373321
     Influence (LOO): fixed 539 labels. Loss 0.04562. Accuracy 0.999.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0060059526
Train loss (w/o reg) on all data: 0.0023098753
Test loss (w/o reg) on all data: 0.0036200483
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3478254e-07
Norm of the params: 8.597764
                Loss: fixed 699 labels. Loss 0.00362. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32911873
Train loss (w/o reg) on all data: 0.3230706
Test loss (w/o reg) on all data: 0.15914066
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.9668325e-05
Norm of the params: 10.998317
              Random: fixed 188 labels. Loss 0.15914. Accuracy 0.983.
### Flips: 820, rs: 32, checks: 1230
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883145
Train loss (w/o reg) on all data: 0.08120137
Test loss (w/o reg) on all data: 0.031103473
Train acc on all data:  0.9730123997082422
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.992483e-06
Norm of the params: 11.927397
     Influence (LOO): fixed 592 labels. Loss 0.03110. Accuracy 0.999.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0056980783
Train loss (w/o reg) on all data: 0.002165936
Test loss (w/o reg) on all data: 0.0034507115
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 9.100544e-08
Norm of the params: 8.40493
                Loss: fixed 700 labels. Loss 0.00345. Accuracy 1.000.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31577873
Train loss (w/o reg) on all data: 0.30971327
Test loss (w/o reg) on all data: 0.14588454
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.601467e-05
Norm of the params: 11.014031
              Random: fixed 223 labels. Loss 0.14588. Accuracy 0.985.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4088102
Train loss (w/o reg) on all data: 0.40330106
Test loss (w/o reg) on all data: 0.21222241
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.08839e-05
Norm of the params: 10.496808
Flipped loss: 0.21222. Accuracy: 0.987
### Flips: 820, rs: 33, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31869057
Train loss (w/o reg) on all data: 0.3091973
Test loss (w/o reg) on all data: 0.15411483
Train acc on all data:  0.8713834184293703
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.666763e-06
Norm of the params: 13.77915
     Influence (LOO): fixed 175 labels. Loss 0.15411. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2845409
Train loss (w/o reg) on all data: 0.27225184
Test loss (w/o reg) on all data: 0.15526368
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9400159e-05
Norm of the params: 15.677402
                Loss: fixed 205 labels. Loss 0.15526. Accuracy 0.966.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4000377
Train loss (w/o reg) on all data: 0.39428967
Test loss (w/o reg) on all data: 0.20423067
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4521075e-05
Norm of the params: 10.72197
              Random: fixed  27 labels. Loss 0.20423. Accuracy 0.985.
### Flips: 820, rs: 33, checks: 410
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252048
Train loss (w/o reg) on all data: 0.24186446
Test loss (w/o reg) on all data: 0.11424182
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 7.070677e-06
Norm of the params: 14.271318
     Influence (LOO): fixed 309 labels. Loss 0.11424. Accuracy 0.995.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17179316
Train loss (w/o reg) on all data: 0.15579809
Test loss (w/o reg) on all data: 0.10413598
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 6.557993e-06
Norm of the params: 17.885792
                Loss: fixed 410 labels. Loss 0.10414. Accuracy 0.969.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3878215
Train loss (w/o reg) on all data: 0.38201594
Test loss (w/o reg) on all data: 0.19127624
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.5089179e-05
Norm of the params: 10.775483
              Random: fixed  63 labels. Loss 0.19128. Accuracy 0.990.
### Flips: 820, rs: 33, checks: 615
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20101482
Train loss (w/o reg) on all data: 0.1913785
Test loss (w/o reg) on all data: 0.08668224
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.9493383e-06
Norm of the params: 13.882585
     Influence (LOO): fixed 405 labels. Loss 0.08668. Accuracy 0.999.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059740465
Train loss (w/o reg) on all data: 0.044652835
Test loss (w/o reg) on all data: 0.04116404
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 3.641149e-06
Norm of the params: 17.371029
                Loss: fixed 611 labels. Loss 0.04116. Accuracy 0.990.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37350816
Train loss (w/o reg) on all data: 0.36773574
Test loss (w/o reg) on all data: 0.17547145
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.7636152e-05
Norm of the params: 10.744689
              Random: fixed 107 labels. Loss 0.17547. Accuracy 0.992.
### Flips: 820, rs: 33, checks: 820
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15995939
Train loss (w/o reg) on all data: 0.15068078
Test loss (w/o reg) on all data: 0.06649494
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0929996e-05
Norm of the params: 13.622488
     Influence (LOO): fixed 479 labels. Loss 0.06649. Accuracy 0.999.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01032252
Train loss (w/o reg) on all data: 0.0053780815
Test loss (w/o reg) on all data: 0.0059444676
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.512536e-07
Norm of the params: 9.944283
                Loss: fixed 706 labels. Loss 0.00594. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35765907
Train loss (w/o reg) on all data: 0.35145813
Test loss (w/o reg) on all data: 0.16521667
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.3547716e-05
Norm of the params: 11.136368
              Random: fixed 143 labels. Loss 0.16522. Accuracy 0.991.
### Flips: 820, rs: 33, checks: 1025
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12365204
Train loss (w/o reg) on all data: 0.11544994
Test loss (w/o reg) on all data: 0.052226014
Train acc on all data:  0.9567225869195235
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5799052e-06
Norm of the params: 12.80789
     Influence (LOO): fixed 538 labels. Loss 0.05223. Accuracy 1.000.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0051760897
Train loss (w/o reg) on all data: 0.002005174
Test loss (w/o reg) on all data: 0.0035210452
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2672966e-08
Norm of the params: 7.963562
                Loss: fixed 713 labels. Loss 0.00352. Accuracy 1.000.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34580436
Train loss (w/o reg) on all data: 0.33949324
Test loss (w/o reg) on all data: 0.1540623
Train acc on all data:  0.8716265499635303
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.6037047e-05
Norm of the params: 11.234869
              Random: fixed 176 labels. Loss 0.15406. Accuracy 0.993.
### Flips: 820, rs: 33, checks: 1230
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096146
Train loss (w/o reg) on all data: 0.08832339
Test loss (w/o reg) on all data: 0.039341606
Train acc on all data:  0.9669341113542427
Test acc on all data:   1.0
Norm of the mean of gradients: 9.300795e-06
Norm of the params: 12.508088
     Influence (LOO): fixed 582 labels. Loss 0.03934. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004452096
Train loss (w/o reg) on all data: 0.0016603386
Test loss (w/o reg) on all data: 0.0032096242
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 5.566128e-08
Norm of the params: 7.472292
                Loss: fixed 714 labels. Loss 0.00321. Accuracy 1.000.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33031067
Train loss (w/o reg) on all data: 0.32372147
Test loss (w/o reg) on all data: 0.14415304
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.8744755e-06
Norm of the params: 11.479727
              Random: fixed 211 labels. Loss 0.14415. Accuracy 0.993.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40795308
Train loss (w/o reg) on all data: 0.40212476
Test loss (w/o reg) on all data: 0.21758664
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.622394e-05
Norm of the params: 10.79658
Flipped loss: 0.21759. Accuracy: 0.986
### Flips: 820, rs: 34, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31301916
Train loss (w/o reg) on all data: 0.3033827
Test loss (w/o reg) on all data: 0.15595256
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.342289e-05
Norm of the params: 13.882698
     Influence (LOO): fixed 188 labels. Loss 0.15595. Accuracy 0.993.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2828678
Train loss (w/o reg) on all data: 0.2697749
Test loss (w/o reg) on all data: 0.17200066
Train acc on all data:  0.8798930221249696
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 5.4311377e-06
Norm of the params: 16.182013
                Loss: fixed 205 labels. Loss 0.17200. Accuracy 0.954.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39343107
Train loss (w/o reg) on all data: 0.3878148
Test loss (w/o reg) on all data: 0.2035317
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 4.544178e-05
Norm of the params: 10.598364
              Random: fixed  48 labels. Loss 0.20353. Accuracy 0.985.
### Flips: 820, rs: 34, checks: 410
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25067478
Train loss (w/o reg) on all data: 0.24066523
Test loss (w/o reg) on all data: 0.115240134
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.4603567e-05
Norm of the params: 14.148891
     Influence (LOO): fixed 310 labels. Loss 0.11524. Accuracy 0.997.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17065002
Train loss (w/o reg) on all data: 0.15359944
Test loss (w/o reg) on all data: 0.10866206
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.2756491e-05
Norm of the params: 18.466497
                Loss: fixed 410 labels. Loss 0.10866. Accuracy 0.968.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38316235
Train loss (w/o reg) on all data: 0.37746596
Test loss (w/o reg) on all data: 0.19289748
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.624824e-05
Norm of the params: 10.673682
              Random: fixed  79 labels. Loss 0.19290. Accuracy 0.988.
### Flips: 820, rs: 34, checks: 615
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19744392
Train loss (w/o reg) on all data: 0.18711211
Test loss (w/o reg) on all data: 0.08848267
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.925351e-06
Norm of the params: 14.374844
     Influence (LOO): fixed 404 labels. Loss 0.08848. Accuracy 0.995.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05959459
Train loss (w/o reg) on all data: 0.044033002
Test loss (w/o reg) on all data: 0.036881097
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.8342153e-06
Norm of the params: 17.641764
                Loss: fixed 615 labels. Loss 0.03688. Accuracy 0.989.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37169108
Train loss (w/o reg) on all data: 0.36607292
Test loss (w/o reg) on all data: 0.18318264
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.4256027e-05
Norm of the params: 10.600133
              Random: fixed 113 labels. Loss 0.18318. Accuracy 0.988.
### Flips: 820, rs: 34, checks: 820
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15339363
Train loss (w/o reg) on all data: 0.14339915
Test loss (w/o reg) on all data: 0.06514702
Train acc on all data:  0.9431072210065645
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.2481956e-06
Norm of the params: 14.138229
     Influence (LOO): fixed 484 labels. Loss 0.06515. Accuracy 0.997.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008913564
Train loss (w/o reg) on all data: 0.004111605
Test loss (w/o reg) on all data: 0.0044506504
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 5.691635e-07
Norm of the params: 9.799957
                Loss: fixed 709 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35864973
Train loss (w/o reg) on all data: 0.3529573
Test loss (w/o reg) on all data: 0.171109
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.0482916e-05
Norm of the params: 10.669983
              Random: fixed 146 labels. Loss 0.17111. Accuracy 0.989.
### Flips: 820, rs: 34, checks: 1025
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1151439
Train loss (w/o reg) on all data: 0.10575992
Test loss (w/o reg) on all data: 0.04692836
Train acc on all data:  0.9591539022611233
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.9072037e-06
Norm of the params: 13.699625
     Influence (LOO): fixed 545 labels. Loss 0.04693. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065856846
Train loss (w/o reg) on all data: 0.0029697318
Test loss (w/o reg) on all data: 0.0039367615
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 9.759016e-08
Norm of the params: 8.504061
                Loss: fixed 713 labels. Loss 0.00394. Accuracy 1.000.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34460178
Train loss (w/o reg) on all data: 0.3387728
Test loss (w/o reg) on all data: 0.16153184
Train acc on all data:  0.8723559445660102
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2538723e-05
Norm of the params: 10.7971945
              Random: fixed 181 labels. Loss 0.16153. Accuracy 0.991.
### Flips: 820, rs: 34, checks: 1230
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09841027
Train loss (w/o reg) on all data: 0.08965059
Test loss (w/o reg) on all data: 0.040125586
Train acc on all data:  0.9657184536834428
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.7287904e-06
Norm of the params: 13.236071
     Influence (LOO): fixed 573 labels. Loss 0.04013. Accuracy 0.998.
Using normal model
LBFGS training took [26] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0037254128
Train loss (w/o reg) on all data: 0.0014285754
Test loss (w/o reg) on all data: 0.0032918197
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 4.98379e-08
Norm of the params: 6.7776656
                Loss: fixed 716 labels. Loss 0.00329. Accuracy 1.000.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3296639
Train loss (w/o reg) on all data: 0.32362148
Test loss (w/o reg) on all data: 0.14887537
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.1473244e-05
Norm of the params: 10.993118
              Random: fixed 216 labels. Loss 0.14888. Accuracy 0.991.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4078686
Train loss (w/o reg) on all data: 0.40185556
Test loss (w/o reg) on all data: 0.22116934
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.8490595e-05
Norm of the params: 10.966348
Flipped loss: 0.22117. Accuracy: 0.979
### Flips: 820, rs: 35, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3206586
Train loss (w/o reg) on all data: 0.31081513
Test loss (w/o reg) on all data: 0.16563676
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 6.809471e-06
Norm of the params: 14.031008
     Influence (LOO): fixed 174 labels. Loss 0.16564. Accuracy 0.989.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2861131
Train loss (w/o reg) on all data: 0.2731375
Test loss (w/o reg) on all data: 0.17001554
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 8.950845e-06
Norm of the params: 16.109377
                Loss: fixed 205 labels. Loss 0.17002. Accuracy 0.963.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39544284
Train loss (w/o reg) on all data: 0.3892165
Test loss (w/o reg) on all data: 0.21004944
Train acc on all data:  0.8373450036469731
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.0893659e-05
Norm of the params: 11.1591625
              Random: fixed  35 labels. Loss 0.21005. Accuracy 0.979.
### Flips: 820, rs: 35, checks: 410
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2577966
Train loss (w/o reg) on all data: 0.24822079
Test loss (w/o reg) on all data: 0.12161475
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0277017e-05
Norm of the params: 13.838938
     Influence (LOO): fixed 310 labels. Loss 0.12161. Accuracy 0.993.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1735403
Train loss (w/o reg) on all data: 0.15551256
Test loss (w/o reg) on all data: 0.11352164
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.1363021e-05
Norm of the params: 18.98828
                Loss: fixed 410 labels. Loss 0.11352. Accuracy 0.965.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3823439
Train loss (w/o reg) on all data: 0.37577885
Test loss (w/o reg) on all data: 0.19886371
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.947788e-05
Norm of the params: 11.458645
              Random: fixed  70 labels. Loss 0.19886. Accuracy 0.979.
### Flips: 820, rs: 35, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1982506
Train loss (w/o reg) on all data: 0.18865554
Test loss (w/o reg) on all data: 0.08826081
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.766773e-06
Norm of the params: 13.852848
     Influence (LOO): fixed 423 labels. Loss 0.08826. Accuracy 0.996.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063783765
Train loss (w/o reg) on all data: 0.047294874
Test loss (w/o reg) on all data: 0.04089194
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.7049585e-06
Norm of the params: 18.159788
                Loss: fixed 612 labels. Loss 0.04089. Accuracy 0.985.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36768445
Train loss (w/o reg) on all data: 0.3611076
Test loss (w/o reg) on all data: 0.18698408
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.422113e-05
Norm of the params: 11.468968
              Random: fixed 112 labels. Loss 0.18698. Accuracy 0.983.
### Flips: 820, rs: 35, checks: 820
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15917729
Train loss (w/o reg) on all data: 0.15107334
Test loss (w/o reg) on all data: 0.06287981
Train acc on all data:  0.9443228786773644
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9213363e-06
Norm of the params: 12.7310295
     Influence (LOO): fixed 497 labels. Loss 0.06288. Accuracy 1.000.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011982882
Train loss (w/o reg) on all data: 0.0057522473
Test loss (w/o reg) on all data: 0.0057809995
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 3.648443e-07
Norm of the params: 11.163007
                Loss: fixed 709 labels. Loss 0.00578. Accuracy 1.000.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35476783
Train loss (w/o reg) on all data: 0.34836057
Test loss (w/o reg) on all data: 0.17490916
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 5.9893013e-05
Norm of the params: 11.320113
              Random: fixed 146 labels. Loss 0.17491. Accuracy 0.986.
### Flips: 820, rs: 35, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11977202
Train loss (w/o reg) on all data: 0.11232134
Test loss (w/o reg) on all data: 0.045373812
Train acc on all data:  0.9606126914660832
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5929522e-06
Norm of the params: 12.207109
     Influence (LOO): fixed 562 labels. Loss 0.04537. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00810018
Train loss (w/o reg) on all data: 0.003644195
Test loss (w/o reg) on all data: 0.0045579267
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8023566e-07
Norm of the params: 9.440324
                Loss: fixed 717 labels. Loss 0.00456. Accuracy 1.000.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34030828
Train loss (w/o reg) on all data: 0.33336726
Test loss (w/o reg) on all data: 0.165776
Train acc on all data:  0.8718696814976903
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.2811789e-05
Norm of the params: 11.782213
              Random: fixed 177 labels. Loss 0.16578. Accuracy 0.985.
### Flips: 820, rs: 35, checks: 1230
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09193147
Train loss (w/o reg) on all data: 0.085053004
Test loss (w/o reg) on all data: 0.034508582
Train acc on all data:  0.9713104789691223
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7517887e-06
Norm of the params: 11.728992
     Influence (LOO): fixed 605 labels. Loss 0.03451. Accuracy 1.000.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0061537195
Train loss (w/o reg) on all data: 0.0025956316
Test loss (w/o reg) on all data: 0.0036159747
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4376636e-08
Norm of the params: 8.435743
                Loss: fixed 720 labels. Loss 0.00362. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3284105
Train loss (w/o reg) on all data: 0.3212316
Test loss (w/o reg) on all data: 0.15800641
Train acc on all data:  0.8774617067833698
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.840335e-05
Norm of the params: 11.982407
              Random: fixed 204 labels. Loss 0.15801. Accuracy 0.985.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40528017
Train loss (w/o reg) on all data: 0.39920887
Test loss (w/o reg) on all data: 0.22684105
Train acc on all data:  0.8302941891563336
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.5314801e-05
Norm of the params: 11.019359
Flipped loss: 0.22684. Accuracy: 0.973
### Flips: 820, rs: 36, checks: 205
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3106071
Train loss (w/o reg) on all data: 0.30146173
Test loss (w/o reg) on all data: 0.16601664
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3794364e-05
Norm of the params: 13.524338
     Influence (LOO): fixed 183 labels. Loss 0.16602. Accuracy 0.985.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27948415
Train loss (w/o reg) on all data: 0.26763198
Test loss (w/o reg) on all data: 0.17666255
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 6.0045886e-06
Norm of the params: 15.396219
                Loss: fixed 205 labels. Loss 0.17666. Accuracy 0.954.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39283663
Train loss (w/o reg) on all data: 0.38686442
Test loss (w/o reg) on all data: 0.21306524
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.781702e-05
Norm of the params: 10.929037
              Random: fixed  38 labels. Loss 0.21307. Accuracy 0.975.
### Flips: 820, rs: 36, checks: 410
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24353388
Train loss (w/o reg) on all data: 0.233591
Test loss (w/o reg) on all data: 0.11999946
Train acc on all data:  0.9044493070751276
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.972899e-06
Norm of the params: 14.10168
     Influence (LOO): fixed 316 labels. Loss 0.12000. Accuracy 0.997.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16430748
Train loss (w/o reg) on all data: 0.14856686
Test loss (w/o reg) on all data: 0.12996113
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.6181215e-06
Norm of the params: 17.74295
                Loss: fixed 409 labels. Loss 0.12996. Accuracy 0.962.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38363
Train loss (w/o reg) on all data: 0.37736884
Test loss (w/o reg) on all data: 0.20362753
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.1067346e-05
Norm of the params: 11.190332
              Random: fixed  65 labels. Loss 0.20363. Accuracy 0.978.
### Flips: 820, rs: 36, checks: 615
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19893713
Train loss (w/o reg) on all data: 0.18980576
Test loss (w/o reg) on all data: 0.091947414
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0278859e-05
Norm of the params: 13.51397
     Influence (LOO): fixed 403 labels. Loss 0.09195. Accuracy 0.997.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056316428
Train loss (w/o reg) on all data: 0.042913802
Test loss (w/o reg) on all data: 0.045346897
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.392657e-06
Norm of the params: 16.37231
                Loss: fixed 611 labels. Loss 0.04535. Accuracy 0.983.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37367576
Train loss (w/o reg) on all data: 0.36736777
Test loss (w/o reg) on all data: 0.19288045
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.792745e-05
Norm of the params: 11.232076
              Random: fixed  94 labels. Loss 0.19288. Accuracy 0.983.
### Flips: 820, rs: 36, checks: 820
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15496555
Train loss (w/o reg) on all data: 0.14601691
Test loss (w/o reg) on all data: 0.064815156
Train acc on all data:  0.9438366156090445
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.999264e-06
Norm of the params: 13.378068
     Influence (LOO): fixed 484 labels. Loss 0.06482. Accuracy 0.999.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013259575
Train loss (w/o reg) on all data: 0.006473471
Test loss (w/o reg) on all data: 0.00637928
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.0976115e-07
Norm of the params: 11.649982
                Loss: fixed 697 labels. Loss 0.00638. Accuracy 0.998.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35886854
Train loss (w/o reg) on all data: 0.35232762
Test loss (w/o reg) on all data: 0.17964906
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 4.041893e-05
Norm of the params: 11.4376
              Random: fixed 133 labels. Loss 0.17965. Accuracy 0.986.
### Flips: 820, rs: 36, checks: 1025
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12704495
Train loss (w/o reg) on all data: 0.1181654
Test loss (w/o reg) on all data: 0.051736612
Train acc on all data:  0.9547775346462436
Test acc on all data:   1.0
Norm of the mean of gradients: 2.9011119e-06
Norm of the params: 13.326325
     Influence (LOO): fixed 529 labels. Loss 0.05174. Accuracy 1.000.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00912368
Train loss (w/o reg) on all data: 0.004330108
Test loss (w/o reg) on all data: 0.0040136673
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 4.598539e-07
Norm of the params: 9.791396
                Loss: fixed 704 labels. Loss 0.00401. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34487033
Train loss (w/o reg) on all data: 0.33780882
Test loss (w/o reg) on all data: 0.16759127
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.0147123e-05
Norm of the params: 11.884025
              Random: fixed 169 labels. Loss 0.16759. Accuracy 0.989.
### Flips: 820, rs: 36, checks: 1230
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0965671
Train loss (w/o reg) on all data: 0.08877853
Test loss (w/o reg) on all data: 0.038329348
Train acc on all data:  0.9662047167517627
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.1378338e-06
Norm of the params: 12.480841
     Influence (LOO): fixed 578 labels. Loss 0.03833. Accuracy 0.999.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0055775736
Train loss (w/o reg) on all data: 0.0024535896
Test loss (w/o reg) on all data: 0.0031448407
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.9245353e-07
Norm of the params: 7.9044094
                Loss: fixed 709 labels. Loss 0.00314. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33251882
Train loss (w/o reg) on all data: 0.32554078
Test loss (w/o reg) on all data: 0.15658066
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.334796e-06
Norm of the params: 11.813572
              Random: fixed 203 labels. Loss 0.15658. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40272722
Train loss (w/o reg) on all data: 0.39666718
Test loss (w/o reg) on all data: 0.21326882
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1316731e-05
Norm of the params: 11.009108
Flipped loss: 0.21327. Accuracy: 0.982
### Flips: 820, rs: 37, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31260705
Train loss (w/o reg) on all data: 0.30339798
Test loss (w/o reg) on all data: 0.1590489
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.3116753e-06
Norm of the params: 13.571351
     Influence (LOO): fixed 180 labels. Loss 0.15905. Accuracy 0.982.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27644908
Train loss (w/o reg) on all data: 0.26340187
Test loss (w/o reg) on all data: 0.16026819
Train acc on all data:  0.888402625820569
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.6980797e-05
Norm of the params: 16.153765
                Loss: fixed 205 labels. Loss 0.16027. Accuracy 0.962.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39153358
Train loss (w/o reg) on all data: 0.3853967
Test loss (w/o reg) on all data: 0.20204982
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.6830749e-05
Norm of the params: 11.078708
              Random: fixed  34 labels. Loss 0.20205. Accuracy 0.983.
### Flips: 820, rs: 37, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24104473
Train loss (w/o reg) on all data: 0.2306856
Test loss (w/o reg) on all data: 0.11519419
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.84462e-05
Norm of the params: 14.393836
     Influence (LOO): fixed 319 labels. Loss 0.11519. Accuracy 0.990.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16167484
Train loss (w/o reg) on all data: 0.14265458
Test loss (w/o reg) on all data: 0.10215914
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.6170993e-06
Norm of the params: 19.503979
                Loss: fixed 410 labels. Loss 0.10216. Accuracy 0.970.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3768929
Train loss (w/o reg) on all data: 0.37077212
Test loss (w/o reg) on all data: 0.18738914
Train acc on all data:  0.8529054218332117
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.4833293e-05
Norm of the params: 11.064152
              Random: fixed  75 labels. Loss 0.18739. Accuracy 0.986.
### Flips: 820, rs: 37, checks: 615
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19201677
Train loss (w/o reg) on all data: 0.18170151
Test loss (w/o reg) on all data: 0.07960233
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.2296144e-05
Norm of the params: 14.363322
     Influence (LOO): fixed 416 labels. Loss 0.07960. Accuracy 0.997.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05396717
Train loss (w/o reg) on all data: 0.03832447
Test loss (w/o reg) on all data: 0.033031832
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5132854e-06
Norm of the params: 17.687677
                Loss: fixed 612 labels. Loss 0.03303. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3635261
Train loss (w/o reg) on all data: 0.35742968
Test loss (w/o reg) on all data: 0.17731659
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.5212548e-05
Norm of the params: 11.042115
              Random: fixed 113 labels. Loss 0.17732. Accuracy 0.986.
### Flips: 820, rs: 37, checks: 820
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15415622
Train loss (w/o reg) on all data: 0.144814
Test loss (w/o reg) on all data: 0.058998317
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.659751e-06
Norm of the params: 13.669108
     Influence (LOO): fixed 486 labels. Loss 0.05900. Accuracy 0.998.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010335671
Train loss (w/o reg) on all data: 0.00482343
Test loss (w/o reg) on all data: 0.0052164546
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3479644e-07
Norm of the params: 10.499754
                Loss: fixed 698 labels. Loss 0.00522. Accuracy 1.000.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35231042
Train loss (w/o reg) on all data: 0.34600133
Test loss (w/o reg) on all data: 0.168428
Train acc on all data:  0.8682227084852906
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.8358863e-05
Norm of the params: 11.233074
              Random: fixed 143 labels. Loss 0.16843. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 1025
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11638697
Train loss (w/o reg) on all data: 0.107702546
Test loss (w/o reg) on all data: 0.04372216
Train acc on all data:  0.9603695599319232
Test acc on all data:   1.0
Norm of the mean of gradients: 8.942919e-06
Norm of the params: 13.179097
     Influence (LOO): fixed 544 labels. Loss 0.04372. Accuracy 1.000.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074169827
Train loss (w/o reg) on all data: 0.0032096147
Test loss (w/o reg) on all data: 0.0041665393
Train acc on all data:  0.9992706053975201
Test acc on all data:   1.0
Norm of the mean of gradients: 1.327264e-07
Norm of the params: 9.173187
                Loss: fixed 702 labels. Loss 0.00417. Accuracy 1.000.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3357873
Train loss (w/o reg) on all data: 0.32937667
Test loss (w/o reg) on all data: 0.15693042
Train acc on all data:  0.8772185752492099
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3532325e-05
Norm of the params: 11.323092
              Random: fixed 182 labels. Loss 0.15693. Accuracy 0.989.
### Flips: 820, rs: 37, checks: 1230
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08253237
Train loss (w/o reg) on all data: 0.07503556
Test loss (w/o reg) on all data: 0.031308755
Train acc on all data:  0.9734986627765622
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.8390405e-06
Norm of the params: 12.244847
     Influence (LOO): fixed 597 labels. Loss 0.03131. Accuracy 0.998.
Using normal model
LBFGS training took [29] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005600677
Train loss (w/o reg) on all data: 0.00219801
Test loss (w/o reg) on all data: 0.0038119256
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 2.2595823e-07
Norm of the params: 8.249445
                Loss: fixed 705 labels. Loss 0.00381. Accuracy 1.000.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31682518
Train loss (w/o reg) on all data: 0.309851
Test loss (w/o reg) on all data: 0.14383803
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.4086454e-06
Norm of the params: 11.810322
              Random: fixed 225 labels. Loss 0.14384. Accuracy 0.991.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40761065
Train loss (w/o reg) on all data: 0.40099198
Test loss (w/o reg) on all data: 0.22315423
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.8422683e-05
Norm of the params: 11.505363
Flipped loss: 0.22315. Accuracy: 0.980
### Flips: 820, rs: 38, checks: 205
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32446152
Train loss (w/o reg) on all data: 0.31373355
Test loss (w/o reg) on all data: 0.16281533
Train acc on all data:  0.8730853391684902
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.623776e-06
Norm of the params: 14.647846
     Influence (LOO): fixed 170 labels. Loss 0.16282. Accuracy 0.990.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28837785
Train loss (w/o reg) on all data: 0.27573645
Test loss (w/o reg) on all data: 0.15955134
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.929883e-05
Norm of the params: 15.900566
                Loss: fixed 204 labels. Loss 0.15955. Accuracy 0.969.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39894745
Train loss (w/o reg) on all data: 0.3922211
Test loss (w/o reg) on all data: 0.21576661
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.1342034e-05
Norm of the params: 11.598589
              Random: fixed  28 labels. Loss 0.21577. Accuracy 0.979.
### Flips: 820, rs: 38, checks: 410
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25385064
Train loss (w/o reg) on all data: 0.24342279
Test loss (w/o reg) on all data: 0.11616461
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.708916e-06
Norm of the params: 14.441509
     Influence (LOO): fixed 311 labels. Loss 0.11616. Accuracy 0.996.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17193566
Train loss (w/o reg) on all data: 0.15441558
Test loss (w/o reg) on all data: 0.10213748
Train acc on all data:  0.9350838803792852
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 7.329867e-06
Norm of the params: 18.719019
                Loss: fixed 409 labels. Loss 0.10214. Accuracy 0.973.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38282952
Train loss (w/o reg) on all data: 0.3755145
Test loss (w/o reg) on all data: 0.20288587
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.9472736e-05
Norm of the params: 12.095472
              Random: fixed  72 labels. Loss 0.20289. Accuracy 0.981.
### Flips: 820, rs: 38, checks: 615
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19885561
Train loss (w/o reg) on all data: 0.18931197
Test loss (w/o reg) on all data: 0.08727172
Train acc on all data:  0.9280330658886458
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1333848e-05
Norm of the params: 13.815677
     Influence (LOO): fixed 417 labels. Loss 0.08727. Accuracy 0.998.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06390304
Train loss (w/o reg) on all data: 0.047554515
Test loss (w/o reg) on all data: 0.038718976
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.3123234e-06
Norm of the params: 18.082327
                Loss: fixed 603 labels. Loss 0.03872. Accuracy 0.990.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3708103
Train loss (w/o reg) on all data: 0.3633856
Test loss (w/o reg) on all data: 0.19124182
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1551434e-05
Norm of the params: 12.185819
              Random: fixed 103 labels. Loss 0.19124. Accuracy 0.982.
### Flips: 820, rs: 38, checks: 820
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15601823
Train loss (w/o reg) on all data: 0.14716099
Test loss (w/o reg) on all data: 0.06610238
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.782765e-06
Norm of the params: 13.309568
     Influence (LOO): fixed 488 labels. Loss 0.06610. Accuracy 0.997.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01153878
Train loss (w/o reg) on all data: 0.0057897656
Test loss (w/o reg) on all data: 0.0054598325
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4915933e-07
Norm of the params: 10.722886
                Loss: fixed 701 labels. Loss 0.00546. Accuracy 1.000.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35559714
Train loss (w/o reg) on all data: 0.34799138
Test loss (w/o reg) on all data: 0.17724226
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3228935e-05
Norm of the params: 12.333495
              Random: fixed 141 labels. Loss 0.17724. Accuracy 0.985.
### Flips: 820, rs: 38, checks: 1025
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11284207
Train loss (w/o reg) on all data: 0.10446165
Test loss (w/o reg) on all data: 0.043824103
Train acc on all data:  0.9608558230002431
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0795368e-05
Norm of the params: 12.946368
     Influence (LOO): fixed 558 labels. Loss 0.04382. Accuracy 0.998.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069361
Train loss (w/o reg) on all data: 0.0030195187
Test loss (w/o reg) on all data: 0.003864247
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 1.11416234e-07
Norm of the params: 8.850515
                Loss: fixed 711 labels. Loss 0.00386. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34320793
Train loss (w/o reg) on all data: 0.33561507
Test loss (w/o reg) on all data: 0.1671156
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.7786655e-05
Norm of the params: 12.32303
              Random: fixed 170 labels. Loss 0.16712. Accuracy 0.988.
### Flips: 820, rs: 38, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084530026
Train loss (w/o reg) on all data: 0.07702118
Test loss (w/o reg) on all data: 0.030955747
Train acc on all data:  0.9720398735716023
Test acc on all data:   1.0
Norm of the mean of gradients: 6.044333e-06
Norm of the params: 12.25467
     Influence (LOO): fixed 603 labels. Loss 0.03096. Accuracy 1.000.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0053858883
Train loss (w/o reg) on all data: 0.002201286
Test loss (w/o reg) on all data: 0.0038172593
Train acc on all data:  1.0
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.1141993e-07
Norm of the params: 7.9807296
                Loss: fixed 714 labels. Loss 0.00382. Accuracy 0.999.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32871833
Train loss (w/o reg) on all data: 0.32137343
Test loss (w/o reg) on all data: 0.15666638
Train acc on all data:  0.8777048383175298
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 7.3223046e-06
Norm of the params: 12.120148
              Random: fixed 206 labels. Loss 0.15667. Accuracy 0.989.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40393576
Train loss (w/o reg) on all data: 0.39810276
Test loss (w/o reg) on all data: 0.21662666
Train acc on all data:  0.8315098468271335
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.1111703e-05
Norm of the params: 10.8009205
Flipped loss: 0.21663. Accuracy: 0.970
### Flips: 820, rs: 39, checks: 205
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31846952
Train loss (w/o reg) on all data: 0.30930126
Test loss (w/o reg) on all data: 0.15793177
Train acc on all data:  0.8738147337709701
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.5423404e-05
Norm of the params: 13.541236
     Influence (LOO): fixed 174 labels. Loss 0.15793. Accuracy 0.988.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27745607
Train loss (w/o reg) on all data: 0.26434344
Test loss (w/o reg) on all data: 0.16761528
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.9737496e-05
Norm of the params: 16.194218
                Loss: fixed 205 labels. Loss 0.16762. Accuracy 0.960.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3927844
Train loss (w/o reg) on all data: 0.38692376
Test loss (w/o reg) on all data: 0.20287353
Train acc on all data:  0.838560661317773
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.3334203e-05
Norm of the params: 10.826468
              Random: fixed  35 labels. Loss 0.20287. Accuracy 0.978.
### Flips: 820, rs: 39, checks: 410
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25273886
Train loss (w/o reg) on all data: 0.24268636
Test loss (w/o reg) on all data: 0.11939357
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.311946e-05
Norm of the params: 14.17921
     Influence (LOO): fixed 305 labels. Loss 0.11939. Accuracy 0.996.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16609399
Train loss (w/o reg) on all data: 0.1478406
Test loss (w/o reg) on all data: 0.11946393
Train acc on all data:  0.936785801118405
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.388429e-06
Norm of the params: 19.106749
                Loss: fixed 409 labels. Loss 0.11946. Accuracy 0.962.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37887126
Train loss (w/o reg) on all data: 0.37284398
Test loss (w/o reg) on all data: 0.191225
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.7970545e-05
Norm of the params: 10.97934
              Random: fixed  72 labels. Loss 0.19123. Accuracy 0.979.
### Flips: 820, rs: 39, checks: 615
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19429027
Train loss (w/o reg) on all data: 0.18487729
Test loss (w/o reg) on all data: 0.084832266
Train acc on all data:  0.9275468028203258
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.4685382e-06
Norm of the params: 13.720769
     Influence (LOO): fixed 414 labels. Loss 0.08483. Accuracy 0.994.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061486706
Train loss (w/o reg) on all data: 0.04572577
Test loss (w/o reg) on all data: 0.04205362
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.2667047e-06
Norm of the params: 17.754398
                Loss: fixed 606 labels. Loss 0.04205. Accuracy 0.988.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36311695
Train loss (w/o reg) on all data: 0.35711977
Test loss (w/o reg) on all data: 0.1764315
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.097417e-06
Norm of the params: 10.951876
              Random: fixed 117 labels. Loss 0.17643. Accuracy 0.982.
### Flips: 820, rs: 39, checks: 820
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15099573
Train loss (w/o reg) on all data: 0.14205982
Test loss (w/o reg) on all data: 0.06061706
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.2166445e-05
Norm of the params: 13.368554
     Influence (LOO): fixed 496 labels. Loss 0.06062. Accuracy 0.999.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0101350155
Train loss (w/o reg) on all data: 0.0049942466
Test loss (w/o reg) on all data: 0.005779917
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 7.407877e-08
Norm of the params: 10.139792
                Loss: fixed 701 labels. Loss 0.00578. Accuracy 1.000.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.345336
Train loss (w/o reg) on all data: 0.33934087
Test loss (w/o reg) on all data: 0.1618117
Train acc on all data:  0.8670070508144906
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 4.8872345e-05
Norm of the params: 10.949997
              Random: fixed 162 labels. Loss 0.16181. Accuracy 0.987.
### Flips: 820, rs: 39, checks: 1025
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1110613
Train loss (w/o reg) on all data: 0.10290795
Test loss (w/o reg) on all data: 0.042462517
Train acc on all data:  0.962800875273523
Test acc on all data:   1.0
Norm of the mean of gradients: 8.5396905e-06
Norm of the params: 12.769769
     Influence (LOO): fixed 562 labels. Loss 0.04246. Accuracy 1.000.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.004622599
Train loss (w/o reg) on all data: 0.0017597389
Test loss (w/o reg) on all data: 0.0026826751
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4823085e-07
Norm of the params: 7.5668488
                Loss: fixed 710 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33362752
Train loss (w/o reg) on all data: 0.32770845
Test loss (w/o reg) on all data: 0.15297045
Train acc on all data:  0.8743009968392901
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.5320104e-06
Norm of the params: 10.88032
              Random: fixed 194 labels. Loss 0.15297. Accuracy 0.988.
### Flips: 820, rs: 39, checks: 1230
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08688802
Train loss (w/o reg) on all data: 0.07922453
Test loss (w/o reg) on all data: 0.032620117
Train acc on all data:  0.9722830051057623
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0653179e-05
Norm of the params: 12.380221
     Influence (LOO): fixed 599 labels. Loss 0.03262. Accuracy 1.000.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0046225986
Train loss (w/o reg) on all data: 0.0017597473
Test loss (w/o reg) on all data: 0.0026827042
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 9.5191716e-08
Norm of the params: 7.5668373
                Loss: fixed 710 labels. Loss 0.00268. Accuracy 1.000.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32155272
Train loss (w/o reg) on all data: 0.3155388
Test loss (w/o reg) on all data: 0.14158374
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.922551e-06
Norm of the params: 10.9671545
              Random: fixed 224 labels. Loss 0.14158. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4584187
Train loss (w/o reg) on all data: 0.4528632
Test loss (w/o reg) on all data: 0.2752328
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.4588626e-05
Norm of the params: 10.540882
Flipped loss: 0.27523. Accuracy: 0.961
### Flips: 1025, rs: 0, checks: 205
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837506
Train loss (w/o reg) on all data: 0.37424788
Test loss (w/o reg) on all data: 0.21460105
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1299305e-05
Norm of the params: 13.786014
     Influence (LOO): fixed 170 labels. Loss 0.21460. Accuracy 0.986.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34637833
Train loss (w/o reg) on all data: 0.33434746
Test loss (w/o reg) on all data: 0.21775956
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.6909336e-05
Norm of the params: 15.5118475
                Loss: fixed 205 labels. Loss 0.21776. Accuracy 0.941.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44782382
Train loss (w/o reg) on all data: 0.44228223
Test loss (w/o reg) on all data: 0.26032913
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.071351e-06
Norm of the params: 10.527666
              Random: fixed  41 labels. Loss 0.26033. Accuracy 0.970.
### Flips: 1025, rs: 0, checks: 410
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32645655
Train loss (w/o reg) on all data: 0.31622738
Test loss (w/o reg) on all data: 0.17152749
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.705976e-06
Norm of the params: 14.303262
     Influence (LOO): fixed 304 labels. Loss 0.17153. Accuracy 0.990.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24572088
Train loss (w/o reg) on all data: 0.2287436
Test loss (w/o reg) on all data: 0.15909587
Train acc on all data:  0.8927789934354485
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 6.8592162e-06
Norm of the params: 18.426765
                Loss: fixed 410 labels. Loss 0.15910. Accuracy 0.946.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43759337
Train loss (w/o reg) on all data: 0.4322327
Test loss (w/o reg) on all data: 0.2447761
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.7814515e-05
Norm of the params: 10.354378
              Random: fixed  83 labels. Loss 0.24478. Accuracy 0.978.
### Flips: 1025, rs: 0, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27982768
Train loss (w/o reg) on all data: 0.26883525
Test loss (w/o reg) on all data: 0.13943535
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0855276e-05
Norm of the params: 14.8273
     Influence (LOO): fixed 405 labels. Loss 0.13944. Accuracy 0.990.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14644626
Train loss (w/o reg) on all data: 0.12803596
Test loss (w/o reg) on all data: 0.10161318
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.4054794e-05
Norm of the params: 19.188696
                Loss: fixed 610 labels. Loss 0.10161. Accuracy 0.970.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42406604
Train loss (w/o reg) on all data: 0.4185315
Test loss (w/o reg) on all data: 0.23080528
Train acc on all data:  0.8157062971067347
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4615957e-05
Norm of the params: 10.520973
              Random: fixed 128 labels. Loss 0.23081. Accuracy 0.979.
### Flips: 1025, rs: 0, checks: 820
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23231682
Train loss (w/o reg) on all data: 0.22146611
Test loss (w/o reg) on all data: 0.10810287
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 5.3035064e-06
Norm of the params: 14.731401
     Influence (LOO): fixed 504 labels. Loss 0.10810. Accuracy 0.995.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060467463
Train loss (w/o reg) on all data: 0.045496885
Test loss (w/o reg) on all data: 0.03533968
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.1558042e-06
Norm of the params: 17.303514
                Loss: fixed 790 labels. Loss 0.03534. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40957683
Train loss (w/o reg) on all data: 0.40391028
Test loss (w/o reg) on all data: 0.2162004
Train acc on all data:  0.8278628738147338
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.2614168e-05
Norm of the params: 10.6456995
              Random: fixed 177 labels. Loss 0.21620. Accuracy 0.983.
### Flips: 1025, rs: 0, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19280358
Train loss (w/o reg) on all data: 0.18204708
Test loss (w/o reg) on all data: 0.0864536
Train acc on all data:  0.9229273036712862
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.6419473e-06
Norm of the params: 14.667308
     Influence (LOO): fixed 578 labels. Loss 0.08645. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023438744
Train loss (w/o reg) on all data: 0.014475854
Test loss (w/o reg) on all data: 0.012820492
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.882281e-07
Norm of the params: 13.38872
                Loss: fixed 860 labels. Loss 0.01282. Accuracy 0.999.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39676484
Train loss (w/o reg) on all data: 0.39091462
Test loss (w/o reg) on all data: 0.20610465
Train acc on all data:  0.8368587405786531
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.2572134e-05
Norm of the params: 10.816858
              Random: fixed 215 labels. Loss 0.20610. Accuracy 0.985.
### Flips: 1025, rs: 0, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16122654
Train loss (w/o reg) on all data: 0.15135412
Test loss (w/o reg) on all data: 0.06860803
Train acc on all data:  0.9382445903233649
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.241051e-06
Norm of the params: 14.051631
     Influence (LOO): fixed 642 labels. Loss 0.06861. Accuracy 0.998.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014079605
Train loss (w/o reg) on all data: 0.0076575014
Test loss (w/o reg) on all data: 0.0073956805
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.032844e-07
Norm of the params: 11.33323
                Loss: fixed 875 labels. Loss 0.00740. Accuracy 0.999.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38058445
Train loss (w/o reg) on all data: 0.37434712
Test loss (w/o reg) on all data: 0.19181372
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.4117277e-05
Norm of the params: 11.169007
              Random: fixed 259 labels. Loss 0.19181. Accuracy 0.988.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45258984
Train loss (w/o reg) on all data: 0.4466688
Test loss (w/o reg) on all data: 0.28347364
Train acc on all data:  0.7969851689764162
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.658727e-05
Norm of the params: 10.882134
Flipped loss: 0.28347. Accuracy: 0.953
### Flips: 1025, rs: 1, checks: 205
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37893152
Train loss (w/o reg) on all data: 0.36940008
Test loss (w/o reg) on all data: 0.23300818
Train acc on all data:  0.8373450036469731
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0865578e-05
Norm of the params: 13.806834
     Influence (LOO): fixed 171 labels. Loss 0.23301. Accuracy 0.955.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34180328
Train loss (w/o reg) on all data: 0.32847303
Test loss (w/o reg) on all data: 0.2377719
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 8.125715e-06
Norm of the params: 16.32804
                Loss: fixed 205 labels. Loss 0.23777. Accuracy 0.930.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43939197
Train loss (w/o reg) on all data: 0.43333632
Test loss (w/o reg) on all data: 0.26716903
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.7324277e-05
Norm of the params: 11.00515
              Random: fixed  49 labels. Loss 0.26717. Accuracy 0.955.
### Flips: 1025, rs: 1, checks: 410
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32245308
Train loss (w/o reg) on all data: 0.31179485
Test loss (w/o reg) on all data: 0.18056951
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 9.586884e-06
Norm of the params: 14.60017
     Influence (LOO): fixed 307 labels. Loss 0.18057. Accuracy 0.972.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24102512
Train loss (w/o reg) on all data: 0.22325842
Test loss (w/o reg) on all data: 0.18853778
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.1967677e-05
Norm of the params: 18.850307
                Loss: fixed 408 labels. Loss 0.18854. Accuracy 0.942.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42896882
Train loss (w/o reg) on all data: 0.42291555
Test loss (w/o reg) on all data: 0.25596672
Train acc on all data:  0.8174082178458546
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.5339516e-05
Norm of the params: 11.00298
              Random: fixed  89 labels. Loss 0.25597. Accuracy 0.952.
### Flips: 1025, rs: 1, checks: 615
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27554357
Train loss (w/o reg) on all data: 0.2642047
Test loss (w/o reg) on all data: 0.14260057
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.4304412e-05
Norm of the params: 15.059115
     Influence (LOO): fixed 410 labels. Loss 0.14260. Accuracy 0.985.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1422704
Train loss (w/o reg) on all data: 0.12116943
Test loss (w/o reg) on all data: 0.14117426
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.323344e-06
Norm of the params: 20.543108
                Loss: fixed 607 labels. Loss 0.14117. Accuracy 0.952.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41149238
Train loss (w/o reg) on all data: 0.40517205
Test loss (w/o reg) on all data: 0.23517208
Train acc on all data:  0.8264040846097739
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 9.287301e-05
Norm of the params: 11.243067
              Random: fixed 144 labels. Loss 0.23517. Accuracy 0.961.
### Flips: 1025, rs: 1, checks: 820
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23292008
Train loss (w/o reg) on all data: 0.22219363
Test loss (w/o reg) on all data: 0.11219355
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.2359231e-05
Norm of the params: 14.646812
     Influence (LOO): fixed 499 labels. Loss 0.11219. Accuracy 0.991.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05977019
Train loss (w/o reg) on all data: 0.041872125
Test loss (w/o reg) on all data: 0.0654908
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.9002233e-06
Norm of the params: 18.919865
                Loss: fixed 779 labels. Loss 0.06549. Accuracy 0.980.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39017543
Train loss (w/o reg) on all data: 0.38360542
Test loss (w/o reg) on all data: 0.21221863
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.880408e-05
Norm of the params: 11.462999
              Random: fixed 207 labels. Loss 0.21222. Accuracy 0.965.
### Flips: 1025, rs: 1, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19605373
Train loss (w/o reg) on all data: 0.18604226
Test loss (w/o reg) on all data: 0.08876433
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5372007e-05
Norm of the params: 14.150236
     Influence (LOO): fixed 575 labels. Loss 0.08876. Accuracy 0.997.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02749683
Train loss (w/o reg) on all data: 0.016378684
Test loss (w/o reg) on all data: 0.020288413
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.1504515e-07
Norm of the params: 14.9118395
                Loss: fixed 846 labels. Loss 0.02029. Accuracy 0.995.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37688538
Train loss (w/o reg) on all data: 0.37029067
Test loss (w/o reg) on all data: 0.19656315
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 9.332449e-06
Norm of the params: 11.48453
              Random: fixed 247 labels. Loss 0.19656. Accuracy 0.973.
### Flips: 1025, rs: 1, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15808728
Train loss (w/o reg) on all data: 0.14844428
Test loss (w/o reg) on all data: 0.06859958
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.745969e-06
Norm of the params: 13.887403
     Influence (LOO): fixed 644 labels. Loss 0.06860. Accuracy 0.999.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01911565
Train loss (w/o reg) on all data: 0.010343756
Test loss (w/o reg) on all data: 0.0125707835
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.9031284e-07
Norm of the params: 13.245295
                Loss: fixed 866 labels. Loss 0.01257. Accuracy 0.997.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3633437
Train loss (w/o reg) on all data: 0.3567006
Test loss (w/o reg) on all data: 0.18617065
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4289515e-05
Norm of the params: 11.526562
              Random: fixed 286 labels. Loss 0.18617. Accuracy 0.979.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45846382
Train loss (w/o reg) on all data: 0.453344
Test loss (w/o reg) on all data: 0.26657262
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.1168205e-05
Norm of the params: 10.119118
Flipped loss: 0.26657. Accuracy: 0.975
### Flips: 1025, rs: 2, checks: 205
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38196677
Train loss (w/o reg) on all data: 0.3725928
Test loss (w/o reg) on all data: 0.21228409
Train acc on all data:  0.8353999513736932
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.6400112e-05
Norm of the params: 13.692313
     Influence (LOO): fixed 172 labels. Loss 0.21228. Accuracy 0.981.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34923792
Train loss (w/o reg) on all data: 0.33789897
Test loss (w/o reg) on all data: 0.21606527
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.5750436e-05
Norm of the params: 15.059189
                Loss: fixed 205 labels. Loss 0.21607. Accuracy 0.954.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44855416
Train loss (w/o reg) on all data: 0.44325942
Test loss (w/o reg) on all data: 0.25725138
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.356268e-05
Norm of the params: 10.290521
              Random: fixed  38 labels. Loss 0.25725. Accuracy 0.978.
### Flips: 1025, rs: 2, checks: 410
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32173613
Train loss (w/o reg) on all data: 0.31078282
Test loss (w/o reg) on all data: 0.16847958
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.061141e-05
Norm of the params: 14.8008795
     Influence (LOO): fixed 307 labels. Loss 0.16848. Accuracy 0.983.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24350595
Train loss (w/o reg) on all data: 0.22694486
Test loss (w/o reg) on all data: 0.16343425
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.6656122e-05
Norm of the params: 18.199503
                Loss: fixed 409 labels. Loss 0.16343. Accuracy 0.954.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43610606
Train loss (w/o reg) on all data: 0.43058154
Test loss (w/o reg) on all data: 0.24294879
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.7417133e-05
Norm of the params: 10.511429
              Random: fixed  83 labels. Loss 0.24295. Accuracy 0.983.
### Flips: 1025, rs: 2, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27619085
Train loss (w/o reg) on all data: 0.26531145
Test loss (w/o reg) on all data: 0.13550003
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.7824348e-05
Norm of the params: 14.750855
     Influence (LOO): fixed 411 labels. Loss 0.13550. Accuracy 0.995.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13977394
Train loss (w/o reg) on all data: 0.11915414
Test loss (w/o reg) on all data: 0.089759775
Train acc on all data:  0.9474835886214442
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.13909e-06
Norm of the params: 20.307533
                Loss: fixed 612 labels. Loss 0.08976. Accuracy 0.972.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42555177
Train loss (w/o reg) on all data: 0.42004248
Test loss (w/o reg) on all data: 0.23318134
Train acc on all data:  0.8174082178458546
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.0591585e-05
Norm of the params: 10.496949
              Random: fixed 118 labels. Loss 0.23318. Accuracy 0.983.
### Flips: 1025, rs: 2, checks: 820
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23674801
Train loss (w/o reg) on all data: 0.22660081
Test loss (w/o reg) on all data: 0.1083745
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0644875e-05
Norm of the params: 14.245843
     Influence (LOO): fixed 496 labels. Loss 0.10837. Accuracy 0.999.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048898473
Train loss (w/o reg) on all data: 0.03359837
Test loss (w/o reg) on all data: 0.025737906
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.0649994e-07
Norm of the params: 17.492914
                Loss: fixed 798 labels. Loss 0.02574. Accuracy 0.995.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40880758
Train loss (w/o reg) on all data: 0.40307546
Test loss (w/o reg) on all data: 0.2173714
Train acc on all data:  0.8281060053488938
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1054177e-05
Norm of the params: 10.707131
              Random: fixed 166 labels. Loss 0.21737. Accuracy 0.986.
### Flips: 1025, rs: 2, checks: 1025
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19423206
Train loss (w/o reg) on all data: 0.18455961
Test loss (w/o reg) on all data: 0.08584832
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.995974e-06
Norm of the params: 13.908597
     Influence (LOO): fixed 575 labels. Loss 0.08585. Accuracy 0.999.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021746384
Train loss (w/o reg) on all data: 0.012644506
Test loss (w/o reg) on all data: 0.00800417
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 7.2921006e-07
Norm of the params: 13.49213
                Loss: fixed 855 labels. Loss 0.00800. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39396095
Train loss (w/o reg) on all data: 0.38841873
Test loss (w/o reg) on all data: 0.2006801
Train acc on all data:  0.8402625820568927
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.1931508e-05
Norm of the params: 10.528262
              Random: fixed 212 labels. Loss 0.20068. Accuracy 0.987.
### Flips: 1025, rs: 2, checks: 1230
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15326637
Train loss (w/o reg) on all data: 0.14425515
Test loss (w/o reg) on all data: 0.066741094
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.46549555e-05
Norm of the params: 13.424774
     Influence (LOO): fixed 648 labels. Loss 0.06674. Accuracy 0.998.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013026288
Train loss (w/o reg) on all data: 0.006567702
Test loss (w/o reg) on all data: 0.0058467644
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6489934e-07
Norm of the params: 11.365374
                Loss: fixed 869 labels. Loss 0.00585. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37683085
Train loss (w/o reg) on all data: 0.37127492
Test loss (w/o reg) on all data: 0.18524322
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.962442e-06
Norm of the params: 10.541275
              Random: fixed 263 labels. Loss 0.18524. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46247506
Train loss (w/o reg) on all data: 0.45659465
Test loss (w/o reg) on all data: 0.2750838
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.1116647e-05
Norm of the params: 10.84475
Flipped loss: 0.27508. Accuracy: 0.959
### Flips: 1025, rs: 3, checks: 205
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3887701
Train loss (w/o reg) on all data: 0.3785477
Test loss (w/o reg) on all data: 0.2147248
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.331425e-05
Norm of the params: 14.298528
     Influence (LOO): fixed 170 labels. Loss 0.21472. Accuracy 0.983.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3580523
Train loss (w/o reg) on all data: 0.34577414
Test loss (w/o reg) on all data: 0.2243247
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.9419656e-05
Norm of the params: 15.670458
                Loss: fixed 205 labels. Loss 0.22432. Accuracy 0.941.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44929525
Train loss (w/o reg) on all data: 0.4433062
Test loss (w/o reg) on all data: 0.26094687
Train acc on all data:  0.799659615852176
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.1075746e-05
Norm of the params: 10.944442
              Random: fixed  47 labels. Loss 0.26095. Accuracy 0.964.
### Flips: 1025, rs: 3, checks: 410
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33302283
Train loss (w/o reg) on all data: 0.32173353
Test loss (w/o reg) on all data: 0.18159509
Train acc on all data:  0.8580111840505713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.113109e-06
Norm of the params: 15.0261755
     Influence (LOO): fixed 292 labels. Loss 0.18160. Accuracy 0.982.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25775975
Train loss (w/o reg) on all data: 0.24102986
Test loss (w/o reg) on all data: 0.17035855
Train acc on all data:  0.8903476780938487
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 4.85194e-05
Norm of the params: 18.29202
                Loss: fixed 410 labels. Loss 0.17036. Accuracy 0.949.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43900636
Train loss (w/o reg) on all data: 0.43308115
Test loss (w/o reg) on all data: 0.24808201
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 9.506527e-06
Norm of the params: 10.885958
              Random: fixed  86 labels. Loss 0.24808. Accuracy 0.967.
### Flips: 1025, rs: 3, checks: 615
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2836868
Train loss (w/o reg) on all data: 0.27252984
Test loss (w/o reg) on all data: 0.14412193
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 9.172257e-06
Norm of the params: 14.93784
     Influence (LOO): fixed 401 labels. Loss 0.14412. Accuracy 0.990.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15027778
Train loss (w/o reg) on all data: 0.13064554
Test loss (w/o reg) on all data: 0.11647975
Train acc on all data:  0.9428640894724045
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.252389e-06
Norm of the params: 19.815268
                Loss: fixed 612 labels. Loss 0.11648. Accuracy 0.960.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4245186
Train loss (w/o reg) on all data: 0.41840324
Test loss (w/o reg) on all data: 0.23083057
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.0326682e-05
Norm of the params: 11.059252
              Random: fixed 134 labels. Loss 0.23083. Accuracy 0.978.
### Flips: 1025, rs: 3, checks: 820
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24637417
Train loss (w/o reg) on all data: 0.23497225
Test loss (w/o reg) on all data: 0.12338851
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.5506548e-05
Norm of the params: 15.100941
     Influence (LOO): fixed 481 labels. Loss 0.12339. Accuracy 0.990.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055991765
Train loss (w/o reg) on all data: 0.040190764
Test loss (w/o reg) on all data: 0.04299808
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.210357e-06
Norm of the params: 17.77695
                Loss: fixed 802 labels. Loss 0.04300. Accuracy 0.984.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4109719
Train loss (w/o reg) on all data: 0.40485552
Test loss (w/o reg) on all data: 0.21454403
Train acc on all data:  0.8283491368830538
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.083541e-05
Norm of the params: 11.06018
              Random: fixed 179 labels. Loss 0.21454. Accuracy 0.982.
### Flips: 1025, rs: 3, checks: 1025
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211398
Train loss (w/o reg) on all data: 0.2008093
Test loss (w/o reg) on all data: 0.09903758
Train acc on all data:  0.9166058837831267
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.7493233e-06
Norm of the params: 14.552464
     Influence (LOO): fixed 558 labels. Loss 0.09904. Accuracy 0.998.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021091778
Train loss (w/o reg) on all data: 0.012695636
Test loss (w/o reg) on all data: 0.009364312
Train acc on all data:  0.9956236323851203
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0633615e-07
Norm of the params: 12.958505
                Loss: fixed 870 labels. Loss 0.00936. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3965817
Train loss (w/o reg) on all data: 0.39030534
Test loss (w/o reg) on all data: 0.20485288
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.7212335e-05
Norm of the params: 11.2038965
              Random: fixed 220 labels. Loss 0.20485. Accuracy 0.983.
### Flips: 1025, rs: 3, checks: 1230
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17904162
Train loss (w/o reg) on all data: 0.16893339
Test loss (w/o reg) on all data: 0.07815152
Train acc on all data:  0.9333819596401653
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.302161e-06
Norm of the params: 14.218458
     Influence (LOO): fixed 623 labels. Loss 0.07815. Accuracy 0.998.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010662418
Train loss (w/o reg) on all data: 0.00536223
Test loss (w/o reg) on all data: 0.004989982
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4637452e-07
Norm of the params: 10.295814
                Loss: fixed 887 labels. Loss 0.00499. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38327795
Train loss (w/o reg) on all data: 0.3768762
Test loss (w/o reg) on all data: 0.19450721
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.5759786e-05
Norm of the params: 11.315265
              Random: fixed 257 labels. Loss 0.19451. Accuracy 0.984.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45059514
Train loss (w/o reg) on all data: 0.4454505
Test loss (w/o reg) on all data: 0.26458505
Train acc on all data:  0.7960126428397764
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.9586248e-05
Norm of the params: 10.143583
Flipped loss: 0.26459. Accuracy: 0.970
### Flips: 1025, rs: 4, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3730755
Train loss (w/o reg) on all data: 0.36368158
Test loss (w/o reg) on all data: 0.21430562
Train acc on all data:  0.838074398249453
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 8.310627e-06
Norm of the params: 13.70685
     Influence (LOO): fixed 170 labels. Loss 0.21431. Accuracy 0.971.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33614206
Train loss (w/o reg) on all data: 0.32441536
Test loss (w/o reg) on all data: 0.22057293
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 4.3204596e-05
Norm of the params: 15.314503
                Loss: fixed 205 labels. Loss 0.22057. Accuracy 0.943.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43935755
Train loss (w/o reg) on all data: 0.43390915
Test loss (w/o reg) on all data: 0.24995504
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.0268024e-05
Norm of the params: 10.438763
              Random: fixed  44 labels. Loss 0.24996. Accuracy 0.973.
### Flips: 1025, rs: 4, checks: 410
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31854403
Train loss (w/o reg) on all data: 0.3080642
Test loss (w/o reg) on all data: 0.1696723
Train acc on all data:  0.8653051300753708
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.564008e-06
Norm of the params: 14.477462
     Influence (LOO): fixed 302 labels. Loss 0.16967. Accuracy 0.985.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22772808
Train loss (w/o reg) on all data: 0.21160458
Test loss (w/o reg) on all data: 0.17464611
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.8775206e-05
Norm of the params: 17.95745
                Loss: fixed 410 labels. Loss 0.17465. Accuracy 0.938.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42696503
Train loss (w/o reg) on all data: 0.42137787
Test loss (w/o reg) on all data: 0.23340917
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.310132e-05
Norm of the params: 10.570853
              Random: fixed  89 labels. Loss 0.23341. Accuracy 0.977.
### Flips: 1025, rs: 4, checks: 615
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26947546
Train loss (w/o reg) on all data: 0.25796553
Test loss (w/o reg) on all data: 0.14019455
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.639449e-05
Norm of the params: 15.172287
     Influence (LOO): fixed 402 labels. Loss 0.14019. Accuracy 0.989.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12753673
Train loss (w/o reg) on all data: 0.10935974
Test loss (w/o reg) on all data: 0.10241905
Train acc on all data:  0.9533187454412837
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.853655e-06
Norm of the params: 19.06672
                Loss: fixed 613 labels. Loss 0.10242. Accuracy 0.970.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41197926
Train loss (w/o reg) on all data: 0.4059985
Test loss (w/o reg) on all data: 0.21412507
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.5053676e-05
Norm of the params: 10.936883
              Random: fixed 136 labels. Loss 0.21413. Accuracy 0.988.
### Flips: 1025, rs: 4, checks: 820
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22497272
Train loss (w/o reg) on all data: 0.21357024
Test loss (w/o reg) on all data: 0.1081982
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9635783e-05
Norm of the params: 15.1013155
     Influence (LOO): fixed 499 labels. Loss 0.10820. Accuracy 0.993.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04748253
Train loss (w/o reg) on all data: 0.03323503
Test loss (w/o reg) on all data: 0.03181915
Train acc on all data:  0.987357160223681
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.414988e-06
Norm of the params: 16.880463
                Loss: fixed 790 labels. Loss 0.03182. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40086943
Train loss (w/o reg) on all data: 0.39483652
Test loss (w/o reg) on all data: 0.20382065
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.58536e-06
Norm of the params: 10.984466
              Random: fixed 175 labels. Loss 0.20382. Accuracy 0.986.
### Flips: 1025, rs: 4, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18835516
Train loss (w/o reg) on all data: 0.17737614
Test loss (w/o reg) on all data: 0.08577778
Train acc on all data:  0.9265742766836859
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.8847587e-06
Norm of the params: 14.818249
     Influence (LOO): fixed 569 labels. Loss 0.08578. Accuracy 0.997.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019493435
Train loss (w/o reg) on all data: 0.010653477
Test loss (w/o reg) on all data: 0.0138987815
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5028052e-06
Norm of the params: 13.296586
                Loss: fixed 843 labels. Loss 0.01390. Accuracy 0.997.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3826843
Train loss (w/o reg) on all data: 0.3765083
Test loss (w/o reg) on all data: 0.18443622
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.0125266e-05
Norm of the params: 11.113954
              Random: fixed 228 labels. Loss 0.18444. Accuracy 0.991.
### Flips: 1025, rs: 4, checks: 1230
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16109782
Train loss (w/o reg) on all data: 0.1503815
Test loss (w/o reg) on all data: 0.070022315
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.63607e-06
Norm of the params: 14.639887
     Influence (LOO): fixed 623 labels. Loss 0.07002. Accuracy 0.997.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011162255
Train loss (w/o reg) on all data: 0.0054937457
Test loss (w/o reg) on all data: 0.0072519034
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.4745285e-07
Norm of the params: 10.647544
                Loss: fixed 861 labels. Loss 0.00725. Accuracy 0.998.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36643335
Train loss (w/o reg) on all data: 0.35993376
Test loss (w/o reg) on all data: 0.17211191
Train acc on all data:  0.8555798687089715
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.5358276e-05
Norm of the params: 11.401382
              Random: fixed 266 labels. Loss 0.17211. Accuracy 0.991.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46434167
Train loss (w/o reg) on all data: 0.45935872
Test loss (w/o reg) on all data: 0.27853683
Train acc on all data:  0.7865305130075371
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.998119e-05
Norm of the params: 9.982937
Flipped loss: 0.27854. Accuracy: 0.967
### Flips: 1025, rs: 5, checks: 205
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39286166
Train loss (w/o reg) on all data: 0.38386774
Test loss (w/o reg) on all data: 0.2220584
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.289335e-05
Norm of the params: 13.411867
     Influence (LOO): fixed 166 labels. Loss 0.22206. Accuracy 0.972.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3552713
Train loss (w/o reg) on all data: 0.34438345
Test loss (w/o reg) on all data: 0.22147836
Train acc on all data:  0.837588135181133
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.800876e-06
Norm of the params: 14.756598
                Loss: fixed 205 labels. Loss 0.22148. Accuracy 0.953.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45299977
Train loss (w/o reg) on all data: 0.4480032
Test loss (w/o reg) on all data: 0.26445293
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.1247407e-05
Norm of the params: 9.996579
              Random: fixed  45 labels. Loss 0.26445. Accuracy 0.972.
### Flips: 1025, rs: 5, checks: 410
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33347762
Train loss (w/o reg) on all data: 0.3233778
Test loss (w/o reg) on all data: 0.17803629
Train acc on all data:  0.8550936056406516
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.0148225e-05
Norm of the params: 14.212553
     Influence (LOO): fixed 305 labels. Loss 0.17804. Accuracy 0.983.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25560278
Train loss (w/o reg) on all data: 0.24087438
Test loss (w/o reg) on all data: 0.17130943
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 6.7644924e-06
Norm of the params: 17.162977
                Loss: fixed 408 labels. Loss 0.17131. Accuracy 0.952.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4429711
Train loss (w/o reg) on all data: 0.43784493
Test loss (w/o reg) on all data: 0.2529439
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.1351894e-05
Norm of the params: 10.125379
              Random: fixed  83 labels. Loss 0.25294. Accuracy 0.971.
### Flips: 1025, rs: 5, checks: 615
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2804801
Train loss (w/o reg) on all data: 0.26959965
Test loss (w/o reg) on all data: 0.14229324
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.094082e-06
Norm of the params: 14.751567
     Influence (LOO): fixed 418 labels. Loss 0.14229. Accuracy 0.991.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14889333
Train loss (w/o reg) on all data: 0.13092059
Test loss (w/o reg) on all data: 0.10966605
Train acc on all data:  0.9421346948699246
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.970597e-06
Norm of the params: 18.95929
                Loss: fixed 611 labels. Loss 0.10967. Accuracy 0.966.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4302876
Train loss (w/o reg) on all data: 0.42483056
Test loss (w/o reg) on all data: 0.23704764
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0180767e-05
Norm of the params: 10.447049
              Random: fixed 127 labels. Loss 0.23705. Accuracy 0.976.
### Flips: 1025, rs: 5, checks: 820
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24122721
Train loss (w/o reg) on all data: 0.23084547
Test loss (w/o reg) on all data: 0.114171945
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2865836e-05
Norm of the params: 14.409545
     Influence (LOO): fixed 503 labels. Loss 0.11417. Accuracy 0.994.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061369028
Train loss (w/o reg) on all data: 0.045365244
Test loss (w/o reg) on all data: 0.042265326
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.6782546e-06
Norm of the params: 17.89066
                Loss: fixed 796 labels. Loss 0.04227. Accuracy 0.987.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41636762
Train loss (w/o reg) on all data: 0.41109815
Test loss (w/o reg) on all data: 0.22014475
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.6374442e-05
Norm of the params: 10.265936
              Random: fixed 176 labels. Loss 0.22014. Accuracy 0.983.
### Flips: 1025, rs: 5, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2001826
Train loss (w/o reg) on all data: 0.19001888
Test loss (w/o reg) on all data: 0.09294639
Train acc on all data:  0.9202528567955264
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.103713e-05
Norm of the params: 14.257434
     Influence (LOO): fixed 581 labels. Loss 0.09295. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022335555
Train loss (w/o reg) on all data: 0.013417798
Test loss (w/o reg) on all data: 0.014767987
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.7624553e-07
Norm of the params: 13.354967
                Loss: fixed 876 labels. Loss 0.01477. Accuracy 0.995.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4001451
Train loss (w/o reg) on all data: 0.3944971
Test loss (w/o reg) on all data: 0.20738356
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.4136813e-05
Norm of the params: 10.628283
              Random: fixed 220 labels. Loss 0.20738. Accuracy 0.980.
### Flips: 1025, rs: 5, checks: 1230
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16338715
Train loss (w/o reg) on all data: 0.15416966
Test loss (w/o reg) on all data: 0.074316844
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.3592763e-06
Norm of the params: 13.577542
     Influence (LOO): fixed 652 labels. Loss 0.07432. Accuracy 0.998.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014869398
Train loss (w/o reg) on all data: 0.0077900896
Test loss (w/o reg) on all data: 0.01132685
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.4914714e-07
Norm of the params: 11.898999
                Loss: fixed 891 labels. Loss 0.01133. Accuracy 0.997.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38351792
Train loss (w/o reg) on all data: 0.37761566
Test loss (w/o reg) on all data: 0.1926442
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.5862726e-05
Norm of the params: 10.864863
              Random: fixed 266 labels. Loss 0.19264. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4573757
Train loss (w/o reg) on all data: 0.4521632
Test loss (w/o reg) on all data: 0.28217247
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.71304e-05
Norm of the params: 10.210292
Flipped loss: 0.28217. Accuracy: 0.957
### Flips: 1025, rs: 6, checks: 205
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38444054
Train loss (w/o reg) on all data: 0.37504277
Test loss (w/o reg) on all data: 0.22168428
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.2958948e-05
Norm of the params: 13.709696
     Influence (LOO): fixed 169 labels. Loss 0.22168. Accuracy 0.976.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3457046
Train loss (w/o reg) on all data: 0.33413398
Test loss (w/o reg) on all data: 0.22951429
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.6435417e-05
Norm of the params: 15.212232
                Loss: fixed 205 labels. Loss 0.22951. Accuracy 0.938.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4447904
Train loss (w/o reg) on all data: 0.43957952
Test loss (w/o reg) on all data: 0.26488775
Train acc on all data:  0.799173352783856
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.1314792e-05
Norm of the params: 10.208696
              Random: fixed  49 labels. Loss 0.26489. Accuracy 0.968.
### Flips: 1025, rs: 6, checks: 410
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33061096
Train loss (w/o reg) on all data: 0.3210502
Test loss (w/o reg) on all data: 0.17573118
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.701711e-05
Norm of the params: 13.828065
     Influence (LOO): fixed 296 labels. Loss 0.17573. Accuracy 0.988.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24243394
Train loss (w/o reg) on all data: 0.22604243
Test loss (w/o reg) on all data: 0.16641755
Train acc on all data:  0.8954534403112083
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 3.73091e-06
Norm of the params: 18.10608
                Loss: fixed 409 labels. Loss 0.16642. Accuracy 0.947.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4326663
Train loss (w/o reg) on all data: 0.42733774
Test loss (w/o reg) on all data: 0.2529676
Train acc on all data:  0.8103574033552152
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.8003916e-05
Norm of the params: 10.323325
              Random: fixed  89 labels. Loss 0.25297. Accuracy 0.972.
### Flips: 1025, rs: 6, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28270793
Train loss (w/o reg) on all data: 0.2723742
Test loss (w/o reg) on all data: 0.14239292
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.2859433e-05
Norm of the params: 14.376183
     Influence (LOO): fixed 401 labels. Loss 0.14239. Accuracy 0.989.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14641544
Train loss (w/o reg) on all data: 0.12756364
Test loss (w/o reg) on all data: 0.09709504
Train acc on all data:  0.9443228786773644
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.4167022e-06
Norm of the params: 19.417418
                Loss: fixed 610 labels. Loss 0.09710. Accuracy 0.970.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41792214
Train loss (w/o reg) on all data: 0.41216257
Test loss (w/o reg) on all data: 0.23615511
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.801239e-05
Norm of the params: 10.732728
              Random: fixed 135 labels. Loss 0.23616. Accuracy 0.975.
### Flips: 1025, rs: 6, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2350587
Train loss (w/o reg) on all data: 0.2245676
Test loss (w/o reg) on all data: 0.11048341
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9196175e-05
Norm of the params: 14.485228
     Influence (LOO): fixed 500 labels. Loss 0.11048. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05878845
Train loss (w/o reg) on all data: 0.042484492
Test loss (w/o reg) on all data: 0.038108893
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.550449e-06
Norm of the params: 18.057661
                Loss: fixed 788 labels. Loss 0.03811. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40464696
Train loss (w/o reg) on all data: 0.39880073
Test loss (w/o reg) on all data: 0.22207825
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.650197e-05
Norm of the params: 10.813171
              Random: fixed 179 labels. Loss 0.22208. Accuracy 0.976.
### Flips: 1025, rs: 6, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1950334
Train loss (w/o reg) on all data: 0.18488963
Test loss (w/o reg) on all data: 0.0888304
Train acc on all data:  0.9231704352054462
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 8.45309e-06
Norm of the params: 14.243432
     Influence (LOO): fixed 577 labels. Loss 0.08883. Accuracy 0.997.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021609187
Train loss (w/o reg) on all data: 0.01266306
Test loss (w/o reg) on all data: 0.014054333
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0641755e-06
Norm of the params: 13.376194
                Loss: fixed 862 labels. Loss 0.01405. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39020285
Train loss (w/o reg) on all data: 0.38416663
Test loss (w/o reg) on all data: 0.20371656
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.6178535e-05
Norm of the params: 10.987471
              Random: fixed 220 labels. Loss 0.20372. Accuracy 0.983.
### Flips: 1025, rs: 6, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15431912
Train loss (w/o reg) on all data: 0.14470948
Test loss (w/o reg) on all data: 0.06788693
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5703501e-05
Norm of the params: 13.863364
     Influence (LOO): fixed 651 labels. Loss 0.06789. Accuracy 0.995.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01337192
Train loss (w/o reg) on all data: 0.0070696594
Test loss (w/o reg) on all data: 0.009789967
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.1656159e-07
Norm of the params: 11.226986
                Loss: fixed 879 labels. Loss 0.00979. Accuracy 0.997.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3764642
Train loss (w/o reg) on all data: 0.370373
Test loss (w/o reg) on all data: 0.19296648
Train acc on all data:  0.849987843423292
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.21880585e-05
Norm of the params: 11.037369
              Random: fixed 260 labels. Loss 0.19297. Accuracy 0.986.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45782146
Train loss (w/o reg) on all data: 0.45265225
Test loss (w/o reg) on all data: 0.27381274
Train acc on all data:  0.7940675905664965
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.1465191e-05
Norm of the params: 10.167803
Flipped loss: 0.27381. Accuracy: 0.965
### Flips: 1025, rs: 7, checks: 205
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38191035
Train loss (w/o reg) on all data: 0.37257484
Test loss (w/o reg) on all data: 0.22076282
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.412172e-05
Norm of the params: 13.664192
     Influence (LOO): fixed 169 labels. Loss 0.22076. Accuracy 0.980.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34755868
Train loss (w/o reg) on all data: 0.33667406
Test loss (w/o reg) on all data: 0.22782052
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 8.506292e-06
Norm of the params: 14.754401
                Loss: fixed 205 labels. Loss 0.22782. Accuracy 0.945.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44736207
Train loss (w/o reg) on all data: 0.44217396
Test loss (w/o reg) on all data: 0.26164228
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.3351542e-05
Norm of the params: 10.186368
              Random: fixed  43 labels. Loss 0.26164. Accuracy 0.971.
### Flips: 1025, rs: 7, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3287685
Train loss (w/o reg) on all data: 0.31878692
Test loss (w/o reg) on all data: 0.17414531
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.6419733e-05
Norm of the params: 14.129099
     Influence (LOO): fixed 299 labels. Loss 0.17415. Accuracy 0.986.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23967777
Train loss (w/o reg) on all data: 0.22365488
Test loss (w/o reg) on all data: 0.177015
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 1.99318e-05
Norm of the params: 17.901337
                Loss: fixed 410 labels. Loss 0.17702. Accuracy 0.940.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4343372
Train loss (w/o reg) on all data: 0.42905796
Test loss (w/o reg) on all data: 0.24373853
Train acc on all data:  0.812545587162655
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.4080955e-05
Norm of the params: 10.27545
              Random: fixed  90 labels. Loss 0.24374. Accuracy 0.977.
### Flips: 1025, rs: 7, checks: 615
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2764349
Train loss (w/o reg) on all data: 0.2663858
Test loss (w/o reg) on all data: 0.14197733
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.806568e-06
Norm of the params: 14.176812
     Influence (LOO): fixed 413 labels. Loss 0.14198. Accuracy 0.988.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13594365
Train loss (w/o reg) on all data: 0.11625029
Test loss (w/o reg) on all data: 0.101885304
Train acc on all data:  0.9511305616338439
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.635402e-06
Norm of the params: 19.846092
                Loss: fixed 609 labels. Loss 0.10189. Accuracy 0.966.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42229077
Train loss (w/o reg) on all data: 0.4168318
Test loss (w/o reg) on all data: 0.23146588
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.184685e-05
Norm of the params: 10.448904
              Random: fixed 129 labels. Loss 0.23147. Accuracy 0.979.
### Flips: 1025, rs: 7, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23376492
Train loss (w/o reg) on all data: 0.22372267
Test loss (w/o reg) on all data: 0.111452125
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.4354175e-05
Norm of the params: 14.171979
     Influence (LOO): fixed 504 labels. Loss 0.11145. Accuracy 0.997.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049016967
Train loss (w/o reg) on all data: 0.03374184
Test loss (w/o reg) on all data: 0.036315102
Train acc on all data:  0.987114028689521
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.5889545e-06
Norm of the params: 17.47863
                Loss: fixed 785 labels. Loss 0.03632. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40579727
Train loss (w/o reg) on all data: 0.39999166
Test loss (w/o reg) on all data: 0.21894011
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.2259507e-05
Norm of the params: 10.775529
              Random: fixed 174 labels. Loss 0.21894. Accuracy 0.982.
### Flips: 1025, rs: 7, checks: 1025
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19699958
Train loss (w/o reg) on all data: 0.18692702
Test loss (w/o reg) on all data: 0.08972033
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7291002e-06
Norm of the params: 14.193345
     Influence (LOO): fixed 577 labels. Loss 0.08972. Accuracy 0.999.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019746661
Train loss (w/o reg) on all data: 0.010953951
Test loss (w/o reg) on all data: 0.011721595
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.0589196e-07
Norm of the params: 13.261004
                Loss: fixed 852 labels. Loss 0.01172. Accuracy 0.997.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3921833
Train loss (w/o reg) on all data: 0.38658467
Test loss (w/o reg) on all data: 0.20419945
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.6678968e-05
Norm of the params: 10.581725
              Random: fixed 222 labels. Loss 0.20420. Accuracy 0.981.
### Flips: 1025, rs: 7, checks: 1230
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562993
Train loss (w/o reg) on all data: 0.1469295
Test loss (w/o reg) on all data: 0.06850541
Train acc on all data:  0.9435934840748845
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6996998e-06
Norm of the params: 13.1912365
     Influence (LOO): fixed 649 labels. Loss 0.06851. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01386467
Train loss (w/o reg) on all data: 0.0071239695
Test loss (w/o reg) on all data: 0.0068975673
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.2861707e-07
Norm of the params: 11.610944
                Loss: fixed 864 labels. Loss 0.00690. Accuracy 0.999.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37333465
Train loss (w/o reg) on all data: 0.3676119
Test loss (w/o reg) on all data: 0.1834237
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.9395842e-05
Norm of the params: 10.698359
              Random: fixed 274 labels. Loss 0.18342. Accuracy 0.986.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45490015
Train loss (w/o reg) on all data: 0.44941226
Test loss (w/o reg) on all data: 0.27305242
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.4540522e-05
Norm of the params: 10.476525
Flipped loss: 0.27305. Accuracy: 0.960
### Flips: 1025, rs: 8, checks: 205
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37979603
Train loss (w/o reg) on all data: 0.37007722
Test loss (w/o reg) on all data: 0.21720207
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.229696e-06
Norm of the params: 13.941879
     Influence (LOO): fixed 172 labels. Loss 0.21720. Accuracy 0.974.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34307545
Train loss (w/o reg) on all data: 0.33219436
Test loss (w/o reg) on all data: 0.22596748
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 2.3561579e-05
Norm of the params: 14.75201
                Loss: fixed 205 labels. Loss 0.22597. Accuracy 0.941.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44324028
Train loss (w/o reg) on all data: 0.43754762
Test loss (w/o reg) on all data: 0.25611234
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.7171962e-05
Norm of the params: 10.670199
              Random: fixed  44 labels. Loss 0.25611. Accuracy 0.969.
### Flips: 1025, rs: 8, checks: 410
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31880227
Train loss (w/o reg) on all data: 0.30821472
Test loss (w/o reg) on all data: 0.17401627
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4275287e-05
Norm of the params: 14.55166
     Influence (LOO): fixed 307 labels. Loss 0.17402. Accuracy 0.979.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773435
Train loss (w/o reg) on all data: 0.22256592
Test loss (w/o reg) on all data: 0.16879576
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.41924e-05
Norm of the params: 17.41748
                Loss: fixed 410 labels. Loss 0.16880. Accuracy 0.946.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4313264
Train loss (w/o reg) on all data: 0.42558727
Test loss (w/o reg) on all data: 0.23960024
Train acc on all data:  0.812059324094335
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.5046833e-05
Norm of the params: 10.713662
              Random: fixed  88 labels. Loss 0.23960. Accuracy 0.975.
### Flips: 1025, rs: 8, checks: 615
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2708011
Train loss (w/o reg) on all data: 0.25870413
Test loss (w/o reg) on all data: 0.13916385
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.622948e-06
Norm of the params: 15.554401
     Influence (LOO): fixed 412 labels. Loss 0.13916. Accuracy 0.994.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13200389
Train loss (w/o reg) on all data: 0.115025006
Test loss (w/o reg) on all data: 0.11213356
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.4469805e-06
Norm of the params: 18.427635
                Loss: fixed 612 labels. Loss 0.11213. Accuracy 0.958.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41634545
Train loss (w/o reg) on all data: 0.41065636
Test loss (w/o reg) on all data: 0.22749685
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.2010415e-05
Norm of the params: 10.666859
              Random: fixed 136 labels. Loss 0.22750. Accuracy 0.978.
### Flips: 1025, rs: 8, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23153271
Train loss (w/o reg) on all data: 0.21993396
Test loss (w/o reg) on all data: 0.10826402
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0614121e-05
Norm of the params: 15.230728
     Influence (LOO): fixed 493 labels. Loss 0.10826. Accuracy 0.997.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048726976
Train loss (w/o reg) on all data: 0.03470749
Test loss (w/o reg) on all data: 0.037699368
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.92621e-06
Norm of the params: 16.74484
                Loss: fixed 790 labels. Loss 0.03770. Accuracy 0.987.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40026557
Train loss (w/o reg) on all data: 0.39464504
Test loss (w/o reg) on all data: 0.20877294
Train acc on all data:  0.8334548991004134
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.8092323e-05
Norm of the params: 10.602386
              Random: fixed 186 labels. Loss 0.20877. Accuracy 0.982.
### Flips: 1025, rs: 8, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1913657
Train loss (w/o reg) on all data: 0.18047422
Test loss (w/o reg) on all data: 0.08193214
Train acc on all data:  0.9263311451495259
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4387109e-05
Norm of the params: 14.759055
     Influence (LOO): fixed 571 labels. Loss 0.08193. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01529442
Train loss (w/o reg) on all data: 0.008140363
Test loss (w/o reg) on all data: 0.010286334
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.6251996e-07
Norm of the params: 11.961654
                Loss: fixed 860 labels. Loss 0.01029. Accuracy 0.997.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38474646
Train loss (w/o reg) on all data: 0.37895373
Test loss (w/o reg) on all data: 0.19955672
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1782729e-05
Norm of the params: 10.763582
              Random: fixed 227 labels. Loss 0.19956. Accuracy 0.984.
### Flips: 1025, rs: 8, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15777363
Train loss (w/o reg) on all data: 0.14737016
Test loss (w/o reg) on all data: 0.06588962
Train acc on all data:  0.9399465110624848
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.2154378e-05
Norm of the params: 14.424607
     Influence (LOO): fixed 632 labels. Loss 0.06589. Accuracy 0.999.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010454077
Train loss (w/o reg) on all data: 0.0050872704
Test loss (w/o reg) on all data: 0.0068140533
Train acc on all data:  0.9992706053975201
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.8302562e-07
Norm of the params: 10.360316
                Loss: fixed 870 labels. Loss 0.00681. Accuracy 0.999.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36840472
Train loss (w/o reg) on all data: 0.36267778
Test loss (w/o reg) on all data: 0.1851501
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.069208e-05
Norm of the params: 10.702287
              Random: fixed 273 labels. Loss 0.18515. Accuracy 0.983.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45652112
Train loss (w/o reg) on all data: 0.4514613
Test loss (w/o reg) on all data: 0.27666968
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 3.655826e-05
Norm of the params: 10.059667
Flipped loss: 0.27667. Accuracy: 0.963
### Flips: 1025, rs: 9, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38051823
Train loss (w/o reg) on all data: 0.37098473
Test loss (w/o reg) on all data: 0.22193871
Train acc on all data:  0.8312667152929735
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.9667701e-05
Norm of the params: 13.808331
     Influence (LOO): fixed 168 labels. Loss 0.22194. Accuracy 0.974.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34135497
Train loss (w/o reg) on all data: 0.32922938
Test loss (w/o reg) on all data: 0.23066035
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.6150012e-05
Norm of the params: 15.572792
                Loss: fixed 205 labels. Loss 0.23066. Accuracy 0.944.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44517902
Train loss (w/o reg) on all data: 0.43987623
Test loss (w/o reg) on all data: 0.2645447
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.5974081e-05
Norm of the params: 10.298331
              Random: fixed  42 labels. Loss 0.26454. Accuracy 0.965.
### Flips: 1025, rs: 9, checks: 410
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31712833
Train loss (w/o reg) on all data: 0.3057726
Test loss (w/o reg) on all data: 0.17486322
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.6930724e-05
Norm of the params: 15.070316
     Influence (LOO): fixed 304 labels. Loss 0.17486. Accuracy 0.981.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23907113
Train loss (w/o reg) on all data: 0.22236584
Test loss (w/o reg) on all data: 0.17093298
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.0639746e-06
Norm of the params: 18.278564
                Loss: fixed 409 labels. Loss 0.17093. Accuracy 0.944.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43287572
Train loss (w/o reg) on all data: 0.42756993
Test loss (w/o reg) on all data: 0.24953714
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.6040612e-05
Norm of the params: 10.301267
              Random: fixed  84 labels. Loss 0.24954. Accuracy 0.973.
### Flips: 1025, rs: 9, checks: 615
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733754
Train loss (w/o reg) on all data: 0.26198527
Test loss (w/o reg) on all data: 0.14113005
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 8.093368e-06
Norm of the params: 15.093126
     Influence (LOO): fixed 400 labels. Loss 0.14113. Accuracy 0.983.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13706999
Train loss (w/o reg) on all data: 0.11684107
Test loss (w/o reg) on all data: 0.10811223
Train acc on all data:  0.9484561147580841
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.4371499e-05
Norm of the params: 20.114132
                Loss: fixed 612 labels. Loss 0.10811. Accuracy 0.966.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41753098
Train loss (w/o reg) on all data: 0.4123081
Test loss (w/o reg) on all data: 0.23205565
Train acc on all data:  0.8205689277899344
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.135995e-05
Norm of the params: 10.220464
              Random: fixed 132 labels. Loss 0.23206. Accuracy 0.977.
### Flips: 1025, rs: 9, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2296869
Train loss (w/o reg) on all data: 0.2182392
Test loss (w/o reg) on all data: 0.113535345
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.0794415e-06
Norm of the params: 15.131229
     Influence (LOO): fixed 490 labels. Loss 0.11354. Accuracy 0.988.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049394146
Train loss (w/o reg) on all data: 0.034687553
Test loss (w/o reg) on all data: 0.034373324
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.4760045e-07
Norm of the params: 17.150272
                Loss: fixed 796 labels. Loss 0.03437. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40385923
Train loss (w/o reg) on all data: 0.39846754
Test loss (w/o reg) on all data: 0.21638255
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.1919319e-05
Norm of the params: 10.3843
              Random: fixed 172 labels. Loss 0.21638. Accuracy 0.979.
### Flips: 1025, rs: 9, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19609466
Train loss (w/o reg) on all data: 0.18515365
Test loss (w/o reg) on all data: 0.0934153
Train acc on all data:  0.9195234621930465
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.6859398e-06
Norm of the params: 14.792578
     Influence (LOO): fixed 557 labels. Loss 0.09342. Accuracy 0.990.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017151233
Train loss (w/o reg) on all data: 0.009593885
Test loss (w/o reg) on all data: 0.015192219
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.445726e-07
Norm of the params: 12.294184
                Loss: fixed 859 labels. Loss 0.01519. Accuracy 0.995.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38648126
Train loss (w/o reg) on all data: 0.38044512
Test loss (w/o reg) on all data: 0.20368992
Train acc on all data:  0.8395331874544129
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.099426e-05
Norm of the params: 10.987388
              Random: fixed 217 labels. Loss 0.20369. Accuracy 0.981.
### Flips: 1025, rs: 9, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16673155
Train loss (w/o reg) on all data: 0.1564795
Test loss (w/o reg) on all data: 0.07695882
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.539532e-06
Norm of the params: 14.319256
     Influence (LOO): fixed 612 labels. Loss 0.07696. Accuracy 0.995.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01277959
Train loss (w/o reg) on all data: 0.006693552
Test loss (w/o reg) on all data: 0.010418175
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.436564e-07
Norm of the params: 11.032713
                Loss: fixed 870 labels. Loss 0.01042. Accuracy 0.996.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37276405
Train loss (w/o reg) on all data: 0.3668053
Test loss (w/o reg) on all data: 0.19270284
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.424887e-05
Norm of the params: 10.916742
              Random: fixed 254 labels. Loss 0.19270. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4562268
Train loss (w/o reg) on all data: 0.45074454
Test loss (w/o reg) on all data: 0.26963878
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 4.5601053e-05
Norm of the params: 10.471167
Flipped loss: 0.26964. Accuracy: 0.955
### Flips: 1025, rs: 10, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37696394
Train loss (w/o reg) on all data: 0.3674848
Test loss (w/o reg) on all data: 0.2108584
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.0322426e-05
Norm of the params: 13.7689085
     Influence (LOO): fixed 170 labels. Loss 0.21086. Accuracy 0.973.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33845568
Train loss (w/o reg) on all data: 0.32547304
Test loss (w/o reg) on all data: 0.21557394
Train acc on all data:  0.849015317286652
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 6.71394e-06
Norm of the params: 16.113735
                Loss: fixed 205 labels. Loss 0.21557. Accuracy 0.936.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44354963
Train loss (w/o reg) on all data: 0.4379962
Test loss (w/o reg) on all data: 0.25213256
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.8118538e-05
Norm of the params: 10.538908
              Random: fixed  47 labels. Loss 0.25213. Accuracy 0.971.
### Flips: 1025, rs: 10, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3174385
Train loss (w/o reg) on all data: 0.30743393
Test loss (w/o reg) on all data: 0.16457145
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.381646e-05
Norm of the params: 14.145368
     Influence (LOO): fixed 307 labels. Loss 0.16457. Accuracy 0.986.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23842494
Train loss (w/o reg) on all data: 0.22134767
Test loss (w/o reg) on all data: 0.15950999
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 4.580684e-06
Norm of the params: 18.480947
                Loss: fixed 406 labels. Loss 0.15951. Accuracy 0.947.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336881
Train loss (w/o reg) on all data: 0.42821681
Test loss (w/o reg) on all data: 0.2381071
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.8261436e-05
Norm of the params: 10.460688
              Random: fixed  86 labels. Loss 0.23811. Accuracy 0.972.
### Flips: 1025, rs: 10, checks: 615
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25478402
Train loss (w/o reg) on all data: 0.24364756
Test loss (w/o reg) on all data: 0.1306032
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 8.542023e-06
Norm of the params: 14.924114
     Influence (LOO): fixed 429 labels. Loss 0.13060. Accuracy 0.990.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13901976
Train loss (w/o reg) on all data: 0.12039521
Test loss (w/o reg) on all data: 0.110091366
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.8020806e-06
Norm of the params: 19.300026
                Loss: fixed 608 labels. Loss 0.11009. Accuracy 0.960.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42143494
Train loss (w/o reg) on all data: 0.4161551
Test loss (w/o reg) on all data: 0.22295855
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 9.858147e-06
Norm of the params: 10.276028
              Random: fixed 128 labels. Loss 0.22296. Accuracy 0.978.
### Flips: 1025, rs: 10, checks: 820
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21868926
Train loss (w/o reg) on all data: 0.20707771
Test loss (w/o reg) on all data: 0.11190466
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 4.9442533e-06
Norm of the params: 15.239133
     Influence (LOO): fixed 500 labels. Loss 0.11190. Accuracy 0.988.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058711052
Train loss (w/o reg) on all data: 0.043632425
Test loss (w/o reg) on all data: 0.032278936
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 3.5999049e-06
Norm of the params: 17.365845
                Loss: fixed 787 labels. Loss 0.03228. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40748015
Train loss (w/o reg) on all data: 0.40224394
Test loss (w/o reg) on all data: 0.20958415
Train acc on all data:  0.8283491368830538
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.190285e-05
Norm of the params: 10.233486
              Random: fixed 172 labels. Loss 0.20958. Accuracy 0.983.
### Flips: 1025, rs: 10, checks: 1025
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18827975
Train loss (w/o reg) on all data: 0.176769
Test loss (w/o reg) on all data: 0.09309609
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.3006155e-06
Norm of the params: 15.1728325
     Influence (LOO): fixed 562 labels. Loss 0.09310. Accuracy 0.991.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019800767
Train loss (w/o reg) on all data: 0.011329265
Test loss (w/o reg) on all data: 0.013680539
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.099674e-07
Norm of the params: 13.016529
                Loss: fixed 860 labels. Loss 0.01368. Accuracy 0.997.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3899336
Train loss (w/o reg) on all data: 0.38432056
Test loss (w/o reg) on all data: 0.19430849
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.776748e-06
Norm of the params: 10.595317
              Random: fixed 221 labels. Loss 0.19431. Accuracy 0.982.
### Flips: 1025, rs: 10, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15768026
Train loss (w/o reg) on all data: 0.1466486
Test loss (w/o reg) on all data: 0.07417371
Train acc on all data:  0.9360564065159251
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 6.439477e-06
Norm of the params: 14.853722
     Influence (LOO): fixed 623 labels. Loss 0.07417. Accuracy 0.995.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013158156
Train loss (w/o reg) on all data: 0.0067365617
Test loss (w/o reg) on all data: 0.0071906615
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 9.293733e-08
Norm of the params: 11.332779
                Loss: fixed 871 labels. Loss 0.00719. Accuracy 0.999.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37059817
Train loss (w/o reg) on all data: 0.36441407
Test loss (w/o reg) on all data: 0.1790243
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.2888274e-05
Norm of the params: 11.121251
              Random: fixed 268 labels. Loss 0.17902. Accuracy 0.986.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45670733
Train loss (w/o reg) on all data: 0.45124716
Test loss (w/o reg) on all data: 0.27406204
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.1300893e-05
Norm of the params: 10.450032
Flipped loss: 0.27406. Accuracy: 0.962
### Flips: 1025, rs: 11, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3815005
Train loss (w/o reg) on all data: 0.37151256
Test loss (w/o reg) on all data: 0.21743599
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.7587248e-05
Norm of the params: 14.133616
     Influence (LOO): fixed 170 labels. Loss 0.21744. Accuracy 0.976.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34765515
Train loss (w/o reg) on all data: 0.33642787
Test loss (w/o reg) on all data: 0.22250235
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 1.0640229e-05
Norm of the params: 14.984855
                Loss: fixed 205 labels. Loss 0.22250. Accuracy 0.940.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44875893
Train loss (w/o reg) on all data: 0.44348672
Test loss (w/o reg) on all data: 0.26236814
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.7460357e-05
Norm of the params: 10.2685995
              Random: fixed  36 labels. Loss 0.26237. Accuracy 0.972.
### Flips: 1025, rs: 11, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32797164
Train loss (w/o reg) on all data: 0.31696063
Test loss (w/o reg) on all data: 0.18059629
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0734377e-05
Norm of the params: 14.839823
     Influence (LOO): fixed 298 labels. Loss 0.18060. Accuracy 0.977.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24468121
Train loss (w/o reg) on all data: 0.2281322
Test loss (w/o reg) on all data: 0.16633682
Train acc on all data:  0.8978847556528081
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 7.3705505e-06
Norm of the params: 18.192856
                Loss: fixed 409 labels. Loss 0.16634. Accuracy 0.948.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43436715
Train loss (w/o reg) on all data: 0.4289389
Test loss (w/o reg) on all data: 0.24753888
Train acc on all data:  0.8098711402868952
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.057361e-05
Norm of the params: 10.419464
              Random: fixed  87 labels. Loss 0.24754. Accuracy 0.973.
### Flips: 1025, rs: 11, checks: 615
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27805674
Train loss (w/o reg) on all data: 0.26719162
Test loss (w/o reg) on all data: 0.14917643
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 8.695557e-06
Norm of the params: 14.741192
     Influence (LOO): fixed 409 labels. Loss 0.14918. Accuracy 0.981.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14024815
Train loss (w/o reg) on all data: 0.12046213
Test loss (w/o reg) on all data: 0.10419889
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.16739e-06
Norm of the params: 19.892725
                Loss: fixed 610 labels. Loss 0.10420. Accuracy 0.966.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42199612
Train loss (w/o reg) on all data: 0.41662452
Test loss (w/o reg) on all data: 0.23186123
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.51098e-05
Norm of the params: 10.364935
              Random: fixed 132 labels. Loss 0.23186. Accuracy 0.979.
### Flips: 1025, rs: 11, checks: 820
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23095383
Train loss (w/o reg) on all data: 0.22074096
Test loss (w/o reg) on all data: 0.11688024
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 6.082636e-06
Norm of the params: 14.29186
     Influence (LOO): fixed 506 labels. Loss 0.11688. Accuracy 0.986.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058465973
Train loss (w/o reg) on all data: 0.041304544
Test loss (w/o reg) on all data: 0.036711913
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.1019936e-06
Norm of the params: 18.526428
                Loss: fixed 790 labels. Loss 0.03671. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.408202
Train loss (w/o reg) on all data: 0.40283096
Test loss (w/o reg) on all data: 0.21654795
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.93953e-05
Norm of the params: 10.364406
              Random: fixed 175 labels. Loss 0.21655. Accuracy 0.982.
### Flips: 1025, rs: 11, checks: 1025
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19388041
Train loss (w/o reg) on all data: 0.18364623
Test loss (w/o reg) on all data: 0.09156881
Train acc on all data:  0.9234135667396062
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.2835883e-06
Norm of the params: 14.306769
     Influence (LOO): fixed 581 labels. Loss 0.09157. Accuracy 0.990.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025823195
Train loss (w/o reg) on all data: 0.01530558
Test loss (w/o reg) on all data: 0.011733172
Train acc on all data:  0.9946511062484804
Test acc on all data:   1.0
Norm of the mean of gradients: 8.6415844e-07
Norm of the params: 14.503528
                Loss: fixed 859 labels. Loss 0.01173. Accuracy 1.000.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3885755
Train loss (w/o reg) on all data: 0.3829444
Test loss (w/o reg) on all data: 0.19864482
Train acc on all data:  0.8407488451252128
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.7136934e-05
Norm of the params: 10.612339
              Random: fixed 228 labels. Loss 0.19864. Accuracy 0.985.
### Flips: 1025, rs: 11, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16823013
Train loss (w/o reg) on all data: 0.15824842
Test loss (w/o reg) on all data: 0.077256046
Train acc on all data:  0.9338682227084852
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.414954e-06
Norm of the params: 14.1291895
     Influence (LOO): fixed 627 labels. Loss 0.07726. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01587369
Train loss (w/o reg) on all data: 0.008643518
Test loss (w/o reg) on all data: 0.0069181905
Train acc on all data:  0.9970824215900802
Test acc on all data:   1.0
Norm of the mean of gradients: 7.6275074e-07
Norm of the params: 12.025116
                Loss: fixed 878 labels. Loss 0.00692. Accuracy 1.000.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3728787
Train loss (w/o reg) on all data: 0.36712244
Test loss (w/o reg) on all data: 0.18584993
Train acc on all data:  0.8507172380257719
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.21743315e-05
Norm of the params: 10.729646
              Random: fixed 271 labels. Loss 0.18585. Accuracy 0.986.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45779845
Train loss (w/o reg) on all data: 0.4519661
Test loss (w/o reg) on all data: 0.28197494
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 1.481574e-05
Norm of the params: 10.80031
Flipped loss: 0.28197. Accuracy: 0.951
### Flips: 1025, rs: 12, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3840753
Train loss (w/o reg) on all data: 0.37388465
Test loss (w/o reg) on all data: 0.22250423
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.046278e-05
Norm of the params: 14.27632
     Influence (LOO): fixed 172 labels. Loss 0.22250. Accuracy 0.967.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34603846
Train loss (w/o reg) on all data: 0.33304304
Test loss (w/o reg) on all data: 0.23800458
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 3.745468e-05
Norm of the params: 16.12168
                Loss: fixed 205 labels. Loss 0.23800. Accuracy 0.921.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44141448
Train loss (w/o reg) on all data: 0.4353934
Test loss (w/o reg) on all data: 0.26317155
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.0859194e-05
Norm of the params: 10.9736805
              Random: fixed  61 labels. Loss 0.26317. Accuracy 0.962.
### Flips: 1025, rs: 12, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32637477
Train loss (w/o reg) on all data: 0.31456757
Test loss (w/o reg) on all data: 0.18257484
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.1629712e-05
Norm of the params: 15.366985
     Influence (LOO): fixed 305 labels. Loss 0.18257. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24330977
Train loss (w/o reg) on all data: 0.22563426
Test loss (w/o reg) on all data: 0.18404055
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 5.7109664e-06
Norm of the params: 18.801863
                Loss: fixed 409 labels. Loss 0.18404. Accuracy 0.930.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42863238
Train loss (w/o reg) on all data: 0.42210394
Test loss (w/o reg) on all data: 0.24798338
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.5567982e-05
Norm of the params: 11.426676
              Random: fixed 105 labels. Loss 0.24798. Accuracy 0.967.
### Flips: 1025, rs: 12, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28245008
Train loss (w/o reg) on all data: 0.27069014
Test loss (w/o reg) on all data: 0.14866643
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.1420543e-05
Norm of the params: 15.336192
     Influence (LOO): fixed 411 labels. Loss 0.14867. Accuracy 0.988.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14353982
Train loss (w/o reg) on all data: 0.12218842
Test loss (w/o reg) on all data: 0.12213067
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.6606295e-05
Norm of the params: 20.664654
                Loss: fixed 609 labels. Loss 0.12213. Accuracy 0.952.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41240937
Train loss (w/o reg) on all data: 0.40540057
Test loss (w/o reg) on all data: 0.23580118
Train acc on all data:  0.825188426938974
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.3582716e-05
Norm of the params: 11.839577
              Random: fixed 156 labels. Loss 0.23580. Accuracy 0.972.
### Flips: 1025, rs: 12, checks: 820
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2413197
Train loss (w/o reg) on all data: 0.23005351
Test loss (w/o reg) on all data: 0.12223968
Train acc on all data:  0.9032336494043277
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.179423e-06
Norm of the params: 15.010789
     Influence (LOO): fixed 496 labels. Loss 0.12224. Accuracy 0.987.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067878306
Train loss (w/o reg) on all data: 0.050253604
Test loss (w/o reg) on all data: 0.059608262
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.4381487e-06
Norm of the params: 18.774824
                Loss: fixed 779 labels. Loss 0.05961. Accuracy 0.982.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39587638
Train loss (w/o reg) on all data: 0.38895366
Test loss (w/o reg) on all data: 0.21839648
Train acc on all data:  0.8366156090444931
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.1173392e-05
Norm of the params: 11.766662
              Random: fixed 208 labels. Loss 0.21840. Accuracy 0.977.
### Flips: 1025, rs: 12, checks: 1025
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2002114
Train loss (w/o reg) on all data: 0.18944052
Test loss (w/o reg) on all data: 0.09784659
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.6933802e-06
Norm of the params: 14.677122
     Influence (LOO): fixed 573 labels. Loss 0.09785. Accuracy 0.994.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031914458
Train loss (w/o reg) on all data: 0.019969147
Test loss (w/o reg) on all data: 0.015804503
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.514132e-06
Norm of the params: 15.456592
                Loss: fixed 857 labels. Loss 0.01580. Accuracy 0.999.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38481367
Train loss (w/o reg) on all data: 0.3779312
Test loss (w/o reg) on all data: 0.20627418
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 3.7228507e-05
Norm of the params: 11.732404
              Random: fixed 244 labels. Loss 0.20627. Accuracy 0.980.
### Flips: 1025, rs: 12, checks: 1230
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16759482
Train loss (w/o reg) on all data: 0.15722023
Test loss (w/o reg) on all data: 0.07759813
Train acc on all data:  0.9353270119134451
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.0844733e-06
Norm of the params: 14.404574
     Influence (LOO): fixed 638 labels. Loss 0.07760. Accuracy 0.997.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01724381
Train loss (w/o reg) on all data: 0.008891035
Test loss (w/o reg) on all data: 0.009366943
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.8172403e-07
Norm of the params: 12.924996
                Loss: fixed 879 labels. Loss 0.00937. Accuracy 0.998.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3693017
Train loss (w/o reg) on all data: 0.3625381
Test loss (w/o reg) on all data: 0.18973942
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.2062364e-05
Norm of the params: 11.630665
              Random: fixed 290 labels. Loss 0.18974. Accuracy 0.981.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4554349
Train loss (w/o reg) on all data: 0.45021063
Test loss (w/o reg) on all data: 0.2767153
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.4736188e-05
Norm of the params: 10.221798
Flipped loss: 0.27672. Accuracy: 0.961
### Flips: 1025, rs: 13, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38016972
Train loss (w/o reg) on all data: 0.37101957
Test loss (w/o reg) on all data: 0.2190309
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.2343535e-05
Norm of the params: 13.527864
     Influence (LOO): fixed 169 labels. Loss 0.21903. Accuracy 0.968.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34518275
Train loss (w/o reg) on all data: 0.33429646
Test loss (w/o reg) on all data: 0.23080195
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.9746405e-05
Norm of the params: 14.755527
                Loss: fixed 204 labels. Loss 0.23080. Accuracy 0.934.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44514892
Train loss (w/o reg) on all data: 0.43998033
Test loss (w/o reg) on all data: 0.26140714
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.05959725e-05
Norm of the params: 10.16718
              Random: fixed  41 labels. Loss 0.26141. Accuracy 0.969.
### Flips: 1025, rs: 13, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31911057
Train loss (w/o reg) on all data: 0.30808294
Test loss (w/o reg) on all data: 0.18247332
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4035378e-05
Norm of the params: 14.851016
     Influence (LOO): fixed 303 labels. Loss 0.18247. Accuracy 0.980.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24278343
Train loss (w/o reg) on all data: 0.22743872
Test loss (w/o reg) on all data: 0.17894436
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.0795007e-05
Norm of the params: 17.5184
                Loss: fixed 409 labels. Loss 0.17894. Accuracy 0.944.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43320844
Train loss (w/o reg) on all data: 0.42835978
Test loss (w/o reg) on all data: 0.2446555
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.2921746e-05
Norm of the params: 9.847508
              Random: fixed  87 labels. Loss 0.24466. Accuracy 0.975.
### Flips: 1025, rs: 13, checks: 615
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2708994
Train loss (w/o reg) on all data: 0.25998074
Test loss (w/o reg) on all data: 0.14829426
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 9.28808e-06
Norm of the params: 14.777456
     Influence (LOO): fixed 409 labels. Loss 0.14829. Accuracy 0.982.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14250441
Train loss (w/o reg) on all data: 0.12383014
Test loss (w/o reg) on all data: 0.11814427
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8021308e-06
Norm of the params: 19.325768
                Loss: fixed 608 labels. Loss 0.11814. Accuracy 0.958.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42074588
Train loss (w/o reg) on all data: 0.4157927
Test loss (w/o reg) on all data: 0.23265895
Train acc on all data:  0.8188670070508145
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.2089025e-05
Norm of the params: 9.953056
              Random: fixed 126 labels. Loss 0.23266. Accuracy 0.975.
### Flips: 1025, rs: 13, checks: 820
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23064461
Train loss (w/o reg) on all data: 0.21997839
Test loss (w/o reg) on all data: 0.12023815
Train acc on all data:  0.9076100170192074
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.2624143e-05
Norm of the params: 14.605636
     Influence (LOO): fixed 495 labels. Loss 0.12024. Accuracy 0.984.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05076549
Train loss (w/o reg) on all data: 0.035951458
Test loss (w/o reg) on all data: 0.048917823
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.9126069e-06
Norm of the params: 17.212803
                Loss: fixed 788 labels. Loss 0.04892. Accuracy 0.986.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40750962
Train loss (w/o reg) on all data: 0.40257728
Test loss (w/o reg) on all data: 0.2197138
Train acc on all data:  0.8281060053488938
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.4499047e-05
Norm of the params: 9.932102
              Random: fixed 169 labels. Loss 0.21971. Accuracy 0.977.
### Flips: 1025, rs: 13, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19304998
Train loss (w/o reg) on all data: 0.18231808
Test loss (w/o reg) on all data: 0.09783757
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.5857982e-05
Norm of the params: 14.650535
     Influence (LOO): fixed 567 labels. Loss 0.09784. Accuracy 0.986.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018986132
Train loss (w/o reg) on all data: 0.0107249
Test loss (w/o reg) on all data: 0.013506995
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.115851e-07
Norm of the params: 12.853974
                Loss: fixed 853 labels. Loss 0.01351. Accuracy 0.998.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39396533
Train loss (w/o reg) on all data: 0.38886815
Test loss (w/o reg) on all data: 0.20363358
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1303104e-05
Norm of the params: 10.096706
              Random: fixed 211 labels. Loss 0.20363. Accuracy 0.982.
### Flips: 1025, rs: 13, checks: 1230
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15579247
Train loss (w/o reg) on all data: 0.14524055
Test loss (w/o reg) on all data: 0.07835551
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.132153e-06
Norm of the params: 14.5271635
     Influence (LOO): fixed 636 labels. Loss 0.07836. Accuracy 0.989.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01534229
Train loss (w/o reg) on all data: 0.008421993
Test loss (w/o reg) on all data: 0.008940329
Train acc on all data:  0.9975686846584002
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1073905e-07
Norm of the params: 11.764606
                Loss: fixed 865 labels. Loss 0.00894. Accuracy 1.000.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37940386
Train loss (w/o reg) on all data: 0.37373576
Test loss (w/o reg) on all data: 0.19323003
Train acc on all data:  0.8477996596158521
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2647636e-05
Norm of the params: 10.647161
              Random: fixed 249 labels. Loss 0.19323. Accuracy 0.983.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45584434
Train loss (w/o reg) on all data: 0.45156336
Test loss (w/o reg) on all data: 0.26268202
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.6793627e-05
Norm of the params: 9.253077
Flipped loss: 0.26268. Accuracy: 0.976
### Flips: 1025, rs: 14, checks: 205
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37830403
Train loss (w/o reg) on all data: 0.3697381
Test loss (w/o reg) on all data: 0.19903146
Train acc on all data:  0.8349136883053732
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 8.404022e-06
Norm of the params: 13.088879
     Influence (LOO): fixed 175 labels. Loss 0.19903. Accuracy 0.984.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34058973
Train loss (w/o reg) on all data: 0.3296081
Test loss (w/o reg) on all data: 0.22395614
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.9319486e-05
Norm of the params: 14.820009
                Loss: fixed 205 labels. Loss 0.22396. Accuracy 0.938.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44051078
Train loss (w/o reg) on all data: 0.43606648
Test loss (w/o reg) on all data: 0.24704657
Train acc on all data:  0.8045222465353756
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.6076e-05
Norm of the params: 9.427947
              Random: fixed  48 labels. Loss 0.24705. Accuracy 0.977.
### Flips: 1025, rs: 14, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31954423
Train loss (w/o reg) on all data: 0.3098751
Test loss (w/o reg) on all data: 0.15893076
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3404895e-05
Norm of the params: 13.906212
     Influence (LOO): fixed 312 labels. Loss 0.15893. Accuracy 0.987.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23255354
Train loss (w/o reg) on all data: 0.21599628
Test loss (w/o reg) on all data: 0.16376425
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4558793e-05
Norm of the params: 18.197392
                Loss: fixed 410 labels. Loss 0.16376. Accuracy 0.956.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4283376
Train loss (w/o reg) on all data: 0.42382777
Test loss (w/o reg) on all data: 0.2346464
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5181726e-05
Norm of the params: 9.497204
              Random: fixed  88 labels. Loss 0.23465. Accuracy 0.976.
### Flips: 1025, rs: 14, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27396068
Train loss (w/o reg) on all data: 0.2631993
Test loss (w/o reg) on all data: 0.13066797
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.139156e-06
Norm of the params: 14.670628
     Influence (LOO): fixed 410 labels. Loss 0.13067. Accuracy 0.993.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12974042
Train loss (w/o reg) on all data: 0.110791594
Test loss (w/o reg) on all data: 0.106244735
Train acc on all data:  0.9491855093605641
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.2466519e-06
Norm of the params: 19.467316
                Loss: fixed 612 labels. Loss 0.10624. Accuracy 0.973.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.416428
Train loss (w/o reg) on all data: 0.41167626
Test loss (w/o reg) on all data: 0.22374514
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.4954333e-05
Norm of the params: 9.748596
              Random: fixed 126 labels. Loss 0.22375. Accuracy 0.982.
### Flips: 1025, rs: 14, checks: 820
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22896503
Train loss (w/o reg) on all data: 0.21835299
Test loss (w/o reg) on all data: 0.10820946
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.5757967e-05
Norm of the params: 14.568487
     Influence (LOO): fixed 500 labels. Loss 0.10821. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049289133
Train loss (w/o reg) on all data: 0.03454115
Test loss (w/o reg) on all data: 0.040560316
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0399876e-06
Norm of the params: 17.17439
                Loss: fixed 792 labels. Loss 0.04056. Accuracy 0.990.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4030697
Train loss (w/o reg) on all data: 0.39846033
Test loss (w/o reg) on all data: 0.21160783
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.6090333e-05
Norm of the params: 9.601441
              Random: fixed 169 labels. Loss 0.21161. Accuracy 0.979.
### Flips: 1025, rs: 14, checks: 1025
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260609
Train loss (w/o reg) on all data: 0.18231994
Test loss (w/o reg) on all data: 0.08597405
Train acc on all data:  0.925601750547046
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5636666e-05
Norm of the params: 14.343043
     Influence (LOO): fixed 572 labels. Loss 0.08597. Accuracy 0.995.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02032661
Train loss (w/o reg) on all data: 0.0117239235
Test loss (w/o reg) on all data: 0.009317574
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.0096903e-06
Norm of the params: 13.116926
                Loss: fixed 854 labels. Loss 0.00932. Accuracy 0.999.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3894111
Train loss (w/o reg) on all data: 0.38456678
Test loss (w/o reg) on all data: 0.19721355
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.6016678e-05
Norm of the params: 9.843063
              Random: fixed 211 labels. Loss 0.19721. Accuracy 0.982.
### Flips: 1025, rs: 14, checks: 1230
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1545756
Train loss (w/o reg) on all data: 0.14521816
Test loss (w/o reg) on all data: 0.069076836
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.963603e-06
Norm of the params: 13.680228
     Influence (LOO): fixed 642 labels. Loss 0.06908. Accuracy 0.994.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011795572
Train loss (w/o reg) on all data: 0.0058887415
Test loss (w/o reg) on all data: 0.0051523824
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 1.57598e-07
Norm of the params: 10.869067
                Loss: fixed 867 labels. Loss 0.00515. Accuracy 1.000.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36877567
Train loss (w/o reg) on all data: 0.36369985
Test loss (w/o reg) on all data: 0.17935051
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.8141581e-05
Norm of the params: 10.075517
              Random: fixed 266 labels. Loss 0.17935. Accuracy 0.983.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4519427
Train loss (w/o reg) on all data: 0.44569594
Test loss (w/o reg) on all data: 0.26974815
Train acc on all data:  0.7916362752248967
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 5.3960426e-05
Norm of the params: 11.177457
Flipped loss: 0.26975. Accuracy: 0.965
### Flips: 1025, rs: 15, checks: 205
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37608027
Train loss (w/o reg) on all data: 0.36550426
Test loss (w/o reg) on all data: 0.21651232
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.601209e-05
Norm of the params: 14.543737
     Influence (LOO): fixed 168 labels. Loss 0.21651. Accuracy 0.975.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34230596
Train loss (w/o reg) on all data: 0.3294433
Test loss (w/o reg) on all data: 0.21266459
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 6.796438e-06
Norm of the params: 16.039112
                Loss: fixed 205 labels. Loss 0.21266. Accuracy 0.956.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4409179
Train loss (w/o reg) on all data: 0.43473923
Test loss (w/o reg) on all data: 0.2557293
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.1920052e-05
Norm of the params: 11.116364
              Random: fixed  48 labels. Loss 0.25573. Accuracy 0.969.
### Flips: 1025, rs: 15, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32616574
Train loss (w/o reg) on all data: 0.31458563
Test loss (w/o reg) on all data: 0.17192586
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2845895e-05
Norm of the params: 15.2184725
     Influence (LOO): fixed 296 labels. Loss 0.17193. Accuracy 0.990.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2403428
Train loss (w/o reg) on all data: 0.22330528
Test loss (w/o reg) on all data: 0.15798606
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.10617975e-05
Norm of the params: 18.459415
                Loss: fixed 409 labels. Loss 0.15799. Accuracy 0.954.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42968124
Train loss (w/o reg) on all data: 0.42366505
Test loss (w/o reg) on all data: 0.2406591
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.283388e-05
Norm of the params: 10.969218
              Random: fixed  90 labels. Loss 0.24066. Accuracy 0.974.
### Flips: 1025, rs: 15, checks: 615
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28125796
Train loss (w/o reg) on all data: 0.2690615
Test loss (w/o reg) on all data: 0.1395385
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7760855e-05
Norm of the params: 15.618221
     Influence (LOO): fixed 402 labels. Loss 0.13954. Accuracy 0.994.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14081505
Train loss (w/o reg) on all data: 0.1196543
Test loss (w/o reg) on all data: 0.10245421
Train acc on all data:  0.9486992462922441
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.29190175e-05
Norm of the params: 20.57219
                Loss: fixed 608 labels. Loss 0.10245. Accuracy 0.973.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4160057
Train loss (w/o reg) on all data: 0.40998754
Test loss (w/o reg) on all data: 0.2268162
Train acc on all data:  0.8227571115973742
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.535883e-05
Norm of the params: 10.9710245
              Random: fixed 141 labels. Loss 0.22682. Accuracy 0.975.
### Flips: 1025, rs: 15, checks: 820
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23770148
Train loss (w/o reg) on all data: 0.22616936
Test loss (w/o reg) on all data: 0.111147836
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.953373e-06
Norm of the params: 15.186908
     Influence (LOO): fixed 495 labels. Loss 0.11115. Accuracy 0.997.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054383695
Train loss (w/o reg) on all data: 0.03855531
Test loss (w/o reg) on all data: 0.038727395
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1437393e-06
Norm of the params: 17.792353
                Loss: fixed 783 labels. Loss 0.03873. Accuracy 0.990.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40563744
Train loss (w/o reg) on all data: 0.3994491
Test loss (w/o reg) on all data: 0.21662354
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.557339e-05
Norm of the params: 11.125048
              Random: fixed 175 labels. Loss 0.21662. Accuracy 0.976.
### Flips: 1025, rs: 15, checks: 1025
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1967819
Train loss (w/o reg) on all data: 0.18562965
Test loss (w/o reg) on all data: 0.09053326
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.677428e-06
Norm of the params: 14.934694
     Influence (LOO): fixed 572 labels. Loss 0.09053. Accuracy 0.998.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021416895
Train loss (w/o reg) on all data: 0.012407047
Test loss (w/o reg) on all data: 0.013247588
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 8.7235765e-07
Norm of the params: 13.423747
                Loss: fixed 856 labels. Loss 0.01325. Accuracy 0.999.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39030746
Train loss (w/o reg) on all data: 0.38405734
Test loss (w/o reg) on all data: 0.20268482
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.31430315e-05
Norm of the params: 11.180437
              Random: fixed 223 labels. Loss 0.20268. Accuracy 0.980.
### Flips: 1025, rs: 15, checks: 1230
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16366363
Train loss (w/o reg) on all data: 0.15320668
Test loss (w/o reg) on all data: 0.07378102
Train acc on all data:  0.937028932652565
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3872191e-05
Norm of the params: 14.46164
     Influence (LOO): fixed 634 labels. Loss 0.07378. Accuracy 1.000.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016593792
Train loss (w/o reg) on all data: 0.009240018
Test loss (w/o reg) on all data: 0.0085834935
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 7.1961597e-07
Norm of the params: 12.127467
                Loss: fixed 867 labels. Loss 0.00858. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37395346
Train loss (w/o reg) on all data: 0.3677958
Test loss (w/o reg) on all data: 0.1877652
Train acc on all data:  0.8538779479698517
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.8674878e-05
Norm of the params: 11.097454
              Random: fixed 272 labels. Loss 0.18777. Accuracy 0.983.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4560412
Train loss (w/o reg) on all data: 0.45066908
Test loss (w/o reg) on all data: 0.26157874
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3112769e-05
Norm of the params: 10.365436
Flipped loss: 0.26158. Accuracy: 0.975
### Flips: 1025, rs: 16, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37794274
Train loss (w/o reg) on all data: 0.3688167
Test loss (w/o reg) on all data: 0.20614965
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 7.7750665e-06
Norm of the params: 13.510031
     Influence (LOO): fixed 173 labels. Loss 0.20615. Accuracy 0.987.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3423682
Train loss (w/o reg) on all data: 0.32970494
Test loss (w/o reg) on all data: 0.21076015
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4294314e-05
Norm of the params: 15.91429
                Loss: fixed 205 labels. Loss 0.21076. Accuracy 0.949.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44206157
Train loss (w/o reg) on all data: 0.4362592
Test loss (w/o reg) on all data: 0.24675749
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.803388e-05
Norm of the params: 10.772518
              Random: fixed  47 labels. Loss 0.24676. Accuracy 0.981.
### Flips: 1025, rs: 16, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31794727
Train loss (w/o reg) on all data: 0.30772093
Test loss (w/o reg) on all data: 0.16422684
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.091899e-06
Norm of the params: 14.301295
     Influence (LOO): fixed 310 labels. Loss 0.16423. Accuracy 0.988.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23948461
Train loss (w/o reg) on all data: 0.22226968
Test loss (w/o reg) on all data: 0.1506725
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 6.5534755e-06
Norm of the params: 18.55528
                Loss: fixed 410 labels. Loss 0.15067. Accuracy 0.957.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43047997
Train loss (w/o reg) on all data: 0.42451382
Test loss (w/o reg) on all data: 0.23653091
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2757497e-05
Norm of the params: 10.923514
              Random: fixed  90 labels. Loss 0.23653. Accuracy 0.979.
### Flips: 1025, rs: 16, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27049896
Train loss (w/o reg) on all data: 0.26016504
Test loss (w/o reg) on all data: 0.13444263
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.3574678e-05
Norm of the params: 14.376326
     Influence (LOO): fixed 413 labels. Loss 0.13444. Accuracy 0.993.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13716236
Train loss (w/o reg) on all data: 0.11742647
Test loss (w/o reg) on all data: 0.08280599
Train acc on all data:  0.950401167031364
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 6.1529e-06
Norm of the params: 19.867502
                Loss: fixed 615 labels. Loss 0.08281. Accuracy 0.976.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41640943
Train loss (w/o reg) on all data: 0.41027552
Test loss (w/o reg) on all data: 0.21914667
Train acc on all data:  0.825674690007294
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2221768e-05
Norm of the params: 11.07601
              Random: fixed 139 labels. Loss 0.21915. Accuracy 0.979.
### Flips: 1025, rs: 16, checks: 820
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22021368
Train loss (w/o reg) on all data: 0.20996518
Test loss (w/o reg) on all data: 0.106908225
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 8.891797e-06
Norm of the params: 14.316768
     Influence (LOO): fixed 509 labels. Loss 0.10691. Accuracy 0.994.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04187885
Train loss (w/o reg) on all data: 0.027412346
Test loss (w/o reg) on all data: 0.024505263
Train acc on all data:  0.9895453440311208
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 7.4242394e-07
Norm of the params: 17.009705
                Loss: fixed 803 labels. Loss 0.02451. Accuracy 0.993.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40300354
Train loss (w/o reg) on all data: 0.39675418
Test loss (w/o reg) on all data: 0.20656441
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.1795243e-05
Norm of the params: 11.1797695
              Random: fixed 180 labels. Loss 0.20656. Accuracy 0.982.
### Flips: 1025, rs: 16, checks: 1025
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18310489
Train loss (w/o reg) on all data: 0.17279874
Test loss (w/o reg) on all data: 0.08425222
Train acc on all data:  0.9277899343544858
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.9792893e-05
Norm of the params: 14.356981
     Influence (LOO): fixed 579 labels. Loss 0.08425. Accuracy 0.998.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020652216
Train loss (w/o reg) on all data: 0.011360529
Test loss (w/o reg) on all data: 0.011154525
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.4418339e-06
Norm of the params: 13.632086
                Loss: fixed 853 labels. Loss 0.01115. Accuracy 0.999.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38706145
Train loss (w/o reg) on all data: 0.38029733
Test loss (w/o reg) on all data: 0.19374633
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6034039e-05
Norm of the params: 11.6311035
              Random: fixed 223 labels. Loss 0.19375. Accuracy 0.984.
### Flips: 1025, rs: 16, checks: 1230
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15621883
Train loss (w/o reg) on all data: 0.14662716
Test loss (w/o reg) on all data: 0.07042802
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 8.318121e-06
Norm of the params: 13.850392
     Influence (LOO): fixed 629 labels. Loss 0.07043. Accuracy 0.998.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012743596
Train loss (w/o reg) on all data: 0.0062685134
Test loss (w/o reg) on all data: 0.006224231
Train acc on all data:  0.9985412107950401
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0843663e-07
Norm of the params: 11.379879
                Loss: fixed 868 labels. Loss 0.00622. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37135735
Train loss (w/o reg) on all data: 0.36459833
Test loss (w/o reg) on all data: 0.1786801
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2371845e-05
Norm of the params: 11.626701
              Random: fixed 271 labels. Loss 0.17868. Accuracy 0.986.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4492957
Train loss (w/o reg) on all data: 0.4430098
Test loss (w/o reg) on all data: 0.26138517
Train acc on all data:  0.7947969851689765
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3895e-05
Norm of the params: 11.212406
Flipped loss: 0.26139. Accuracy: 0.972
### Flips: 1025, rs: 17, checks: 205
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38043353
Train loss (w/o reg) on all data: 0.37090525
Test loss (w/o reg) on all data: 0.2082738
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.0319988e-05
Norm of the params: 13.804549
     Influence (LOO): fixed 162 labels. Loss 0.20827. Accuracy 0.977.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34013516
Train loss (w/o reg) on all data: 0.32770854
Test loss (w/o reg) on all data: 0.20816372
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4064537e-05
Norm of the params: 15.764903
                Loss: fixed 205 labels. Loss 0.20816. Accuracy 0.949.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4347162
Train loss (w/o reg) on all data: 0.42811212
Test loss (w/o reg) on all data: 0.24739158
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.5390693e-05
Norm of the params: 11.492678
              Random: fixed  48 labels. Loss 0.24739. Accuracy 0.971.
### Flips: 1025, rs: 17, checks: 410
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3245089
Train loss (w/o reg) on all data: 0.31344
Test loss (w/o reg) on all data: 0.16219985
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2304865e-05
Norm of the params: 14.878781
     Influence (LOO): fixed 298 labels. Loss 0.16220. Accuracy 0.988.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23856835
Train loss (w/o reg) on all data: 0.2215144
Test loss (w/o reg) on all data: 0.15366451
Train acc on all data:  0.899829807926088
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 5.7977923e-06
Norm of the params: 18.468325
                Loss: fixed 408 labels. Loss 0.15366. Accuracy 0.956.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4234893
Train loss (w/o reg) on all data: 0.41686073
Test loss (w/o reg) on all data: 0.23593229
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.3217084e-05
Norm of the params: 11.513979
              Random: fixed  87 labels. Loss 0.23593. Accuracy 0.973.
### Flips: 1025, rs: 17, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2743718
Train loss (w/o reg) on all data: 0.26316398
Test loss (w/o reg) on all data: 0.13091049
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.47397195e-05
Norm of the params: 14.971855
     Influence (LOO): fixed 409 labels. Loss 0.13091. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13520508
Train loss (w/o reg) on all data: 0.11493914
Test loss (w/o reg) on all data: 0.09290709
Train acc on all data:  0.9523462193046438
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.321746e-06
Norm of the params: 20.132526
                Loss: fixed 607 labels. Loss 0.09291. Accuracy 0.976.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40745685
Train loss (w/o reg) on all data: 0.400589
Test loss (w/o reg) on all data: 0.22422035
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.0179403e-05
Norm of the params: 11.719949
              Random: fixed 133 labels. Loss 0.22422. Accuracy 0.969.
### Flips: 1025, rs: 17, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22440553
Train loss (w/o reg) on all data: 0.21342805
Test loss (w/o reg) on all data: 0.10194998
Train acc on all data:  0.9115001215657671
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2094015e-05
Norm of the params: 14.817203
     Influence (LOO): fixed 514 labels. Loss 0.10195. Accuracy 0.994.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049666554
Train loss (w/o reg) on all data: 0.03377505
Test loss (w/o reg) on all data: 0.039527155
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 8.167066e-06
Norm of the params: 17.82779
                Loss: fixed 780 labels. Loss 0.03953. Accuracy 0.988.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3928749
Train loss (w/o reg) on all data: 0.3858308
Test loss (w/o reg) on all data: 0.20963284
Train acc on all data:  0.8363724775103331
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0808669e-05
Norm of the params: 11.869377
              Random: fixed 176 labels. Loss 0.20963. Accuracy 0.975.
### Flips: 1025, rs: 17, checks: 1025
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19157916
Train loss (w/o reg) on all data: 0.18124764
Test loss (w/o reg) on all data: 0.084141895
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.14086e-06
Norm of the params: 14.374645
     Influence (LOO): fixed 579 labels. Loss 0.08414. Accuracy 0.996.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027317205
Train loss (w/o reg) on all data: 0.016258737
Test loss (w/o reg) on all data: 0.014285675
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.318626e-06
Norm of the params: 14.871764
                Loss: fixed 832 labels. Loss 0.01429. Accuracy 0.998.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3766378
Train loss (w/o reg) on all data: 0.36958843
Test loss (w/o reg) on all data: 0.19492115
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.4489496e-05
Norm of the params: 11.873805
              Random: fixed 226 labels. Loss 0.19492. Accuracy 0.976.
### Flips: 1025, rs: 17, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15486585
Train loss (w/o reg) on all data: 0.14555076
Test loss (w/o reg) on all data: 0.06706514
Train acc on all data:  0.9426209579382446
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.2875004e-06
Norm of the params: 13.649236
     Influence (LOO): fixed 649 labels. Loss 0.06707. Accuracy 0.994.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020251218
Train loss (w/o reg) on all data: 0.011429893
Test loss (w/o reg) on all data: 0.009101208
Train acc on all data:  0.9968392900559202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.193463e-07
Norm of the params: 13.282564
                Loss: fixed 849 labels. Loss 0.00910. Accuracy 0.999.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3601671
Train loss (w/o reg) on all data: 0.35279182
Test loss (w/o reg) on all data: 0.18525583
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.6241236e-05
Norm of the params: 12.14518
              Random: fixed 269 labels. Loss 0.18526. Accuracy 0.981.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46249828
Train loss (w/o reg) on all data: 0.45749184
Test loss (w/o reg) on all data: 0.27889532
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.7472166e-05
Norm of the params: 10.00642
Flipped loss: 0.27890. Accuracy: 0.966
### Flips: 1025, rs: 18, checks: 205
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38607186
Train loss (w/o reg) on all data: 0.37693682
Test loss (w/o reg) on all data: 0.22011676
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.5251851e-05
Norm of the params: 13.516694
     Influence (LOO): fixed 170 labels. Loss 0.22012. Accuracy 0.983.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3512927
Train loss (w/o reg) on all data: 0.34055752
Test loss (w/o reg) on all data: 0.22659884
Train acc on all data:  0.8397763189885729
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 2.580885e-05
Norm of the params: 14.65277
                Loss: fixed 205 labels. Loss 0.22660. Accuracy 0.940.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44922546
Train loss (w/o reg) on all data: 0.44408983
Test loss (w/o reg) on all data: 0.2588999
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6720967e-05
Norm of the params: 10.134711
              Random: fixed  52 labels. Loss 0.25890. Accuracy 0.973.
### Flips: 1025, rs: 18, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3279386
Train loss (w/o reg) on all data: 0.31711134
Test loss (w/o reg) on all data: 0.1764967
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.8798644e-05
Norm of the params: 14.715468
     Influence (LOO): fixed 301 labels. Loss 0.17650. Accuracy 0.983.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24778362
Train loss (w/o reg) on all data: 0.2326822
Test loss (w/o reg) on all data: 0.16608396
Train acc on all data:  0.8918064672988086
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 8.243191e-06
Norm of the params: 17.378963
                Loss: fixed 409 labels. Loss 0.16608. Accuracy 0.955.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4405687
Train loss (w/o reg) on all data: 0.43526796
Test loss (w/o reg) on all data: 0.24849145
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 6.9340414e-05
Norm of the params: 10.296325
              Random: fixed  83 labels. Loss 0.24849. Accuracy 0.979.
### Flips: 1025, rs: 18, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28241724
Train loss (w/o reg) on all data: 0.27193883
Test loss (w/o reg) on all data: 0.14236763
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.5233707e-05
Norm of the params: 14.476469
     Influence (LOO): fixed 400 labels. Loss 0.14237. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14662786
Train loss (w/o reg) on all data: 0.12816687
Test loss (w/o reg) on all data: 0.102737084
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.2173546e-06
Norm of the params: 19.215092
                Loss: fixed 612 labels. Loss 0.10274. Accuracy 0.970.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42563024
Train loss (w/o reg) on all data: 0.42034212
Test loss (w/o reg) on all data: 0.23173675
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.25087845e-05
Norm of the params: 10.284098
              Random: fixed 130 labels. Loss 0.23174. Accuracy 0.984.
### Flips: 1025, rs: 18, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23954569
Train loss (w/o reg) on all data: 0.22881626
Test loss (w/o reg) on all data: 0.11716475
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.593086e-05
Norm of the params: 14.648842
     Influence (LOO): fixed 484 labels. Loss 0.11716. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05404059
Train loss (w/o reg) on all data: 0.03961494
Test loss (w/o reg) on all data: 0.036243472
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6740314e-06
Norm of the params: 16.985672
                Loss: fixed 805 labels. Loss 0.03624. Accuracy 0.989.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41410384
Train loss (w/o reg) on all data: 0.40860918
Test loss (w/o reg) on all data: 0.22047462
Train acc on all data:  0.824702163870654
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.2338478e-05
Norm of the params: 10.482986
              Random: fixed 168 labels. Loss 0.22047. Accuracy 0.983.
### Flips: 1025, rs: 18, checks: 1025
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2017337
Train loss (w/o reg) on all data: 0.19147015
Test loss (w/o reg) on all data: 0.09350866
Train acc on all data:  0.9204959883296864
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.935562e-06
Norm of the params: 14.327275
     Influence (LOO): fixed 565 labels. Loss 0.09351. Accuracy 0.997.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014521884
Train loss (w/o reg) on all data: 0.0076785865
Test loss (w/o reg) on all data: 0.007935281
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.254144e-07
Norm of the params: 11.698972
                Loss: fixed 883 labels. Loss 0.00794. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40128472
Train loss (w/o reg) on all data: 0.39550823
Test loss (w/o reg) on all data: 0.21217333
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.3076031e-05
Norm of the params: 10.748489
              Random: fixed 203 labels. Loss 0.21217. Accuracy 0.983.
### Flips: 1025, rs: 18, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16479409
Train loss (w/o reg) on all data: 0.15435527
Test loss (w/o reg) on all data: 0.074067295
Train acc on all data:  0.937028932652565
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.23713e-06
Norm of the params: 14.449092
     Influence (LOO): fixed 638 labels. Loss 0.07407. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00980421
Train loss (w/o reg) on all data: 0.004831326
Test loss (w/o reg) on all data: 0.0050345045
Train acc on all data:  0.9987843423292001
Test acc on all data:   1.0
Norm of the mean of gradients: 1.785917e-07
Norm of the params: 9.972847
                Loss: fixed 892 labels. Loss 0.00503. Accuracy 1.000.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38475314
Train loss (w/o reg) on all data: 0.37864938
Test loss (w/o reg) on all data: 0.1974452
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0947793e-05
Norm of the params: 11.048771
              Random: fixed 249 labels. Loss 0.19745. Accuracy 0.983.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45182028
Train loss (w/o reg) on all data: 0.4468468
Test loss (w/o reg) on all data: 0.26221377
Train acc on all data:  0.7923656698273767
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.841089e-05
Norm of the params: 9.973425
Flipped loss: 0.26221. Accuracy: 0.974
### Flips: 1025, rs: 19, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3688234
Train loss (w/o reg) on all data: 0.35981536
Test loss (w/o reg) on all data: 0.19719717
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.921659e-05
Norm of the params: 13.422409
     Influence (LOO): fixed 174 labels. Loss 0.19720. Accuracy 0.979.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33776066
Train loss (w/o reg) on all data: 0.32618582
Test loss (w/o reg) on all data: 0.20307826
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.236674e-06
Norm of the params: 15.215015
                Loss: fixed 204 labels. Loss 0.20308. Accuracy 0.954.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.441239
Train loss (w/o reg) on all data: 0.43603933
Test loss (w/o reg) on all data: 0.24722494
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.1320475e-05
Norm of the params: 10.197714
              Random: fixed  42 labels. Loss 0.24722. Accuracy 0.975.
### Flips: 1025, rs: 19, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3143834
Train loss (w/o reg) on all data: 0.30420604
Test loss (w/o reg) on all data: 0.15594102
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.2510261e-05
Norm of the params: 14.266977
     Influence (LOO): fixed 303 labels. Loss 0.15594. Accuracy 0.990.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23514712
Train loss (w/o reg) on all data: 0.21801051
Test loss (w/o reg) on all data: 0.15602697
Train acc on all data:  0.8952103087770484
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 5.3189196e-06
Norm of the params: 18.513023
                Loss: fixed 409 labels. Loss 0.15603. Accuracy 0.952.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4282373
Train loss (w/o reg) on all data: 0.42283016
Test loss (w/o reg) on all data: 0.2321981
Train acc on all data:  0.812302455628495
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.288717e-05
Norm of the params: 10.399152
              Random: fixed  89 labels. Loss 0.23220. Accuracy 0.982.
### Flips: 1025, rs: 19, checks: 615
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26464653
Train loss (w/o reg) on all data: 0.2533173
Test loss (w/o reg) on all data: 0.12927097
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.144095e-06
Norm of the params: 15.052725
     Influence (LOO): fixed 408 labels. Loss 0.12927. Accuracy 0.988.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13418792
Train loss (w/o reg) on all data: 0.114423074
Test loss (w/o reg) on all data: 0.08378211
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.1214e-06
Norm of the params: 19.882074
                Loss: fixed 608 labels. Loss 0.08378. Accuracy 0.970.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4147871
Train loss (w/o reg) on all data: 0.4091371
Test loss (w/o reg) on all data: 0.21985453
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.7827896e-05
Norm of the params: 10.630151
              Random: fixed 134 labels. Loss 0.21985. Accuracy 0.983.
### Flips: 1025, rs: 19, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22348022
Train loss (w/o reg) on all data: 0.21227664
Test loss (w/o reg) on all data: 0.1028849
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.372843e-06
Norm of the params: 14.969027
     Influence (LOO): fixed 493 labels. Loss 0.10288. Accuracy 0.995.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045842923
Train loss (w/o reg) on all data: 0.031647634
Test loss (w/o reg) on all data: 0.029324692
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.3218055e-06
Norm of the params: 16.849504
                Loss: fixed 791 labels. Loss 0.02932. Accuracy 0.989.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39885244
Train loss (w/o reg) on all data: 0.39295417
Test loss (w/o reg) on all data: 0.20513785
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.914158e-06
Norm of the params: 10.861179
              Random: fixed 179 labels. Loss 0.20514. Accuracy 0.983.
### Flips: 1025, rs: 19, checks: 1025
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864627
Train loss (w/o reg) on all data: 0.17849195
Test loss (w/o reg) on all data: 0.084811114
Train acc on all data:  0.925115487478726
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 2.969912e-05
Norm of the params: 14.250835
     Influence (LOO): fixed 562 labels. Loss 0.08481. Accuracy 0.997.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017918944
Train loss (w/o reg) on all data: 0.010635327
Test loss (w/o reg) on all data: 0.006929225
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 2.3012907e-07
Norm of the params: 12.06948
                Loss: fixed 854 labels. Loss 0.00693. Accuracy 1.000.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38462192
Train loss (w/o reg) on all data: 0.37862527
Test loss (w/o reg) on all data: 0.19044143
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.4666646e-05
Norm of the params: 10.951379
              Random: fixed 219 labels. Loss 0.19044. Accuracy 0.985.
### Flips: 1025, rs: 19, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15327477
Train loss (w/o reg) on all data: 0.14328615
Test loss (w/o reg) on all data: 0.06910455
Train acc on all data:  0.9389739849258448
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.2322186e-06
Norm of the params: 14.134085
     Influence (LOO): fixed 624 labels. Loss 0.06910. Accuracy 0.998.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010347586
Train loss (w/o reg) on all data: 0.0053877453
Test loss (w/o reg) on all data: 0.0042151813
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 8.587097e-08
Norm of the params: 9.959761
                Loss: fixed 865 labels. Loss 0.00422. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36744273
Train loss (w/o reg) on all data: 0.3611915
Test loss (w/o reg) on all data: 0.17694746
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 9.648843e-06
Norm of the params: 11.181429
              Random: fixed 265 labels. Loss 0.17695. Accuracy 0.988.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4482012
Train loss (w/o reg) on all data: 0.44268134
Test loss (w/o reg) on all data: 0.26756346
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.2179357e-05
Norm of the params: 10.507021
Flipped loss: 0.26756. Accuracy: 0.957
### Flips: 1025, rs: 20, checks: 205
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3778715
Train loss (w/o reg) on all data: 0.36831608
Test loss (w/o reg) on all data: 0.21543477
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.964154e-05
Norm of the params: 13.824209
     Influence (LOO): fixed 165 labels. Loss 0.21543. Accuracy 0.972.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33696982
Train loss (w/o reg) on all data: 0.3257069
Test loss (w/o reg) on all data: 0.21484326
Train acc on all data:  0.850230974957452
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 3.9168404e-05
Norm of the params: 15.008612
                Loss: fixed 205 labels. Loss 0.21484. Accuracy 0.938.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43549517
Train loss (w/o reg) on all data: 0.42961398
Test loss (w/o reg) on all data: 0.25077817
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.345523e-05
Norm of the params: 10.845445
              Random: fixed  45 labels. Loss 0.25078. Accuracy 0.965.
### Flips: 1025, rs: 20, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32376012
Train loss (w/o reg) on all data: 0.31337208
Test loss (w/o reg) on all data: 0.16709465
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.1961424e-05
Norm of the params: 14.4139185
     Influence (LOO): fixed 296 labels. Loss 0.16709. Accuracy 0.984.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23289675
Train loss (w/o reg) on all data: 0.21719769
Test loss (w/o reg) on all data: 0.16338019
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 9.4378165e-06
Norm of the params: 17.719519
                Loss: fixed 408 labels. Loss 0.16338. Accuracy 0.948.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42291296
Train loss (w/o reg) on all data: 0.41683412
Test loss (w/o reg) on all data: 0.23925382
Train acc on all data:  0.8135181132992949
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8120433e-05
Norm of the params: 11.026197
              Random: fixed  88 labels. Loss 0.23925. Accuracy 0.967.
### Flips: 1025, rs: 20, checks: 615
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27361792
Train loss (w/o reg) on all data: 0.2630944
Test loss (w/o reg) on all data: 0.134885
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.4649352e-05
Norm of the params: 14.507605
     Influence (LOO): fixed 405 labels. Loss 0.13488. Accuracy 0.990.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12834178
Train loss (w/o reg) on all data: 0.10845916
Test loss (w/o reg) on all data: 0.103540055
Train acc on all data:  0.9545344031120836
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.1557169e-06
Norm of the params: 19.941223
                Loss: fixed 609 labels. Loss 0.10354. Accuracy 0.967.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4098097
Train loss (w/o reg) on all data: 0.40365425
Test loss (w/o reg) on all data: 0.22444472
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.0676594e-05
Norm of the params: 11.095466
              Random: fixed 133 labels. Loss 0.22444. Accuracy 0.975.
### Flips: 1025, rs: 20, checks: 820
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22576025
Train loss (w/o reg) on all data: 0.21495828
Test loss (w/o reg) on all data: 0.11074521
Train acc on all data:  0.9078531485533674
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.7974588e-05
Norm of the params: 14.6982765
     Influence (LOO): fixed 499 labels. Loss 0.11075. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05296564
Train loss (w/o reg) on all data: 0.037365526
Test loss (w/o reg) on all data: 0.03744796
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.7492774e-06
Norm of the params: 17.663586
                Loss: fixed 773 labels. Loss 0.03745. Accuracy 0.990.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39814603
Train loss (w/o reg) on all data: 0.39193526
Test loss (w/o reg) on all data: 0.21313941
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6002192e-05
Norm of the params: 11.145205
              Random: fixed 167 labels. Loss 0.21314. Accuracy 0.973.
### Flips: 1025, rs: 20, checks: 1025
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18667577
Train loss (w/o reg) on all data: 0.17590983
Test loss (w/o reg) on all data: 0.08812589
Train acc on all data:  0.925358619012886
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.942851e-06
Norm of the params: 14.673747
     Influence (LOO): fixed 574 labels. Loss 0.08813. Accuracy 0.998.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027025394
Train loss (w/o reg) on all data: 0.016796073
Test loss (w/o reg) on all data: 0.012316063
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 7.35763e-07
Norm of the params: 14.30337
                Loss: fixed 837 labels. Loss 0.01232. Accuracy 0.998.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38519225
Train loss (w/o reg) on all data: 0.378831
Test loss (w/o reg) on all data: 0.20072138
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.7870114e-05
Norm of the params: 11.279406
              Random: fixed 205 labels. Loss 0.20072. Accuracy 0.975.
### Flips: 1025, rs: 20, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15832204
Train loss (w/o reg) on all data: 0.14777866
Test loss (w/o reg) on all data: 0.07144405
Train acc on all data:  0.9387308533916849
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.004599e-06
Norm of the params: 14.521281
     Influence (LOO): fixed 629 labels. Loss 0.07144. Accuracy 0.999.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01506971
Train loss (w/o reg) on all data: 0.007901257
Test loss (w/o reg) on all data: 0.0069428165
Train acc on all data:  0.9978118161925602
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6049474e-07
Norm of the params: 11.973682
                Loss: fixed 863 labels. Loss 0.00694. Accuracy 1.000.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37198117
Train loss (w/o reg) on all data: 0.36555338
Test loss (w/o reg) on all data: 0.18677305
Train acc on all data:  0.8521760272307318
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.0172004e-05
Norm of the params: 11.338246
              Random: fixed 247 labels. Loss 0.18677. Accuracy 0.983.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.457394
Train loss (w/o reg) on all data: 0.4519908
Test loss (w/o reg) on all data: 0.27977362
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.5506763e-05
Norm of the params: 10.395388
Flipped loss: 0.27977. Accuracy: 0.961
### Flips: 1025, rs: 21, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38254863
Train loss (w/o reg) on all data: 0.37300345
Test loss (w/o reg) on all data: 0.22250308
Train acc on all data:  0.8293216630196937
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 9.578038e-06
Norm of the params: 13.81678
     Influence (LOO): fixed 167 labels. Loss 0.22250. Accuracy 0.973.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3406743
Train loss (w/o reg) on all data: 0.32783562
Test loss (w/o reg) on all data: 0.23744819
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 3.1966065e-05
Norm of the params: 16.024166
                Loss: fixed 204 labels. Loss 0.23745. Accuracy 0.925.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4458603
Train loss (w/o reg) on all data: 0.44024947
Test loss (w/o reg) on all data: 0.2670555
Train acc on all data:  0.8001458789204959
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 4.619061e-05
Norm of the params: 10.593231
              Random: fixed  42 labels. Loss 0.26706. Accuracy 0.965.
### Flips: 1025, rs: 21, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32285902
Train loss (w/o reg) on all data: 0.31142256
Test loss (w/o reg) on all data: 0.17710733
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.657779e-06
Norm of the params: 15.123801
     Influence (LOO): fixed 302 labels. Loss 0.17711. Accuracy 0.983.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24187572
Train loss (w/o reg) on all data: 0.22455394
Test loss (w/o reg) on all data: 0.17025943
Train acc on all data:  0.8947240457087284
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 1.2391139e-05
Norm of the params: 18.612783
                Loss: fixed 407 labels. Loss 0.17026. Accuracy 0.942.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4346554
Train loss (w/o reg) on all data: 0.42899293
Test loss (w/o reg) on all data: 0.25170347
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.2284617e-05
Norm of the params: 10.641863
              Random: fixed  86 labels. Loss 0.25170. Accuracy 0.972.
### Flips: 1025, rs: 21, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2789208
Train loss (w/o reg) on all data: 0.26694378
Test loss (w/o reg) on all data: 0.15083922
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.9856727e-05
Norm of the params: 15.477084
     Influence (LOO): fixed 400 labels. Loss 0.15084. Accuracy 0.987.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14398277
Train loss (w/o reg) on all data: 0.12371153
Test loss (w/o reg) on all data: 0.09967751
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.416016e-06
Norm of the params: 20.135159
                Loss: fixed 610 labels. Loss 0.09968. Accuracy 0.969.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41938278
Train loss (w/o reg) on all data: 0.41344562
Test loss (w/o reg) on all data: 0.23739731
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 4.256727e-05
Norm of the params: 10.896944
              Random: fixed 134 labels. Loss 0.23740. Accuracy 0.975.
### Flips: 1025, rs: 21, checks: 820
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773022
Train loss (w/o reg) on all data: 0.22589456
Test loss (w/o reg) on all data: 0.12057792
Train acc on all data:  0.900316070994408
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2145192e-05
Norm of the params: 15.385491
     Influence (LOO): fixed 494 labels. Loss 0.12058. Accuracy 0.994.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05845254
Train loss (w/o reg) on all data: 0.04203002
Test loss (w/o reg) on all data: 0.037921052
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.3798059e-06
Norm of the params: 18.123198
                Loss: fixed 787 labels. Loss 0.03792. Accuracy 0.987.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40787506
Train loss (w/o reg) on all data: 0.4019208
Test loss (w/o reg) on all data: 0.22128226
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.6899387e-05
Norm of the params: 10.912622
              Random: fixed 175 labels. Loss 0.22128. Accuracy 0.979.
### Flips: 1025, rs: 21, checks: 1025
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19619827
Train loss (w/o reg) on all data: 0.18451492
Test loss (w/o reg) on all data: 0.09646781
Train acc on all data:  0.9212253829321663
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.014776e-06
Norm of the params: 15.286165
     Influence (LOO): fixed 573 labels. Loss 0.09647. Accuracy 0.993.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024493326
Train loss (w/o reg) on all data: 0.014707129
Test loss (w/o reg) on all data: 0.013652317
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 7.772997e-07
Norm of the params: 13.990137
                Loss: fixed 858 labels. Loss 0.01365. Accuracy 0.997.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39492902
Train loss (w/o reg) on all data: 0.3892212
Test loss (w/o reg) on all data: 0.20945695
Train acc on all data:  0.8368587405786531
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2894676e-05
Norm of the params: 10.684406
              Random: fixed 214 labels. Loss 0.20946. Accuracy 0.983.
### Flips: 1025, rs: 21, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167008
Train loss (w/o reg) on all data: 0.15616827
Test loss (w/o reg) on all data: 0.07999391
Train acc on all data:  0.9348407488451252
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 9.118193e-06
Norm of the params: 14.723943
     Influence (LOO): fixed 631 labels. Loss 0.07999. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018418279
Train loss (w/o reg) on all data: 0.010493911
Test loss (w/o reg) on all data: 0.008312402
Train acc on all data:  0.9965961585217603
Test acc on all data:   1.0
Norm of the mean of gradients: 3.036382e-06
Norm of the params: 12.589175
                Loss: fixed 874 labels. Loss 0.00831. Accuracy 1.000.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37814373
Train loss (w/o reg) on all data: 0.37241516
Test loss (w/o reg) on all data: 0.19534497
Train acc on all data:  0.8473133965475322
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 9.685841e-06
Norm of the params: 10.703812
              Random: fixed 260 labels. Loss 0.19534. Accuracy 0.983.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4574934
Train loss (w/o reg) on all data: 0.45228183
Test loss (w/o reg) on all data: 0.26814216
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2005291e-05
Norm of the params: 10.209359
Flipped loss: 0.26814. Accuracy: 0.976
### Flips: 1025, rs: 22, checks: 205
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38142058
Train loss (w/o reg) on all data: 0.3712625
Test loss (w/o reg) on all data: 0.20994608
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.6273184e-05
Norm of the params: 14.253485
     Influence (LOO): fixed 169 labels. Loss 0.20995. Accuracy 0.973.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34561422
Train loss (w/o reg) on all data: 0.33352485
Test loss (w/o reg) on all data: 0.21480799
Train acc on all data:  0.8470702650133722
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 3.8324513e-05
Norm of the params: 15.549521
                Loss: fixed 205 labels. Loss 0.21481. Accuracy 0.947.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44546258
Train loss (w/o reg) on all data: 0.440107
Test loss (w/o reg) on all data: 0.25361636
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.0346156e-05
Norm of the params: 10.349487
              Random: fixed  44 labels. Loss 0.25362. Accuracy 0.974.
### Flips: 1025, rs: 22, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32051337
Train loss (w/o reg) on all data: 0.3086564
Test loss (w/o reg) on all data: 0.1704361
Train acc on all data:  0.8638463408704109
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.82828e-06
Norm of the params: 15.399332
     Influence (LOO): fixed 309 labels. Loss 0.17044. Accuracy 0.981.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24138877
Train loss (w/o reg) on all data: 0.2259
Test loss (w/o reg) on all data: 0.16370656
Train acc on all data:  0.8971553610503282
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.0214381e-05
Norm of the params: 17.600445
                Loss: fixed 409 labels. Loss 0.16371. Accuracy 0.945.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43221498
Train loss (w/o reg) on all data: 0.4267491
Test loss (w/o reg) on all data: 0.23878717
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0379993e-05
Norm of the params: 10.455489
              Random: fixed  92 labels. Loss 0.23879. Accuracy 0.976.
### Flips: 1025, rs: 22, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27220225
Train loss (w/o reg) on all data: 0.2595248
Test loss (w/o reg) on all data: 0.13738142
Train acc on all data:  0.888645757354729
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.839097e-06
Norm of the params: 15.923238
     Influence (LOO): fixed 414 labels. Loss 0.13738. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13862363
Train loss (w/o reg) on all data: 0.12026225
Test loss (w/o reg) on all data: 0.09232742
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.803825e-06
Norm of the params: 19.163176
                Loss: fixed 613 labels. Loss 0.09233. Accuracy 0.970.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4135188
Train loss (w/o reg) on all data: 0.4077887
Test loss (w/o reg) on all data: 0.22153586
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1508473e-05
Norm of the params: 10.70523
              Random: fixed 150 labels. Loss 0.22154. Accuracy 0.979.
### Flips: 1025, rs: 22, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23470119
Train loss (w/o reg) on all data: 0.22270454
Test loss (w/o reg) on all data: 0.11630609
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2210032e-05
Norm of the params: 15.489766
     Influence (LOO): fixed 489 labels. Loss 0.11631. Accuracy 0.994.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050101835
Train loss (w/o reg) on all data: 0.03586258
Test loss (w/o reg) on all data: 0.030677678
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.113209e-06
Norm of the params: 16.875576
                Loss: fixed 796 labels. Loss 0.03068. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40072608
Train loss (w/o reg) on all data: 0.39492518
Test loss (w/o reg) on all data: 0.20801279
Train acc on all data:  0.8351568198395332
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.3818207e-05
Norm of the params: 10.771169
              Random: fixed 192 labels. Loss 0.20801. Accuracy 0.984.
### Flips: 1025, rs: 22, checks: 1025
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19821818
Train loss (w/o reg) on all data: 0.18694563
Test loss (w/o reg) on all data: 0.09154862
Train acc on all data:  0.9214685144663263
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.259518e-06
Norm of the params: 15.015023
     Influence (LOO): fixed 563 labels. Loss 0.09155. Accuracy 0.998.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011289705
Train loss (w/o reg) on all data: 0.0058989283
Test loss (w/o reg) on all data: 0.0066110194
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.8899274e-07
Norm of the params: 10.383426
                Loss: fixed 870 labels. Loss 0.00661. Accuracy 0.999.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38725913
Train loss (w/o reg) on all data: 0.381241
Test loss (w/o reg) on all data: 0.19596112
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.9677866e-05
Norm of the params: 10.971002
              Random: fixed 229 labels. Loss 0.19596. Accuracy 0.986.
### Flips: 1025, rs: 22, checks: 1230
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16387987
Train loss (w/o reg) on all data: 0.15360941
Test loss (w/o reg) on all data: 0.07449488
Train acc on all data:  0.9362995380500851
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.0556407e-06
Norm of the params: 14.332103
     Influence (LOO): fixed 621 labels. Loss 0.07449. Accuracy 0.998.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008192946
Train loss (w/o reg) on all data: 0.0039579016
Test loss (w/o reg) on all data: 0.00586431
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 1.2704268e-07
Norm of the params: 9.203309
                Loss: fixed 875 labels. Loss 0.00586. Accuracy 0.999.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3677604
Train loss (w/o reg) on all data: 0.36163273
Test loss (w/o reg) on all data: 0.18075357
Train acc on all data:  0.8563092633114515
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.1550855e-05
Norm of the params: 11.070366
              Random: fixed 280 labels. Loss 0.18075. Accuracy 0.988.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4551509
Train loss (w/o reg) on all data: 0.4495559
Test loss (w/o reg) on all data: 0.26728356
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 6.241033e-05
Norm of the params: 10.57828
Flipped loss: 0.26728. Accuracy: 0.962
### Flips: 1025, rs: 23, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37867603
Train loss (w/o reg) on all data: 0.36848882
Test loss (w/o reg) on all data: 0.21432056
Train acc on all data:  0.8305373206904936
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 7.860136e-06
Norm of the params: 14.273904
     Influence (LOO): fixed 176 labels. Loss 0.21432. Accuracy 0.978.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3481109
Train loss (w/o reg) on all data: 0.3360983
Test loss (w/o reg) on all data: 0.21534225
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 8.094193e-06
Norm of the params: 15.500059
                Loss: fixed 204 labels. Loss 0.21534. Accuracy 0.946.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44642362
Train loss (w/o reg) on all data: 0.4406748
Test loss (w/o reg) on all data: 0.25589022
Train acc on all data:  0.798930221249696
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.4373006e-05
Norm of the params: 10.722686
              Random: fixed  38 labels. Loss 0.25589. Accuracy 0.967.
### Flips: 1025, rs: 23, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32614213
Train loss (w/o reg) on all data: 0.31525835
Test loss (w/o reg) on all data: 0.17648256
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.2070939e-05
Norm of the params: 14.753834
     Influence (LOO): fixed 299 labels. Loss 0.17648. Accuracy 0.982.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24512556
Train loss (w/o reg) on all data: 0.22849241
Test loss (w/o reg) on all data: 0.15568282
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.6653903e-05
Norm of the params: 18.239048
                Loss: fixed 409 labels. Loss 0.15568. Accuracy 0.959.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4343516
Train loss (w/o reg) on all data: 0.4284002
Test loss (w/o reg) on all data: 0.24338758
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.8700599e-05
Norm of the params: 10.909994
              Random: fixed  81 labels. Loss 0.24339. Accuracy 0.970.
### Flips: 1025, rs: 23, checks: 615
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27624366
Train loss (w/o reg) on all data: 0.26493493
Test loss (w/o reg) on all data: 0.14204209
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.7774115e-05
Norm of the params: 15.039107
     Influence (LOO): fixed 407 labels. Loss 0.14204. Accuracy 0.989.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14390928
Train loss (w/o reg) on all data: 0.12462291
Test loss (w/o reg) on all data: 0.08964045
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.742522e-06
Norm of the params: 19.639942
                Loss: fixed 610 labels. Loss 0.08964. Accuracy 0.970.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42219153
Train loss (w/o reg) on all data: 0.4161876
Test loss (w/o reg) on all data: 0.2286194
Train acc on all data:  0.8154631655725748
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2560128e-05
Norm of the params: 10.958026
              Random: fixed 125 labels. Loss 0.22862. Accuracy 0.978.
### Flips: 1025, rs: 23, checks: 820
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23128583
Train loss (w/o reg) on all data: 0.22008516
Test loss (w/o reg) on all data: 0.113088734
Train acc on all data:  0.9068806224167274
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.324163e-06
Norm of the params: 14.967072
     Influence (LOO): fixed 502 labels. Loss 0.11309. Accuracy 0.993.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060881846
Train loss (w/o reg) on all data: 0.045259185
Test loss (w/o reg) on all data: 0.035350773
Train acc on all data:  0.9832239241429613
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.0618354e-06
Norm of the params: 17.676348
                Loss: fixed 778 labels. Loss 0.03535. Accuracy 0.990.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40782982
Train loss (w/o reg) on all data: 0.40196002
Test loss (w/o reg) on all data: 0.21316876
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.014551e-05
Norm of the params: 10.834961
              Random: fixed 173 labels. Loss 0.21317. Accuracy 0.980.
### Flips: 1025, rs: 23, checks: 1025
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19145
Train loss (w/o reg) on all data: 0.18102504
Test loss (w/o reg) on all data: 0.08592858
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 4.0729365e-06
Norm of the params: 14.4395
     Influence (LOO): fixed 578 labels. Loss 0.08593. Accuracy 0.995.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020749543
Train loss (w/o reg) on all data: 0.011651804
Test loss (w/o reg) on all data: 0.0108534815
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 9.653916e-07
Norm of the params: 13.48906
                Loss: fixed 855 labels. Loss 0.01085. Accuracy 0.998.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39355057
Train loss (w/o reg) on all data: 0.38758713
Test loss (w/o reg) on all data: 0.1953058
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.2278698e-05
Norm of the params: 10.921027
              Random: fixed 218 labels. Loss 0.19531. Accuracy 0.988.
### Flips: 1025, rs: 23, checks: 1230
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16184191
Train loss (w/o reg) on all data: 0.15194209
Test loss (w/o reg) on all data: 0.07230654
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.1323317e-05
Norm of the params: 14.071123
     Influence (LOO): fixed 633 labels. Loss 0.07231. Accuracy 0.998.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0147462115
Train loss (w/o reg) on all data: 0.007643876
Test loss (w/o reg) on all data: 0.006668423
Train acc on all data:  0.9968392900559202
Test acc on all data:   1.0
Norm of the mean of gradients: 1.0190614e-06
Norm of the params: 11.918336
                Loss: fixed 870 labels. Loss 0.00667. Accuracy 1.000.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37851107
Train loss (w/o reg) on all data: 0.37253976
Test loss (w/o reg) on all data: 0.18126588
Train acc on all data:  0.8487721857524921
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 3.3020773e-05
Norm of the params: 10.92824
              Random: fixed 262 labels. Loss 0.18127. Accuracy 0.988.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45264798
Train loss (w/o reg) on all data: 0.4466591
Test loss (w/o reg) on all data: 0.27235547
Train acc on all data:  0.7938244590323364
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.0201767e-05
Norm of the params: 10.944298
Flipped loss: 0.27236. Accuracy: 0.966
### Flips: 1025, rs: 24, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37472293
Train loss (w/o reg) on all data: 0.36483902
Test loss (w/o reg) on all data: 0.21859153
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.097397e-05
Norm of the params: 14.059807
     Influence (LOO): fixed 167 labels. Loss 0.21859. Accuracy 0.974.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34267315
Train loss (w/o reg) on all data: 0.33050296
Test loss (w/o reg) on all data: 0.22300684
Train acc on all data:  0.8448820812059324
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 7.5196267e-06
Norm of the params: 15.601416
                Loss: fixed 205 labels. Loss 0.22301. Accuracy 0.939.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44163868
Train loss (w/o reg) on all data: 0.43571877
Test loss (w/o reg) on all data: 0.25599086
Train acc on all data:  0.8040359834670556
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.0340569e-05
Norm of the params: 10.881094
              Random: fixed  44 labels. Loss 0.25599. Accuracy 0.971.
### Flips: 1025, rs: 24, checks: 410
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3211674
Train loss (w/o reg) on all data: 0.30995464
Test loss (w/o reg) on all data: 0.1694673
Train acc on all data:  0.862630683199611
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1562252e-05
Norm of the params: 14.975161
     Influence (LOO): fixed 303 labels. Loss 0.16947. Accuracy 0.989.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23901959
Train loss (w/o reg) on all data: 0.22201775
Test loss (w/o reg) on all data: 0.16363695
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 4.49953e-06
Norm of the params: 18.440086
                Loss: fixed 410 labels. Loss 0.16364. Accuracy 0.950.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42807835
Train loss (w/o reg) on all data: 0.42207363
Test loss (w/o reg) on all data: 0.23760091
Train acc on all data:  0.8157062971067347
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.8153188e-05
Norm of the params: 10.958761
              Random: fixed  93 labels. Loss 0.23760. Accuracy 0.978.
### Flips: 1025, rs: 24, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26452324
Train loss (w/o reg) on all data: 0.25288662
Test loss (w/o reg) on all data: 0.1388754
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 6.8081954e-06
Norm of the params: 15.25556
     Influence (LOO): fixed 421 labels. Loss 0.13888. Accuracy 0.991.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13666208
Train loss (w/o reg) on all data: 0.117848255
Test loss (w/o reg) on all data: 0.09718825
Train acc on all data:  0.949428640894724
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.0454907e-05
Norm of the params: 19.397848
                Loss: fixed 614 labels. Loss 0.09719. Accuracy 0.970.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4173519
Train loss (w/o reg) on all data: 0.4113016
Test loss (w/o reg) on all data: 0.22616984
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.7730323e-05
Norm of the params: 11.000271
              Random: fixed 131 labels. Loss 0.22617. Accuracy 0.979.
### Flips: 1025, rs: 24, checks: 820
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21997157
Train loss (w/o reg) on all data: 0.20839013
Test loss (w/o reg) on all data: 0.11004566
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.3409417e-06
Norm of the params: 15.219355
     Influence (LOO): fixed 509 labels. Loss 0.11005. Accuracy 0.996.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05188679
Train loss (w/o reg) on all data: 0.036661725
Test loss (w/o reg) on all data: 0.03119034
Train acc on all data:  0.986627765621201
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.8740986e-06
Norm of the params: 17.449965
                Loss: fixed 791 labels. Loss 0.03119. Accuracy 0.994.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4004843
Train loss (w/o reg) on all data: 0.39412323
Test loss (w/o reg) on all data: 0.2154727
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.9770941e-05
Norm of the params: 11.279236
              Random: fixed 180 labels. Loss 0.21547. Accuracy 0.983.
### Flips: 1025, rs: 24, checks: 1025
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1928452
Train loss (w/o reg) on all data: 0.1817233
Test loss (w/o reg) on all data: 0.09605089
Train acc on all data:  0.9224410406029662
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.6707738e-05
Norm of the params: 14.91435
     Influence (LOO): fixed 563 labels. Loss 0.09605. Accuracy 0.997.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021883056
Train loss (w/o reg) on all data: 0.012225456
Test loss (w/o reg) on all data: 0.010827803
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.7767016e-07
Norm of the params: 13.897914
                Loss: fixed 849 labels. Loss 0.01083. Accuracy 0.999.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39101738
Train loss (w/o reg) on all data: 0.3846063
Test loss (w/o reg) on all data: 0.2035948
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.949175e-06
Norm of the params: 11.323486
              Random: fixed 210 labels. Loss 0.20359. Accuracy 0.984.
### Flips: 1025, rs: 24, checks: 1230
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16363741
Train loss (w/o reg) on all data: 0.15253991
Test loss (w/o reg) on all data: 0.07964786
Train acc on all data:  0.9355701434476051
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.0350119e-05
Norm of the params: 14.897993
     Influence (LOO): fixed 619 labels. Loss 0.07965. Accuracy 0.998.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01192976
Train loss (w/o reg) on all data: 0.0059566554
Test loss (w/o reg) on all data: 0.005341676
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 2.7098972e-07
Norm of the params: 10.929872
                Loss: fixed 869 labels. Loss 0.00534. Accuracy 1.000.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37734306
Train loss (w/o reg) on all data: 0.3708212
Test loss (w/o reg) on all data: 0.18997686
Train acc on all data:  0.849258448820812
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 5.2057967e-05
Norm of the params: 11.420914
              Random: fixed 248 labels. Loss 0.18998. Accuracy 0.985.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45621532
Train loss (w/o reg) on all data: 0.45064592
Test loss (w/o reg) on all data: 0.27837864
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.859613e-05
Norm of the params: 10.554052
Flipped loss: 0.27838. Accuracy: 0.954
### Flips: 1025, rs: 25, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38044953
Train loss (w/o reg) on all data: 0.37133127
Test loss (w/o reg) on all data: 0.22098954
Train acc on all data:  0.8288353999513737
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.7264392e-05
Norm of the params: 13.504274
     Influence (LOO): fixed 163 labels. Loss 0.22099. Accuracy 0.971.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34621617
Train loss (w/o reg) on all data: 0.3346974
Test loss (w/o reg) on all data: 0.23706912
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.1359048e-05
Norm of the params: 15.178131
                Loss: fixed 205 labels. Loss 0.23707. Accuracy 0.925.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44524756
Train loss (w/o reg) on all data: 0.4396174
Test loss (w/o reg) on all data: 0.26389635
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.4746724e-05
Norm of the params: 10.611481
              Random: fixed  40 labels. Loss 0.26390. Accuracy 0.966.
### Flips: 1025, rs: 25, checks: 410
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3249725
Train loss (w/o reg) on all data: 0.31479216
Test loss (w/o reg) on all data: 0.18024515
Train acc on all data:  0.8577680525164114
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.3717373e-05
Norm of the params: 14.269104
     Influence (LOO): fixed 294 labels. Loss 0.18025. Accuracy 0.980.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24336469
Train loss (w/o reg) on all data: 0.22735888
Test loss (w/o reg) on all data: 0.18239681
Train acc on all data:  0.8949671772428884
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 8.954279e-06
Norm of the params: 17.891792
                Loss: fixed 409 labels. Loss 0.18240. Accuracy 0.939.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43114766
Train loss (w/o reg) on all data: 0.42526758
Test loss (w/o reg) on all data: 0.24957174
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.848065e-05
Norm of the params: 10.844428
              Random: fixed  85 labels. Loss 0.24957. Accuracy 0.967.
### Flips: 1025, rs: 25, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2742975
Train loss (w/o reg) on all data: 0.26305807
Test loss (w/o reg) on all data: 0.14255333
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.7152987e-06
Norm of the params: 14.992967
     Influence (LOO): fixed 409 labels. Loss 0.14255. Accuracy 0.985.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14309219
Train loss (w/o reg) on all data: 0.12272564
Test loss (w/o reg) on all data: 0.11308082
Train acc on all data:  0.9482129832239241
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 7.788208e-06
Norm of the params: 20.182438
                Loss: fixed 609 labels. Loss 0.11308. Accuracy 0.960.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41600204
Train loss (w/o reg) on all data: 0.40993774
Test loss (w/o reg) on all data: 0.23548357
Train acc on all data:  0.8198395331874544
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.5733998e-05
Norm of the params: 11.0130005
              Random: fixed 130 labels. Loss 0.23548. Accuracy 0.968.
### Flips: 1025, rs: 25, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23705573
Train loss (w/o reg) on all data: 0.22659506
Test loss (w/o reg) on all data: 0.12224157
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.4236337e-06
Norm of the params: 14.464211
     Influence (LOO): fixed 483 labels. Loss 0.12224. Accuracy 0.987.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053725004
Train loss (w/o reg) on all data: 0.03758747
Test loss (w/o reg) on all data: 0.040740903
Train acc on all data:  0.9858983710187211
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 4.322568e-06
Norm of the params: 17.965263
                Loss: fixed 792 labels. Loss 0.04074. Accuracy 0.991.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4030861
Train loss (w/o reg) on all data: 0.3969724
Test loss (w/o reg) on all data: 0.21891075
Train acc on all data:  0.8300510576221736
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.396128e-05
Norm of the params: 11.05776
              Random: fixed 174 labels. Loss 0.21891. Accuracy 0.975.
### Flips: 1025, rs: 25, checks: 1025
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19474241
Train loss (w/o reg) on all data: 0.18549356
Test loss (w/o reg) on all data: 0.09312153
Train acc on all data:  0.9236566982737661
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 8.158279e-06
Norm of the params: 13.600629
     Influence (LOO): fixed 572 labels. Loss 0.09312. Accuracy 0.995.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020416234
Train loss (w/o reg) on all data: 0.011401596
Test loss (w/o reg) on all data: 0.013877205
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.5154069e-06
Norm of the params: 13.427313
                Loss: fixed 859 labels. Loss 0.01388. Accuracy 0.997.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38801953
Train loss (w/o reg) on all data: 0.38171238
Test loss (w/o reg) on all data: 0.20649925
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.925354e-05
Norm of the params: 11.23135
              Random: fixed 213 labels. Loss 0.20650. Accuracy 0.975.
### Flips: 1025, rs: 25, checks: 1230
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15755783
Train loss (w/o reg) on all data: 0.14846511
Test loss (w/o reg) on all data: 0.07479096
Train acc on all data:  0.9404327741308047
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 4.5965476e-06
Norm of the params: 13.485336
     Influence (LOO): fixed 639 labels. Loss 0.07479. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011839092
Train loss (w/o reg) on all data: 0.0056333714
Test loss (w/o reg) on all data: 0.007724261
Train acc on all data:  0.9990274738633601
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.7550104e-07
Norm of the params: 11.140665
                Loss: fixed 873 labels. Loss 0.00772. Accuracy 0.998.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37068257
Train loss (w/o reg) on all data: 0.36429557
Test loss (w/o reg) on all data: 0.19116531
Train acc on all data:  0.8526622902990518
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.0866511e-05
Norm of the params: 11.302217
              Random: fixed 264 labels. Loss 0.19117. Accuracy 0.976.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4490468
Train loss (w/o reg) on all data: 0.44262215
Test loss (w/o reg) on all data: 0.27471882
Train acc on all data:  0.7957695113056164
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.1461046e-05
Norm of the params: 11.335452
Flipped loss: 0.27472. Accuracy: 0.953
### Flips: 1025, rs: 26, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37749302
Train loss (w/o reg) on all data: 0.3672436
Test loss (w/o reg) on all data: 0.22848897
Train acc on all data:  0.8329686360320934
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.429743e-06
Norm of the params: 14.317423
     Influence (LOO): fixed 159 labels. Loss 0.22849. Accuracy 0.960.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33634007
Train loss (w/o reg) on all data: 0.322732
Test loss (w/o reg) on all data: 0.22695386
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9319727891156463
Norm of the mean of gradients: 2.1126325e-05
Norm of the params: 16.497309
                Loss: fixed 205 labels. Loss 0.22695. Accuracy 0.932.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43682522
Train loss (w/o reg) on all data: 0.43020993
Test loss (w/o reg) on all data: 0.25888675
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 2.7687356e-05
Norm of the params: 11.502424
              Random: fixed  46 labels. Loss 0.25889. Accuracy 0.961.
### Flips: 1025, rs: 26, checks: 410
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32374486
Train loss (w/o reg) on all data: 0.31245774
Test loss (w/o reg) on all data: 0.17918707
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.1086208e-05
Norm of the params: 15.024725
     Influence (LOO): fixed 295 labels. Loss 0.17919. Accuracy 0.981.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2384246
Train loss (w/o reg) on all data: 0.22024179
Test loss (w/o reg) on all data: 0.16956604
Train acc on all data:  0.8964259664478483
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 7.4841155e-06
Norm of the params: 19.069769
                Loss: fixed 407 labels. Loss 0.16957. Accuracy 0.943.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42108056
Train loss (w/o reg) on all data: 0.41430372
Test loss (w/o reg) on all data: 0.24227649
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.336508e-06
Norm of the params: 11.642024
              Random: fixed 101 labels. Loss 0.24228. Accuracy 0.966.
### Flips: 1025, rs: 26, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27301076
Train loss (w/o reg) on all data: 0.2617169
Test loss (w/o reg) on all data: 0.14554726
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1197393e-05
Norm of the params: 15.029219
     Influence (LOO): fixed 403 labels. Loss 0.14555. Accuracy 0.987.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13938504
Train loss (w/o reg) on all data: 0.11841672
Test loss (w/o reg) on all data: 0.10245044
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.555228e-06
Norm of the params: 20.478437
                Loss: fixed 610 labels. Loss 0.10245. Accuracy 0.963.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4100583
Train loss (w/o reg) on all data: 0.40318972
Test loss (w/o reg) on all data: 0.22847112
Train acc on all data:  0.824945295404814
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.3153654e-05
Norm of the params: 11.7205515
              Random: fixed 140 labels. Loss 0.22847. Accuracy 0.968.
### Flips: 1025, rs: 26, checks: 820
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231764
Train loss (w/o reg) on all data: 0.22082753
Test loss (w/o reg) on all data: 0.115613535
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.5781836e-05
Norm of the params: 14.789504
     Influence (LOO): fixed 496 labels. Loss 0.11561. Accuracy 0.991.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060720444
Train loss (w/o reg) on all data: 0.04384524
Test loss (w/o reg) on all data: 0.04208387
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 2.8723982e-06
Norm of the params: 18.371284
                Loss: fixed 780 labels. Loss 0.04208. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39583102
Train loss (w/o reg) on all data: 0.38866362
Test loss (w/o reg) on all data: 0.21679333
Train acc on all data:  0.8339411621687333
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.1681471e-05
Norm of the params: 11.972794
              Random: fixed 184 labels. Loss 0.21679. Accuracy 0.971.
### Flips: 1025, rs: 26, checks: 1025
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1907042
Train loss (w/o reg) on all data: 0.18015853
Test loss (w/o reg) on all data: 0.09275575
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7460632e-05
Norm of the params: 14.522863
     Influence (LOO): fixed 569 labels. Loss 0.09276. Accuracy 0.991.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029512152
Train loss (w/o reg) on all data: 0.019283554
Test loss (w/o reg) on all data: 0.021330275
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 5.984482e-07
Norm of the params: 14.302866
                Loss: fixed 842 labels. Loss 0.02133. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3828411
Train loss (w/o reg) on all data: 0.37554133
Test loss (w/o reg) on all data: 0.20169589
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.4034178e-05
Norm of the params: 12.082873
              Random: fixed 226 labels. Loss 0.20170. Accuracy 0.978.
### Flips: 1025, rs: 26, checks: 1230
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15342456
Train loss (w/o reg) on all data: 0.14379087
Test loss (w/o reg) on all data: 0.07241406
Train acc on all data:  0.9406759056649647
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.2885416e-06
Norm of the params: 13.880702
     Influence (LOO): fixed 635 labels. Loss 0.07241. Accuracy 0.995.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013027685
Train loss (w/o reg) on all data: 0.007183336
Test loss (w/o reg) on all data: 0.0102075795
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.5246137e-07
Norm of the params: 10.811427
                Loss: fixed 872 labels. Loss 0.01021. Accuracy 0.998.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36805817
Train loss (w/o reg) on all data: 0.36053044
Test loss (w/o reg) on all data: 0.19132747
Train acc on all data:  0.8548504741064916
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.654108e-05
Norm of the params: 12.270064
              Random: fixed 267 labels. Loss 0.19133. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44958055
Train loss (w/o reg) on all data: 0.4439531
Test loss (w/o reg) on all data: 0.27525023
Train acc on all data:  0.7972283005105762
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 5.5762663e-05
Norm of the params: 10.608921
Flipped loss: 0.27525. Accuracy: 0.956
### Flips: 1025, rs: 27, checks: 205
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37934557
Train loss (w/o reg) on all data: 0.3698151
Test loss (w/o reg) on all data: 0.22363168
Train acc on all data:  0.8324823729637734
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 2.3155122e-05
Norm of the params: 13.806135
     Influence (LOO): fixed 163 labels. Loss 0.22363. Accuracy 0.972.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33384535
Train loss (w/o reg) on all data: 0.32222646
Test loss (w/o reg) on all data: 0.23098354
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 9.187484e-06
Norm of the params: 15.243945
                Loss: fixed 205 labels. Loss 0.23098. Accuracy 0.928.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43749788
Train loss (w/o reg) on all data: 0.43169814
Test loss (w/o reg) on all data: 0.26258224
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.2734337e-05
Norm of the params: 10.7701
              Random: fixed  41 labels. Loss 0.26258. Accuracy 0.959.
### Flips: 1025, rs: 27, checks: 410
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32362124
Train loss (w/o reg) on all data: 0.31278446
Test loss (w/o reg) on all data: 0.17799567
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.57e-05
Norm of the params: 14.721947
     Influence (LOO): fixed 303 labels. Loss 0.17800. Accuracy 0.977.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23123011
Train loss (w/o reg) on all data: 0.21473525
Test loss (w/o reg) on all data: 0.17781074
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.6814352e-05
Norm of the params: 18.163067
                Loss: fixed 409 labels. Loss 0.17781. Accuracy 0.937.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4277896
Train loss (w/o reg) on all data: 0.42175686
Test loss (w/o reg) on all data: 0.24970533
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 0.0001073355
Norm of the params: 10.984297
              Random: fixed  75 labels. Loss 0.24971. Accuracy 0.962.
### Flips: 1025, rs: 27, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275273
Train loss (w/o reg) on all data: 0.26458207
Test loss (w/o reg) on all data: 0.1433001
Train acc on all data:  0.888159494286409
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.153034e-05
Norm of the params: 14.622545
     Influence (LOO): fixed 413 labels. Loss 0.14330. Accuracy 0.985.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13344605
Train loss (w/o reg) on all data: 0.11382699
Test loss (w/o reg) on all data: 0.11371741
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 7.68649e-06
Norm of the params: 19.808613
                Loss: fixed 610 labels. Loss 0.11372. Accuracy 0.955.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4156409
Train loss (w/o reg) on all data: 0.4096436
Test loss (w/o reg) on all data: 0.23886417
Train acc on all data:  0.825188426938974
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.6652542e-05
Norm of the params: 10.951995
              Random: fixed 117 labels. Loss 0.23886. Accuracy 0.967.
### Flips: 1025, rs: 27, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22452095
Train loss (w/o reg) on all data: 0.21388438
Test loss (w/o reg) on all data: 0.107867286
Train acc on all data:  0.9112569900316071
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.089251e-06
Norm of the params: 14.585317
     Influence (LOO): fixed 516 labels. Loss 0.10787. Accuracy 0.993.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05310968
Train loss (w/o reg) on all data: 0.037791133
Test loss (w/o reg) on all data: 0.046841405
Train acc on all data:  0.9856552394845611
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.62954e-06
Norm of the params: 17.503454
                Loss: fixed 779 labels. Loss 0.04684. Accuracy 0.981.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40104356
Train loss (w/o reg) on all data: 0.3949051
Test loss (w/o reg) on all data: 0.22662903
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.8158287e-05
Norm of the params: 11.080146
              Random: fixed 165 labels. Loss 0.22663. Accuracy 0.968.
### Flips: 1025, rs: 27, checks: 1025
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18264869
Train loss (w/o reg) on all data: 0.17247202
Test loss (w/o reg) on all data: 0.08874439
Train acc on all data:  0.9294918550936057
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.0204195e-06
Norm of the params: 14.266516
     Influence (LOO): fixed 586 labels. Loss 0.08874. Accuracy 0.994.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022330888
Train loss (w/o reg) on all data: 0.012980679
Test loss (w/o reg) on all data: 0.018399768
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.240631e-06
Norm of the params: 13.674948
                Loss: fixed 840 labels. Loss 0.01840. Accuracy 0.996.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38728255
Train loss (w/o reg) on all data: 0.3809995
Test loss (w/o reg) on all data: 0.21395533
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.3667448e-05
Norm of the params: 11.209854
              Random: fixed 204 labels. Loss 0.21396. Accuracy 0.971.
### Flips: 1025, rs: 27, checks: 1230
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14628448
Train loss (w/o reg) on all data: 0.13682002
Test loss (w/o reg) on all data: 0.07178673
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0490221e-05
Norm of the params: 13.758236
     Influence (LOO): fixed 652 labels. Loss 0.07179. Accuracy 0.994.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014806569
Train loss (w/o reg) on all data: 0.007559031
Test loss (w/o reg) on all data: 0.010929859
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.3311777e-07
Norm of the params: 12.039551
                Loss: fixed 858 labels. Loss 0.01093. Accuracy 0.998.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36798185
Train loss (w/o reg) on all data: 0.3616449
Test loss (w/o reg) on all data: 0.19860849
Train acc on all data:  0.8565523948456115
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.773768e-05
Norm of the params: 11.257837
              Random: fixed 258 labels. Loss 0.19861. Accuracy 0.971.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4540684
Train loss (w/o reg) on all data: 0.450153
Test loss (w/o reg) on all data: 0.2667503
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.9948584e-05
Norm of the params: 8.849171
Flipped loss: 0.26675. Accuracy: 0.964
### Flips: 1025, rs: 28, checks: 205
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37766838
Train loss (w/o reg) on all data: 0.3686985
Test loss (w/o reg) on all data: 0.2052502
Train acc on all data:  0.8322392414296135
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.968411e-05
Norm of the params: 13.393936
     Influence (LOO): fixed 174 labels. Loss 0.20525. Accuracy 0.977.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34018147
Train loss (w/o reg) on all data: 0.33023787
Test loss (w/o reg) on all data: 0.2128866
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.22532865e-05
Norm of the params: 14.102195
                Loss: fixed 204 labels. Loss 0.21289. Accuracy 0.938.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4419818
Train loss (w/o reg) on all data: 0.43779826
Test loss (w/o reg) on all data: 0.25550848
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.45969e-05
Norm of the params: 9.14716
              Random: fixed  41 labels. Loss 0.25551. Accuracy 0.963.
### Flips: 1025, rs: 28, checks: 410
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32015142
Train loss (w/o reg) on all data: 0.3100721
Test loss (w/o reg) on all data: 0.16666171
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.760118e-05
Norm of the params: 14.198112
     Influence (LOO): fixed 299 labels. Loss 0.16666. Accuracy 0.983.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23455852
Train loss (w/o reg) on all data: 0.21949388
Test loss (w/o reg) on all data: 0.15391575
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 5.380495e-06
Norm of the params: 17.357792
                Loss: fixed 409 labels. Loss 0.15392. Accuracy 0.949.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42247534
Train loss (w/o reg) on all data: 0.41801688
Test loss (w/o reg) on all data: 0.23392767
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0827055e-05
Norm of the params: 9.442952
              Random: fixed 106 labels. Loss 0.23393. Accuracy 0.975.
### Flips: 1025, rs: 28, checks: 615
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27515402
Train loss (w/o reg) on all data: 0.26411805
Test loss (w/o reg) on all data: 0.13289103
Train acc on all data:  0.886457573547289
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 4.8831707e-06
Norm of the params: 14.856631
     Influence (LOO): fixed 408 labels. Loss 0.13289. Accuracy 0.993.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13255838
Train loss (w/o reg) on all data: 0.116062716
Test loss (w/o reg) on all data: 0.094605096
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.6404043e-06
Norm of the params: 18.163513
                Loss: fixed 608 labels. Loss 0.09461. Accuracy 0.971.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4076105
Train loss (w/o reg) on all data: 0.40314755
Test loss (w/o reg) on all data: 0.21713546
Train acc on all data:  0.8261609530756139
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.3751583e-05
Norm of the params: 9.4477215
              Random: fixed 156 labels. Loss 0.21714. Accuracy 0.979.
### Flips: 1025, rs: 28, checks: 820
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24061312
Train loss (w/o reg) on all data: 0.22973315
Test loss (w/o reg) on all data: 0.11322858
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1129394e-05
Norm of the params: 14.751246
     Influence (LOO): fixed 481 labels. Loss 0.11323. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04928376
Train loss (w/o reg) on all data: 0.03600045
Test loss (w/o reg) on all data: 0.042801242
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.781501e-07
Norm of the params: 16.299273
                Loss: fixed 784 labels. Loss 0.04280. Accuracy 0.987.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3954448
Train loss (w/o reg) on all data: 0.39067498
Test loss (w/o reg) on all data: 0.20309804
Train acc on all data:  0.8353999513736932
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.5040354e-05
Norm of the params: 9.767121
              Random: fixed 194 labels. Loss 0.20310. Accuracy 0.984.
### Flips: 1025, rs: 28, checks: 1025
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20325717
Train loss (w/o reg) on all data: 0.19274932
Test loss (w/o reg) on all data: 0.091080084
Train acc on all data:  0.9207391198638464
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.956726e-06
Norm of the params: 14.496799
     Influence (LOO): fixed 557 labels. Loss 0.09108. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017946538
Train loss (w/o reg) on all data: 0.010348224
Test loss (w/o reg) on all data: 0.016210863
Train acc on all data:  0.9965961585217603
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.1581584e-07
Norm of the params: 12.327461
                Loss: fixed 849 labels. Loss 0.01621. Accuracy 0.995.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38106582
Train loss (w/o reg) on all data: 0.37589514
Test loss (w/o reg) on all data: 0.18986629
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.5529553e-05
Norm of the params: 10.169235
              Random: fixed 239 labels. Loss 0.18987. Accuracy 0.988.
### Flips: 1025, rs: 28, checks: 1230
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15897529
Train loss (w/o reg) on all data: 0.14908856
Test loss (w/o reg) on all data: 0.069607586
Train acc on all data:  0.9384877218575249
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.748169e-06
Norm of the params: 14.061811
     Influence (LOO): fixed 638 labels. Loss 0.06961. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010771645
Train loss (w/o reg) on all data: 0.0055479845
Test loss (w/o reg) on all data: 0.0073591373
Train acc on all data:  0.9985412107950401
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 2.2870972e-07
Norm of the params: 10.221214
                Loss: fixed 868 labels. Loss 0.00736. Accuracy 0.998.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36533922
Train loss (w/o reg) on all data: 0.35992408
Test loss (w/o reg) on all data: 0.17598093
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 4.2128027e-05
Norm of the params: 10.40686
              Random: fixed 283 labels. Loss 0.17598. Accuracy 0.989.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45753968
Train loss (w/o reg) on all data: 0.45234317
Test loss (w/o reg) on all data: 0.27437064
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.6016994e-05
Norm of the params: 10.19463
Flipped loss: 0.27437. Accuracy: 0.967
### Flips: 1025, rs: 29, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3827662
Train loss (w/o reg) on all data: 0.37340316
Test loss (w/o reg) on all data: 0.22259536
Train acc on all data:  0.8310235837588135
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.490719e-05
Norm of the params: 13.684324
     Influence (LOO): fixed 172 labels. Loss 0.22260. Accuracy 0.973.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.348393
Train loss (w/o reg) on all data: 0.33694968
Test loss (w/o reg) on all data: 0.23249203
Train acc on all data:  0.8426938973984925
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 1.3423197e-05
Norm of the params: 15.128326
                Loss: fixed 203 labels. Loss 0.23249. Accuracy 0.939.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44533074
Train loss (w/o reg) on all data: 0.43979862
Test loss (w/o reg) on all data: 0.261823
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.27556095e-05
Norm of the params: 10.518679
              Random: fixed  45 labels. Loss 0.26182. Accuracy 0.968.
### Flips: 1025, rs: 29, checks: 410
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3230626
Train loss (w/o reg) on all data: 0.31276348
Test loss (w/o reg) on all data: 0.18125339
Train acc on all data:  0.8611718939946511
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 6.4218866e-06
Norm of the params: 14.352085
     Influence (LOO): fixed 307 labels. Loss 0.18125. Accuracy 0.980.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24536552
Train loss (w/o reg) on all data: 0.22995454
Test loss (w/o reg) on all data: 0.17586327
Train acc on all data:  0.8925358619012886
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 8.8664465e-06
Norm of the params: 17.556185
                Loss: fixed 408 labels. Loss 0.17586. Accuracy 0.942.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43156356
Train loss (w/o reg) on all data: 0.42609718
Test loss (w/o reg) on all data: 0.24647334
Train acc on all data:  0.8103574033552152
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.859255e-05
Norm of the params: 10.455975
              Random: fixed  94 labels. Loss 0.24647. Accuracy 0.975.
### Flips: 1025, rs: 29, checks: 615
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2648088
Train loss (w/o reg) on all data: 0.25416765
Test loss (w/o reg) on all data: 0.14681283
Train acc on all data:  0.8888888888888888
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.0891806e-06
Norm of the params: 14.588454
     Influence (LOO): fixed 422 labels. Loss 0.14681. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14149085
Train loss (w/o reg) on all data: 0.12330551
Test loss (w/o reg) on all data: 0.10453207
Train acc on all data:  0.9418915633357646
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.5343934e-05
Norm of the params: 19.071098
                Loss: fixed 610 labels. Loss 0.10453. Accuracy 0.968.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42012486
Train loss (w/o reg) on all data: 0.41464752
Test loss (w/o reg) on all data: 0.23526573
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2573289e-05
Norm of the params: 10.466449
              Random: fixed 132 labels. Loss 0.23527. Accuracy 0.976.
### Flips: 1025, rs: 29, checks: 820
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173692
Train loss (w/o reg) on all data: 0.21146293
Test loss (w/o reg) on all data: 0.113477066
Train acc on all data:  0.9093119377583273
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.799812e-06
Norm of the params: 14.334568
     Influence (LOO): fixed 516 labels. Loss 0.11348. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049532235
Train loss (w/o reg) on all data: 0.0349564
Test loss (w/o reg) on all data: 0.028932484
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.2455495e-06
Norm of the params: 17.07386
                Loss: fixed 802 labels. Loss 0.02893. Accuracy 0.993.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4057873
Train loss (w/o reg) on all data: 0.40022224
Test loss (w/o reg) on all data: 0.2199807
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.1140001e-05
Norm of the params: 10.549923
              Random: fixed 179 labels. Loss 0.21998. Accuracy 0.979.
### Flips: 1025, rs: 29, checks: 1025
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17952316
Train loss (w/o reg) on all data: 0.16903171
Test loss (w/o reg) on all data: 0.090132974
Train acc on all data:  0.9297349866277657
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.9780695e-05
Norm of the params: 14.485477
     Influence (LOO): fixed 593 labels. Loss 0.09013. Accuracy 0.993.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018242676
Train loss (w/o reg) on all data: 0.0094019
Test loss (w/o reg) on all data: 0.008447472
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 7.0592154e-07
Norm of the params: 13.297198
                Loss: fixed 861 labels. Loss 0.00845. Accuracy 0.999.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39255506
Train loss (w/o reg) on all data: 0.38669696
Test loss (w/o reg) on all data: 0.20722592
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.5386719e-05
Norm of the params: 10.824139
              Random: fixed 219 labels. Loss 0.20723. Accuracy 0.981.
### Flips: 1025, rs: 29, checks: 1230
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14882445
Train loss (w/o reg) on all data: 0.13854562
Test loss (w/o reg) on all data: 0.072633564
Train acc on all data:  0.9416484318016046
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 5.626377e-06
Norm of the params: 14.337951
     Influence (LOO): fixed 648 labels. Loss 0.07263. Accuracy 0.997.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010413123
Train loss (w/o reg) on all data: 0.0046182713
Test loss (w/o reg) on all data: 0.00458579
Train acc on all data:  0.99975686846584
Test acc on all data:   1.0
Norm of the mean of gradients: 3.4374636e-07
Norm of the params: 10.76555
                Loss: fixed 880 labels. Loss 0.00459. Accuracy 1.000.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37562293
Train loss (w/o reg) on all data: 0.36966285
Test loss (w/o reg) on all data: 0.19348058
Train acc on all data:  0.8485290542183321
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 9.395284e-06
Norm of the params: 10.917934
              Random: fixed 265 labels. Loss 0.19348. Accuracy 0.979.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45093253
Train loss (w/o reg) on all data: 0.44535735
Test loss (w/o reg) on all data: 0.28389448
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.3617437e-05
Norm of the params: 10.559517
Flipped loss: 0.28389. Accuracy: 0.964
### Flips: 1025, rs: 30, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37447396
Train loss (w/o reg) on all data: 0.36515006
Test loss (w/o reg) on all data: 0.22903241
Train acc on all data:  0.8319961098954535
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.5852243e-05
Norm of the params: 13.655696
     Influence (LOO): fixed 171 labels. Loss 0.22903. Accuracy 0.968.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3383243
Train loss (w/o reg) on all data: 0.32647014
Test loss (w/o reg) on all data: 0.2319889
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 9.86883e-06
Norm of the params: 15.397507
                Loss: fixed 204 labels. Loss 0.23199. Accuracy 0.940.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43587956
Train loss (w/o reg) on all data: 0.43002042
Test loss (w/o reg) on all data: 0.26663288
Train acc on all data:  0.8062241672744955
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 3.0198067e-05
Norm of the params: 10.825105
              Random: fixed  51 labels. Loss 0.26663. Accuracy 0.962.
### Flips: 1025, rs: 30, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32230595
Train loss (w/o reg) on all data: 0.31234545
Test loss (w/o reg) on all data: 0.17850384
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 9.6066915e-06
Norm of the params: 14.114182
     Influence (LOO): fixed 296 labels. Loss 0.17850. Accuracy 0.978.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23284392
Train loss (w/o reg) on all data: 0.21552841
Test loss (w/o reg) on all data: 0.18077624
Train acc on all data:  0.8983710187211281
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 7.3336446e-06
Norm of the params: 18.60941
                Loss: fixed 408 labels. Loss 0.18078. Accuracy 0.946.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42433423
Train loss (w/o reg) on all data: 0.4184011
Test loss (w/o reg) on all data: 0.25230816
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.1770766e-05
Norm of the params: 10.893235
              Random: fixed  92 labels. Loss 0.25231. Accuracy 0.965.
### Flips: 1025, rs: 30, checks: 615
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27276337
Train loss (w/o reg) on all data: 0.262523
Test loss (w/o reg) on all data: 0.13872539
Train acc on all data:  0.8876732312180889
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.4482546e-05
Norm of the params: 14.311105
     Influence (LOO): fixed 407 labels. Loss 0.13873. Accuracy 0.989.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13659927
Train loss (w/o reg) on all data: 0.11705416
Test loss (w/o reg) on all data: 0.11570354
Train acc on all data:  0.949428640894724
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 9.2812925e-06
Norm of the params: 19.771248
                Loss: fixed 606 labels. Loss 0.11570. Accuracy 0.968.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41058508
Train loss (w/o reg) on all data: 0.40469262
Test loss (w/o reg) on all data: 0.23296048
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.3315714e-05
Norm of the params: 10.855829
              Random: fixed 142 labels. Loss 0.23296. Accuracy 0.974.
### Flips: 1025, rs: 30, checks: 820
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23218215
Train loss (w/o reg) on all data: 0.22098932
Test loss (w/o reg) on all data: 0.11484489
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.7321607e-06
Norm of the params: 14.961835
     Influence (LOO): fixed 489 labels. Loss 0.11484. Accuracy 0.993.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056515846
Train loss (w/o reg) on all data: 0.040542156
Test loss (w/o reg) on all data: 0.04289599
Train acc on all data:  0.9846827133479212
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9430245e-06
Norm of the params: 17.87383
                Loss: fixed 779 labels. Loss 0.04290. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39314598
Train loss (w/o reg) on all data: 0.38681564
Test loss (w/o reg) on all data: 0.21415287
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.4242128e-05
Norm of the params: 11.2519655
              Random: fixed 194 labels. Loss 0.21415. Accuracy 0.983.
### Flips: 1025, rs: 30, checks: 1025
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19234109
Train loss (w/o reg) on all data: 0.18163839
Test loss (w/o reg) on all data: 0.091122545
Train acc on all data:  0.926088013615366
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.996773e-06
Norm of the params: 14.630584
     Influence (LOO): fixed 570 labels. Loss 0.09112. Accuracy 0.994.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026105512
Train loss (w/o reg) on all data: 0.015595371
Test loss (w/o reg) on all data: 0.01862358
Train acc on all data:  0.9958667639192803
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 3.6471015e-06
Norm of the params: 14.498374
                Loss: fixed 838 labels. Loss 0.01862. Accuracy 0.997.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37893066
Train loss (w/o reg) on all data: 0.37282717
Test loss (w/o reg) on all data: 0.1976033
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.84873e-05
Norm of the params: 11.048504
              Random: fixed 241 labels. Loss 0.19760. Accuracy 0.984.
### Flips: 1025, rs: 30, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16256042
Train loss (w/o reg) on all data: 0.15241788
Test loss (w/o reg) on all data: 0.07332643
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.521684e-05
Norm of the params: 14.242569
     Influence (LOO): fixed 626 labels. Loss 0.07333. Accuracy 0.997.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019535946
Train loss (w/o reg) on all data: 0.010950994
Test loss (w/o reg) on all data: 0.015217306
Train acc on all data:  0.9963530269876003
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.6524207e-07
Norm of the params: 13.103398
                Loss: fixed 855 labels. Loss 0.01522. Accuracy 0.999.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.36090264
Train loss (w/o reg) on all data: 0.35459718
Test loss (w/o reg) on all data: 0.18071243
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 7.7271325e-06
Norm of the params: 11.229829
              Random: fixed 287 labels. Loss 0.18071. Accuracy 0.988.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45397356
Train loss (w/o reg) on all data: 0.44761252
Test loss (w/o reg) on all data: 0.28073063
Train acc on all data:  0.7906637490882568
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 7.027623e-05
Norm of the params: 11.279208
Flipped loss: 0.28073. Accuracy: 0.961
### Flips: 1025, rs: 31, checks: 205
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38034433
Train loss (w/o reg) on all data: 0.3707392
Test loss (w/o reg) on all data: 0.22172217
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.4703498e-05
Norm of the params: 13.860119
     Influence (LOO): fixed 170 labels. Loss 0.22172. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3446513
Train loss (w/o reg) on all data: 0.33193144
Test loss (w/o reg) on all data: 0.2320115
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.0277639e-05
Norm of the params: 15.949847
                Loss: fixed 205 labels. Loss 0.23201. Accuracy 0.941.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44112107
Train loss (w/o reg) on all data: 0.43468004
Test loss (w/o reg) on all data: 0.2626884
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.2978216e-05
Norm of the params: 11.349922
              Random: fixed  48 labels. Loss 0.26269. Accuracy 0.961.
### Flips: 1025, rs: 31, checks: 410
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32396963
Train loss (w/o reg) on all data: 0.31292737
Test loss (w/o reg) on all data: 0.17777202
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.303897e-05
Norm of the params: 14.86086
     Influence (LOO): fixed 302 labels. Loss 0.17777. Accuracy 0.984.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24518412
Train loss (w/o reg) on all data: 0.22773212
Test loss (w/o reg) on all data: 0.17674321
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 3.301353e-05
Norm of the params: 18.682615
                Loss: fixed 410 labels. Loss 0.17674. Accuracy 0.944.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4267953
Train loss (w/o reg) on all data: 0.4207821
Test loss (w/o reg) on all data: 0.24401638
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 3.228376e-05
Norm of the params: 10.966519
              Random: fixed 100 labels. Loss 0.24402. Accuracy 0.969.
### Flips: 1025, rs: 31, checks: 615
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27469936
Train loss (w/o reg) on all data: 0.26361045
Test loss (w/o reg) on all data: 0.13864377
Train acc on all data:  0.8840262582056893
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.951254e-06
Norm of the params: 14.89222
     Influence (LOO): fixed 411 labels. Loss 0.13864. Accuracy 0.987.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14688577
Train loss (w/o reg) on all data: 0.12569585
Test loss (w/o reg) on all data: 0.113512166
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 3.4819839e-06
Norm of the params: 20.58636
                Loss: fixed 610 labels. Loss 0.11351. Accuracy 0.960.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4158153
Train loss (w/o reg) on all data: 0.40964827
Test loss (w/o reg) on all data: 0.23329948
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.5860169e-05
Norm of the params: 11.105873
              Random: fixed 136 labels. Loss 0.23330. Accuracy 0.976.
### Flips: 1025, rs: 31, checks: 820
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22680844
Train loss (w/o reg) on all data: 0.21474123
Test loss (w/o reg) on all data: 0.10842062
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.0296274e-05
Norm of the params: 15.535259
     Influence (LOO): fixed 507 labels. Loss 0.10842. Accuracy 0.994.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056658387
Train loss (w/o reg) on all data: 0.038859297
Test loss (w/o reg) on all data: 0.048157297
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.2437616e-06
Norm of the params: 18.86748
                Loss: fixed 794 labels. Loss 0.04816. Accuracy 0.986.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4024329
Train loss (w/o reg) on all data: 0.3960994
Test loss (w/o reg) on all data: 0.21562426
Train acc on all data:  0.8336980306345733
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.7783326e-05
Norm of the params: 11.254766
              Random: fixed 178 labels. Loss 0.21562. Accuracy 0.981.
### Flips: 1025, rs: 31, checks: 1025
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19892941
Train loss (w/o reg) on all data: 0.18800895
Test loss (w/o reg) on all data: 0.0929074
Train acc on all data:  0.9190371991247265
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.144221e-06
Norm of the params: 14.778676
     Influence (LOO): fixed 565 labels. Loss 0.09291. Accuracy 0.994.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02666018
Train loss (w/o reg) on all data: 0.01545795
Test loss (w/o reg) on all data: 0.015807657
Train acc on all data:  0.9946511062484804
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 3.2269616e-06
Norm of the params: 14.968119
                Loss: fixed 860 labels. Loss 0.01581. Accuracy 0.998.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38728008
Train loss (w/o reg) on all data: 0.3807896
Test loss (w/o reg) on all data: 0.20210576
Train acc on all data:  0.8422076343301726
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1450613e-05
Norm of the params: 11.393393
              Random: fixed 222 labels. Loss 0.20211. Accuracy 0.983.
### Flips: 1025, rs: 31, checks: 1230
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16476779
Train loss (w/o reg) on all data: 0.15430798
Test loss (w/o reg) on all data: 0.07425186
Train acc on all data:  0.9341113542426452
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.8552056e-06
Norm of the params: 14.4636135
     Influence (LOO): fixed 627 labels. Loss 0.07425. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017608475
Train loss (w/o reg) on all data: 0.009102568
Test loss (w/o reg) on all data: 0.011500696
Train acc on all data:  0.9970824215900802
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 6.7411247e-07
Norm of the params: 13.042934
                Loss: fixed 873 labels. Loss 0.01150. Accuracy 0.997.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3729878
Train loss (w/o reg) on all data: 0.36639023
Test loss (w/o reg) on all data: 0.18966453
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.308385e-06
Norm of the params: 11.487012
              Random: fixed 263 labels. Loss 0.18966. Accuracy 0.986.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4580301
Train loss (w/o reg) on all data: 0.45231462
Test loss (w/o reg) on all data: 0.27820918
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 3.5209443e-05
Norm of the params: 10.69157
Flipped loss: 0.27821. Accuracy: 0.969
### Flips: 1025, rs: 32, checks: 205
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3820804
Train loss (w/o reg) on all data: 0.37244254
Test loss (w/o reg) on all data: 0.22468854
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 2.929118e-05
Norm of the params: 13.883715
     Influence (LOO): fixed 168 labels. Loss 0.22469. Accuracy 0.981.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34753364
Train loss (w/o reg) on all data: 0.33601463
Test loss (w/o reg) on all data: 0.22747749
Train acc on all data:  0.8395331874544129
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.4464417e-05
Norm of the params: 15.178294
                Loss: fixed 205 labels. Loss 0.22748. Accuracy 0.945.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4460506
Train loss (w/o reg) on all data: 0.44015032
Test loss (w/o reg) on all data: 0.26346645
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.354696e-05
Norm of the params: 10.863059
              Random: fixed  43 labels. Loss 0.26347. Accuracy 0.978.
### Flips: 1025, rs: 32, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32534862
Train loss (w/o reg) on all data: 0.31416994
Test loss (w/o reg) on all data: 0.183851
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.301856e-06
Norm of the params: 14.952381
     Influence (LOO): fixed 299 labels. Loss 0.18385. Accuracy 0.987.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24647781
Train loss (w/o reg) on all data: 0.2311799
Test loss (w/o reg) on all data: 0.16780026
Train acc on all data:  0.8937515195720885
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 2.4497853e-05
Norm of the params: 17.491665
                Loss: fixed 410 labels. Loss 0.16780. Accuracy 0.952.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43557894
Train loss (w/o reg) on all data: 0.42965776
Test loss (w/o reg) on all data: 0.25497678
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.0341486e-05
Norm of the params: 10.882258
              Random: fixed  77 labels. Loss 0.25498. Accuracy 0.971.
### Flips: 1025, rs: 32, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28269
Train loss (w/o reg) on all data: 0.27152738
Test loss (w/o reg) on all data: 0.14842312
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.3080776e-05
Norm of the params: 14.941628
     Influence (LOO): fixed 400 labels. Loss 0.14842. Accuracy 0.990.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14016758
Train loss (w/o reg) on all data: 0.12201685
Test loss (w/o reg) on all data: 0.10606205
Train acc on all data:  0.9462679309506443
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.3384748e-05
Norm of the params: 19.05294
                Loss: fixed 613 labels. Loss 0.10606. Accuracy 0.964.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42210454
Train loss (w/o reg) on all data: 0.41618878
Test loss (w/o reg) on all data: 0.24013624
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.8119737e-05
Norm of the params: 10.877293
              Random: fixed 124 labels. Loss 0.24014. Accuracy 0.979.
### Flips: 1025, rs: 32, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23345643
Train loss (w/o reg) on all data: 0.22211018
Test loss (w/o reg) on all data: 0.11865602
Train acc on all data:  0.9049355701434476
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.545173e-06
Norm of the params: 15.064036
     Influence (LOO): fixed 495 labels. Loss 0.11866. Accuracy 0.993.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053748406
Train loss (w/o reg) on all data: 0.0393198
Test loss (w/o reg) on all data: 0.039507974
Train acc on all data:  0.9849258448820812
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.1710001e-06
Norm of the params: 16.987411
                Loss: fixed 801 labels. Loss 0.03951. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40514654
Train loss (w/o reg) on all data: 0.39898515
Test loss (w/o reg) on all data: 0.22298257
Train acc on all data:  0.8276197422805738
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.2352026e-05
Norm of the params: 11.100814
              Random: fixed 173 labels. Loss 0.22298. Accuracy 0.977.
### Flips: 1025, rs: 32, checks: 1025
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19618191
Train loss (w/o reg) on all data: 0.1848508
Test loss (w/o reg) on all data: 0.09198365
Train acc on all data:  0.9226841721371262
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.5274134e-05
Norm of the params: 15.05398
     Influence (LOO): fixed 570 labels. Loss 0.09198. Accuracy 0.995.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014639707
Train loss (w/o reg) on all data: 0.007591403
Test loss (w/o reg) on all data: 0.008203067
Train acc on all data:  0.9975686846584002
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 4.2758504e-07
Norm of the params: 11.872913
                Loss: fixed 873 labels. Loss 0.00820. Accuracy 0.999.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39168465
Train loss (w/o reg) on all data: 0.38549438
Test loss (w/o reg) on all data: 0.20657438
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.0909112e-05
Norm of the params: 11.12678
              Random: fixed 218 labels. Loss 0.20657. Accuracy 0.982.
### Flips: 1025, rs: 32, checks: 1230
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1570502
Train loss (w/o reg) on all data: 0.14628601
Test loss (w/o reg) on all data: 0.07313635
Train acc on all data:  0.9394602479941648
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.020832e-05
Norm of the params: 14.672558
     Influence (LOO): fixed 640 labels. Loss 0.07314. Accuracy 0.994.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008824365
Train loss (w/o reg) on all data: 0.0040984172
Test loss (w/o reg) on all data: 0.004623014
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3688826e-07
Norm of the params: 9.722086
                Loss: fixed 883 labels. Loss 0.00462. Accuracy 1.000.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37857524
Train loss (w/o reg) on all data: 0.37244016
Test loss (w/o reg) on all data: 0.19267127
Train acc on all data:  0.8477996596158521
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 1.2204376e-05
Norm of the params: 11.077074
              Random: fixed 256 labels. Loss 0.19267. Accuracy 0.983.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45586264
Train loss (w/o reg) on all data: 0.4507094
Test loss (w/o reg) on all data: 0.2779382
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 4.3062304e-05
Norm of the params: 10.152081
Flipped loss: 0.27794. Accuracy: 0.950
### Flips: 1025, rs: 33, checks: 205
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3793803
Train loss (w/o reg) on all data: 0.3699992
Test loss (w/o reg) on all data: 0.22283082
Train acc on all data:  0.8285922684172137
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0714492e-05
Norm of the params: 13.697503
     Influence (LOO): fixed 175 labels. Loss 0.22283. Accuracy 0.955.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34004647
Train loss (w/o reg) on all data: 0.32681325
Test loss (w/o reg) on all data: 0.23927224
Train acc on all data:  0.8439095550692924
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 2.1135582e-05
Norm of the params: 16.268501
                Loss: fixed 205 labels. Loss 0.23927. Accuracy 0.915.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44503957
Train loss (w/o reg) on all data: 0.43991268
Test loss (w/o reg) on all data: 0.2619617
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 3.10268e-05
Norm of the params: 10.1261
              Random: fixed  46 labels. Loss 0.26196. Accuracy 0.957.
### Flips: 1025, rs: 33, checks: 410
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3214204
Train loss (w/o reg) on all data: 0.31107643
Test loss (w/o reg) on all data: 0.1818159
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.7986625e-05
Norm of the params: 14.383304
     Influence (LOO): fixed 304 labels. Loss 0.18182. Accuracy 0.966.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23440565
Train loss (w/o reg) on all data: 0.21490207
Test loss (w/o reg) on all data: 0.18479007
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 4.404537e-06
Norm of the params: 19.75023
                Loss: fixed 410 labels. Loss 0.18479. Accuracy 0.930.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43344894
Train loss (w/o reg) on all data: 0.42800727
Test loss (w/o reg) on all data: 0.24746634
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 5.9454043e-05
Norm of the params: 10.432312
              Random: fixed  86 labels. Loss 0.24747. Accuracy 0.964.
### Flips: 1025, rs: 33, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2773005
Train loss (w/o reg) on all data: 0.26685938
Test loss (w/o reg) on all data: 0.15211931
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 4.64857e-06
Norm of the params: 14.450698
     Influence (LOO): fixed 403 labels. Loss 0.15212. Accuracy 0.979.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14355908
Train loss (w/o reg) on all data: 0.12180511
Test loss (w/o reg) on all data: 0.115549274
Train acc on all data:  0.9477267201556042
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.0234554e-05
Norm of the params: 20.858559
                Loss: fixed 608 labels. Loss 0.11555. Accuracy 0.955.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42033583
Train loss (w/o reg) on all data: 0.41464227
Test loss (w/o reg) on all data: 0.2339366
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 2.6571839e-05
Norm of the params: 10.671044
              Random: fixed 127 labels. Loss 0.23394. Accuracy 0.966.
### Flips: 1025, rs: 33, checks: 820
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23068945
Train loss (w/o reg) on all data: 0.21996422
Test loss (w/o reg) on all data: 0.11959996
Train acc on all data:  0.9054218332117676
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.912724e-06
Norm of the params: 14.645978
     Influence (LOO): fixed 503 labels. Loss 0.11960. Accuracy 0.987.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06266421
Train loss (w/o reg) on all data: 0.045499418
Test loss (w/o reg) on all data: 0.048783567
Train acc on all data:  0.9807926088013615
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.3710264e-06
Norm of the params: 18.528246
                Loss: fixed 782 labels. Loss 0.04878. Accuracy 0.985.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40743455
Train loss (w/o reg) on all data: 0.4017654
Test loss (w/o reg) on all data: 0.2176203
Train acc on all data:  0.8298079260880136
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.3294775e-05
Norm of the params: 10.648136
              Random: fixed 174 labels. Loss 0.21762. Accuracy 0.979.
### Flips: 1025, rs: 33, checks: 1025
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19164039
Train loss (w/o reg) on all data: 0.18132584
Test loss (w/o reg) on all data: 0.09708158
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.6992995e-05
Norm of the params: 14.362837
     Influence (LOO): fixed 580 labels. Loss 0.09708. Accuracy 0.991.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023392703
Train loss (w/o reg) on all data: 0.013773534
Test loss (w/o reg) on all data: 0.0172156
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.7896575e-07
Norm of the params: 13.870234
                Loss: fixed 864 labels. Loss 0.01722. Accuracy 0.995.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39603043
Train loss (w/o reg) on all data: 0.3904997
Test loss (w/o reg) on all data: 0.20211808
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 6.206532e-06
Norm of the params: 10.517327
              Random: fixed 214 labels. Loss 0.20212. Accuracy 0.985.
### Flips: 1025, rs: 33, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15866816
Train loss (w/o reg) on all data: 0.14827152
Test loss (w/o reg) on all data: 0.0790988
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.146179e-06
Norm of the params: 14.419883
     Influence (LOO): fixed 645 labels. Loss 0.07910. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014599968
Train loss (w/o reg) on all data: 0.007856891
Test loss (w/o reg) on all data: 0.008044442
Train acc on all data:  0.9978118161925602
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 2.0372678e-07
Norm of the params: 11.61299
                Loss: fixed 881 labels. Loss 0.00804. Accuracy 0.999.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37631708
Train loss (w/o reg) on all data: 0.37050942
Test loss (w/o reg) on all data: 0.18495801
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.072985e-05
Norm of the params: 10.777444
              Random: fixed 269 labels. Loss 0.18496. Accuracy 0.989.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4471832
Train loss (w/o reg) on all data: 0.44139257
Test loss (w/o reg) on all data: 0.28515393
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.7146567e-05
Norm of the params: 10.761604
Flipped loss: 0.28515. Accuracy: 0.956
### Flips: 1025, rs: 34, checks: 205
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3717565
Train loss (w/o reg) on all data: 0.36200973
Test loss (w/o reg) on all data: 0.2288069
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.47932e-05
Norm of the params: 13.961928
     Influence (LOO): fixed 168 labels. Loss 0.22881. Accuracy 0.967.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33263215
Train loss (w/o reg) on all data: 0.31988758
Test loss (w/o reg) on all data: 0.24702123
Train acc on all data:  0.849258448820812
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 1.2882593e-05
Norm of the params: 15.965324
                Loss: fixed 205 labels. Loss 0.24702. Accuracy 0.927.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43794793
Train loss (w/o reg) on all data: 0.43242103
Test loss (w/o reg) on all data: 0.27037656
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 5.388004e-05
Norm of the params: 10.513711
              Random: fixed  38 labels. Loss 0.27038. Accuracy 0.960.
### Flips: 1025, rs: 34, checks: 410
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3166928
Train loss (w/o reg) on all data: 0.305298
Test loss (w/o reg) on all data: 0.17990679
Train acc on all data:  0.8677364454169706
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.50631695e-05
Norm of the params: 15.096216
     Influence (LOO): fixed 293 labels. Loss 0.17991. Accuracy 0.977.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22960919
Train loss (w/o reg) on all data: 0.21181694
Test loss (w/o reg) on all data: 0.19186312
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 6.772788e-06
Norm of the params: 18.86386
                Loss: fixed 407 labels. Loss 0.19186. Accuracy 0.948.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4273568
Train loss (w/o reg) on all data: 0.42160824
Test loss (w/o reg) on all data: 0.25413805
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.7047394e-05
Norm of the params: 10.722483
              Random: fixed  77 labels. Loss 0.25414. Accuracy 0.968.
### Flips: 1025, rs: 34, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26550338
Train loss (w/o reg) on all data: 0.25324813
Test loss (w/o reg) on all data: 0.14744295
Train acc on all data:  0.8913202042304887
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 7.95748e-06
Norm of the params: 15.65583
     Influence (LOO): fixed 403 labels. Loss 0.14744. Accuracy 0.985.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13207264
Train loss (w/o reg) on all data: 0.11173128
Test loss (w/o reg) on all data: 0.13350695
Train acc on all data:  0.9518599562363238
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 4.871747e-06
Norm of the params: 20.169956
                Loss: fixed 608 labels. Loss 0.13351. Accuracy 0.964.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41665405
Train loss (w/o reg) on all data: 0.41074154
Test loss (w/o reg) on all data: 0.24028537
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.31955185e-05
Norm of the params: 10.874303
              Random: fixed 112 labels. Loss 0.24029. Accuracy 0.968.
### Flips: 1025, rs: 34, checks: 820
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22665523
Train loss (w/o reg) on all data: 0.2145559
Test loss (w/o reg) on all data: 0.1156654
Train acc on all data:  0.9107707269632871
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 2.3522458e-05
Norm of the params: 15.555915
     Influence (LOO): fixed 490 labels. Loss 0.11567. Accuracy 0.987.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05326591
Train loss (w/o reg) on all data: 0.036821797
Test loss (w/o reg) on all data: 0.07422977
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.5649733e-06
Norm of the params: 18.13511
                Loss: fixed 774 labels. Loss 0.07423. Accuracy 0.978.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40496543
Train loss (w/o reg) on all data: 0.39918104
Test loss (w/o reg) on all data: 0.22910017
Train acc on all data:  0.8288353999513737
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.2720046e-05
Norm of the params: 10.755828
              Random: fixed 151 labels. Loss 0.22910. Accuracy 0.972.
### Flips: 1025, rs: 34, checks: 1025
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18526568
Train loss (w/o reg) on all data: 0.17410062
Test loss (w/o reg) on all data: 0.09015882
Train acc on all data:  0.9292487235594457
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.9648578e-05
Norm of the params: 14.943258
     Influence (LOO): fixed 573 labels. Loss 0.09016. Accuracy 0.994.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025645632
Train loss (w/o reg) on all data: 0.015496802
Test loss (w/o reg) on all data: 0.030259306
Train acc on all data:  0.9953805008509604
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 4.851084e-07
Norm of the params: 14.246986
                Loss: fixed 832 labels. Loss 0.03026. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3925038
Train loss (w/o reg) on all data: 0.3867776
Test loss (w/o reg) on all data: 0.21438734
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.1710677e-05
Norm of the params: 10.70157
              Random: fixed 194 labels. Loss 0.21439. Accuracy 0.973.
### Flips: 1025, rs: 34, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15283822
Train loss (w/o reg) on all data: 0.14301169
Test loss (w/o reg) on all data: 0.06855248
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.0660636e-05
Norm of the params: 14.018938
     Influence (LOO): fixed 642 labels. Loss 0.06855. Accuracy 0.997.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015907245
Train loss (w/o reg) on all data: 0.008722977
Test loss (w/o reg) on all data: 0.015642364
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 3.3974368e-07
Norm of the params: 11.986882
                Loss: fixed 854 labels. Loss 0.01564. Accuracy 0.995.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37982213
Train loss (w/o reg) on all data: 0.3738912
Test loss (w/o reg) on all data: 0.20439252
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.0104733e-05
Norm of the params: 10.891229
              Random: fixed 227 labels. Loss 0.20439. Accuracy 0.976.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45443696
Train loss (w/o reg) on all data: 0.4491248
Test loss (w/o reg) on all data: 0.28732872
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.13373335e-05
Norm of the params: 10.307406
Flipped loss: 0.28733. Accuracy: 0.950
### Flips: 1025, rs: 35, checks: 205
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3804947
Train loss (w/o reg) on all data: 0.37108082
Test loss (w/o reg) on all data: 0.21977088
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 3.5621888e-05
Norm of the params: 13.72145
     Influence (LOO): fixed 168 labels. Loss 0.21977. Accuracy 0.976.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33905873
Train loss (w/o reg) on all data: 0.3273184
Test loss (w/o reg) on all data: 0.24207567
Train acc on all data:  0.837831266715293
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.1926437e-05
Norm of the params: 15.323406
                Loss: fixed 205 labels. Loss 0.24208. Accuracy 0.931.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44071865
Train loss (w/o reg) on all data: 0.43492222
Test loss (w/o reg) on all data: 0.26980877
Train acc on all data:  0.7982008266472161
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 7.343597e-05
Norm of the params: 10.767011
              Random: fixed  45 labels. Loss 0.26981. Accuracy 0.958.
### Flips: 1025, rs: 35, checks: 410
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31928316
Train loss (w/o reg) on all data: 0.30840373
Test loss (w/o reg) on all data: 0.16988559
Train acc on all data:  0.8592268417213713
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 2.9879693e-05
Norm of the params: 14.750888
     Influence (LOO): fixed 307 labels. Loss 0.16989. Accuracy 0.989.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23570746
Train loss (w/o reg) on all data: 0.21966548
Test loss (w/o reg) on all data: 0.18373771
Train acc on all data:  0.8920495988329686
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.346775e-06
Norm of the params: 17.911999
                Loss: fixed 410 labels. Loss 0.18374. Accuracy 0.938.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42827535
Train loss (w/o reg) on all data: 0.42239237
Test loss (w/o reg) on all data: 0.25531122
Train acc on all data:  0.8064672988086555
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 3.2433003e-05
Norm of the params: 10.847086
              Random: fixed  84 labels. Loss 0.25531. Accuracy 0.959.
### Flips: 1025, rs: 35, checks: 615
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733321
Train loss (w/o reg) on all data: 0.26222694
Test loss (w/o reg) on all data: 0.1405348
Train acc on all data:  0.8835399951373694
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 5.9264353e-06
Norm of the params: 14.903111
     Influence (LOO): fixed 412 labels. Loss 0.14053. Accuracy 0.990.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14185306
Train loss (w/o reg) on all data: 0.1242321
Test loss (w/o reg) on all data: 0.11194819
Train acc on all data:  0.9435934840748845
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.3166758e-05
Norm of the params: 18.772833
                Loss: fixed 609 labels. Loss 0.11195. Accuracy 0.958.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41813943
Train loss (w/o reg) on all data: 0.41256934
Test loss (w/o reg) on all data: 0.24389161
Train acc on all data:  0.8169219547775346
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 4.319736e-05
Norm of the params: 10.554687
              Random: fixed 122 labels. Loss 0.24389. Accuracy 0.964.
### Flips: 1025, rs: 35, checks: 820
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23486644
Train loss (w/o reg) on all data: 0.2234253
Test loss (w/o reg) on all data: 0.117796704
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.481621e-06
Norm of the params: 15.126897
     Influence (LOO): fixed 487 labels. Loss 0.11780. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060801357
Train loss (w/o reg) on all data: 0.04537556
Test loss (w/o reg) on all data: 0.04775567
Train acc on all data:  0.9803063457330415
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.8362222e-06
Norm of the params: 17.564623
                Loss: fixed 775 labels. Loss 0.04776. Accuracy 0.980.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4084551
Train loss (w/o reg) on all data: 0.40300056
Test loss (w/o reg) on all data: 0.23123923
Train acc on all data:  0.824945295404814
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 7.78909e-06
Norm of the params: 10.44466
              Random: fixed 157 labels. Loss 0.23124. Accuracy 0.974.
### Flips: 1025, rs: 35, checks: 1025
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19245684
Train loss (w/o reg) on all data: 0.18123144
Test loss (w/o reg) on all data: 0.09433278
Train acc on all data:  0.9217116460004863
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 1.1895313e-05
Norm of the params: 14.983589
     Influence (LOO): fixed 567 labels. Loss 0.09433. Accuracy 0.996.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026004422
Train loss (w/o reg) on all data: 0.015432868
Test loss (w/o reg) on all data: 0.013610072
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.5295686e-07
Norm of the params: 14.54067
                Loss: fixed 852 labels. Loss 0.01361. Accuracy 0.998.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3947741
Train loss (w/o reg) on all data: 0.3894945
Test loss (w/o reg) on all data: 0.2109512
Train acc on all data:  0.838074398249453
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.345575e-05
Norm of the params: 10.275802
              Random: fixed 209 labels. Loss 0.21095. Accuracy 0.983.
### Flips: 1025, rs: 35, checks: 1230
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15772817
Train loss (w/o reg) on all data: 0.14733705
Test loss (w/o reg) on all data: 0.07268444
Train acc on all data:  0.9377583272550449
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 3.7974833e-06
Norm of the params: 14.4160385
     Influence (LOO): fixed 634 labels. Loss 0.07268. Accuracy 0.999.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015217355
Train loss (w/o reg) on all data: 0.007880276
Test loss (w/o reg) on all data: 0.0076369406
Train acc on all data:  0.9982980792608801
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 5.868496e-07
Norm of the params: 12.113694
                Loss: fixed 869 labels. Loss 0.00764. Accuracy 0.999.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37841117
Train loss (w/o reg) on all data: 0.37256894
Test loss (w/o reg) on all data: 0.19606833
Train acc on all data:  0.8482859226841721
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 3.809409e-05
Norm of the params: 10.809481
              Random: fixed 255 labels. Loss 0.19607. Accuracy 0.985.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46266285
Train loss (w/o reg) on all data: 0.45796564
Test loss (w/o reg) on all data: 0.27695218
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.3929368e-05
Norm of the params: 9.692479
Flipped loss: 0.27695. Accuracy: 0.961
### Flips: 1025, rs: 36, checks: 205
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38010052
Train loss (w/o reg) on all data: 0.37135962
Test loss (w/o reg) on all data: 0.22324581
Train acc on all data:  0.8295647945538537
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.700064e-05
Norm of the params: 13.221878
     Influence (LOO): fixed 176 labels. Loss 0.22325. Accuracy 0.973.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3523404
Train loss (w/o reg) on all data: 0.34120736
Test loss (w/o reg) on all data: 0.22623338
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 4.4082863e-05
Norm of the params: 14.921837
                Loss: fixed 205 labels. Loss 0.22623. Accuracy 0.937.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45247597
Train loss (w/o reg) on all data: 0.44771
Test loss (w/o reg) on all data: 0.26175007
Train acc on all data:  0.7986870897155361
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 5.16882e-05
Norm of the params: 9.76316
              Random: fixed  42 labels. Loss 0.26175. Accuracy 0.969.
### Flips: 1025, rs: 36, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.32420582
Train loss (w/o reg) on all data: 0.31466398
Test loss (w/o reg) on all data: 0.1713545
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 6.5347917e-06
Norm of the params: 13.81437
     Influence (LOO): fixed 307 labels. Loss 0.17135. Accuracy 0.988.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2470191
Train loss (w/o reg) on all data: 0.23151492
Test loss (w/o reg) on all data: 0.17482631
Train acc on all data:  0.8905908096280087
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.06069e-05
Norm of the params: 17.609192
                Loss: fixed 409 labels. Loss 0.17483. Accuracy 0.944.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4395411
Train loss (w/o reg) on all data: 0.43447387
Test loss (w/o reg) on all data: 0.24984409
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.826785e-05
Norm of the params: 10.066998
              Random: fixed  86 labels. Loss 0.24984. Accuracy 0.969.
### Flips: 1025, rs: 36, checks: 615
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27242747
Train loss (w/o reg) on all data: 0.26172903
Test loss (w/o reg) on all data: 0.1383203
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.159519e-05
Norm of the params: 14.627676
     Influence (LOO): fixed 414 labels. Loss 0.13832. Accuracy 0.993.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146081
Train loss (w/o reg) on all data: 0.12677307
Test loss (w/o reg) on all data: 0.11138716
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 8.890969e-06
Norm of the params: 19.650915
                Loss: fixed 612 labels. Loss 0.11139. Accuracy 0.961.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4250637
Train loss (w/o reg) on all data: 0.41990027
Test loss (w/o reg) on all data: 0.23501822
Train acc on all data:  0.8178944809141746
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.099161e-05
Norm of the params: 10.162113
              Random: fixed 133 labels. Loss 0.23502. Accuracy 0.973.
### Flips: 1025, rs: 36, checks: 820
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2368121
Train loss (w/o reg) on all data: 0.22588457
Test loss (w/o reg) on all data: 0.11444828
Train acc on all data:  0.9010454655968879
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.13031465e-05
Norm of the params: 14.78346
     Influence (LOO): fixed 492 labels. Loss 0.11445. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054281205
Train loss (w/o reg) on all data: 0.039052356
Test loss (w/o reg) on all data: 0.045134645
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.396421e-06
Norm of the params: 17.452135
                Loss: fixed 800 labels. Loss 0.04513. Accuracy 0.984.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41082323
Train loss (w/o reg) on all data: 0.40541416
Test loss (w/o reg) on all data: 0.21558087
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.9149118e-05
Norm of the params: 10.4010105
              Random: fixed 184 labels. Loss 0.21558. Accuracy 0.980.
### Flips: 1025, rs: 36, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20091774
Train loss (w/o reg) on all data: 0.19000629
Test loss (w/o reg) on all data: 0.09309894
Train acc on all data:  0.9178215414539266
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 5.2550645e-06
Norm of the params: 14.77258
     Influence (LOO): fixed 564 labels. Loss 0.09310. Accuracy 0.998.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020420041
Train loss (w/o reg) on all data: 0.012119658
Test loss (w/o reg) on all data: 0.012891593
Train acc on all data:  0.9961098954534403
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 6.97371e-07
Norm of the params: 12.884396
                Loss: fixed 870 labels. Loss 0.01289. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3988007
Train loss (w/o reg) on all data: 0.39333534
Test loss (w/o reg) on all data: 0.20455846
Train acc on all data:  0.838560661317773
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.6188907e-05
Norm of the params: 10.455013
              Random: fixed 218 labels. Loss 0.20456. Accuracy 0.981.
### Flips: 1025, rs: 36, checks: 1230
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1685893
Train loss (w/o reg) on all data: 0.15866542
Test loss (w/o reg) on all data: 0.0762717
Train acc on all data:  0.9348407488451252
Test acc on all data:   1.0
Norm of the mean of gradients: 6.598716e-06
Norm of the params: 14.088206
     Influence (LOO): fixed 631 labels. Loss 0.07627. Accuracy 1.000.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008141652
Train loss (w/o reg) on all data: 0.003864219
Test loss (w/o reg) on all data: 0.004447329
Train acc on all data:  0.9990274738633601
Test acc on all data:   1.0
Norm of the mean of gradients: 6.1689724e-07
Norm of the params: 9.249251
                Loss: fixed 892 labels. Loss 0.00445. Accuracy 1.000.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38217935
Train loss (w/o reg) on all data: 0.37648705
Test loss (w/o reg) on all data: 0.1911808
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.589113e-05
Norm of the params: 10.669874
              Random: fixed 265 labels. Loss 0.19118. Accuracy 0.983.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45629716
Train loss (w/o reg) on all data: 0.45146155
Test loss (w/o reg) on all data: 0.27476794
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.6070082e-05
Norm of the params: 9.83422
Flipped loss: 0.27477. Accuracy: 0.959
### Flips: 1025, rs: 37, checks: 205
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3796233
Train loss (w/o reg) on all data: 0.3703362
Test loss (w/o reg) on all data: 0.21602182
Train acc on all data:  0.8327255044979334
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 8.850798e-06
Norm of the params: 13.628704
     Influence (LOO): fixed 168 labels. Loss 0.21602. Accuracy 0.973.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34150285
Train loss (w/o reg) on all data: 0.32973787
Test loss (w/o reg) on all data: 0.23299098
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.258325e-06
Norm of the params: 15.339479
                Loss: fixed 205 labels. Loss 0.23299. Accuracy 0.938.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44252166
Train loss (w/o reg) on all data: 0.43767327
Test loss (w/o reg) on all data: 0.2596113
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.7752885e-05
Norm of the params: 9.847213
              Random: fixed  48 labels. Loss 0.25961. Accuracy 0.961.
### Flips: 1025, rs: 37, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3229689
Train loss (w/o reg) on all data: 0.31260717
Test loss (w/o reg) on all data: 0.17635523
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 6.5395965e-05
Norm of the params: 14.395639
     Influence (LOO): fixed 299 labels. Loss 0.17636. Accuracy 0.984.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23522606
Train loss (w/o reg) on all data: 0.21818773
Test loss (w/o reg) on all data: 0.17788564
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 9.789233e-06
Norm of the params: 18.459866
                Loss: fixed 409 labels. Loss 0.17789. Accuracy 0.948.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43190503
Train loss (w/o reg) on all data: 0.42703763
Test loss (w/o reg) on all data: 0.24649288
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.5115519e-05
Norm of the params: 9.866503
              Random: fixed  91 labels. Loss 0.24649. Accuracy 0.960.
### Flips: 1025, rs: 37, checks: 615
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27338842
Train loss (w/o reg) on all data: 0.2621621
Test loss (w/o reg) on all data: 0.14297149
Train acc on all data:  0.8893751519572088
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 7.948464e-06
Norm of the params: 14.984217
     Influence (LOO): fixed 411 labels. Loss 0.14297. Accuracy 0.990.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13700157
Train loss (w/o reg) on all data: 0.117287055
Test loss (w/o reg) on all data: 0.11226727
Train acc on all data:  0.9460247994164843
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 7.1012573e-06
Norm of the params: 19.856743
                Loss: fixed 611 labels. Loss 0.11227. Accuracy 0.965.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4183232
Train loss (w/o reg) on all data: 0.4135113
Test loss (w/o reg) on all data: 0.22925182
Train acc on all data:  0.825188426938974
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.6634542e-05
Norm of the params: 9.810084
              Random: fixed 140 labels. Loss 0.22925. Accuracy 0.967.
### Flips: 1025, rs: 37, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22702655
Train loss (w/o reg) on all data: 0.21579584
Test loss (w/o reg) on all data: 0.11604945
Train acc on all data:  0.9090688062241673
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.299731e-05
Norm of the params: 14.987133
     Influence (LOO): fixed 505 labels. Loss 0.11605. Accuracy 0.994.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05173573
Train loss (w/o reg) on all data: 0.036500603
Test loss (w/o reg) on all data: 0.0476925
Train acc on all data:  0.9861415025528811
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.1779238e-06
Norm of the params: 17.45573
                Loss: fixed 786 labels. Loss 0.04769. Accuracy 0.989.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40319416
Train loss (w/o reg) on all data: 0.39803016
Test loss (w/o reg) on all data: 0.2153859
Train acc on all data:  0.836129345976173
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.347866e-05
Norm of the params: 10.162686
              Random: fixed 186 labels. Loss 0.21539. Accuracy 0.970.
### Flips: 1025, rs: 37, checks: 1025
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864788
Train loss (w/o reg) on all data: 0.17815427
Test loss (w/o reg) on all data: 0.08942221
Train acc on all data:  0.9268174082178459
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.090098e-06
Norm of the params: 14.486967
     Influence (LOO): fixed 582 labels. Loss 0.08942. Accuracy 0.994.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019021474
Train loss (w/o reg) on all data: 0.010670327
Test loss (w/o reg) on all data: 0.0111967595
Train acc on all data:  0.9973255531242402
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.6792775e-07
Norm of the params: 12.923736
                Loss: fixed 854 labels. Loss 0.01120. Accuracy 0.998.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38751543
Train loss (w/o reg) on all data: 0.38207474
Test loss (w/o reg) on all data: 0.19944996
Train acc on all data:  0.8465840019450522
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.0258634e-05
Norm of the params: 10.431365
              Random: fixed 233 labels. Loss 0.19945. Accuracy 0.973.
### Flips: 1025, rs: 37, checks: 1230
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15542395
Train loss (w/o reg) on all data: 0.14576058
Test loss (w/o reg) on all data: 0.07120835
Train acc on all data:  0.9414053002674447
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.528187e-05
Norm of the params: 13.902067
     Influence (LOO): fixed 646 labels. Loss 0.07121. Accuracy 0.994.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014780827
Train loss (w/o reg) on all data: 0.0077660633
Test loss (w/o reg) on all data: 0.007249986
Train acc on all data:  0.9982980792608801
Test acc on all data:   1.0
Norm of the mean of gradients: 3.94207e-07
Norm of the params: 11.844631
                Loss: fixed 866 labels. Loss 0.00725. Accuracy 1.000.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3693578
Train loss (w/o reg) on all data: 0.36360848
Test loss (w/o reg) on all data: 0.18239518
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.2262912e-05
Norm of the params: 10.723176
              Random: fixed 282 labels. Loss 0.18240. Accuracy 0.979.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4570764
Train loss (w/o reg) on all data: 0.45138207
Test loss (w/o reg) on all data: 0.2800536
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.0360774e-05
Norm of the params: 10.671768
Flipped loss: 0.28005. Accuracy: 0.963
### Flips: 1025, rs: 38, checks: 205
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3850462
Train loss (w/o reg) on all data: 0.3758408
Test loss (w/o reg) on all data: 0.21814552
Train acc on all data:  0.8317529783612935
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.559845e-05
Norm of the params: 13.568635
     Influence (LOO): fixed 170 labels. Loss 0.21815. Accuracy 0.975.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3446472
Train loss (w/o reg) on all data: 0.33305565
Test loss (w/o reg) on all data: 0.2303886
Train acc on all data:  0.8456114758084123
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.5335241e-05
Norm of the params: 15.225996
                Loss: fixed 205 labels. Loss 0.23039. Accuracy 0.937.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44471863
Train loss (w/o reg) on all data: 0.43887952
Test loss (w/o reg) on all data: 0.2648147
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.5354553e-05
Norm of the params: 10.806586
              Random: fixed  46 labels. Loss 0.26481. Accuracy 0.969.
### Flips: 1025, rs: 38, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3307038
Train loss (w/o reg) on all data: 0.3198753
Test loss (w/o reg) on all data: 0.17766389
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8155013e-05
Norm of the params: 14.71631
     Influence (LOO): fixed 297 labels. Loss 0.17766. Accuracy 0.983.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24097289
Train loss (w/o reg) on all data: 0.22407795
Test loss (w/o reg) on all data: 0.17719147
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.5893469e-05
Norm of the params: 18.382025
                Loss: fixed 408 labels. Loss 0.17719. Accuracy 0.938.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43265018
Train loss (w/o reg) on all data: 0.42682633
Test loss (w/o reg) on all data: 0.25052726
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.2539671e-05
Norm of the params: 10.792447
              Random: fixed  91 labels. Loss 0.25053. Accuracy 0.967.
### Flips: 1025, rs: 38, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27797177
Train loss (w/o reg) on all data: 0.26703805
Test loss (w/o reg) on all data: 0.13674733
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.134658e-06
Norm of the params: 14.787646
     Influence (LOO): fixed 415 labels. Loss 0.13675. Accuracy 0.991.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13226269
Train loss (w/o reg) on all data: 0.111077175
Test loss (w/o reg) on all data: 0.111575186
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.4931904e-05
Norm of the params: 20.584225
                Loss: fixed 613 labels. Loss 0.11158. Accuracy 0.953.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4187497
Train loss (w/o reg) on all data: 0.41280067
Test loss (w/o reg) on all data: 0.23332813
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.5727404e-05
Norm of the params: 10.90782
              Random: fixed 138 labels. Loss 0.23333. Accuracy 0.967.
### Flips: 1025, rs: 38, checks: 820
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23486009
Train loss (w/o reg) on all data: 0.22450148
Test loss (w/o reg) on all data: 0.11058515
Train acc on all data:  0.9063943593484075
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 8.6131495e-06
Norm of the params: 14.39348
     Influence (LOO): fixed 503 labels. Loss 0.11059. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051175933
Train loss (w/o reg) on all data: 0.034712862
Test loss (w/o reg) on all data: 0.029206507
Train acc on all data:  0.987114028689521
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0884195e-06
Norm of the params: 18.145563
                Loss: fixed 793 labels. Loss 0.02921. Accuracy 0.995.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4023179
Train loss (w/o reg) on all data: 0.39650667
Test loss (w/o reg) on all data: 0.21464607
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.3779483e-05
Norm of the params: 10.78077
              Random: fixed 192 labels. Loss 0.21465. Accuracy 0.978.
### Flips: 1025, rs: 38, checks: 1025
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139095
Train loss (w/o reg) on all data: 0.18118247
Test loss (w/o reg) on all data: 0.08667886
Train acc on all data:  0.9246292244104061
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.0982738e-05
Norm of the params: 14.288789
     Influence (LOO): fixed 586 labels. Loss 0.08668. Accuracy 0.995.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026291538
Train loss (w/o reg) on all data: 0.0151804695
Test loss (w/o reg) on all data: 0.014549064
Train acc on all data:  0.9948942377826404
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 4.923903e-07
Norm of the params: 14.907091
                Loss: fixed 846 labels. Loss 0.01455. Accuracy 0.997.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38528788
Train loss (w/o reg) on all data: 0.37948912
Test loss (w/o reg) on all data: 0.19935209
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.755552e-05
Norm of the params: 10.769187
              Random: fixed 241 labels. Loss 0.19935. Accuracy 0.978.
### Flips: 1025, rs: 38, checks: 1230
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15501486
Train loss (w/o reg) on all data: 0.14550452
Test loss (w/o reg) on all data: 0.06533627
Train acc on all data:  0.9404327741308047
Test acc on all data:   1.0
Norm of the mean of gradients: 6.4248884e-06
Norm of the params: 13.791542
     Influence (LOO): fixed 654 labels. Loss 0.06534. Accuracy 1.000.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016674351
Train loss (w/o reg) on all data: 0.0091157025
Test loss (w/o reg) on all data: 0.0066748657
Train acc on all data:  0.9973255531242402
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0004408e-07
Norm of the params: 12.295241
                Loss: fixed 869 labels. Loss 0.00667. Accuracy 1.000.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37155935
Train loss (w/o reg) on all data: 0.3658135
Test loss (w/o reg) on all data: 0.18688205
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.779472e-06
Norm of the params: 10.71995
              Random: fixed 282 labels. Loss 0.18688. Accuracy 0.980.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4508928
Train loss (w/o reg) on all data: 0.44501084
Test loss (w/o reg) on all data: 0.27614775
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.8157254e-05
Norm of the params: 10.846169
Flipped loss: 0.27615. Accuracy: 0.956
### Flips: 1025, rs: 39, checks: 205
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37669322
Train loss (w/o reg) on all data: 0.36690193
Test loss (w/o reg) on all data: 0.22210631
Train acc on all data:  0.8344274252370533
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.0489904e-05
Norm of the params: 13.993771
     Influence (LOO): fixed 166 labels. Loss 0.22211. Accuracy 0.972.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33685312
Train loss (w/o reg) on all data: 0.323617
Test loss (w/o reg) on all data: 0.2289085
Train acc on all data:  0.8480427911500121
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.2639598e-05
Norm of the params: 16.27028
                Loss: fixed 204 labels. Loss 0.22891. Accuracy 0.938.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43977445
Train loss (w/o reg) on all data: 0.4338547
Test loss (w/o reg) on all data: 0.26238412
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.3369546e-05
Norm of the params: 10.880964
              Random: fixed  44 labels. Loss 0.26238. Accuracy 0.956.
### Flips: 1025, rs: 39, checks: 410
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31773332
Train loss (w/o reg) on all data: 0.3072587
Test loss (w/o reg) on all data: 0.17756417
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4848721e-05
Norm of the params: 14.473848
     Influence (LOO): fixed 298 labels. Loss 0.17756. Accuracy 0.982.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2377851
Train loss (w/o reg) on all data: 0.2202852
Test loss (w/o reg) on all data: 0.17020033
Train acc on all data:  0.899343544857768
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 5.5770847e-06
Norm of the params: 18.708225
                Loss: fixed 406 labels. Loss 0.17020. Accuracy 0.950.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43017548
Train loss (w/o reg) on all data: 0.42423785
Test loss (w/o reg) on all data: 0.24792738
Train acc on all data:  0.8096280087527352
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.1924617e-05
Norm of the params: 10.897375
              Random: fixed  79 labels. Loss 0.24793. Accuracy 0.967.
### Flips: 1025, rs: 39, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27242345
Train loss (w/o reg) on all data: 0.26150492
Test loss (w/o reg) on all data: 0.14361922
Train acc on all data:  0.8854850474106492
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.1571676e-05
Norm of the params: 14.777374
     Influence (LOO): fixed 402 labels. Loss 0.14362. Accuracy 0.988.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13851103
Train loss (w/o reg) on all data: 0.11861428
Test loss (w/o reg) on all data: 0.105425134
Train acc on all data:  0.9489423778264041
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.332758e-06
Norm of the params: 19.948309
                Loss: fixed 605 labels. Loss 0.10543. Accuracy 0.970.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41674423
Train loss (w/o reg) on all data: 0.41056985
Test loss (w/o reg) on all data: 0.23334599
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 7.2866937e-06
Norm of the params: 11.112499
              Random: fixed 126 labels. Loss 0.23335. Accuracy 0.974.
### Flips: 1025, rs: 39, checks: 820
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22395031
Train loss (w/o reg) on all data: 0.21336381
Test loss (w/o reg) on all data: 0.107709564
Train acc on all data:  0.9085825431558473
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 4.7993217e-06
Norm of the params: 14.550942
     Influence (LOO): fixed 502 labels. Loss 0.10771. Accuracy 0.998.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056367833
Train loss (w/o reg) on all data: 0.040394183
Test loss (w/o reg) on all data: 0.048165392
Train acc on all data:  0.9844395818137612
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.1822012e-06
Norm of the params: 17.87381
                Loss: fixed 777 labels. Loss 0.04817. Accuracy 0.987.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40177932
Train loss (w/o reg) on all data: 0.39551935
Test loss (w/o reg) on all data: 0.21696417
Train acc on all data:  0.8332117675662534
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.7077578e-05
Norm of the params: 11.18927
              Random: fixed 173 labels. Loss 0.21696. Accuracy 0.980.
### Flips: 1025, rs: 39, checks: 1025
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1896139
Train loss (w/o reg) on all data: 0.17968622
Test loss (w/o reg) on all data: 0.086863376
Train acc on all data:  0.924872355944566
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 9.803566e-06
Norm of the params: 14.090899
     Influence (LOO): fixed 572 labels. Loss 0.08686. Accuracy 0.997.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024608962
Train loss (w/o reg) on all data: 0.014939981
Test loss (w/o reg) on all data: 0.016745139
Train acc on all data:  0.9956236323851203
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 4.4191026e-07
Norm of the params: 13.906102
                Loss: fixed 846 labels. Loss 0.01675. Accuracy 0.994.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38782766
Train loss (w/o reg) on all data: 0.38134083
Test loss (w/o reg) on all data: 0.20067953
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.3699281e-05
Norm of the params: 11.390201
              Random: fixed 215 labels. Loss 0.20068. Accuracy 0.985.
### Flips: 1025, rs: 39, checks: 1230
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15464728
Train loss (w/o reg) on all data: 0.14567
Test loss (w/o reg) on all data: 0.06752756
Train acc on all data:  0.9397033795283248
Test acc on all data:   0.9990281827016521
Norm of the mean of gradients: 6.739813e-06
Norm of the params: 13.399457
     Influence (LOO): fixed 637 labels. Loss 0.06753. Accuracy 0.999.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015344581
Train loss (w/o reg) on all data: 0.008079082
Test loss (w/o reg) on all data: 0.015645
Train acc on all data:  0.9980549477267202
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 2.8406933e-07
Norm of the params: 12.054459
                Loss: fixed 859 labels. Loss 0.01564. Accuracy 0.995.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37103972
Train loss (w/o reg) on all data: 0.36439535
Test loss (w/o reg) on all data: 0.18631022
Train acc on all data:  0.8514466326282519
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.742109e-06
Norm of the params: 11.527667
              Random: fixed 261 labels. Loss 0.18631. Accuracy 0.987.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4967608
Train loss (w/o reg) on all data: 0.49179205
Test loss (w/o reg) on all data: 0.3280967
Train acc on all data:  0.7544371504984196
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 3.2178636e-05
Norm of the params: 9.9686775
Flipped loss: 0.32810. Accuracy: 0.950
### Flips: 1230, rs: 0, checks: 205
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43525904
Train loss (w/o reg) on all data: 0.42578787
Test loss (w/o reg) on all data: 0.2721158
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.3450702e-05
Norm of the params: 13.763129
     Influence (LOO): fixed 155 labels. Loss 0.27212. Accuracy 0.965.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39535257
Train loss (w/o reg) on all data: 0.3830104
Test loss (w/o reg) on all data: 0.27352276
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 1.5749125e-05
Norm of the params: 15.711256
                Loss: fixed 203 labels. Loss 0.27352. Accuracy 0.923.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4858729
Train loss (w/o reg) on all data: 0.4807679
Test loss (w/o reg) on all data: 0.31219152
Train acc on all data:  0.7690250425480185
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 4.1953732e-05
Norm of the params: 10.104429
              Random: fixed  52 labels. Loss 0.31219. Accuracy 0.949.
### Flips: 1230, rs: 0, checks: 410
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38446438
Train loss (w/o reg) on all data: 0.3726919
Test loss (w/o reg) on all data: 0.23183258
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.197378e-05
Norm of the params: 15.3443775
     Influence (LOO): fixed 286 labels. Loss 0.23183. Accuracy 0.968.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30696788
Train loss (w/o reg) on all data: 0.2895118
Test loss (w/o reg) on all data: 0.21161975
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 4.2594937e-05
Norm of the params: 18.684792
                Loss: fixed 405 labels. Loss 0.21162. Accuracy 0.941.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47489348
Train loss (w/o reg) on all data: 0.4698561
Test loss (w/o reg) on all data: 0.29182255
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 5.363249e-05
Norm of the params: 10.037299
              Random: fixed 104 labels. Loss 0.29182. Accuracy 0.960.
### Flips: 1230, rs: 0, checks: 615
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34011295
Train loss (w/o reg) on all data: 0.32719794
Test loss (w/o reg) on all data: 0.20767075
Train acc on all data:  0.8402625820568927
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 4.6891248e-05
Norm of the params: 16.071722
     Influence (LOO): fixed 395 labels. Loss 0.20767. Accuracy 0.972.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21641271
Train loss (w/o reg) on all data: 0.1962607
Test loss (w/o reg) on all data: 0.15296766
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4577443e-05
Norm of the params: 20.075857
                Loss: fixed 602 labels. Loss 0.15297. Accuracy 0.956.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46471187
Train loss (w/o reg) on all data: 0.45951468
Test loss (w/o reg) on all data: 0.27676705
Train acc on all data:  0.7858011184050572
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.3332e-05
Norm of the params: 10.19528
              Random: fixed 146 labels. Loss 0.27677. Accuracy 0.968.
### Flips: 1230, rs: 0, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29879177
Train loss (w/o reg) on all data: 0.285799
Test loss (w/o reg) on all data: 0.17116706
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.0368731e-05
Norm of the params: 16.120028
     Influence (LOO): fixed 498 labels. Loss 0.17117. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13185243
Train loss (w/o reg) on all data: 0.11027695
Test loss (w/o reg) on all data: 0.09435313
Train acc on all data:  0.949914903963044
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.4510557e-06
Norm of the params: 20.772808
                Loss: fixed 792 labels. Loss 0.09435. Accuracy 0.969.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45258304
Train loss (w/o reg) on all data: 0.4475686
Test loss (w/o reg) on all data: 0.25935513
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.3313412e-05
Norm of the params: 10.014433
              Random: fixed 195 labels. Loss 0.25936. Accuracy 0.981.
### Flips: 1230, rs: 0, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2614743
Train loss (w/o reg) on all data: 0.2482641
Test loss (w/o reg) on all data: 0.14348066
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 8.2149345e-06
Norm of the params: 16.254353
     Influence (LOO): fixed 584 labels. Loss 0.14348. Accuracy 0.986.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063015565
Train loss (w/o reg) on all data: 0.045754027
Test loss (w/o reg) on all data: 0.037744243
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.3020265e-06
Norm of the params: 18.58039
                Loss: fixed 944 labels. Loss 0.03774. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43846884
Train loss (w/o reg) on all data: 0.4334986
Test loss (w/o reg) on all data: 0.2409161
Train acc on all data:  0.8050085096036956
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.0570075e-05
Norm of the params: 9.9702
              Random: fixed 250 labels. Loss 0.24092. Accuracy 0.982.
### Flips: 1230, rs: 0, checks: 1230
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22971484
Train loss (w/o reg) on all data: 0.2170295
Test loss (w/o reg) on all data: 0.121861465
Train acc on all data:  0.8986141502552881
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 2.276989e-05
Norm of the params: 15.928183
     Influence (LOO): fixed 654 labels. Loss 0.12186. Accuracy 0.993.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034105167
Train loss (w/o reg) on all data: 0.02093731
Test loss (w/o reg) on all data: 0.020092765
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 3.1385719e-06
Norm of the params: 16.228281
                Loss: fixed 1004 labels. Loss 0.02009. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42415953
Train loss (w/o reg) on all data: 0.41893557
Test loss (w/o reg) on all data: 0.22584958
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 8.842516e-06
Norm of the params: 10.221517
              Random: fixed 299 labels. Loss 0.22585. Accuracy 0.985.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4988366
Train loss (w/o reg) on all data: 0.49396574
Test loss (w/o reg) on all data: 0.32657197
Train acc on all data:  0.7520058351568198
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.5058e-05
Norm of the params: 9.87001
Flipped loss: 0.32657. Accuracy: 0.948
### Flips: 1230, rs: 1, checks: 205
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43609995
Train loss (w/o reg) on all data: 0.4266837
Test loss (w/o reg) on all data: 0.2747425
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 6.314964e-05
Norm of the params: 13.72316
     Influence (LOO): fixed 159 labels. Loss 0.27474. Accuracy 0.945.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39814463
Train loss (w/o reg) on all data: 0.3866526
Test loss (w/o reg) on all data: 0.2690457
Train acc on all data:  0.8069535618769754
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 3.0795363e-05
Norm of the params: 15.160508
                Loss: fixed 205 labels. Loss 0.26905. Accuracy 0.919.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4886165
Train loss (w/o reg) on all data: 0.48377502
Test loss (w/o reg) on all data: 0.30729085
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.4996024e-05
Norm of the params: 9.840205
              Random: fixed  57 labels. Loss 0.30729. Accuracy 0.952.
### Flips: 1230, rs: 1, checks: 410
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39096907
Train loss (w/o reg) on all data: 0.37942752
Test loss (w/o reg) on all data: 0.23859556
Train acc on all data:  0.8188670070508145
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 4.998756e-05
Norm of the params: 15.193125
     Influence (LOO): fixed 282 labels. Loss 0.23860. Accuracy 0.965.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30429357
Train loss (w/o reg) on all data: 0.28692883
Test loss (w/o reg) on all data: 0.21857794
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 2.9041903e-05
Norm of the params: 18.635849
                Loss: fixed 409 labels. Loss 0.21858. Accuracy 0.926.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47460085
Train loss (w/o reg) on all data: 0.46960565
Test loss (w/o reg) on all data: 0.2885201
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.0550662e-05
Norm of the params: 9.995207
              Random: fixed 112 labels. Loss 0.28852. Accuracy 0.958.
### Flips: 1230, rs: 1, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34826523
Train loss (w/o reg) on all data: 0.33602917
Test loss (w/o reg) on all data: 0.20492314
Train acc on all data:  0.8409919766593728
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.638703e-06
Norm of the params: 15.643569
     Influence (LOO): fixed 393 labels. Loss 0.20492. Accuracy 0.976.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21558754
Train loss (w/o reg) on all data: 0.19404602
Test loss (w/o reg) on all data: 0.16088326
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.3579454e-05
Norm of the params: 20.756453
                Loss: fixed 607 labels. Loss 0.16088. Accuracy 0.946.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46076548
Train loss (w/o reg) on all data: 0.455668
Test loss (w/o reg) on all data: 0.2652882
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.3204288e-05
Norm of the params: 10.09701
              Random: fixed 168 labels. Loss 0.26529. Accuracy 0.968.
### Flips: 1230, rs: 1, checks: 820
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30421206
Train loss (w/o reg) on all data: 0.2919132
Test loss (w/o reg) on all data: 0.17161298
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.9201048e-05
Norm of the params: 15.683649
     Influence (LOO): fixed 499 labels. Loss 0.17161. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13272631
Train loss (w/o reg) on all data: 0.1091408
Test loss (w/o reg) on all data: 0.10296866
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.1486376e-06
Norm of the params: 21.718891
                Loss: fixed 792 labels. Loss 0.10297. Accuracy 0.965.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44648504
Train loss (w/o reg) on all data: 0.4412622
Test loss (w/o reg) on all data: 0.24754481
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.2170648e-05
Norm of the params: 10.220434
              Random: fixed 224 labels. Loss 0.24754. Accuracy 0.978.
### Flips: 1230, rs: 1, checks: 1025
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26424915
Train loss (w/o reg) on all data: 0.2523191
Test loss (w/o reg) on all data: 0.14401257
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.8635837e-05
Norm of the params: 15.446712
     Influence (LOO): fixed 587 labels. Loss 0.14401. Accuracy 0.984.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0667266
Train loss (w/o reg) on all data: 0.048173316
Test loss (w/o reg) on all data: 0.052364215
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 3.1667284e-06
Norm of the params: 19.263067
                Loss: fixed 937 labels. Loss 0.05236. Accuracy 0.982.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4303817
Train loss (w/o reg) on all data: 0.42481396
Test loss (w/o reg) on all data: 0.23396727
Train acc on all data:  0.812545587162655
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.5817956e-05
Norm of the params: 10.552471
              Random: fixed 279 labels. Loss 0.23397. Accuracy 0.982.
### Flips: 1230, rs: 1, checks: 1230
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23000471
Train loss (w/o reg) on all data: 0.21871942
Test loss (w/o reg) on all data: 0.118186526
Train acc on all data:  0.899829807926088
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 7.570932e-06
Norm of the params: 15.023505
     Influence (LOO): fixed 660 labels. Loss 0.11819. Accuracy 0.994.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040536992
Train loss (w/o reg) on all data: 0.026353525
Test loss (w/o reg) on all data: 0.026451295
Train acc on all data:  0.9912472647702407
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 3.1383988e-06
Norm of the params: 16.842487
                Loss: fixed 1004 labels. Loss 0.02645. Accuracy 0.991.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41427958
Train loss (w/o reg) on all data: 0.40849817
Test loss (w/o reg) on all data: 0.21570528
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.4003462e-05
Norm of the params: 10.753056
              Random: fixed 329 labels. Loss 0.21571. Accuracy 0.982.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49609748
Train loss (w/o reg) on all data: 0.4914747
Test loss (w/o reg) on all data: 0.3327336
Train acc on all data:  0.761974228057379
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 5.4712345e-05
Norm of the params: 9.615394
Flipped loss: 0.33273. Accuracy: 0.931
### Flips: 1230, rs: 2, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43355578
Train loss (w/o reg) on all data: 0.42470974
Test loss (w/o reg) on all data: 0.28316444
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.0645467e-05
Norm of the params: 13.301164
     Influence (LOO): fixed 159 labels. Loss 0.28316. Accuracy 0.947.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39332965
Train loss (w/o reg) on all data: 0.382342
Test loss (w/o reg) on all data: 0.28860265
Train acc on all data:  0.8169219547775346
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 2.1799136e-05
Norm of the params: 14.82406
                Loss: fixed 203 labels. Loss 0.28860. Accuracy 0.915.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4887879
Train loss (w/o reg) on all data: 0.4840174
Test loss (w/o reg) on all data: 0.31575498
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.268473e-05
Norm of the params: 9.767795
              Random: fixed  42 labels. Loss 0.31575. Accuracy 0.956.
### Flips: 1230, rs: 2, checks: 410
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38875702
Train loss (w/o reg) on all data: 0.37777156
Test loss (w/o reg) on all data: 0.24022292
Train acc on all data:  0.8195964016532944
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 8.183679e-06
Norm of the params: 14.822592
     Influence (LOO): fixed 274 labels. Loss 0.24022. Accuracy 0.971.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2979805
Train loss (w/o reg) on all data: 0.28231117
Test loss (w/o reg) on all data: 0.23209444
Train acc on all data:  0.8657913931436907
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 8.0633745e-06
Norm of the params: 17.702723
                Loss: fixed 406 labels. Loss 0.23209. Accuracy 0.922.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47553527
Train loss (w/o reg) on all data: 0.4704973
Test loss (w/o reg) on all data: 0.29994494
Train acc on all data:  0.7802090931193776
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.6426778e-05
Norm of the params: 10.037879
              Random: fixed  95 labels. Loss 0.29994. Accuracy 0.956.
### Flips: 1230, rs: 2, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3424614
Train loss (w/o reg) on all data: 0.33048156
Test loss (w/o reg) on all data: 0.19907428
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0239037e-05
Norm of the params: 15.478927
     Influence (LOO): fixed 393 labels. Loss 0.19907. Accuracy 0.983.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20704883
Train loss (w/o reg) on all data: 0.18666674
Test loss (w/o reg) on all data: 0.1753892
Train acc on all data:  0.9139314369073669
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 3.9527033e-05
Norm of the params: 20.190142
                Loss: fixed 604 labels. Loss 0.17539. Accuracy 0.942.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4614031
Train loss (w/o reg) on all data: 0.45594594
Test loss (w/o reg) on all data: 0.27993882
Train acc on all data:  0.7892049598832969
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.47802975e-05
Norm of the params: 10.447169
              Random: fixed 148 labels. Loss 0.27994. Accuracy 0.964.
### Flips: 1230, rs: 2, checks: 820
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29739362
Train loss (w/o reg) on all data: 0.28529516
Test loss (w/o reg) on all data: 0.1685206
Train acc on all data:  0.8701677607585704
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.0997604e-05
Norm of the params: 15.555352
     Influence (LOO): fixed 494 labels. Loss 0.16852. Accuracy 0.980.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12011933
Train loss (w/o reg) on all data: 0.09779823
Test loss (w/o reg) on all data: 0.117824934
Train acc on all data:  0.9564794553853635
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 9.106133e-06
Norm of the params: 21.128702
                Loss: fixed 791 labels. Loss 0.11782. Accuracy 0.962.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44690245
Train loss (w/o reg) on all data: 0.44121215
Test loss (w/o reg) on all data: 0.26346454
Train acc on all data:  0.8023340627279358
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 6.34985e-05
Norm of the params: 10.668006
              Random: fixed 196 labels. Loss 0.26346. Accuracy 0.967.
### Flips: 1230, rs: 2, checks: 1025
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2605065
Train loss (w/o reg) on all data: 0.24838355
Test loss (w/o reg) on all data: 0.14976016
Train acc on all data:  0.886943836615609
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 8.2290935e-06
Norm of the params: 15.571102
     Influence (LOO): fixed 578 labels. Loss 0.14976. Accuracy 0.979.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06347131
Train loss (w/o reg) on all data: 0.044633325
Test loss (w/o reg) on all data: 0.07004807
Train acc on all data:  0.9837101872112813
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.5099243e-06
Norm of the params: 19.4103
                Loss: fixed 923 labels. Loss 0.07005. Accuracy 0.982.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4318539
Train loss (w/o reg) on all data: 0.42581826
Test loss (w/o reg) on all data: 0.24899396
Train acc on all data:  0.8144906394359348
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.095869e-05
Norm of the params: 10.986924
              Random: fixed 245 labels. Loss 0.24899. Accuracy 0.974.
### Flips: 1230, rs: 2, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22715054
Train loss (w/o reg) on all data: 0.21530886
Test loss (w/o reg) on all data: 0.1231506
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.6766979e-05
Norm of the params: 15.3894005
     Influence (LOO): fixed 655 labels. Loss 0.12315. Accuracy 0.990.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0399279
Train loss (w/o reg) on all data: 0.025421716
Test loss (w/o reg) on all data: 0.047069043
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.0357882e-06
Norm of the params: 17.033018
                Loss: fixed 974 labels. Loss 0.04707. Accuracy 0.985.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41889924
Train loss (w/o reg) on all data: 0.41278723
Test loss (w/o reg) on all data: 0.2352116
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.8455244e-05
Norm of the params: 11.056234
              Random: fixed 293 labels. Loss 0.23521. Accuracy 0.970.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49898657
Train loss (w/o reg) on all data: 0.49404323
Test loss (w/o reg) on all data: 0.34989694
Train acc on all data:  0.74981765134938
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 2.886375e-05
Norm of the params: 9.9431715
Flipped loss: 0.34990. Accuracy: 0.917
### Flips: 1230, rs: 3, checks: 205
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44210535
Train loss (w/o reg) on all data: 0.4330348
Test loss (w/o reg) on all data: 0.29781467
Train acc on all data:  0.788475565280817
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 3.2013657e-05
Norm of the params: 13.468889
     Influence (LOO): fixed 151 labels. Loss 0.29781. Accuracy 0.935.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4026018
Train loss (w/o reg) on all data: 0.39202943
Test loss (w/o reg) on all data: 0.3113403
Train acc on all data:  0.8016046681254558
Test acc on all data:   0.8785228377065112
Norm of the mean of gradients: 2.2785367e-05
Norm of the params: 14.541236
                Loss: fixed 203 labels. Loss 0.31134. Accuracy 0.879.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48544213
Train loss (w/o reg) on all data: 0.48033953
Test loss (w/o reg) on all data: 0.32751596
Train acc on all data:  0.7612448334548991
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 2.1710004e-05
Norm of the params: 10.102093
              Random: fixed  60 labels. Loss 0.32752. Accuracy 0.931.
### Flips: 1230, rs: 3, checks: 410
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3939327
Train loss (w/o reg) on all data: 0.38325238
Test loss (w/o reg) on all data: 0.25257754
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 4.6211397e-05
Norm of the params: 14.615272
     Influence (LOO): fixed 280 labels. Loss 0.25258. Accuracy 0.956.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31373823
Train loss (w/o reg) on all data: 0.29901123
Test loss (w/o reg) on all data: 0.26939067
Train acc on all data:  0.8524191587648918
Test acc on all data:   0.892128279883382
Norm of the mean of gradients: 3.6580386e-05
Norm of the params: 17.16217
                Loss: fixed 403 labels. Loss 0.26939. Accuracy 0.892.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47541994
Train loss (w/o reg) on all data: 0.47025692
Test loss (w/o reg) on all data: 0.30743644
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.6280235e-05
Norm of the params: 10.1616955
              Random: fixed 110 labels. Loss 0.30744. Accuracy 0.937.
### Flips: 1230, rs: 3, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3548315
Train loss (w/o reg) on all data: 0.34366983
Test loss (w/o reg) on all data: 0.22133343
Train acc on all data:  0.8392900559202529
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.5667023e-05
Norm of the params: 14.9409895
     Influence (LOO): fixed 384 labels. Loss 0.22133. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22414647
Train loss (w/o reg) on all data: 0.20502988
Test loss (w/o reg) on all data: 0.20492186
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 5.1121415e-06
Norm of the params: 19.553307
                Loss: fixed 602 labels. Loss 0.20492. Accuracy 0.916.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4635362
Train loss (w/o reg) on all data: 0.4584117
Test loss (w/o reg) on all data: 0.2914731
Train acc on all data:  0.7845854607342573
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.6008877e-05
Norm of the params: 10.123747
              Random: fixed 160 labels. Loss 0.29147. Accuracy 0.944.
### Flips: 1230, rs: 3, checks: 820
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31182474
Train loss (w/o reg) on all data: 0.30065873
Test loss (w/o reg) on all data: 0.19085889
Train acc on all data:  0.8633600778020909
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 1.187879e-05
Norm of the params: 14.943899
     Influence (LOO): fixed 488 labels. Loss 0.19086. Accuracy 0.968.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13784537
Train loss (w/o reg) on all data: 0.116832174
Test loss (w/o reg) on all data: 0.14824164
Train acc on all data:  0.950158035497204
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 9.03574e-06
Norm of the params: 20.50034
                Loss: fixed 787 labels. Loss 0.14824. Accuracy 0.948.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44895592
Train loss (w/o reg) on all data: 0.44385642
Test loss (w/o reg) on all data: 0.26526845
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 4.0890045e-05
Norm of the params: 10.099007
              Random: fixed 221 labels. Loss 0.26527. Accuracy 0.966.
### Flips: 1230, rs: 3, checks: 1025
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272144
Train loss (w/o reg) on all data: 0.26014608
Test loss (w/o reg) on all data: 0.157212
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 8.646234e-06
Norm of the params: 15.490588
     Influence (LOO): fixed 580 labels. Loss 0.15721. Accuracy 0.980.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07327852
Train loss (w/o reg) on all data: 0.055541266
Test loss (w/o reg) on all data: 0.066854134
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 7.116932e-06
Norm of the params: 18.834679
                Loss: fixed 934 labels. Loss 0.06685. Accuracy 0.976.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43238035
Train loss (w/o reg) on all data: 0.42706174
Test loss (w/o reg) on all data: 0.24503241
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7315138e-05
Norm of the params: 10.313685
              Random: fixed 280 labels. Loss 0.24503. Accuracy 0.974.
### Flips: 1230, rs: 3, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2253887
Train loss (w/o reg) on all data: 0.21369214
Test loss (w/o reg) on all data: 0.123370536
Train acc on all data:  0.9061512278142475
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 7.3311658e-06
Norm of the params: 15.2948065
     Influence (LOO): fixed 673 labels. Loss 0.12337. Accuracy 0.985.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046980105
Train loss (w/o reg) on all data: 0.032652866
Test loss (w/o reg) on all data: 0.03570935
Train acc on all data:  0.987357160223681
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 1.5045415e-06
Norm of the params: 16.927635
                Loss: fixed 991 labels. Loss 0.03571. Accuracy 0.989.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4126532
Train loss (w/o reg) on all data: 0.40736583
Test loss (w/o reg) on all data: 0.21935356
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.7282525e-05
Norm of the params: 10.283363
              Random: fixed 344 labels. Loss 0.21935. Accuracy 0.982.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49691716
Train loss (w/o reg) on all data: 0.4922665
Test loss (w/o reg) on all data: 0.32494703
Train acc on all data:  0.7554096766350595
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.2768391e-05
Norm of the params: 9.6443205
Flipped loss: 0.32495. Accuracy: 0.956
### Flips: 1230, rs: 4, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43175635
Train loss (w/o reg) on all data: 0.42213696
Test loss (w/o reg) on all data: 0.2821728
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.2657581e-05
Norm of the params: 13.870381
     Influence (LOO): fixed 161 labels. Loss 0.28217. Accuracy 0.948.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39268684
Train loss (w/o reg) on all data: 0.38176066
Test loss (w/o reg) on all data: 0.27616665
Train acc on all data:  0.8093848772185752
Test acc on all data:   0.9076773566569485
Norm of the mean of gradients: 3.8343438e-05
Norm of the params: 14.782555
                Loss: fixed 205 labels. Loss 0.27617. Accuracy 0.908.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48789695
Train loss (w/o reg) on all data: 0.48328227
Test loss (w/o reg) on all data: 0.30985248
Train acc on all data:  0.7631898857281789
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.2408167e-05
Norm of the params: 9.6069565
              Random: fixed  43 labels. Loss 0.30985. Accuracy 0.960.
### Flips: 1230, rs: 4, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38564807
Train loss (w/o reg) on all data: 0.37414047
Test loss (w/o reg) on all data: 0.2382127
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.9480858e-05
Norm of the params: 15.170756
     Influence (LOO): fixed 283 labels. Loss 0.23821. Accuracy 0.959.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30205908
Train loss (w/o reg) on all data: 0.28653955
Test loss (w/o reg) on all data: 0.22513366
Train acc on all data:  0.862144420131291
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 9.275141e-06
Norm of the params: 17.617903
                Loss: fixed 406 labels. Loss 0.22513. Accuracy 0.916.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47356695
Train loss (w/o reg) on all data: 0.46857268
Test loss (w/o reg) on all data: 0.29114777
Train acc on all data:  0.7768052516411379
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 4.3482607e-05
Norm of the params: 9.994258
              Random: fixed  98 labels. Loss 0.29115. Accuracy 0.963.
### Flips: 1230, rs: 4, checks: 615
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3410771
Train loss (w/o reg) on all data: 0.32865664
Test loss (w/o reg) on all data: 0.2042223
Train acc on all data:  0.8463408704108923
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.1740508e-05
Norm of the params: 15.761001
     Influence (LOO): fixed 396 labels. Loss 0.20422. Accuracy 0.966.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21277374
Train loss (w/o reg) on all data: 0.19392456
Test loss (w/o reg) on all data: 0.16384469
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 6.7796595e-06
Norm of the params: 19.416069
                Loss: fixed 602 labels. Loss 0.16384. Accuracy 0.947.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.458096
Train loss (w/o reg) on all data: 0.4532003
Test loss (w/o reg) on all data: 0.271884
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7560838e-05
Norm of the params: 9.895137
              Random: fixed 157 labels. Loss 0.27188. Accuracy 0.964.
### Flips: 1230, rs: 4, checks: 820
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29747024
Train loss (w/o reg) on all data: 0.28469944
Test loss (w/o reg) on all data: 0.1714913
Train acc on all data:  0.8667639192803307
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.0185637e-05
Norm of the params: 15.981747
     Influence (LOO): fixed 497 labels. Loss 0.17149. Accuracy 0.972.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12961824
Train loss (w/o reg) on all data: 0.11093468
Test loss (w/o reg) on all data: 0.092698805
Train acc on all data:  0.9523462193046438
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 9.439616e-06
Norm of the params: 19.330574
                Loss: fixed 787 labels. Loss 0.09270. Accuracy 0.974.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44708622
Train loss (w/o reg) on all data: 0.44225195
Test loss (w/o reg) on all data: 0.25673726
Train acc on all data:  0.799416484318016
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 4.033853e-05
Norm of the params: 9.832867
              Random: fixed 202 labels. Loss 0.25674. Accuracy 0.973.
### Flips: 1230, rs: 4, checks: 1025
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26159567
Train loss (w/o reg) on all data: 0.24889426
Test loss (w/o reg) on all data: 0.14922182
Train acc on all data:  0.887186968149769
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.6089752e-06
Norm of the params: 15.938262
     Influence (LOO): fixed 578 labels. Loss 0.14922. Accuracy 0.976.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0590714
Train loss (w/o reg) on all data: 0.043158565
Test loss (w/o reg) on all data: 0.04835117
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1156254e-06
Norm of the params: 17.839752
                Loss: fixed 937 labels. Loss 0.04835. Accuracy 0.986.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43133432
Train loss (w/o reg) on all data: 0.42631605
Test loss (w/o reg) on all data: 0.23661388
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.497043e-06
Norm of the params: 10.018243
              Random: fixed 262 labels. Loss 0.23661. Accuracy 0.976.
### Flips: 1230, rs: 4, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22939712
Train loss (w/o reg) on all data: 0.21686433
Test loss (w/o reg) on all data: 0.12514192
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8910601e-05
Norm of the params: 15.832112
     Influence (LOO): fixed 653 labels. Loss 0.12514. Accuracy 0.983.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032520905
Train loss (w/o reg) on all data: 0.020397548
Test loss (w/o reg) on all data: 0.023709312
Train acc on all data:  0.9929491855093605
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 6.6811003e-07
Norm of the params: 15.571359
                Loss: fixed 992 labels. Loss 0.02371. Accuracy 0.993.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41300586
Train loss (w/o reg) on all data: 0.4075395
Test loss (w/o reg) on all data: 0.22062804
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 3.0575793e-05
Norm of the params: 10.455982
              Random: fixed 314 labels. Loss 0.22063. Accuracy 0.978.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49810806
Train loss (w/o reg) on all data: 0.49299547
Test loss (w/o reg) on all data: 0.3313857
Train acc on all data:  0.7532214928276197
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.0187234e-05
Norm of the params: 10.111972
Flipped loss: 0.33139. Accuracy: 0.943
### Flips: 1230, rs: 5, checks: 205
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43010002
Train loss (w/o reg) on all data: 0.42106888
Test loss (w/o reg) on all data: 0.274487
Train acc on all data:  0.7957695113056164
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 3.101192e-05
Norm of the params: 13.439609
     Influence (LOO): fixed 168 labels. Loss 0.27449. Accuracy 0.953.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39837393
Train loss (w/o reg) on all data: 0.38627142
Test loss (w/o reg) on all data: 0.28630963
Train acc on all data:  0.8086554826160953
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 8.182315e-06
Norm of the params: 15.557964
                Loss: fixed 203 labels. Loss 0.28631. Accuracy 0.915.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48875567
Train loss (w/o reg) on all data: 0.48348236
Test loss (w/o reg) on all data: 0.31625926
Train acc on all data:  0.7639192803306589
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 8.113506e-05
Norm of the params: 10.269673
              Random: fixed  47 labels. Loss 0.31626. Accuracy 0.946.
### Flips: 1230, rs: 5, checks: 410
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3837493
Train loss (w/o reg) on all data: 0.37292823
Test loss (w/o reg) on all data: 0.24144423
Train acc on all data:  0.8210551908582543
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.0641758e-05
Norm of the params: 14.711275
     Influence (LOO): fixed 279 labels. Loss 0.24144. Accuracy 0.950.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31080782
Train loss (w/o reg) on all data: 0.29347855
Test loss (w/o reg) on all data: 0.2294739
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9290573372206026
Norm of the mean of gradients: 3.768135e-05
Norm of the params: 18.616802
                Loss: fixed 403 labels. Loss 0.22947. Accuracy 0.929.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47606447
Train loss (w/o reg) on all data: 0.4709184
Test loss (w/o reg) on all data: 0.29645434
Train acc on all data:  0.7741308047653781
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.504733e-05
Norm of the params: 10.145045
              Random: fixed 108 labels. Loss 0.29645. Accuracy 0.958.
### Flips: 1230, rs: 5, checks: 615
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33244994
Train loss (w/o reg) on all data: 0.32043618
Test loss (w/o reg) on all data: 0.20149969
Train acc on all data:  0.849501580354972
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.4357285e-05
Norm of the params: 15.500818
     Influence (LOO): fixed 400 labels. Loss 0.20150. Accuracy 0.963.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22162853
Train loss (w/o reg) on all data: 0.19953044
Test loss (w/o reg) on all data: 0.17725065
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.3064807e-05
Norm of the params: 21.022892
                Loss: fixed 605 labels. Loss 0.17725. Accuracy 0.948.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45960847
Train loss (w/o reg) on all data: 0.45408833
Test loss (w/o reg) on all data: 0.27484667
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.1702734e-05
Norm of the params: 10.507268
              Random: fixed 176 labels. Loss 0.27485. Accuracy 0.964.
### Flips: 1230, rs: 5, checks: 820
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29397088
Train loss (w/o reg) on all data: 0.2815757
Test loss (w/o reg) on all data: 0.17248337
Train acc on all data:  0.8691952346219305
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.3210584e-05
Norm of the params: 15.744954
     Influence (LOO): fixed 491 labels. Loss 0.17248. Accuracy 0.970.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1311624
Train loss (w/o reg) on all data: 0.10748605
Test loss (w/o reg) on all data: 0.11888866
Train acc on all data:  0.9530756139071238
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 8.316332e-06
Norm of the params: 21.760681
                Loss: fixed 799 labels. Loss 0.11889. Accuracy 0.969.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44301862
Train loss (w/o reg) on all data: 0.43734074
Test loss (w/o reg) on all data: 0.2550728
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.7556e-05
Norm of the params: 10.656327
              Random: fixed 231 labels. Loss 0.25507. Accuracy 0.967.
### Flips: 1230, rs: 5, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26376456
Train loss (w/o reg) on all data: 0.25148386
Test loss (w/o reg) on all data: 0.14617418
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.703166e-06
Norm of the params: 15.672085
     Influence (LOO): fixed 564 labels. Loss 0.14617. Accuracy 0.981.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065283425
Train loss (w/o reg) on all data: 0.0477299
Test loss (w/o reg) on all data: 0.04961691
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.8446914e-06
Norm of the params: 18.736876
                Loss: fixed 950 labels. Loss 0.04962. Accuracy 0.985.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43089274
Train loss (w/o reg) on all data: 0.42483413
Test loss (w/o reg) on all data: 0.2434319
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.717428e-05
Norm of the params: 11.007812
              Random: fixed 273 labels. Loss 0.24343. Accuracy 0.972.
### Flips: 1230, rs: 5, checks: 1230
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22332136
Train loss (w/o reg) on all data: 0.21065043
Test loss (w/o reg) on all data: 0.12048056
Train acc on all data:  0.9025042548018478
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3782687e-05
Norm of the params: 15.919133
     Influence (LOO): fixed 652 labels. Loss 0.12048. Accuracy 0.988.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038561936
Train loss (w/o reg) on all data: 0.024706744
Test loss (w/o reg) on all data: 0.027134003
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.431029e-06
Norm of the params: 16.646437
                Loss: fixed 1002 labels. Loss 0.02713. Accuracy 0.991.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41416195
Train loss (w/o reg) on all data: 0.40789452
Test loss (w/o reg) on all data: 0.22832045
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.1047566e-05
Norm of the params: 11.195929
              Random: fixed 327 labels. Loss 0.22832. Accuracy 0.972.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49288565
Train loss (w/o reg) on all data: 0.48815596
Test loss (w/o reg) on all data: 0.33221036
Train acc on all data:  0.7522489666909798
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 2.4841045e-05
Norm of the params: 9.725931
Flipped loss: 0.33221. Accuracy: 0.934
### Flips: 1230, rs: 6, checks: 205
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42873523
Train loss (w/o reg) on all data: 0.4195449
Test loss (w/o reg) on all data: 0.27882782
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.5518895e-05
Norm of the params: 13.557528
     Influence (LOO): fixed 158 labels. Loss 0.27883. Accuracy 0.943.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38980088
Train loss (w/o reg) on all data: 0.37939587
Test loss (w/o reg) on all data: 0.2872891
Train acc on all data:  0.8067104303428154
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 2.7260656e-05
Norm of the params: 14.425668
                Loss: fixed 204 labels. Loss 0.28729. Accuracy 0.912.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48199147
Train loss (w/o reg) on all data: 0.47699583
Test loss (w/o reg) on all data: 0.31316495
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 2.423148e-05
Norm of the params: 9.99565
              Random: fixed  51 labels. Loss 0.31316. Accuracy 0.941.
### Flips: 1230, rs: 6, checks: 410
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38456473
Train loss (w/o reg) on all data: 0.3739383
Test loss (w/o reg) on all data: 0.2448046
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.3431081e-05
Norm of the params: 14.578362
     Influence (LOO): fixed 270 labels. Loss 0.24480. Accuracy 0.943.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2978216
Train loss (w/o reg) on all data: 0.28239903
Test loss (w/o reg) on all data: 0.23724033
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 6.250476e-05
Norm of the params: 17.562792
                Loss: fixed 405 labels. Loss 0.23724. Accuracy 0.924.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46894908
Train loss (w/o reg) on all data: 0.46376574
Test loss (w/o reg) on all data: 0.2962672
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 9.2463124e-05
Norm of the params: 10.181696
              Random: fixed 105 labels. Loss 0.29627. Accuracy 0.947.
### Flips: 1230, rs: 6, checks: 615
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3384028
Train loss (w/o reg) on all data: 0.32764438
Test loss (w/o reg) on all data: 0.20916294
Train acc on all data:  0.8419645027960126
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 9.413578e-06
Norm of the params: 14.668634
     Influence (LOO): fixed 390 labels. Loss 0.20916. Accuracy 0.960.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21011594
Train loss (w/o reg) on all data: 0.19100049
Test loss (w/o reg) on all data: 0.17254034
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 4.1485528e-06
Norm of the params: 19.552725
                Loss: fixed 601 labels. Loss 0.17254. Accuracy 0.947.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45514444
Train loss (w/o reg) on all data: 0.44972998
Test loss (w/o reg) on all data: 0.2771373
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 9.767141e-06
Norm of the params: 10.406204
              Random: fixed 160 labels. Loss 0.27714. Accuracy 0.951.
### Flips: 1230, rs: 6, checks: 820
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30344203
Train loss (w/o reg) on all data: 0.29252854
Test loss (w/o reg) on all data: 0.1794703
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.9085337e-05
Norm of the params: 14.7739525
     Influence (LOO): fixed 474 labels. Loss 0.17947. Accuracy 0.965.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12263348
Train loss (w/o reg) on all data: 0.100613005
Test loss (w/o reg) on all data: 0.10823623
Train acc on all data:  0.9569657184536834
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.8543044e-06
Norm of the params: 20.985935
                Loss: fixed 787 labels. Loss 0.10824. Accuracy 0.961.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43990552
Train loss (w/o reg) on all data: 0.43435103
Test loss (w/o reg) on all data: 0.2585082
Train acc on all data:  0.8013615365912959
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.6571327e-05
Norm of the params: 10.539914
              Random: fixed 222 labels. Loss 0.25851. Accuracy 0.958.
### Flips: 1230, rs: 6, checks: 1025
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2650608
Train loss (w/o reg) on all data: 0.25430492
Test loss (w/o reg) on all data: 0.15295485
Train acc on all data:  0.8815949428640895
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.204087e-05
Norm of the params: 14.6669035
     Influence (LOO): fixed 567 labels. Loss 0.15295. Accuracy 0.975.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07086964
Train loss (w/o reg) on all data: 0.05311261
Test loss (w/o reg) on all data: 0.065175615
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5838755e-06
Norm of the params: 18.845175
                Loss: fixed 916 labels. Loss 0.06518. Accuracy 0.976.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42448595
Train loss (w/o reg) on all data: 0.4188525
Test loss (w/o reg) on all data: 0.24423642
Train acc on all data:  0.8135181132992949
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.9043886e-05
Norm of the params: 10.614555
              Random: fixed 274 labels. Loss 0.24424. Accuracy 0.962.
### Flips: 1230, rs: 6, checks: 1230
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22731955
Train loss (w/o reg) on all data: 0.21571942
Test loss (w/o reg) on all data: 0.12760249
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.514496e-06
Norm of the params: 15.231637
     Influence (LOO): fixed 650 labels. Loss 0.12760. Accuracy 0.983.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03940978
Train loss (w/o reg) on all data: 0.025720067
Test loss (w/o reg) on all data: 0.031220391
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.1947877e-06
Norm of the params: 16.54673
                Loss: fixed 986 labels. Loss 0.03122. Accuracy 0.990.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41174054
Train loss (w/o reg) on all data: 0.40604672
Test loss (w/o reg) on all data: 0.22904104
Train acc on all data:  0.8244590323364941
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.3305744e-05
Norm of the params: 10.671298
              Random: fixed 319 labels. Loss 0.22904. Accuracy 0.969.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.50041455
Train loss (w/o reg) on all data: 0.49554905
Test loss (w/o reg) on all data: 0.32859537
Train acc on all data:  0.7517627036226598
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 2.760683e-05
Norm of the params: 9.864599
Flipped loss: 0.32860. Accuracy: 0.952
### Flips: 1230, rs: 7, checks: 205
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4380524
Train loss (w/o reg) on all data: 0.42962533
Test loss (w/o reg) on all data: 0.2672169
Train acc on all data:  0.7860442499392171
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 8.816078e-06
Norm of the params: 12.982332
     Influence (LOO): fixed 163 labels. Loss 0.26722. Accuracy 0.965.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4012851
Train loss (w/o reg) on all data: 0.39041448
Test loss (w/o reg) on all data: 0.28786767
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 9.850248e-06
Norm of the params: 14.744917
                Loss: fixed 203 labels. Loss 0.28787. Accuracy 0.895.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48825938
Train loss (w/o reg) on all data: 0.48337024
Test loss (w/o reg) on all data: 0.30629197
Train acc on all data:  0.7646486749331388
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.6176642e-05
Norm of the params: 9.888505
              Random: fixed  60 labels. Loss 0.30629. Accuracy 0.959.
### Flips: 1230, rs: 7, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.391042
Train loss (w/o reg) on all data: 0.38093323
Test loss (w/o reg) on all data: 0.22977069
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.872984e-06
Norm of the params: 14.218837
     Influence (LOO): fixed 286 labels. Loss 0.22977. Accuracy 0.980.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30803216
Train loss (w/o reg) on all data: 0.29256323
Test loss (w/o reg) on all data: 0.22831592
Train acc on all data:  0.8606856309263311
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 9.448879e-06
Norm of the params: 17.589167
                Loss: fixed 407 labels. Loss 0.22832. Accuracy 0.928.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47731957
Train loss (w/o reg) on all data: 0.47240934
Test loss (w/o reg) on all data: 0.29090017
Train acc on all data:  0.7765621201069779
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.0829406e-05
Norm of the params: 9.909815
              Random: fixed 109 labels. Loss 0.29090. Accuracy 0.965.
### Flips: 1230, rs: 7, checks: 615
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33921325
Train loss (w/o reg) on all data: 0.32753745
Test loss (w/o reg) on all data: 0.19449471
Train acc on all data:  0.8434232920009725
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4253306e-05
Norm of the params: 15.281234
     Influence (LOO): fixed 410 labels. Loss 0.19449. Accuracy 0.982.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21328646
Train loss (w/o reg) on all data: 0.19379328
Test loss (w/o reg) on all data: 0.16251428
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.6889124e-05
Norm of the params: 19.744965
                Loss: fixed 611 labels. Loss 0.16251. Accuracy 0.947.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46215665
Train loss (w/o reg) on all data: 0.4568612
Test loss (w/o reg) on all data: 0.27546474
Train acc on all data:  0.787503039144177
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 7.8964564e-05
Norm of the params: 10.291209
              Random: fixed 167 labels. Loss 0.27546. Accuracy 0.969.
### Flips: 1230, rs: 7, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2991797
Train loss (w/o reg) on all data: 0.2869325
Test loss (w/o reg) on all data: 0.16981016
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.8777702e-05
Norm of the params: 15.650698
     Influence (LOO): fixed 499 labels. Loss 0.16981. Accuracy 0.985.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12413197
Train loss (w/o reg) on all data: 0.10302196
Test loss (w/o reg) on all data: 0.10535765
Train acc on all data:  0.9547775346462436
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 9.531461e-06
Norm of the params: 20.547514
                Loss: fixed 806 labels. Loss 0.10536. Accuracy 0.957.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44698054
Train loss (w/o reg) on all data: 0.44189718
Test loss (w/o reg) on all data: 0.25359532
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.7913837e-05
Norm of the params: 10.082995
              Random: fixed 223 labels. Loss 0.25360. Accuracy 0.979.
### Flips: 1230, rs: 7, checks: 1025
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26296505
Train loss (w/o reg) on all data: 0.24994925
Test loss (w/o reg) on all data: 0.14625958
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.472633e-06
Norm of the params: 16.134315
     Influence (LOO): fixed 577 labels. Loss 0.14626. Accuracy 0.990.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06257322
Train loss (w/o reg) on all data: 0.04571575
Test loss (w/o reg) on all data: 0.044849224
Train acc on all data:  0.9822513980063214
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.8485292e-06
Norm of the params: 18.361626
                Loss: fixed 948 labels. Loss 0.04485. Accuracy 0.986.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4308184
Train loss (w/o reg) on all data: 0.42548406
Test loss (w/o reg) on all data: 0.23252709
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.237838e-05
Norm of the params: 10.328933
              Random: fixed 280 labels. Loss 0.23253. Accuracy 0.985.
### Flips: 1230, rs: 7, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23056945
Train loss (w/o reg) on all data: 0.21794677
Test loss (w/o reg) on all data: 0.12544723
Train acc on all data:  0.899586676391928
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 5.54568e-06
Norm of the params: 15.888794
     Influence (LOO): fixed 651 labels. Loss 0.12545. Accuracy 0.991.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030177912
Train loss (w/o reg) on all data: 0.018450066
Test loss (w/o reg) on all data: 0.016634619
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 7.0707506e-07
Norm of the params: 15.315251
                Loss: fixed 1017 labels. Loss 0.01663. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41650563
Train loss (w/o reg) on all data: 0.4112925
Test loss (w/o reg) on all data: 0.21790357
Train acc on all data:  0.8242159008023341
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.1761949e-05
Norm of the params: 10.210914
              Random: fixed 330 labels. Loss 0.21790. Accuracy 0.986.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4976432
Train loss (w/o reg) on all data: 0.49258703
Test loss (w/o reg) on all data: 0.3327962
Train acc on all data:  0.7571115973741794
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 5.159131e-05
Norm of the params: 10.0560055
Flipped loss: 0.33280. Accuracy: 0.949
### Flips: 1230, rs: 8, checks: 205
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4351969
Train loss (w/o reg) on all data: 0.42557383
Test loss (w/o reg) on all data: 0.28197667
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.0375712e-05
Norm of the params: 13.873053
     Influence (LOO): fixed 154 labels. Loss 0.28198. Accuracy 0.949.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4007701
Train loss (w/o reg) on all data: 0.38958555
Test loss (w/o reg) on all data: 0.28482428
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 2.8644321e-05
Norm of the params: 14.956294
                Loss: fixed 203 labels. Loss 0.28482. Accuracy 0.917.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4829908
Train loss (w/o reg) on all data: 0.47780567
Test loss (w/o reg) on all data: 0.31351057
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 4.6443947e-05
Norm of the params: 10.183443
              Random: fixed  63 labels. Loss 0.31351. Accuracy 0.955.
### Flips: 1230, rs: 8, checks: 410
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38832507
Train loss (w/o reg) on all data: 0.37739724
Test loss (w/o reg) on all data: 0.23063128
Train acc on all data:  0.8227571115973742
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.9021236e-05
Norm of the params: 14.78366
     Influence (LOO): fixed 286 labels. Loss 0.23063. Accuracy 0.968.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31036207
Train loss (w/o reg) on all data: 0.29470044
Test loss (w/o reg) on all data: 0.22447346
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.6617736e-05
Norm of the params: 17.69837
                Loss: fixed 406 labels. Loss 0.22447. Accuracy 0.931.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46983716
Train loss (w/o reg) on all data: 0.46466526
Test loss (w/o reg) on all data: 0.29375148
Train acc on all data:  0.7804522246535376
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.4203713e-05
Norm of the params: 10.170443
              Random: fixed 117 labels. Loss 0.29375. Accuracy 0.963.
### Flips: 1230, rs: 8, checks: 615
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34377426
Train loss (w/o reg) on all data: 0.33233467
Test loss (w/o reg) on all data: 0.19338782
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.7874463e-05
Norm of the params: 15.125857
     Influence (LOO): fixed 401 labels. Loss 0.19339. Accuracy 0.981.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21504779
Train loss (w/o reg) on all data: 0.19589603
Test loss (w/o reg) on all data: 0.1604844
Train acc on all data:  0.9144176999756869
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.1194117e-05
Norm of the params: 19.571283
                Loss: fixed 607 labels. Loss 0.16048. Accuracy 0.941.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4556443
Train loss (w/o reg) on all data: 0.44994348
Test loss (w/o reg) on all data: 0.27933812
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.709349e-05
Norm of the params: 10.677845
              Random: fixed 168 labels. Loss 0.27934. Accuracy 0.971.
### Flips: 1230, rs: 8, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3052305
Train loss (w/o reg) on all data: 0.2943946
Test loss (w/o reg) on all data: 0.16489951
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.2919876e-05
Norm of the params: 14.721331
     Influence (LOO): fixed 491 labels. Loss 0.16490. Accuracy 0.986.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12028439
Train loss (w/o reg) on all data: 0.09910402
Test loss (w/o reg) on all data: 0.10665513
Train acc on all data:  0.9589107707269633
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.180131e-06
Norm of the params: 20.581728
                Loss: fixed 800 labels. Loss 0.10666. Accuracy 0.963.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4402273
Train loss (w/o reg) on all data: 0.4344277
Test loss (w/o reg) on all data: 0.2602445
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.915088e-05
Norm of the params: 10.769943
              Random: fixed 224 labels. Loss 0.26024. Accuracy 0.972.
### Flips: 1230, rs: 8, checks: 1025
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26816753
Train loss (w/o reg) on all data: 0.25774947
Test loss (w/o reg) on all data: 0.13979341
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 2.2046857e-05
Norm of the params: 14.434727
     Influence (LOO): fixed 572 labels. Loss 0.13979. Accuracy 0.994.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057270136
Train loss (w/o reg) on all data: 0.04047893
Test loss (w/o reg) on all data: 0.045867372
Train acc on all data:  0.986870897155361
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.4010152e-06
Norm of the params: 18.325504
                Loss: fixed 946 labels. Loss 0.04587. Accuracy 0.987.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42597777
Train loss (w/o reg) on all data: 0.4198457
Test loss (w/o reg) on all data: 0.24461271
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 2.5486568e-05
Norm of the params: 11.074365
              Random: fixed 271 labels. Loss 0.24461. Accuracy 0.970.
### Flips: 1230, rs: 8, checks: 1230
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22883949
Train loss (w/o reg) on all data: 0.21796162
Test loss (w/o reg) on all data: 0.11509671
Train acc on all data:  0.9059080962800875
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 5.7727257e-06
Norm of the params: 14.749818
     Influence (LOO): fixed 652 labels. Loss 0.11510. Accuracy 0.994.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03129732
Train loss (w/o reg) on all data: 0.018881591
Test loss (w/o reg) on all data: 0.025590464
Train acc on all data:  0.9951373693168004
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.9006316e-06
Norm of the params: 15.757998
                Loss: fixed 998 labels. Loss 0.02559. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40827578
Train loss (w/o reg) on all data: 0.40203387
Test loss (w/o reg) on all data: 0.23085053
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.7326154e-05
Norm of the params: 11.173104
              Random: fixed 320 labels. Loss 0.23085. Accuracy 0.969.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49769023
Train loss (w/o reg) on all data: 0.49316365
Test loss (w/o reg) on all data: 0.33540922
Train acc on all data:  0.7520058351568198
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 5.9296166e-05
Norm of the params: 9.514809
Flipped loss: 0.33541. Accuracy: 0.937
### Flips: 1230, rs: 9, checks: 205
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43796453
Train loss (w/o reg) on all data: 0.42955416
Test loss (w/o reg) on all data: 0.28009093
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.8309802e-05
Norm of the params: 12.969466
     Influence (LOO): fixed 156 labels. Loss 0.28009. Accuracy 0.959.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39867678
Train loss (w/o reg) on all data: 0.3877086
Test loss (w/o reg) on all data: 0.286141
Train acc on all data:  0.8011184050571359
Test acc on all data:   0.9135082604470359
Norm of the mean of gradients: 2.4210123e-05
Norm of the params: 14.810921
                Loss: fixed 202 labels. Loss 0.28614. Accuracy 0.914.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4819071
Train loss (w/o reg) on all data: 0.47679168
Test loss (w/o reg) on all data: 0.31512865
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.2496535e-05
Norm of the params: 10.114772
              Random: fixed  63 labels. Loss 0.31513. Accuracy 0.955.
### Flips: 1230, rs: 9, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38598052
Train loss (w/o reg) on all data: 0.3751417
Test loss (w/o reg) on all data: 0.2397267
Train acc on all data:  0.8208120593240943
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.4008468e-05
Norm of the params: 14.723316
     Influence (LOO): fixed 292 labels. Loss 0.23973. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30539626
Train loss (w/o reg) on all data: 0.28970546
Test loss (w/o reg) on all data: 0.23625816
Train acc on all data:  0.8567955263797714
Test acc on all data:   0.9096209912536443
Norm of the mean of gradients: 1.4885491e-05
Norm of the params: 17.714846
                Loss: fixed 407 labels. Loss 0.23626. Accuracy 0.910.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4699978
Train loss (w/o reg) on all data: 0.46465352
Test loss (w/o reg) on all data: 0.2941488
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 2.5333127e-05
Norm of the params: 10.338532
              Random: fixed 116 labels. Loss 0.29415. Accuracy 0.962.
### Flips: 1230, rs: 9, checks: 615
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34095043
Train loss (w/o reg) on all data: 0.32948688
Test loss (w/o reg) on all data: 0.20485182
Train acc on all data:  0.8414782397276926
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.1751982e-05
Norm of the params: 15.141698
     Influence (LOO): fixed 401 labels. Loss 0.20485. Accuracy 0.964.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21911496
Train loss (w/o reg) on all data: 0.19940808
Test loss (w/o reg) on all data: 0.16950332
Train acc on all data:  0.9046924386092876
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 3.979107e-06
Norm of the params: 19.852894
                Loss: fixed 600 labels. Loss 0.16950. Accuracy 0.940.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45687386
Train loss (w/o reg) on all data: 0.4513119
Test loss (w/o reg) on all data: 0.27994537
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 6.23277e-05
Norm of the params: 10.547007
              Random: fixed 171 labels. Loss 0.27995. Accuracy 0.969.
### Flips: 1230, rs: 9, checks: 820
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30177954
Train loss (w/o reg) on all data: 0.28931063
Test loss (w/o reg) on all data: 0.17296723
Train acc on all data:  0.8648188670070508
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.000701e-05
Norm of the params: 15.791711
     Influence (LOO): fixed 492 labels. Loss 0.17297. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1404486
Train loss (w/o reg) on all data: 0.11951188
Test loss (w/o reg) on all data: 0.11499951
Train acc on all data:  0.9467541940189642
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.543687e-06
Norm of the params: 20.463
                Loss: fixed 786 labels. Loss 0.11500. Accuracy 0.964.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4443078
Train loss (w/o reg) on all data: 0.43903634
Test loss (w/o reg) on all data: 0.26179513
Train acc on all data:  0.7960126428397764
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.2340704e-05
Norm of the params: 10.267863
              Random: fixed 226 labels. Loss 0.26180. Accuracy 0.973.
### Flips: 1230, rs: 9, checks: 1025
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2654505
Train loss (w/o reg) on all data: 0.25270706
Test loss (w/o reg) on all data: 0.14464852
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6366675e-05
Norm of the params: 15.964608
     Influence (LOO): fixed 577 labels. Loss 0.14465. Accuracy 0.984.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074586876
Train loss (w/o reg) on all data: 0.05575164
Test loss (w/o reg) on all data: 0.058020476
Train acc on all data:  0.9771456357889619
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.3165412e-06
Norm of the params: 19.408882
                Loss: fixed 937 labels. Loss 0.05802. Accuracy 0.983.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42549902
Train loss (w/o reg) on all data: 0.42027566
Test loss (w/o reg) on all data: 0.24215019
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.6352968e-05
Norm of the params: 10.220931
              Random: fixed 290 labels. Loss 0.24215. Accuracy 0.977.
### Flips: 1230, rs: 9, checks: 1230
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22117758
Train loss (w/o reg) on all data: 0.2080562
Test loss (w/o reg) on all data: 0.11565098
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.2123363e-05
Norm of the params: 16.199615
     Influence (LOO): fixed 672 labels. Loss 0.11565. Accuracy 0.988.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042933848
Train loss (w/o reg) on all data: 0.028864626
Test loss (w/o reg) on all data: 0.03890931
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.816857e-06
Norm of the params: 16.774519
                Loss: fixed 1003 labels. Loss 0.03891. Accuracy 0.988.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41059035
Train loss (w/o reg) on all data: 0.40512365
Test loss (w/o reg) on all data: 0.2251066
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 8.9235655e-06
Norm of the params: 10.4562845
              Random: fixed 343 labels. Loss 0.22511. Accuracy 0.976.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49444184
Train loss (w/o reg) on all data: 0.48953032
Test loss (w/o reg) on all data: 0.33522192
Train acc on all data:  0.7527352297592997
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 1.2999789e-05
Norm of the params: 9.911107
Flipped loss: 0.33522. Accuracy: 0.919
### Flips: 1230, rs: 10, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43225938
Train loss (w/o reg) on all data: 0.42235634
Test loss (w/o reg) on all data: 0.28525016
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9290573372206026
Norm of the mean of gradients: 1.4040918e-05
Norm of the params: 14.07341
     Influence (LOO): fixed 155 labels. Loss 0.28525. Accuracy 0.929.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3967858
Train loss (w/o reg) on all data: 0.38623923
Test loss (w/o reg) on all data: 0.28696764
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.8999028182701652
Norm of the mean of gradients: 1.3353487e-05
Norm of the params: 14.523476
                Loss: fixed 203 labels. Loss 0.28697. Accuracy 0.900.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48441464
Train loss (w/o reg) on all data: 0.47959977
Test loss (w/o reg) on all data: 0.31915918
Train acc on all data:  0.762703622659859
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 2.0949132e-05
Norm of the params: 9.813124
              Random: fixed  51 labels. Loss 0.31916. Accuracy 0.928.
### Flips: 1230, rs: 10, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38695353
Train loss (w/o reg) on all data: 0.37574333
Test loss (w/o reg) on all data: 0.24332134
Train acc on all data:  0.8212983223924143
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.7805034e-05
Norm of the params: 14.973438
     Influence (LOO): fixed 280 labels. Loss 0.24332. Accuracy 0.951.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30360818
Train loss (w/o reg) on all data: 0.28784266
Test loss (w/o reg) on all data: 0.22941439
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 2.5049325e-05
Norm of the params: 17.75698
                Loss: fixed 406 labels. Loss 0.22941. Accuracy 0.918.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47277057
Train loss (w/o reg) on all data: 0.4675705
Test loss (w/o reg) on all data: 0.30364558
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 9.2285896e-05
Norm of the params: 10.198101
              Random: fixed  99 labels. Loss 0.30365. Accuracy 0.940.
### Flips: 1230, rs: 10, checks: 615
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3427432
Train loss (w/o reg) on all data: 0.33077857
Test loss (w/o reg) on all data: 0.20827258
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.8262464e-05
Norm of the params: 15.469076
     Influence (LOO): fixed 391 labels. Loss 0.20827. Accuracy 0.966.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21168515
Train loss (w/o reg) on all data: 0.19067477
Test loss (w/o reg) on all data: 0.17939919
Train acc on all data:  0.9102844638949672
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 7.941904e-06
Norm of the params: 20.498966
                Loss: fixed 603 labels. Loss 0.17940. Accuracy 0.934.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4590647
Train loss (w/o reg) on all data: 0.45384005
Test loss (w/o reg) on all data: 0.28256708
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.1637003e-05
Norm of the params: 10.2221775
              Random: fixed 152 labels. Loss 0.28257. Accuracy 0.950.
### Flips: 1230, rs: 10, checks: 820
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30721056
Train loss (w/o reg) on all data: 0.29515
Test loss (w/o reg) on all data: 0.17878366
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 8.182439e-06
Norm of the params: 15.530967
     Influence (LOO): fixed 482 labels. Loss 0.17878. Accuracy 0.972.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13098365
Train loss (w/o reg) on all data: 0.108941875
Test loss (w/o reg) on all data: 0.1273787
Train acc on all data:  0.9528324823729638
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.530242e-06
Norm of the params: 20.99608
                Loss: fixed 778 labels. Loss 0.12738. Accuracy 0.953.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44528693
Train loss (w/o reg) on all data: 0.44001955
Test loss (w/o reg) on all data: 0.26093516
Train acc on all data:  0.799173352783856
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.3314852e-05
Norm of the params: 10.263898
              Random: fixed 212 labels. Loss 0.26094. Accuracy 0.957.
### Flips: 1230, rs: 10, checks: 1025
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27103227
Train loss (w/o reg) on all data: 0.2591557
Test loss (w/o reg) on all data: 0.14545316
Train acc on all data:  0.8849987843423291
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.617198e-05
Norm of the params: 15.41206
     Influence (LOO): fixed 573 labels. Loss 0.14545. Accuracy 0.982.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0770452
Train loss (w/o reg) on all data: 0.058130212
Test loss (w/o reg) on all data: 0.07313235
Train acc on all data:  0.976173109652322
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 6.134038e-06
Norm of the params: 19.449928
                Loss: fixed 903 labels. Loss 0.07313. Accuracy 0.978.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4304749
Train loss (w/o reg) on all data: 0.42512745
Test loss (w/o reg) on all data: 0.2428613
Train acc on all data:  0.8106005348893751
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.3425722e-05
Norm of the params: 10.34162
              Random: fixed 262 labels. Loss 0.24286. Accuracy 0.967.
### Flips: 1230, rs: 10, checks: 1230
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23584634
Train loss (w/o reg) on all data: 0.22412845
Test loss (w/o reg) on all data: 0.12181577
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.8050425e-05
Norm of the params: 15.308743
     Influence (LOO): fixed 652 labels. Loss 0.12182. Accuracy 0.988.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046567593
Train loss (w/o reg) on all data: 0.03139157
Test loss (w/o reg) on all data: 0.039759055
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.7310593e-06
Norm of the params: 17.42184
                Loss: fixed 971 labels. Loss 0.03976. Accuracy 0.991.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41233912
Train loss (w/o reg) on all data: 0.40685794
Test loss (w/o reg) on all data: 0.2220068
Train acc on all data:  0.8239727692681741
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 2.7358188e-05
Norm of the params: 10.4701185
              Random: fixed 320 labels. Loss 0.22201. Accuracy 0.974.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49470803
Train loss (w/o reg) on all data: 0.49052903
Test loss (w/o reg) on all data: 0.34079403
Train acc on all data:  0.7597860442499392
Test acc on all data:   0.9203109815354713
Norm of the mean of gradients: 9.3913324e-05
Norm of the params: 9.142202
Flipped loss: 0.34079. Accuracy: 0.920
### Flips: 1230, rs: 11, checks: 205
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43350756
Train loss (w/o reg) on all data: 0.42485008
Test loss (w/o reg) on all data: 0.28836706
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 8.308413e-05
Norm of the params: 13.158634
     Influence (LOO): fixed 151 labels. Loss 0.28837. Accuracy 0.931.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3900232
Train loss (w/o reg) on all data: 0.37898943
Test loss (w/o reg) on all data: 0.29375905
Train acc on all data:  0.8113299294918551
Test acc on all data:   0.892128279883382
Norm of the mean of gradients: 1.3906108e-05
Norm of the params: 14.855149
                Loss: fixed 201 labels. Loss 0.29376. Accuracy 0.892.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4842782
Train loss (w/o reg) on all data: 0.48003784
Test loss (w/o reg) on all data: 0.32389927
Train acc on all data:  0.7665937272064187
Test acc on all data:   0.9280855199222546
Norm of the mean of gradients: 1.19536935e-05
Norm of the params: 9.209077
              Random: fixed  51 labels. Loss 0.32390. Accuracy 0.928.
### Flips: 1230, rs: 11, checks: 410
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3861868
Train loss (w/o reg) on all data: 0.37584713
Test loss (w/o reg) on all data: 0.24585512
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.9464387e-05
Norm of the params: 14.380317
     Influence (LOO): fixed 280 labels. Loss 0.24586. Accuracy 0.944.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2956541
Train loss (w/o reg) on all data: 0.28039968
Test loss (w/o reg) on all data: 0.24848707
Train acc on all data:  0.8636032093362509
Test acc on all data:   0.901846452866861
Norm of the mean of gradients: 1.9584088e-05
Norm of the params: 17.466764
                Loss: fixed 403 labels. Loss 0.24849. Accuracy 0.902.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47045276
Train loss (w/o reg) on all data: 0.46608037
Test loss (w/o reg) on all data: 0.30039802
Train acc on all data:  0.7804522246535376
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 4.7477526e-05
Norm of the params: 9.351352
              Random: fixed 111 labels. Loss 0.30040. Accuracy 0.940.
### Flips: 1230, rs: 11, checks: 615
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34336752
Train loss (w/o reg) on all data: 0.33235478
Test loss (w/o reg) on all data: 0.20880479
Train acc on all data:  0.8436664235351325
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.0912748e-05
Norm of the params: 14.8409815
     Influence (LOO): fixed 391 labels. Loss 0.20880. Accuracy 0.967.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20126712
Train loss (w/o reg) on all data: 0.18090378
Test loss (w/o reg) on all data: 0.19142467
Train acc on all data:  0.9156333576464868
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 1.6406284e-05
Norm of the params: 20.180851
                Loss: fixed 603 labels. Loss 0.19142. Accuracy 0.926.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45997685
Train loss (w/o reg) on all data: 0.45558843
Test loss (w/o reg) on all data: 0.28548604
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.0745103e-05
Norm of the params: 9.36847
              Random: fixed 161 labels. Loss 0.28549. Accuracy 0.948.
### Flips: 1230, rs: 11, checks: 820
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2997615
Train loss (w/o reg) on all data: 0.288271
Test loss (w/o reg) on all data: 0.17295998
Train acc on all data:  0.8665207877461707
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.0561038e-05
Norm of the params: 15.159477
     Influence (LOO): fixed 498 labels. Loss 0.17296. Accuracy 0.978.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11956409
Train loss (w/o reg) on all data: 0.097344525
Test loss (w/o reg) on all data: 0.1259792
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.182942e-06
Norm of the params: 21.080595
                Loss: fixed 784 labels. Loss 0.12598. Accuracy 0.952.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44540542
Train loss (w/o reg) on all data: 0.4409236
Test loss (w/o reg) on all data: 0.26313776
Train acc on all data:  0.8001458789204959
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.066901e-05
Norm of the params: 9.467643
              Random: fixed 220 labels. Loss 0.26314. Accuracy 0.956.
### Flips: 1230, rs: 11, checks: 1025
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25932536
Train loss (w/o reg) on all data: 0.24811776
Test loss (w/o reg) on all data: 0.14211756
Train acc on all data:  0.8879163627522489
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 9.375018e-06
Norm of the params: 14.971713
     Influence (LOO): fixed 592 labels. Loss 0.14212. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071851455
Train loss (w/o reg) on all data: 0.053145394
Test loss (w/o reg) on all data: 0.07481948
Train acc on all data:  0.9788475565280816
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 3.953597e-06
Norm of the params: 19.342213
                Loss: fixed 914 labels. Loss 0.07482. Accuracy 0.975.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42866653
Train loss (w/o reg) on all data: 0.4243217
Test loss (w/o reg) on all data: 0.23750906
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 4.3370386e-05
Norm of the params: 9.321816
              Random: fixed 288 labels. Loss 0.23751. Accuracy 0.971.
### Flips: 1230, rs: 11, checks: 1230
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21689357
Train loss (w/o reg) on all data: 0.20592289
Test loss (w/o reg) on all data: 0.11141248
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.65228e-06
Norm of the params: 14.812614
     Influence (LOO): fixed 682 labels. Loss 0.11141. Accuracy 0.993.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043372385
Train loss (w/o reg) on all data: 0.028524501
Test loss (w/o reg) on all data: 0.04390974
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3895626e-06
Norm of the params: 17.232462
                Loss: fixed 981 labels. Loss 0.04391. Accuracy 0.988.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40923727
Train loss (w/o reg) on all data: 0.40474427
Test loss (w/o reg) on all data: 0.22058383
Train acc on all data:  0.8290785314855337
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.4623772e-05
Norm of the params: 9.479443
              Random: fixed 349 labels. Loss 0.22058. Accuracy 0.973.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49389735
Train loss (w/o reg) on all data: 0.4885865
Test loss (w/o reg) on all data: 0.3389629
Train acc on all data:  0.7585703865791393
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.4792533e-05
Norm of the params: 10.306178
Flipped loss: 0.33896. Accuracy: 0.938
### Flips: 1230, rs: 12, checks: 205
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42476118
Train loss (w/o reg) on all data: 0.41470775
Test loss (w/o reg) on all data: 0.2888504
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.2012113e-05
Norm of the params: 14.1798725
     Influence (LOO): fixed 161 labels. Loss 0.28885. Accuracy 0.943.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3899081
Train loss (w/o reg) on all data: 0.37667257
Test loss (w/o reg) on all data: 0.2964325
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.9057337220602527
Norm of the mean of gradients: 5.412564e-05
Norm of the params: 16.269932
                Loss: fixed 203 labels. Loss 0.29643. Accuracy 0.906.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4812934
Train loss (w/o reg) on all data: 0.47590265
Test loss (w/o reg) on all data: 0.3226474
Train acc on all data:  0.7699975686846584
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 4.5885543e-05
Norm of the params: 10.383413
              Random: fixed  51 labels. Loss 0.32265. Accuracy 0.943.
### Flips: 1230, rs: 12, checks: 410
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38100997
Train loss (w/o reg) on all data: 0.36991432
Test loss (w/o reg) on all data: 0.253325
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 2.987805e-05
Norm of the params: 14.896738
     Influence (LOO): fixed 278 labels. Loss 0.25332. Accuracy 0.950.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3047573
Train loss (w/o reg) on all data: 0.28758425
Test loss (w/o reg) on all data: 0.23993438
Train acc on all data:  0.862873814733771
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 0.000103139966
Norm of the params: 18.532698
                Loss: fixed 404 labels. Loss 0.23993. Accuracy 0.915.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4698742
Train loss (w/o reg) on all data: 0.46445715
Test loss (w/o reg) on all data: 0.3013295
Train acc on all data:  0.7792365669827377
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 9.822205e-05
Norm of the params: 10.408686
              Random: fixed 103 labels. Loss 0.30133. Accuracy 0.952.
### Flips: 1230, rs: 12, checks: 615
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3377144
Train loss (w/o reg) on all data: 0.3255871
Test loss (w/o reg) on all data: 0.21912678
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.9704868e-05
Norm of the params: 15.573894
     Influence (LOO): fixed 386 labels. Loss 0.21913. Accuracy 0.963.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2140279
Train loss (w/o reg) on all data: 0.19401248
Test loss (w/o reg) on all data: 0.18549725
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.884607e-05
Norm of the params: 20.00771
                Loss: fixed 605 labels. Loss 0.18550. Accuracy 0.935.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45848888
Train loss (w/o reg) on all data: 0.45289764
Test loss (w/o reg) on all data: 0.2851703
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 8.890491e-05
Norm of the params: 10.574717
              Random: fixed 149 labels. Loss 0.28517. Accuracy 0.966.
### Flips: 1230, rs: 12, checks: 820
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2960757
Train loss (w/o reg) on all data: 0.28441426
Test loss (w/o reg) on all data: 0.18104935
Train acc on all data:  0.8674933138828106
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 6.286405e-05
Norm of the params: 15.2718315
     Influence (LOO): fixed 495 labels. Loss 0.18105. Accuracy 0.971.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122412495
Train loss (w/o reg) on all data: 0.1010125
Test loss (w/o reg) on all data: 0.12263648
Train acc on all data:  0.9584245076586433
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 3.6897852e-06
Norm of the params: 20.68816
                Loss: fixed 799 labels. Loss 0.12264. Accuracy 0.956.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44345748
Train loss (w/o reg) on all data: 0.43758053
Test loss (w/o reg) on all data: 0.26571605
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.6890193e-05
Norm of the params: 10.841558
              Random: fixed 203 labels. Loss 0.26572. Accuracy 0.970.
### Flips: 1230, rs: 12, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2606696
Train loss (w/o reg) on all data: 0.24825588
Test loss (w/o reg) on all data: 0.15288122
Train acc on all data:  0.8857281789448092
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.7336047e-06
Norm of the params: 15.756726
     Influence (LOO): fixed 576 labels. Loss 0.15288. Accuracy 0.976.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063852325
Train loss (w/o reg) on all data: 0.045894403
Test loss (w/o reg) on all data: 0.06174299
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 1.348556e-06
Norm of the params: 18.951477
                Loss: fixed 931 labels. Loss 0.06174. Accuracy 0.977.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42784652
Train loss (w/o reg) on all data: 0.4216977
Test loss (w/o reg) on all data: 0.24538776
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6431779e-05
Norm of the params: 11.089463
              Random: fixed 255 labels. Loss 0.24539. Accuracy 0.973.
### Flips: 1230, rs: 12, checks: 1230
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22858523
Train loss (w/o reg) on all data: 0.21627532
Test loss (w/o reg) on all data: 0.12978262
Train acc on all data:  0.9039630440068077
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.7172271e-05
Norm of the params: 15.690705
     Influence (LOO): fixed 649 labels. Loss 0.12978. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040388152
Train loss (w/o reg) on all data: 0.025681917
Test loss (w/o reg) on all data: 0.04458088
Train acc on all data:  0.9907610017019207
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.801893e-06
Norm of the params: 17.150064
                Loss: fixed 977 labels. Loss 0.04458. Accuracy 0.984.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41199678
Train loss (w/o reg) on all data: 0.40588456
Test loss (w/o reg) on all data: 0.22905853
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.561154e-05
Norm of the params: 11.056421
              Random: fixed 311 labels. Loss 0.22906. Accuracy 0.976.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5006862
Train loss (w/o reg) on all data: 0.4959083
Test loss (w/o reg) on all data: 0.33140805
Train acc on all data:  0.7486019936785802
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 0.00013073979
Norm of the params: 9.7754345
Flipped loss: 0.33141. Accuracy: 0.947
### Flips: 1230, rs: 13, checks: 205
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4358788
Train loss (w/o reg) on all data: 0.42622185
Test loss (w/o reg) on all data: 0.27219763
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 2.3561177e-05
Norm of the params: 13.897467
     Influence (LOO): fixed 160 labels. Loss 0.27220. Accuracy 0.959.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4045907
Train loss (w/o reg) on all data: 0.39357424
Test loss (w/o reg) on all data: 0.28081045
Train acc on all data:  0.8028203257962557
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 7.415978e-06
Norm of the params: 14.843488
                Loss: fixed 202 labels. Loss 0.28081. Accuracy 0.921.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48781067
Train loss (w/o reg) on all data: 0.48301378
Test loss (w/o reg) on all data: 0.31194723
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 2.9872794e-05
Norm of the params: 9.794797
              Random: fixed  60 labels. Loss 0.31195. Accuracy 0.954.
### Flips: 1230, rs: 13, checks: 410
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3869424
Train loss (w/o reg) on all data: 0.3760599
Test loss (w/o reg) on all data: 0.23617153
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.6911621e-05
Norm of the params: 14.75296
     Influence (LOO): fixed 286 labels. Loss 0.23617. Accuracy 0.964.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3148533
Train loss (w/o reg) on all data: 0.29955873
Test loss (w/o reg) on all data: 0.22823726
Train acc on all data:  0.8553367371748116
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 2.678864e-05
Norm of the params: 17.489765
                Loss: fixed 405 labels. Loss 0.22824. Accuracy 0.925.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47623178
Train loss (w/o reg) on all data: 0.4717034
Test loss (w/o reg) on all data: 0.29403585
Train acc on all data:  0.7731582786287381
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 5.2888714e-05
Norm of the params: 9.516694
              Random: fixed 117 labels. Loss 0.29404. Accuracy 0.964.
### Flips: 1230, rs: 13, checks: 615
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35009322
Train loss (w/o reg) on all data: 0.3379214
Test loss (w/o reg) on all data: 0.2072137
Train acc on all data:  0.8344274252370533
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.0847257e-05
Norm of the params: 15.602435
     Influence (LOO): fixed 378 labels. Loss 0.20721. Accuracy 0.973.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2209985
Train loss (w/o reg) on all data: 0.20109895
Test loss (w/o reg) on all data: 0.1715361
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 5.580543e-06
Norm of the params: 19.949715
                Loss: fixed 606 labels. Loss 0.17154. Accuracy 0.944.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46332362
Train loss (w/o reg) on all data: 0.45857635
Test loss (w/o reg) on all data: 0.27960274
Train acc on all data:  0.7828835399951374
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 4.50332e-05
Norm of the params: 9.743986
              Random: fixed 166 labels. Loss 0.27960. Accuracy 0.968.
### Flips: 1230, rs: 13, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30748922
Train loss (w/o reg) on all data: 0.29458696
Test loss (w/o reg) on all data: 0.17535627
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2567843e-05
Norm of the params: 16.063776
     Influence (LOO): fixed 485 labels. Loss 0.17536. Accuracy 0.976.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13491829
Train loss (w/o reg) on all data: 0.11449195
Test loss (w/o reg) on all data: 0.10608749
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.1817126e-05
Norm of the params: 20.21205
                Loss: fixed 795 labels. Loss 0.10609. Accuracy 0.966.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45230514
Train loss (w/o reg) on all data: 0.4474167
Test loss (w/o reg) on all data: 0.2688309
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.6693964e-05
Norm of the params: 9.887823
              Random: fixed 207 labels. Loss 0.26883. Accuracy 0.972.
### Flips: 1230, rs: 13, checks: 1025
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2679795
Train loss (w/o reg) on all data: 0.255097
Test loss (w/o reg) on all data: 0.14793344
Train acc on all data:  0.87575978604425
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.5200241e-05
Norm of the params: 16.051489
     Influence (LOO): fixed 572 labels. Loss 0.14793. Accuracy 0.982.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07020034
Train loss (w/o reg) on all data: 0.053264458
Test loss (w/o reg) on all data: 0.05074891
Train acc on all data:  0.9786044249939218
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 3.1994498e-06
Norm of the params: 18.404284
                Loss: fixed 942 labels. Loss 0.05075. Accuracy 0.986.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43924427
Train loss (w/o reg) on all data: 0.434227
Test loss (w/o reg) on all data: 0.25486934
Train acc on all data:  0.8045222465353756
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.161906e-05
Norm of the params: 10.01728
              Random: fixed 255 labels. Loss 0.25487. Accuracy 0.967.
### Flips: 1230, rs: 13, checks: 1230
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23619564
Train loss (w/o reg) on all data: 0.22389498
Test loss (w/o reg) on all data: 0.12570117
Train acc on all data:  0.8930221249696085
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 4.9914593e-06
Norm of the params: 15.684808
     Influence (LOO): fixed 646 labels. Loss 0.12570. Accuracy 0.984.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038343206
Train loss (w/o reg) on all data: 0.02551542
Test loss (w/o reg) on all data: 0.028456623
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 3.599836e-06
Norm of the params: 16.017357
                Loss: fixed 1014 labels. Loss 0.02846. Accuracy 0.993.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4271267
Train loss (w/o reg) on all data: 0.42226213
Test loss (w/o reg) on all data: 0.24095598
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.4195219e-05
Norm of the params: 9.863638
              Random: fixed 300 labels. Loss 0.24096. Accuracy 0.973.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49727947
Train loss (w/o reg) on all data: 0.49251822
Test loss (w/o reg) on all data: 0.3366398
Train acc on all data:  0.7566253343058594
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.3433024e-05
Norm of the params: 9.758339
Flipped loss: 0.33664. Accuracy: 0.946
### Flips: 1230, rs: 14, checks: 205
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336527
Train loss (w/o reg) on all data: 0.4246253
Test loss (w/o reg) on all data: 0.2852143
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 1.8290022e-05
Norm of the params: 13.436812
     Influence (LOO): fixed 160 labels. Loss 0.28521. Accuracy 0.952.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39519668
Train loss (w/o reg) on all data: 0.38438475
Test loss (w/o reg) on all data: 0.28802043
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 1.29014325e-05
Norm of the params: 14.705057
                Loss: fixed 205 labels. Loss 0.28802. Accuracy 0.919.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48568895
Train loss (w/o reg) on all data: 0.48079452
Test loss (w/o reg) on all data: 0.31924725
Train acc on all data:  0.7663505956722587
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.5440592e-05
Norm of the params: 9.893867
              Random: fixed  52 labels. Loss 0.31925. Accuracy 0.948.
### Flips: 1230, rs: 14, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38776386
Train loss (w/o reg) on all data: 0.376844
Test loss (w/o reg) on all data: 0.24288663
Train acc on all data:  0.8186238755166545
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.4427998e-05
Norm of the params: 14.778278
     Influence (LOO): fixed 285 labels. Loss 0.24289. Accuracy 0.959.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304603
Train loss (w/o reg) on all data: 0.28942195
Test loss (w/o reg) on all data: 0.23878123
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 1.2092102e-05
Norm of the params: 17.424732
                Loss: fixed 406 labels. Loss 0.23878. Accuracy 0.917.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4725088
Train loss (w/o reg) on all data: 0.4675017
Test loss (w/o reg) on all data: 0.2967365
Train acc on all data:  0.7782640408460978
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 2.6181644e-05
Norm of the params: 10.007075
              Random: fixed 106 labels. Loss 0.29674. Accuracy 0.960.
### Flips: 1230, rs: 14, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3533538
Train loss (w/o reg) on all data: 0.3414711
Test loss (w/o reg) on all data: 0.21234292
Train acc on all data:  0.8356430829078532
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 8.33175e-06
Norm of the params: 15.416021
     Influence (LOO): fixed 377 labels. Loss 0.21234. Accuracy 0.974.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20923671
Train loss (w/o reg) on all data: 0.19034034
Test loss (w/o reg) on all data: 0.18561472
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 7.3678116e-06
Norm of the params: 19.440357
                Loss: fixed 608 labels. Loss 0.18561. Accuracy 0.937.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4615665
Train loss (w/o reg) on all data: 0.45643553
Test loss (w/o reg) on all data: 0.28319365
Train acc on all data:  0.7848285922684172
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.8381444e-05
Norm of the params: 10.130133
              Random: fixed 151 labels. Loss 0.28319. Accuracy 0.958.
### Flips: 1230, rs: 14, checks: 820
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3150272
Train loss (w/o reg) on all data: 0.30284843
Test loss (w/o reg) on all data: 0.18029045
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.7986853e-06
Norm of the params: 15.60691
     Influence (LOO): fixed 476 labels. Loss 0.18029. Accuracy 0.981.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122622535
Train loss (w/o reg) on all data: 0.10243452
Test loss (w/o reg) on all data: 0.12399266
Train acc on all data:  0.9559931923170435
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 3.735744e-06
Norm of the params: 20.093784
                Loss: fixed 793 labels. Loss 0.12399. Accuracy 0.957.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44902012
Train loss (w/o reg) on all data: 0.44393688
Test loss (w/o reg) on all data: 0.26032406
Train acc on all data:  0.799173352783856
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8358237e-05
Norm of the params: 10.082903
              Random: fixed 207 labels. Loss 0.26032. Accuracy 0.967.
### Flips: 1230, rs: 14, checks: 1025
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28109506
Train loss (w/o reg) on all data: 0.26869568
Test loss (w/o reg) on all data: 0.15021352
Train acc on all data:  0.8767323121808899
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.378641e-06
Norm of the params: 15.747615
     Influence (LOO): fixed 560 labels. Loss 0.15021. Accuracy 0.987.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06317333
Train loss (w/o reg) on all data: 0.046312302
Test loss (w/o reg) on all data: 0.06535828
Train acc on all data:  0.9829807926088013
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.6312035e-06
Norm of the params: 18.363567
                Loss: fixed 933 labels. Loss 0.06536. Accuracy 0.977.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4351575
Train loss (w/o reg) on all data: 0.42971686
Test loss (w/o reg) on all data: 0.2414962
Train acc on all data:  0.8098711402868952
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.4678654e-05
Norm of the params: 10.431343
              Random: fixed 257 labels. Loss 0.24150. Accuracy 0.976.
### Flips: 1230, rs: 14, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23886059
Train loss (w/o reg) on all data: 0.22661868
Test loss (w/o reg) on all data: 0.1209309
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.1558428e-05
Norm of the params: 15.647315
     Influence (LOO): fixed 651 labels. Loss 0.12093. Accuracy 0.990.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032341786
Train loss (w/o reg) on all data: 0.020351104
Test loss (w/o reg) on all data: 0.03374367
Train acc on all data:  0.9934354485776805
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.690609e-07
Norm of the params: 15.485918
                Loss: fixed 996 labels. Loss 0.03374. Accuracy 0.986.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41831914
Train loss (w/o reg) on all data: 0.41282678
Test loss (w/o reg) on all data: 0.22706746
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.7433345e-05
Norm of the params: 10.480813
              Random: fixed 312 labels. Loss 0.22707. Accuracy 0.978.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49587384
Train loss (w/o reg) on all data: 0.4909528
Test loss (w/o reg) on all data: 0.3260308
Train acc on all data:  0.74933138828106
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 3.3683908e-05
Norm of the params: 9.920734
Flipped loss: 0.32603. Accuracy: 0.952
### Flips: 1230, rs: 15, checks: 205
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42643344
Train loss (w/o reg) on all data: 0.41666064
Test loss (w/o reg) on all data: 0.27925524
Train acc on all data:  0.7872599076100171
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 8.9876994e-05
Norm of the params: 13.980562
     Influence (LOO): fixed 163 labels. Loss 0.27926. Accuracy 0.955.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39285496
Train loss (w/o reg) on all data: 0.38096604
Test loss (w/o reg) on all data: 0.27529338
Train acc on all data:  0.8042791150012156
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 4.5638004e-05
Norm of the params: 15.420059
                Loss: fixed 205 labels. Loss 0.27529. Accuracy 0.917.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48456174
Train loss (w/o reg) on all data: 0.47952688
Test loss (w/o reg) on all data: 0.3052325
Train acc on all data:  0.7636761487964989
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.7307283e-05
Norm of the params: 10.0348
              Random: fixed  63 labels. Loss 0.30523. Accuracy 0.960.
### Flips: 1230, rs: 15, checks: 410
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3794086
Train loss (w/o reg) on all data: 0.3676442
Test loss (w/o reg) on all data: 0.23259166
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9636183e-05
Norm of the params: 15.339102
     Influence (LOO): fixed 289 labels. Loss 0.23259. Accuracy 0.966.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30287316
Train loss (w/o reg) on all data: 0.28651717
Test loss (w/o reg) on all data: 0.21978042
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 8.594058e-06
Norm of the params: 18.086454
                Loss: fixed 409 labels. Loss 0.21978. Accuracy 0.918.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47639987
Train loss (w/o reg) on all data: 0.471752
Test loss (w/o reg) on all data: 0.29122567
Train acc on all data:  0.7721857524920982
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.128793e-05
Norm of the params: 9.641463
              Random: fixed 104 labels. Loss 0.29123. Accuracy 0.969.
### Flips: 1230, rs: 15, checks: 615
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34457856
Train loss (w/o reg) on all data: 0.33205226
Test loss (w/o reg) on all data: 0.20442908
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 4.8579674e-05
Norm of the params: 15.828019
     Influence (LOO): fixed 380 labels. Loss 0.20443. Accuracy 0.980.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21658252
Train loss (w/o reg) on all data: 0.19742294
Test loss (w/o reg) on all data: 0.15533546
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 8.043811e-06
Norm of the params: 19.575281
                Loss: fixed 612 labels. Loss 0.15534. Accuracy 0.944.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46371868
Train loss (w/o reg) on all data: 0.4586495
Test loss (w/o reg) on all data: 0.2753692
Train acc on all data:  0.7845854607342573
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 6.3647065e-05
Norm of the params: 10.068961
              Random: fixed 155 labels. Loss 0.27537. Accuracy 0.973.
### Flips: 1230, rs: 15, checks: 820
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30642396
Train loss (w/o reg) on all data: 0.29385856
Test loss (w/o reg) on all data: 0.17351975
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 1.1038186e-05
Norm of the params: 15.852698
     Influence (LOO): fixed 474 labels. Loss 0.17352. Accuracy 0.987.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13541424
Train loss (w/o reg) on all data: 0.11569356
Test loss (w/o reg) on all data: 0.10174092
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 3.7035873e-06
Norm of the params: 19.859848
                Loss: fixed 790 labels. Loss 0.10174. Accuracy 0.965.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44824332
Train loss (w/o reg) on all data: 0.44309196
Test loss (w/o reg) on all data: 0.25482705
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 5.0154096e-05
Norm of the params: 10.150222
              Random: fixed 213 labels. Loss 0.25483. Accuracy 0.982.
### Flips: 1230, rs: 15, checks: 1025
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2677146
Train loss (w/o reg) on all data: 0.2545858
Test loss (w/o reg) on all data: 0.14665553
Train acc on all data:  0.8811086797957695
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 7.543125e-06
Norm of the params: 16.204184
     Influence (LOO): fixed 558 labels. Loss 0.14666. Accuracy 0.986.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06854752
Train loss (w/o reg) on all data: 0.05241788
Test loss (w/o reg) on all data: 0.043031063
Train acc on all data:  0.9778750303914417
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.4572305e-06
Norm of the params: 17.960869
                Loss: fixed 935 labels. Loss 0.04303. Accuracy 0.986.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4376119
Train loss (w/o reg) on all data: 0.43250838
Test loss (w/o reg) on all data: 0.24188766
Train acc on all data:  0.8071966934111354
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 3.562679e-05
Norm of the params: 10.10299
              Random: fixed 256 labels. Loss 0.24189. Accuracy 0.984.
### Flips: 1230, rs: 15, checks: 1230
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23342107
Train loss (w/o reg) on all data: 0.22050871
Test loss (w/o reg) on all data: 0.124956004
Train acc on all data:  0.8969122295161682
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 3.9782968e-05
Norm of the params: 16.070078
     Influence (LOO): fixed 632 labels. Loss 0.12496. Accuracy 0.987.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033969373
Train loss (w/o reg) on all data: 0.021744885
Test loss (w/o reg) on all data: 0.016886484
Train acc on all data:  0.9924629224410406
Test acc on all data:   0.9980563654033042
Norm of the mean of gradients: 1.5279069e-06
Norm of the params: 15.636169
                Loss: fixed 1010 labels. Loss 0.01689. Accuracy 0.998.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4247201
Train loss (w/o reg) on all data: 0.41946945
Test loss (w/o reg) on all data: 0.22841713
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.03609e-05
Norm of the params: 10.2476
              Random: fixed 299 labels. Loss 0.22842. Accuracy 0.983.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4993089
Train loss (w/o reg) on all data: 0.4946123
Test loss (w/o reg) on all data: 0.33074114
Train acc on all data:  0.7522489666909798
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.8218334e-05
Norm of the params: 9.691866
Flipped loss: 0.33074. Accuracy: 0.947
### Flips: 1230, rs: 16, checks: 205
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4335985
Train loss (w/o reg) on all data: 0.42478693
Test loss (w/o reg) on all data: 0.2783394
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.7078586e-05
Norm of the params: 13.275211
     Influence (LOO): fixed 162 labels. Loss 0.27834. Accuracy 0.950.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40079775
Train loss (w/o reg) on all data: 0.3899153
Test loss (w/o reg) on all data: 0.27307153
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9183673469387755
Norm of the mean of gradients: 3.8544847e-05
Norm of the params: 14.752936
                Loss: fixed 204 labels. Loss 0.27307. Accuracy 0.918.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4878856
Train loss (w/o reg) on all data: 0.48298702
Test loss (w/o reg) on all data: 0.31210554
Train acc on all data:  0.762217359591539
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 9.407749e-06
Norm of the params: 9.898066
              Random: fixed  56 labels. Loss 0.31211. Accuracy 0.956.
### Flips: 1230, rs: 16, checks: 410
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38778022
Train loss (w/o reg) on all data: 0.37753654
Test loss (w/o reg) on all data: 0.23821828
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 4.015974e-05
Norm of the params: 14.313403
     Influence (LOO): fixed 286 labels. Loss 0.23822. Accuracy 0.953.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30910203
Train loss (w/o reg) on all data: 0.2937462
Test loss (w/o reg) on all data: 0.21340296
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 9.269482e-06
Norm of the params: 17.524742
                Loss: fixed 405 labels. Loss 0.21340. Accuracy 0.927.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47687495
Train loss (w/o reg) on all data: 0.47207198
Test loss (w/o reg) on all data: 0.29520908
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.07975275e-05
Norm of the params: 9.800979
              Random: fixed 108 labels. Loss 0.29521. Accuracy 0.958.
### Flips: 1230, rs: 16, checks: 615
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35155135
Train loss (w/o reg) on all data: 0.33986202
Test loss (w/o reg) on all data: 0.2031264
Train acc on all data:  0.8346705567712133
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.0527225e-05
Norm of the params: 15.290089
     Influence (LOO): fixed 389 labels. Loss 0.20313. Accuracy 0.976.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21904714
Train loss (w/o reg) on all data: 0.19899546
Test loss (w/o reg) on all data: 0.15674928
Train acc on all data:  0.9034767809384877
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 9.755859e-06
Norm of the params: 20.025826
                Loss: fixed 603 labels. Loss 0.15675. Accuracy 0.941.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46507165
Train loss (w/o reg) on all data: 0.46005306
Test loss (w/o reg) on all data: 0.28237727
Train acc on all data:  0.7828835399951374
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.1613936e-05
Norm of the params: 10.018562
              Random: fixed 154 labels. Loss 0.28238. Accuracy 0.963.
### Flips: 1230, rs: 16, checks: 820
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3155032
Train loss (w/o reg) on all data: 0.30310813
Test loss (w/o reg) on all data: 0.1812764
Train acc on all data:  0.8533916849015317
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 7.018226e-05
Norm of the params: 15.744903
     Influence (LOO): fixed 472 labels. Loss 0.18128. Accuracy 0.975.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13738835
Train loss (w/o reg) on all data: 0.11601808
Test loss (w/o reg) on all data: 0.109226786
Train acc on all data:  0.9469973255531242
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 3.609322e-06
Norm of the params: 20.673782
                Loss: fixed 790 labels. Loss 0.10923. Accuracy 0.960.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45064822
Train loss (w/o reg) on all data: 0.44568253
Test loss (w/o reg) on all data: 0.26187548
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.7941638e-05
Norm of the params: 9.96564
              Random: fixed 208 labels. Loss 0.26188. Accuracy 0.966.
### Flips: 1230, rs: 16, checks: 1025
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2748263
Train loss (w/o reg) on all data: 0.26311052
Test loss (w/o reg) on all data: 0.15115865
Train acc on all data:  0.8796498905908097
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.613042e-06
Norm of the params: 15.307356
     Influence (LOO): fixed 570 labels. Loss 0.15116. Accuracy 0.983.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06673139
Train loss (w/o reg) on all data: 0.04880143
Test loss (w/o reg) on all data: 0.04954192
Train acc on all data:  0.9805494772672015
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.1053455e-06
Norm of the params: 18.936718
                Loss: fixed 947 labels. Loss 0.04954. Accuracy 0.981.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4336961
Train loss (w/o reg) on all data: 0.42883873
Test loss (w/o reg) on all data: 0.24334443
Train acc on all data:  0.8106005348893751
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.1534724e-05
Norm of the params: 9.85634
              Random: fixed 270 labels. Loss 0.24334. Accuracy 0.966.
### Flips: 1230, rs: 16, checks: 1230
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24263553
Train loss (w/o reg) on all data: 0.23163722
Test loss (w/o reg) on all data: 0.1268603
Train acc on all data:  0.8952103087770484
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 5.634756e-06
Norm of the params: 14.831259
     Influence (LOO): fixed 641 labels. Loss 0.12686. Accuracy 0.988.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042710308
Train loss (w/o reg) on all data: 0.028663239
Test loss (w/o reg) on all data: 0.030845145
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 9.5249004e-07
Norm of the params: 16.761307
                Loss: fixed 1003 labels. Loss 0.03085. Accuracy 0.989.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41672155
Train loss (w/o reg) on all data: 0.4117001
Test loss (w/o reg) on all data: 0.22503176
Train acc on all data:  0.8244590323364941
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.6460337e-05
Norm of the params: 10.021436
              Random: fixed 321 labels. Loss 0.22503. Accuracy 0.973.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5014724
Train loss (w/o reg) on all data: 0.49689496
Test loss (w/o reg) on all data: 0.33875754
Train acc on all data:  0.7461706783369803
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 2.2039792e-05
Norm of the params: 9.568117
Flipped loss: 0.33876. Accuracy: 0.937
### Flips: 1230, rs: 17, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43378592
Train loss (w/o reg) on all data: 0.4243938
Test loss (w/o reg) on all data: 0.28650758
Train acc on all data:  0.7892049598832969
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.2523435e-05
Norm of the params: 13.705545
     Influence (LOO): fixed 162 labels. Loss 0.28651. Accuracy 0.935.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39999905
Train loss (w/o reg) on all data: 0.38881376
Test loss (w/o reg) on all data: 0.28446034
Train acc on all data:  0.8054947726720155
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 3.0978168e-05
Norm of the params: 14.956794
                Loss: fixed 205 labels. Loss 0.28446. Accuracy 0.913.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48998326
Train loss (w/o reg) on all data: 0.48531076
Test loss (w/o reg) on all data: 0.31991044
Train acc on all data:  0.7583272550449793
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 7.353727e-05
Norm of the params: 9.666957
              Random: fixed  59 labels. Loss 0.31991. Accuracy 0.947.
### Flips: 1230, rs: 17, checks: 410
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38587284
Train loss (w/o reg) on all data: 0.375738
Test loss (w/o reg) on all data: 0.24243875
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 2.6948564e-05
Norm of the params: 14.237163
     Influence (LOO): fixed 289 labels. Loss 0.24244. Accuracy 0.963.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3107168
Train loss (w/o reg) on all data: 0.29533508
Test loss (w/o reg) on all data: 0.22935307
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 2.1668116e-05
Norm of the params: 17.539503
                Loss: fixed 407 labels. Loss 0.22935. Accuracy 0.921.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47968218
Train loss (w/o reg) on all data: 0.47478402
Test loss (w/o reg) on all data: 0.30578202
Train acc on all data:  0.7690250425480185
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 4.0418752e-05
Norm of the params: 9.897639
              Random: fixed 105 labels. Loss 0.30578. Accuracy 0.951.
### Flips: 1230, rs: 17, checks: 615
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34606275
Train loss (w/o reg) on all data: 0.33452067
Test loss (w/o reg) on all data: 0.21288916
Train acc on all data:  0.8371018721128131
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.054606e-05
Norm of the params: 15.193468
     Influence (LOO): fixed 386 labels. Loss 0.21289. Accuracy 0.968.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22010455
Train loss (w/o reg) on all data: 0.20030841
Test loss (w/o reg) on all data: 0.1777864
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 8.142007e-06
Norm of the params: 19.89781
                Loss: fixed 606 labels. Loss 0.17779. Accuracy 0.930.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46314785
Train loss (w/o reg) on all data: 0.4580305
Test loss (w/o reg) on all data: 0.28549474
Train acc on all data:  0.7855579868708972
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 6.361544e-05
Norm of the params: 10.116668
              Random: fixed 171 labels. Loss 0.28549. Accuracy 0.956.
### Flips: 1230, rs: 17, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3125991
Train loss (w/o reg) on all data: 0.30040422
Test loss (w/o reg) on all data: 0.18477318
Train acc on all data:  0.8560661317772915
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.9225343e-05
Norm of the params: 15.617215
     Influence (LOO): fixed 469 labels. Loss 0.18477. Accuracy 0.973.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13051143
Train loss (w/o reg) on all data: 0.108675964
Test loss (w/o reg) on all data: 0.11133565
Train acc on all data:  0.9538050085096037
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 9.051377e-06
Norm of the params: 20.897593
                Loss: fixed 799 labels. Loss 0.11134. Accuracy 0.958.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45184433
Train loss (w/o reg) on all data: 0.4465041
Test loss (w/o reg) on all data: 0.26873446
Train acc on all data:  0.7945538536348165
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.6507842e-05
Norm of the params: 10.334643
              Random: fixed 214 labels. Loss 0.26873. Accuracy 0.971.
### Flips: 1230, rs: 17, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27752724
Train loss (w/o reg) on all data: 0.2652869
Test loss (w/o reg) on all data: 0.15918216
Train acc on all data:  0.8767323121808899
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.223437e-05
Norm of the params: 15.646308
     Influence (LOO): fixed 554 labels. Loss 0.15918. Accuracy 0.976.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071897894
Train loss (w/o reg) on all data: 0.053456325
Test loss (w/o reg) on all data: 0.056721326
Train acc on all data:  0.9800632141988816
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.3641155e-06
Norm of the params: 19.204987
                Loss: fixed 930 labels. Loss 0.05672. Accuracy 0.978.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43804166
Train loss (w/o reg) on all data: 0.43287316
Test loss (w/o reg) on all data: 0.25103095
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.713428e-05
Norm of the params: 10.16709
              Random: fixed 265 labels. Loss 0.25103. Accuracy 0.974.
### Flips: 1230, rs: 17, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24465127
Train loss (w/o reg) on all data: 0.23220158
Test loss (w/o reg) on all data: 0.13762836
Train acc on all data:  0.8942377826404084
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.3839644e-05
Norm of the params: 15.779541
     Influence (LOO): fixed 627 labels. Loss 0.13763. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03771825
Train loss (w/o reg) on all data: 0.024248453
Test loss (w/o reg) on all data: 0.030236073
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 1.687171e-06
Norm of the params: 16.413288
                Loss: fixed 1011 labels. Loss 0.03024. Accuracy 0.990.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41949382
Train loss (w/o reg) on all data: 0.41416296
Test loss (w/o reg) on all data: 0.23302034
Train acc on all data:  0.8205689277899344
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4700622e-05
Norm of the params: 10.325558
              Random: fixed 328 labels. Loss 0.23302. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49424252
Train loss (w/o reg) on all data: 0.48867956
Test loss (w/o reg) on all data: 0.34115124
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 3.0189522e-05
Norm of the params: 10.547958
Flipped loss: 0.34115. Accuracy: 0.934
### Flips: 1230, rs: 18, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4308268
Train loss (w/o reg) on all data: 0.4204114
Test loss (w/o reg) on all data: 0.29522294
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9319727891156463
Norm of the mean of gradients: 2.0539428e-05
Norm of the params: 14.432892
     Influence (LOO): fixed 151 labels. Loss 0.29522. Accuracy 0.932.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3961607
Train loss (w/o reg) on all data: 0.38355196
Test loss (w/o reg) on all data: 0.291339
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.9135082604470359
Norm of the mean of gradients: 6.6835114e-06
Norm of the params: 15.880011
                Loss: fixed 201 labels. Loss 0.29134. Accuracy 0.914.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48213854
Train loss (w/o reg) on all data: 0.47670054
Test loss (w/o reg) on all data: 0.31969485
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.3340226e-05
Norm of the params: 10.4288
              Random: fixed  61 labels. Loss 0.31969. Accuracy 0.942.
### Flips: 1230, rs: 18, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38116252
Train loss (w/o reg) on all data: 0.36899891
Test loss (w/o reg) on all data: 0.24859568
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 2.8413577e-05
Norm of the params: 15.597184
     Influence (LOO): fixed 288 labels. Loss 0.24860. Accuracy 0.949.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31085458
Train loss (w/o reg) on all data: 0.2937375
Test loss (w/o reg) on all data: 0.24238665
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 6.6769375e-05
Norm of the params: 18.502478
                Loss: fixed 399 labels. Loss 0.24239. Accuracy 0.914.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46692488
Train loss (w/o reg) on all data: 0.4614745
Test loss (w/o reg) on all data: 0.29570222
Train acc on all data:  0.7775346462436178
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.8840787e-05
Norm of the params: 10.440663
              Random: fixed 130 labels. Loss 0.29570. Accuracy 0.954.
### Flips: 1230, rs: 18, checks: 615
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33697388
Train loss (w/o reg) on all data: 0.32412896
Test loss (w/o reg) on all data: 0.20484124
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 3.9827726e-05
Norm of the params: 16.028053
     Influence (LOO): fixed 402 labels. Loss 0.20484. Accuracy 0.967.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2263171
Train loss (w/o reg) on all data: 0.205342
Test loss (w/o reg) on all data: 0.17805009
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 2.063606e-05
Norm of the params: 20.481747
                Loss: fixed 597 labels. Loss 0.17805. Accuracy 0.940.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45348853
Train loss (w/o reg) on all data: 0.44765553
Test loss (w/o reg) on all data: 0.27755487
Train acc on all data:  0.7894480914174569
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.9503914e-05
Norm of the params: 10.800919
              Random: fixed 180 labels. Loss 0.27755. Accuracy 0.957.
### Flips: 1230, rs: 18, checks: 820
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30321756
Train loss (w/o reg) on all data: 0.29020676
Test loss (w/o reg) on all data: 0.1776946
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.5427904e-05
Norm of the params: 16.131214
     Influence (LOO): fixed 493 labels. Loss 0.17769. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14391607
Train loss (w/o reg) on all data: 0.12213265
Test loss (w/o reg) on all data: 0.115847126
Train acc on all data:  0.9457816678823243
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.04379515e-05
Norm of the params: 20.87267
                Loss: fixed 784 labels. Loss 0.11585. Accuracy 0.958.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4414137
Train loss (w/o reg) on all data: 0.43581963
Test loss (w/o reg) on all data: 0.25875533
Train acc on all data:  0.798930221249696
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.0518602e-05
Norm of the params: 10.577403
              Random: fixed 234 labels. Loss 0.25876. Accuracy 0.975.
### Flips: 1230, rs: 18, checks: 1025
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26496404
Train loss (w/o reg) on all data: 0.25136968
Test loss (w/o reg) on all data: 0.15236953
Train acc on all data:  0.8808655482616096
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 6.922714e-06
Norm of the params: 16.489004
     Influence (LOO): fixed 577 labels. Loss 0.15237. Accuracy 0.979.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0807173
Train loss (w/o reg) on all data: 0.061788447
Test loss (w/o reg) on all data: 0.062419146
Train acc on all data:  0.9769025042548019
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 6.379778e-06
Norm of the params: 19.457058
                Loss: fixed 925 labels. Loss 0.06242. Accuracy 0.981.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4291813
Train loss (w/o reg) on all data: 0.42343923
Test loss (w/o reg) on all data: 0.24486813
Train acc on all data:  0.8091417456844152
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.816218e-05
Norm of the params: 10.716401
              Random: fixed 277 labels. Loss 0.24487. Accuracy 0.977.
### Flips: 1230, rs: 18, checks: 1230
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22773623
Train loss (w/o reg) on all data: 0.21454394
Test loss (w/o reg) on all data: 0.12758364
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 5.9233244e-06
Norm of the params: 16.243336
     Influence (LOO): fixed 657 labels. Loss 0.12758. Accuracy 0.981.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045752328
Train loss (w/o reg) on all data: 0.03176361
Test loss (w/o reg) on all data: 0.029977025
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 2.0947496e-06
Norm of the params: 16.726456
                Loss: fixed 994 labels. Loss 0.02998. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41182977
Train loss (w/o reg) on all data: 0.40621004
Test loss (w/o reg) on all data: 0.22334519
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.8085582e-05
Norm of the params: 10.601646
              Random: fixed 337 labels. Loss 0.22335. Accuracy 0.981.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5016705
Train loss (w/o reg) on all data: 0.49742082
Test loss (w/o reg) on all data: 0.32860476
Train acc on all data:  0.7527352297592997
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.87467e-05
Norm of the params: 9.21916
Flipped loss: 0.32860. Accuracy: 0.958
### Flips: 1230, rs: 19, checks: 205
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44063815
Train loss (w/o reg) on all data: 0.43117845
Test loss (w/o reg) on all data: 0.27873453
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.1317095e-05
Norm of the params: 13.754782
     Influence (LOO): fixed 158 labels. Loss 0.27873. Accuracy 0.970.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40615562
Train loss (w/o reg) on all data: 0.39632225
Test loss (w/o reg) on all data: 0.2774264
Train acc on all data:  0.8025771942620958
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 2.1502086e-05
Norm of the params: 14.023822
                Loss: fixed 201 labels. Loss 0.27743. Accuracy 0.927.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4896266
Train loss (w/o reg) on all data: 0.48515415
Test loss (w/o reg) on all data: 0.311045
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.0718952e-05
Norm of the params: 9.457742
              Random: fixed  51 labels. Loss 0.31104. Accuracy 0.959.
### Flips: 1230, rs: 19, checks: 410
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3926049
Train loss (w/o reg) on all data: 0.38188785
Test loss (w/o reg) on all data: 0.22992054
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 7.2068215e-05
Norm of the params: 14.640377
     Influence (LOO): fixed 296 labels. Loss 0.22992. Accuracy 0.980.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31393623
Train loss (w/o reg) on all data: 0.29954478
Test loss (w/o reg) on all data: 0.22254136
Train acc on all data:  0.8589837101872113
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.0052953e-05
Norm of the params: 16.965523
                Loss: fixed 405 labels. Loss 0.22254. Accuracy 0.935.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47912723
Train loss (w/o reg) on all data: 0.4747723
Test loss (w/o reg) on all data: 0.29374927
Train acc on all data:  0.7734014101628981
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.7874298e-05
Norm of the params: 9.332661
              Random: fixed 103 labels. Loss 0.29375. Accuracy 0.962.
### Flips: 1230, rs: 19, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34913883
Train loss (w/o reg) on all data: 0.33836246
Test loss (w/o reg) on all data: 0.19877295
Train acc on all data:  0.8390469243860929
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 2.2162863e-05
Norm of the params: 14.680854
     Influence (LOO): fixed 405 labels. Loss 0.19877. Accuracy 0.983.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22058222
Train loss (w/o reg) on all data: 0.20190261
Test loss (w/o reg) on all data: 0.1721523
Train acc on all data:  0.9095550692924872
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.7329676e-06
Norm of the params: 19.32853
                Loss: fixed 604 labels. Loss 0.17215. Accuracy 0.948.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4642941
Train loss (w/o reg) on all data: 0.45981494
Test loss (w/o reg) on all data: 0.27307358
Train acc on all data:  0.787989302212497
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.4184443e-05
Norm of the params: 9.464864
              Random: fixed 160 labels. Loss 0.27307. Accuracy 0.974.
### Flips: 1230, rs: 19, checks: 820
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3122495
Train loss (w/o reg) on all data: 0.30107394
Test loss (w/o reg) on all data: 0.17632876
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.0516791e-05
Norm of the params: 14.950302
     Influence (LOO): fixed 488 labels. Loss 0.17633. Accuracy 0.983.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12972245
Train loss (w/o reg) on all data: 0.10858144
Test loss (w/o reg) on all data: 0.10672312
Train acc on all data:  0.9535618769754437
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.386494e-05
Norm of the params: 20.56259
                Loss: fixed 798 labels. Loss 0.10672. Accuracy 0.965.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4495845
Train loss (w/o reg) on all data: 0.44466016
Test loss (w/o reg) on all data: 0.25346372
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 9.81784e-06
Norm of the params: 9.924075
              Random: fixed 217 labels. Loss 0.25346. Accuracy 0.979.
### Flips: 1230, rs: 19, checks: 1025
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2673526
Train loss (w/o reg) on all data: 0.2560632
Test loss (w/o reg) on all data: 0.15088671
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.243371e-06
Norm of the params: 15.02625
     Influence (LOO): fixed 583 labels. Loss 0.15089. Accuracy 0.983.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059294656
Train loss (w/o reg) on all data: 0.04294224
Test loss (w/o reg) on all data: 0.052528944
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.945002e-06
Norm of the params: 18.084478
                Loss: fixed 958 labels. Loss 0.05253. Accuracy 0.988.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43379843
Train loss (w/o reg) on all data: 0.42835656
Test loss (w/o reg) on all data: 0.23710291
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 2.2722299e-05
Norm of the params: 10.432524
              Random: fixed 267 labels. Loss 0.23710. Accuracy 0.977.
### Flips: 1230, rs: 19, checks: 1230
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23693201
Train loss (w/o reg) on all data: 0.22590694
Test loss (w/o reg) on all data: 0.12849905
Train acc on all data:  0.900072939460248
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.8965766e-05
Norm of the params: 14.849296
     Influence (LOO): fixed 654 labels. Loss 0.12850. Accuracy 0.985.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033622563
Train loss (w/o reg) on all data: 0.021493226
Test loss (w/o reg) on all data: 0.04046124
Train acc on all data:  0.9927060539752006
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.020592e-06
Norm of the params: 15.575197
                Loss: fixed 1012 labels. Loss 0.04046. Accuracy 0.990.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4168246
Train loss (w/o reg) on all data: 0.41086355
Test loss (w/o reg) on all data: 0.22304589
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 4.0235096e-05
Norm of the params: 10.91885
              Random: fixed 320 labels. Loss 0.22305. Accuracy 0.977.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4954253
Train loss (w/o reg) on all data: 0.48987606
Test loss (w/o reg) on all data: 0.3329862
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 5.4992357e-05
Norm of the params: 10.534957
Flipped loss: 0.33299. Accuracy: 0.936
### Flips: 1230, rs: 20, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43075642
Train loss (w/o reg) on all data: 0.42181188
Test loss (w/o reg) on all data: 0.27771762
Train acc on all data:  0.7947969851689765
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.7677881e-05
Norm of the params: 13.375002
     Influence (LOO): fixed 158 labels. Loss 0.27772. Accuracy 0.947.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39617223
Train loss (w/o reg) on all data: 0.38409844
Test loss (w/o reg) on all data: 0.27370623
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.43409925e-05
Norm of the params: 15.539486
                Loss: fixed 205 labels. Loss 0.27371. Accuracy 0.925.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48512563
Train loss (w/o reg) on all data: 0.47955614
Test loss (w/o reg) on all data: 0.3115725
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.970136e-05
Norm of the params: 10.554143
              Random: fixed  51 labels. Loss 0.31157. Accuracy 0.944.
### Flips: 1230, rs: 20, checks: 410
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38221067
Train loss (w/o reg) on all data: 0.37093997
Test loss (w/o reg) on all data: 0.2389771
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.5594558e-05
Norm of the params: 15.0138
     Influence (LOO): fixed 282 labels. Loss 0.23898. Accuracy 0.953.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3032152
Train loss (w/o reg) on all data: 0.28541103
Test loss (w/o reg) on all data: 0.2165552
Train acc on all data:  0.8609287624604911
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 2.145216e-05
Norm of the params: 18.870169
                Loss: fixed 409 labels. Loss 0.21656. Accuracy 0.921.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4742791
Train loss (w/o reg) on all data: 0.4685724
Test loss (w/o reg) on all data: 0.29718226
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 6.0916773e-05
Norm of the params: 10.683341
              Random: fixed  94 labels. Loss 0.29718. Accuracy 0.951.
### Flips: 1230, rs: 20, checks: 615
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33950257
Train loss (w/o reg) on all data: 0.32853815
Test loss (w/o reg) on all data: 0.20042793
Train acc on all data:  0.8412351081935328
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.3503115e-05
Norm of the params: 14.808385
     Influence (LOO): fixed 393 labels. Loss 0.20043. Accuracy 0.965.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21366656
Train loss (w/o reg) on all data: 0.19217259
Test loss (w/o reg) on all data: 0.15541531
Train acc on all data:  0.9141745684415269
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 7.818876e-06
Norm of the params: 20.733538
                Loss: fixed 607 labels. Loss 0.15542. Accuracy 0.947.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46205446
Train loss (w/o reg) on all data: 0.45631203
Test loss (w/o reg) on all data: 0.2757871
Train acc on all data:  0.7843423292000973
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.3958065e-05
Norm of the params: 10.716733
              Random: fixed 148 labels. Loss 0.27579. Accuracy 0.961.
### Flips: 1230, rs: 20, checks: 820
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293643
Train loss (w/o reg) on all data: 0.2821251
Test loss (w/o reg) on all data: 0.16788426
Train acc on all data:  0.8660345246778507
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 6.3627845e-06
Norm of the params: 15.177552
     Influence (LOO): fixed 499 labels. Loss 0.16788. Accuracy 0.977.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13086975
Train loss (w/o reg) on all data: 0.108219326
Test loss (w/o reg) on all data: 0.091882624
Train acc on all data:  0.9557500607828835
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.6680136e-06
Norm of the params: 21.283997
                Loss: fixed 793 labels. Loss 0.09188. Accuracy 0.970.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44886795
Train loss (w/o reg) on all data: 0.44278032
Test loss (w/o reg) on all data: 0.2568386
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.653175e-05
Norm of the params: 11.034157
              Random: fixed 201 labels. Loss 0.25684. Accuracy 0.973.
### Flips: 1230, rs: 20, checks: 1025
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2574367
Train loss (w/o reg) on all data: 0.24564695
Test loss (w/o reg) on all data: 0.14215316
Train acc on all data:  0.8842693897398493
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 9.35146e-06
Norm of the params: 15.355612
     Influence (LOO): fixed 578 labels. Loss 0.14215. Accuracy 0.983.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06915779
Train loss (w/o reg) on all data: 0.05053554
Test loss (w/o reg) on all data: 0.04769276
Train acc on all data:  0.9824945295404814
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.9843665e-06
Norm of the params: 19.298832
                Loss: fixed 925 labels. Loss 0.04769. Accuracy 0.984.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43247217
Train loss (w/o reg) on all data: 0.42646083
Test loss (w/o reg) on all data: 0.24028632
Train acc on all data:  0.8071966934111354
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.0889393e-05
Norm of the params: 10.9648075
              Random: fixed 262 labels. Loss 0.24029. Accuracy 0.979.
### Flips: 1230, rs: 20, checks: 1230
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22598203
Train loss (w/o reg) on all data: 0.21398124
Test loss (w/o reg) on all data: 0.11772218
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 5.926952e-06
Norm of the params: 15.492436
     Influence (LOO): fixed 650 labels. Loss 0.11772. Accuracy 0.987.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044236112
Train loss (w/o reg) on all data: 0.029667307
Test loss (w/o reg) on all data: 0.02221721
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.01045e-06
Norm of the params: 17.069742
                Loss: fixed 988 labels. Loss 0.02222. Accuracy 0.996.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41489768
Train loss (w/o reg) on all data: 0.40858707
Test loss (w/o reg) on all data: 0.22544436
Train acc on all data:  0.8215414539265743
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.9258845e-05
Norm of the params: 11.234428
              Random: fixed 313 labels. Loss 0.22544. Accuracy 0.978.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49176192
Train loss (w/o reg) on all data: 0.48606423
Test loss (w/o reg) on all data: 0.35225502
Train acc on all data:  0.7539508874300996
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 5.3360447e-05
Norm of the params: 10.674919
Flipped loss: 0.35226. Accuracy: 0.921
### Flips: 1230, rs: 21, checks: 205
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43026397
Train loss (w/o reg) on all data: 0.42069733
Test loss (w/o reg) on all data: 0.29694036
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 1.40117e-05
Norm of the params: 13.832297
     Influence (LOO): fixed 157 labels. Loss 0.29694. Accuracy 0.935.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39439586
Train loss (w/o reg) on all data: 0.3825867
Test loss (w/o reg) on all data: 0.30272275
Train acc on all data:  0.8081692195477753
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 6.269727e-05
Norm of the params: 15.368257
                Loss: fixed 202 labels. Loss 0.30272. Accuracy 0.895.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48247635
Train loss (w/o reg) on all data: 0.47685295
Test loss (w/o reg) on all data: 0.33332276
Train acc on all data:  0.7648918064672988
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 1.641204e-05
Norm of the params: 10.60509
              Random: fixed  51 labels. Loss 0.33332. Accuracy 0.926.
### Flips: 1230, rs: 21, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38157603
Train loss (w/o reg) on all data: 0.37011185
Test loss (w/o reg) on all data: 0.2503776
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.0695159e-05
Norm of the params: 15.142112
     Influence (LOO): fixed 284 labels. Loss 0.25038. Accuracy 0.951.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30001795
Train loss (w/o reg) on all data: 0.28420484
Test loss (w/o reg) on all data: 0.25933176
Train acc on all data:  0.8597131047896912
Test acc on all data:   0.9008746355685131
Norm of the mean of gradients: 1.4050616e-05
Norm of the params: 17.78376
                Loss: fixed 402 labels. Loss 0.25933. Accuracy 0.901.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47156048
Train loss (w/o reg) on all data: 0.4659528
Test loss (w/o reg) on all data: 0.31460562
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.46282555e-05
Norm of the params: 10.590251
              Random: fixed 100 labels. Loss 0.31461. Accuracy 0.941.
### Flips: 1230, rs: 21, checks: 615
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33599243
Train loss (w/o reg) on all data: 0.32394627
Test loss (w/o reg) on all data: 0.21290022
Train acc on all data:  0.8468271334792122
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.3675562e-05
Norm of the params: 15.521701
     Influence (LOO): fixed 397 labels. Loss 0.21290. Accuracy 0.959.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20979956
Train loss (w/o reg) on all data: 0.19053534
Test loss (w/o reg) on all data: 0.19982699
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9310009718172984
Norm of the mean of gradients: 1.25678025e-05
Norm of the params: 19.628668
                Loss: fixed 596 labels. Loss 0.19983. Accuracy 0.931.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45987126
Train loss (w/o reg) on all data: 0.45425186
Test loss (w/o reg) on all data: 0.29808253
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 3.0760835e-05
Norm of the params: 10.601322
              Random: fixed 149 labels. Loss 0.29808. Accuracy 0.945.
### Flips: 1230, rs: 21, checks: 820
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29770976
Train loss (w/o reg) on all data: 0.28499678
Test loss (w/o reg) on all data: 0.18305433
Train acc on all data:  0.8650619985412108
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.1861758e-05
Norm of the params: 15.945528
     Influence (LOO): fixed 492 labels. Loss 0.18305. Accuracy 0.969.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13565221
Train loss (w/o reg) on all data: 0.115858644
Test loss (w/o reg) on all data: 0.14006364
Train acc on all data:  0.9450522732798444
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.688069e-06
Norm of the params: 19.89652
                Loss: fixed 770 labels. Loss 0.14006. Accuracy 0.953.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44420013
Train loss (w/o reg) on all data: 0.43837625
Test loss (w/o reg) on all data: 0.27963966
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 2.8630862e-05
Norm of the params: 10.792489
              Random: fixed 206 labels. Loss 0.27964. Accuracy 0.945.
### Flips: 1230, rs: 21, checks: 1025
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26596808
Train loss (w/o reg) on all data: 0.253657
Test loss (w/o reg) on all data: 0.15123074
Train acc on all data:  0.8820812059324095
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 7.5000626e-06
Norm of the params: 15.691452
     Influence (LOO): fixed 576 labels. Loss 0.15123. Accuracy 0.980.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07481564
Train loss (w/o reg) on all data: 0.05686137
Test loss (w/o reg) on all data: 0.08355781
Train acc on all data:  0.975443715049842
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.6824167e-06
Norm of the params: 18.949549
                Loss: fixed 911 labels. Loss 0.08356. Accuracy 0.974.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42941502
Train loss (w/o reg) on all data: 0.42301434
Test loss (w/o reg) on all data: 0.25663
Train acc on all data:  0.812302455628495
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 4.1982992e-05
Norm of the params: 11.314306
              Random: fixed 262 labels. Loss 0.25663. Accuracy 0.961.
### Flips: 1230, rs: 21, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23160233
Train loss (w/o reg) on all data: 0.21989992
Test loss (w/o reg) on all data: 0.123403534
Train acc on all data:  0.9012885971310479
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.0357936e-05
Norm of the params: 15.298627
     Influence (LOO): fixed 655 labels. Loss 0.12340. Accuracy 0.988.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04547131
Train loss (w/o reg) on all data: 0.031586297
Test loss (w/o reg) on all data: 0.056603193
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 8.5650004e-07
Norm of the params: 16.664343
                Loss: fixed 973 labels. Loss 0.05660. Accuracy 0.982.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4160917
Train loss (w/o reg) on all data: 0.4095217
Test loss (w/o reg) on all data: 0.24249889
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.2324181e-05
Norm of the params: 11.462986
              Random: fixed 307 labels. Loss 0.24250. Accuracy 0.970.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4987831
Train loss (w/o reg) on all data: 0.4941123
Test loss (w/o reg) on all data: 0.32084218
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 3.9054463e-05
Norm of the params: 9.665204
Flipped loss: 0.32084. Accuracy: 0.949
### Flips: 1230, rs: 22, checks: 205
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43323147
Train loss (w/o reg) on all data: 0.42368788
Test loss (w/o reg) on all data: 0.26787117
Train acc on all data:  0.7918794067590567
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.0844877e-05
Norm of the params: 13.815649
     Influence (LOO): fixed 162 labels. Loss 0.26787. Accuracy 0.950.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39949962
Train loss (w/o reg) on all data: 0.38907343
Test loss (w/o reg) on all data: 0.26920196
Train acc on all data:  0.8110867979576951
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 9.786515e-06
Norm of the params: 14.440361
                Loss: fixed 203 labels. Loss 0.26920. Accuracy 0.919.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48617932
Train loss (w/o reg) on all data: 0.4813909
Test loss (w/o reg) on all data: 0.3001715
Train acc on all data:  0.7697544371504984
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 4.7782672e-05
Norm of the params: 9.786154
              Random: fixed  60 labels. Loss 0.30017. Accuracy 0.959.
### Flips: 1230, rs: 22, checks: 410
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38340497
Train loss (w/o reg) on all data: 0.37297896
Test loss (w/o reg) on all data: 0.2275369
Train acc on all data:  0.825188426938974
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 3.0059718e-05
Norm of the params: 14.440221
     Influence (LOO): fixed 296 labels. Loss 0.22754. Accuracy 0.970.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30504096
Train loss (w/o reg) on all data: 0.2898404
Test loss (w/o reg) on all data: 0.22198308
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 2.4174118e-05
Norm of the params: 17.43591
                Loss: fixed 407 labels. Loss 0.22198. Accuracy 0.926.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47503132
Train loss (w/o reg) on all data: 0.47048652
Test loss (w/o reg) on all data: 0.27993745
Train acc on all data:  0.7802090931193776
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.8089822e-05
Norm of the params: 9.533946
              Random: fixed 113 labels. Loss 0.27994. Accuracy 0.966.
### Flips: 1230, rs: 22, checks: 615
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3403634
Train loss (w/o reg) on all data: 0.32911307
Test loss (w/o reg) on all data: 0.1972594
Train acc on all data:  0.8460977388767323
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 8.229216e-06
Norm of the params: 15.000227
     Influence (LOO): fixed 400 labels. Loss 0.19726. Accuracy 0.978.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20642021
Train loss (w/o reg) on all data: 0.18643914
Test loss (w/o reg) on all data: 0.1664464
Train acc on all data:  0.9146608315098468
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 4.6905034e-06
Norm of the params: 19.990534
                Loss: fixed 609 labels. Loss 0.16645. Accuracy 0.944.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4617291
Train loss (w/o reg) on all data: 0.45704874
Test loss (w/o reg) on all data: 0.26096556
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.2578256e-05
Norm of the params: 9.675096
              Random: fixed 168 labels. Loss 0.26097. Accuracy 0.974.
### Flips: 1230, rs: 22, checks: 820
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3031448
Train loss (w/o reg) on all data: 0.2913971
Test loss (w/o reg) on all data: 0.16936465
Train acc on all data:  0.8655482616095308
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.9873043e-05
Norm of the params: 15.328216
     Influence (LOO): fixed 490 labels. Loss 0.16936. Accuracy 0.979.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122375816
Train loss (w/o reg) on all data: 0.10070727
Test loss (w/o reg) on all data: 0.103906736
Train acc on all data:  0.9562363238512035
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 7.4567847e-06
Norm of the params: 20.81756
                Loss: fixed 796 labels. Loss 0.10391. Accuracy 0.965.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44615093
Train loss (w/o reg) on all data: 0.44107038
Test loss (w/o reg) on all data: 0.24559706
Train acc on all data:  0.8064672988086555
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.3918379e-05
Norm of the params: 10.0802355
              Random: fixed 224 labels. Loss 0.24560. Accuracy 0.976.
### Flips: 1230, rs: 22, checks: 1025
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27340955
Train loss (w/o reg) on all data: 0.2615153
Test loss (w/o reg) on all data: 0.14607282
Train acc on all data:  0.8823243374665695
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 8.047971e-06
Norm of the params: 15.423516
     Influence (LOO): fixed 564 labels. Loss 0.14607. Accuracy 0.987.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0638784
Train loss (w/o reg) on all data: 0.046415854
Test loss (w/o reg) on all data: 0.048395015
Train acc on all data:  0.9817651349380014
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 3.0921851e-06
Norm of the params: 18.688255
                Loss: fixed 936 labels. Loss 0.04840. Accuracy 0.983.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43198314
Train loss (w/o reg) on all data: 0.42666128
Test loss (w/o reg) on all data: 0.23188482
Train acc on all data:  0.8142475079017749
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.6999434e-05
Norm of the params: 10.316851
              Random: fixed 274 labels. Loss 0.23188. Accuracy 0.980.
### Flips: 1230, rs: 22, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2349242
Train loss (w/o reg) on all data: 0.22304896
Test loss (w/o reg) on all data: 0.12366014
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 9.223123e-06
Norm of the params: 15.411187
     Influence (LOO): fixed 644 labels. Loss 0.12366. Accuracy 0.986.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03785694
Train loss (w/o reg) on all data: 0.02498669
Test loss (w/o reg) on all data: 0.025368152
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 1.2957036e-06
Norm of the params: 16.043846
                Loss: fixed 988 labels. Loss 0.02537. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41870213
Train loss (w/o reg) on all data: 0.41344568
Test loss (w/o reg) on all data: 0.21975806
Train acc on all data:  0.825431558473134
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.4724632e-05
Norm of the params: 10.253225
              Random: fixed 319 labels. Loss 0.21976. Accuracy 0.982.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49610233
Train loss (w/o reg) on all data: 0.49092954
Test loss (w/o reg) on all data: 0.34909847
Train acc on all data:  0.7546802820325796
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 2.4077963e-05
Norm of the params: 10.171313
Flipped loss: 0.34910. Accuracy: 0.927
### Flips: 1230, rs: 23, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4322936
Train loss (w/o reg) on all data: 0.4226773
Test loss (w/o reg) on all data: 0.29576996
Train acc on all data:  0.7935813274981766
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 1.1794211e-05
Norm of the params: 13.868157
     Influence (LOO): fixed 163 labels. Loss 0.29577. Accuracy 0.938.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39867932
Train loss (w/o reg) on all data: 0.3878231
Test loss (w/o reg) on all data: 0.3004529
Train acc on all data:  0.8079260880136153
Test acc on all data:   0.9037900874635568
Norm of the mean of gradients: 1.4930913e-05
Norm of the params: 14.73514
                Loss: fixed 204 labels. Loss 0.30045. Accuracy 0.904.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4847195
Train loss (w/o reg) on all data: 0.47942725
Test loss (w/o reg) on all data: 0.32935417
Train acc on all data:  0.762703622659859
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 5.3258962e-05
Norm of the params: 10.288125
              Random: fixed  56 labels. Loss 0.32935. Accuracy 0.937.
### Flips: 1230, rs: 23, checks: 410
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38558865
Train loss (w/o reg) on all data: 0.37367707
Test loss (w/o reg) on all data: 0.2566312
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.2220778e-05
Norm of the params: 15.434754
     Influence (LOO): fixed 289 labels. Loss 0.25663. Accuracy 0.945.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30823082
Train loss (w/o reg) on all data: 0.2927486
Test loss (w/o reg) on all data: 0.25225574
Train acc on all data:  0.8582543155847313
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 1.1920927e-05
Norm of the params: 17.596716
                Loss: fixed 404 labels. Loss 0.25226. Accuracy 0.913.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47064415
Train loss (w/o reg) on all data: 0.46513295
Test loss (w/o reg) on all data: 0.31036142
Train acc on all data:  0.7782640408460978
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 6.0863073e-05
Norm of the params: 10.498759
              Random: fixed 116 labels. Loss 0.31036. Accuracy 0.948.
### Flips: 1230, rs: 23, checks: 615
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34678462
Train loss (w/o reg) on all data: 0.33514497
Test loss (w/o reg) on all data: 0.2166005
Train acc on all data:  0.8405057135910527
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 2.4697727e-05
Norm of the params: 15.257554
     Influence (LOO): fixed 397 labels. Loss 0.21660. Accuracy 0.956.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2167979
Train loss (w/o reg) on all data: 0.19742708
Test loss (w/o reg) on all data: 0.19263762
Train acc on all data:  0.9105275954291272
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 4.157699e-05
Norm of the params: 19.682896
                Loss: fixed 603 labels. Loss 0.19264. Accuracy 0.940.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45553616
Train loss (w/o reg) on all data: 0.4497876
Test loss (w/o reg) on all data: 0.29123095
Train acc on all data:  0.7913931436907367
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 0.00010043105
Norm of the params: 10.722467
              Random: fixed 174 labels. Loss 0.29123. Accuracy 0.952.
### Flips: 1230, rs: 23, checks: 820
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30175486
Train loss (w/o reg) on all data: 0.28944924
Test loss (w/o reg) on all data: 0.1803909
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.109048e-05
Norm of the params: 15.687965
     Influence (LOO): fixed 501 labels. Loss 0.18039. Accuracy 0.971.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13551825
Train loss (w/o reg) on all data: 0.11343998
Test loss (w/o reg) on all data: 0.12967423
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 4.5651905e-06
Norm of the params: 21.01346
                Loss: fixed 784 labels. Loss 0.12967. Accuracy 0.957.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4429563
Train loss (w/o reg) on all data: 0.4373621
Test loss (w/o reg) on all data: 0.26922446
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.673181e-05
Norm of the params: 10.57753
              Random: fixed 228 labels. Loss 0.26922. Accuracy 0.959.
### Flips: 1230, rs: 23, checks: 1025
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26530105
Train loss (w/o reg) on all data: 0.25316194
Test loss (w/o reg) on all data: 0.15200469
Train acc on all data:  0.8845125212740093
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 5.2383193e-06
Norm of the params: 15.581475
     Influence (LOO): fixed 588 labels. Loss 0.15200. Accuracy 0.977.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079168804
Train loss (w/o reg) on all data: 0.06047699
Test loss (w/o reg) on all data: 0.05755741
Train acc on all data:  0.975929978118162
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.3048844e-06
Norm of the params: 19.334846
                Loss: fixed 919 labels. Loss 0.05756. Accuracy 0.983.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4256083
Train loss (w/o reg) on all data: 0.41973266
Test loss (w/o reg) on all data: 0.25258955
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.2699664e-05
Norm of the params: 10.840332
              Random: fixed 282 labels. Loss 0.25259. Accuracy 0.967.
### Flips: 1230, rs: 23, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23143797
Train loss (w/o reg) on all data: 0.21942592
Test loss (w/o reg) on all data: 0.13024862
Train acc on all data:  0.9020179917335278
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 4.597397e-06
Norm of the params: 15.499705
     Influence (LOO): fixed 662 labels. Loss 0.13025. Accuracy 0.983.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04300254
Train loss (w/o reg) on all data: 0.028676046
Test loss (w/o reg) on all data: 0.027062234
Train acc on all data:  0.9897884755652808
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.5055875e-06
Norm of the params: 16.92719
                Loss: fixed 994 labels. Loss 0.02706. Accuracy 0.994.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41249812
Train loss (w/o reg) on all data: 0.40664294
Test loss (w/o reg) on all data: 0.23595913
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.020684e-05
Norm of the params: 10.821442
              Random: fixed 328 labels. Loss 0.23596. Accuracy 0.971.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4896999
Train loss (w/o reg) on all data: 0.4835923
Test loss (w/o reg) on all data: 0.3328841
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9261418853255587
Norm of the mean of gradients: 5.883449e-05
Norm of the params: 11.052241
Flipped loss: 0.33288. Accuracy: 0.926
### Flips: 1230, rs: 24, checks: 205
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42500713
Train loss (w/o reg) on all data: 0.41476536
Test loss (w/o reg) on all data: 0.2773836
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.8364819e-05
Norm of the params: 14.312074
     Influence (LOO): fixed 156 labels. Loss 0.27738. Accuracy 0.946.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39015853
Train loss (w/o reg) on all data: 0.37641796
Test loss (w/o reg) on all data: 0.26908726
Train acc on all data:  0.8130318502309749
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 1.47558485e-05
Norm of the params: 16.577429
                Loss: fixed 203 labels. Loss 0.26909. Accuracy 0.923.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4790836
Train loss (w/o reg) on all data: 0.4729912
Test loss (w/o reg) on all data: 0.3136082
Train acc on all data:  0.7653780695356188
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.6106873e-05
Norm of the params: 11.038472
              Random: fixed  50 labels. Loss 0.31361. Accuracy 0.943.
### Flips: 1230, rs: 24, checks: 410
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37823227
Train loss (w/o reg) on all data: 0.3667268
Test loss (w/o reg) on all data: 0.23415922
Train acc on all data:  0.8234865061998541
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.5183886e-05
Norm of the params: 15.169375
     Influence (LOO): fixed 286 labels. Loss 0.23416. Accuracy 0.957.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3037972
Train loss (w/o reg) on all data: 0.28567123
Test loss (w/o reg) on all data: 0.21585283
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 9.762428e-06
Norm of the params: 19.039925
                Loss: fixed 407 labels. Loss 0.21585. Accuracy 0.927.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.466504
Train loss (w/o reg) on all data: 0.46016347
Test loss (w/o reg) on all data: 0.29518944
Train acc on all data:  0.7789934354485777
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.1214693e-05
Norm of the params: 11.261015
              Random: fixed 107 labels. Loss 0.29519. Accuracy 0.946.
### Flips: 1230, rs: 24, checks: 615
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3370346
Train loss (w/o reg) on all data: 0.3247203
Test loss (w/o reg) on all data: 0.2032487
Train acc on all data:  0.8451252127400923
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.399026e-05
Norm of the params: 15.693523
     Influence (LOO): fixed 388 labels. Loss 0.20325. Accuracy 0.970.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21707107
Train loss (w/o reg) on all data: 0.19540021
Test loss (w/o reg) on all data: 0.15662597
Train acc on all data:  0.9097982008266472
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.4804014e-05
Norm of the params: 20.818678
                Loss: fixed 600 labels. Loss 0.15663. Accuracy 0.949.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45563355
Train loss (w/o reg) on all data: 0.44938254
Test loss (w/o reg) on all data: 0.2756076
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 4.2288062e-05
Norm of the params: 11.181249
              Random: fixed 157 labels. Loss 0.27561. Accuracy 0.960.
### Flips: 1230, rs: 24, checks: 820
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30230758
Train loss (w/o reg) on all data: 0.2899463
Test loss (w/o reg) on all data: 0.17164236
Train acc on all data:  0.8645757354728908
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.4871972e-05
Norm of the params: 15.72341
     Influence (LOO): fixed 482 labels. Loss 0.17164. Accuracy 0.974.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1366086
Train loss (w/o reg) on all data: 0.11393823
Test loss (w/o reg) on all data: 0.09755233
Train acc on all data:  0.9508874300996839
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 3.7356267e-06
Norm of the params: 21.29337
                Loss: fixed 777 labels. Loss 0.09755. Accuracy 0.973.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44362193
Train loss (w/o reg) on all data: 0.43717724
Test loss (w/o reg) on all data: 0.259114
Train acc on all data:  0.8008752735229759
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 3.3104316e-05
Norm of the params: 11.353136
              Random: fixed 206 labels. Loss 0.25911. Accuracy 0.976.
### Flips: 1230, rs: 24, checks: 1025
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26498175
Train loss (w/o reg) on all data: 0.25275767
Test loss (w/o reg) on all data: 0.14966643
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 3.1304666e-05
Norm of the params: 15.635916
     Influence (LOO): fixed 564 labels. Loss 0.14967. Accuracy 0.979.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07435261
Train loss (w/o reg) on all data: 0.056133863
Test loss (w/o reg) on all data: 0.051298488
Train acc on all data:  0.9793338195964016
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.9605496e-06
Norm of the params: 19.08861
                Loss: fixed 915 labels. Loss 0.05130. Accuracy 0.986.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42953947
Train loss (w/o reg) on all data: 0.42310497
Test loss (w/o reg) on all data: 0.23961525
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.2592199e-05
Norm of the params: 11.344151
              Random: fixed 256 labels. Loss 0.23962. Accuracy 0.980.
### Flips: 1230, rs: 24, checks: 1230
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22644185
Train loss (w/o reg) on all data: 0.21417734
Test loss (w/o reg) on all data: 0.122716166
Train acc on all data:  0.9029905178701677
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 5.4032826e-06
Norm of the params: 15.661738
     Influence (LOO): fixed 650 labels. Loss 0.12272. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03830815
Train loss (w/o reg) on all data: 0.025261462
Test loss (w/o reg) on all data: 0.022543004
Train acc on all data:  0.9905178701677607
Test acc on all data:   0.9951409135082604
Norm of the mean of gradients: 1.4708758e-06
Norm of the params: 16.153446
                Loss: fixed 992 labels. Loss 0.02254. Accuracy 0.995.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4143345
Train loss (w/o reg) on all data: 0.4079569
Test loss (w/o reg) on all data: 0.22583508
Train acc on all data:  0.8225139800632142
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.080399e-05
Norm of the params: 11.293907
              Random: fixed 310 labels. Loss 0.22584. Accuracy 0.983.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49517465
Train loss (w/o reg) on all data: 0.48954788
Test loss (w/o reg) on all data: 0.33931044
Train acc on all data:  0.7537077558959397
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.0596502e-05
Norm of the params: 10.608266
Flipped loss: 0.33931. Accuracy: 0.937
### Flips: 1230, rs: 25, checks: 205
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4286913
Train loss (w/o reg) on all data: 0.41875485
Test loss (w/o reg) on all data: 0.29198253
Train acc on all data:  0.7911500121565767
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.6003029e-05
Norm of the params: 14.097118
     Influence (LOO): fixed 162 labels. Loss 0.29198. Accuracy 0.947.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3926891
Train loss (w/o reg) on all data: 0.37952504
Test loss (w/o reg) on all data: 0.29208815
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9164237123420796
Norm of the mean of gradients: 1.6501985e-05
Norm of the params: 16.225952
                Loss: fixed 205 labels. Loss 0.29209. Accuracy 0.916.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48336288
Train loss (w/o reg) on all data: 0.47754094
Test loss (w/o reg) on all data: 0.3230644
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 2.398658e-05
Norm of the params: 10.790675
              Random: fixed  57 labels. Loss 0.32306. Accuracy 0.938.
### Flips: 1230, rs: 25, checks: 410
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38142544
Train loss (w/o reg) on all data: 0.370085
Test loss (w/o reg) on all data: 0.24351916
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 9.505347e-06
Norm of the params: 15.060172
     Influence (LOO): fixed 294 labels. Loss 0.24352. Accuracy 0.963.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30429482
Train loss (w/o reg) on all data: 0.28633922
Test loss (w/o reg) on all data: 0.23723325
Train acc on all data:  0.8572817894480914
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 9.253903e-06
Norm of the params: 18.950245
                Loss: fixed 407 labels. Loss 0.23723. Accuracy 0.921.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47002938
Train loss (w/o reg) on all data: 0.46381256
Test loss (w/o reg) on all data: 0.30058578
Train acc on all data:  0.775346462436178
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 1.1002233e-05
Norm of the params: 11.150623
              Random: fixed 115 labels. Loss 0.30059. Accuracy 0.948.
### Flips: 1230, rs: 25, checks: 615
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34019193
Train loss (w/o reg) on all data: 0.32813665
Test loss (w/o reg) on all data: 0.21353997
Train acc on all data:  0.8458546073425723
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 7.583256e-06
Norm of the params: 15.52757
     Influence (LOO): fixed 397 labels. Loss 0.21354. Accuracy 0.963.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2157282
Train loss (w/o reg) on all data: 0.19381613
Test loss (w/o reg) on all data: 0.17536661
Train acc on all data:  0.9073668854850474
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.054826e-05
Norm of the params: 20.934214
                Loss: fixed 609 labels. Loss 0.17537. Accuracy 0.943.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45597312
Train loss (w/o reg) on all data: 0.44973505
Test loss (w/o reg) on all data: 0.28121987
Train acc on all data:  0.7867736445416971
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.6798596e-05
Norm of the params: 11.169668
              Random: fixed 171 labels. Loss 0.28122. Accuracy 0.958.
### Flips: 1230, rs: 25, checks: 820
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30027097
Train loss (w/o reg) on all data: 0.28782287
Test loss (w/o reg) on all data: 0.18244004
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.6104377e-05
Norm of the params: 15.778532
     Influence (LOO): fixed 489 labels. Loss 0.18244. Accuracy 0.973.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13129334
Train loss (w/o reg) on all data: 0.10783482
Test loss (w/o reg) on all data: 0.1111666
Train acc on all data:  0.9552637977145636
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 8.148647e-06
Norm of the params: 21.660343
                Loss: fixed 798 labels. Loss 0.11117. Accuracy 0.969.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44036493
Train loss (w/o reg) on all data: 0.43398386
Test loss (w/o reg) on all data: 0.2581312
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 9.663387e-06
Norm of the params: 11.296956
              Random: fixed 237 labels. Loss 0.25813. Accuracy 0.966.
### Flips: 1230, rs: 25, checks: 1025
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2678728
Train loss (w/o reg) on all data: 0.2551896
Test loss (w/o reg) on all data: 0.15988714
Train acc on all data:  0.8791636275224897
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 6.6312386e-06
Norm of the params: 15.926836
     Influence (LOO): fixed 560 labels. Loss 0.15989. Accuracy 0.974.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07493505
Train loss (w/o reg) on all data: 0.054854166
Test loss (w/o reg) on all data: 0.061083987
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 5.3725753e-06
Norm of the params: 20.0404
                Loss: fixed 926 labels. Loss 0.06108. Accuracy 0.983.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42619595
Train loss (w/o reg) on all data: 0.41986462
Test loss (w/o reg) on all data: 0.24065243
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.2450907e-05
Norm of the params: 11.252854
              Random: fixed 289 labels. Loss 0.24065. Accuracy 0.976.
### Flips: 1230, rs: 25, checks: 1230
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23449051
Train loss (w/o reg) on all data: 0.22192872
Test loss (w/o reg) on all data: 0.13244198
Train acc on all data:  0.8961828349136883
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.4491724e-05
Norm of the params: 15.850423
     Influence (LOO): fixed 634 labels. Loss 0.13244. Accuracy 0.982.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048503824
Train loss (w/o reg) on all data: 0.032541204
Test loss (w/o reg) on all data: 0.03535535
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.4725422e-06
Norm of the params: 17.867638
                Loss: fixed 988 labels. Loss 0.03536. Accuracy 0.991.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41341838
Train loss (w/o reg) on all data: 0.40706074
Test loss (w/o reg) on all data: 0.22589126
Train acc on all data:  0.8232433746656942
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.1616848e-05
Norm of the params: 11.276207
              Random: fixed 331 labels. Loss 0.22589. Accuracy 0.975.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4901399
Train loss (w/o reg) on all data: 0.48440722
Test loss (w/o reg) on all data: 0.33414897
Train acc on all data:  0.7556528081692195
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 5.1181356e-05
Norm of the params: 10.707635
Flipped loss: 0.33415. Accuracy: 0.938
### Flips: 1230, rs: 26, checks: 205
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42797163
Train loss (w/o reg) on all data: 0.4184561
Test loss (w/o reg) on all data: 0.27967575
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.0342833e-05
Norm of the params: 13.795317
     Influence (LOO): fixed 159 labels. Loss 0.27968. Accuracy 0.948.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39190874
Train loss (w/o reg) on all data: 0.37974524
Test loss (w/o reg) on all data: 0.27629757
Train acc on all data:  0.8101142718210552
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 3.8384565e-05
Norm of the params: 15.597108
                Loss: fixed 204 labels. Loss 0.27630. Accuracy 0.912.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47782245
Train loss (w/o reg) on all data: 0.47205067
Test loss (w/o reg) on all data: 0.3113254
Train acc on all data:  0.7704838317529784
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 7.9116166e-05
Norm of the params: 10.744101
              Random: fixed  62 labels. Loss 0.31133. Accuracy 0.953.
### Flips: 1230, rs: 26, checks: 410
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3792752
Train loss (w/o reg) on all data: 0.36827943
Test loss (w/o reg) on all data: 0.232228
Train acc on all data:  0.8230002431315342
Test acc on all data:   0.9630709426627794
Norm of the mean of gradients: 1.0802936e-05
Norm of the params: 14.829549
     Influence (LOO): fixed 295 labels. Loss 0.23223. Accuracy 0.963.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2987111
Train loss (w/o reg) on all data: 0.28158516
Test loss (w/o reg) on all data: 0.22264326
Train acc on all data:  0.861901288597131
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 1.00737125e-05
Norm of the params: 18.507256
                Loss: fixed 405 labels. Loss 0.22264. Accuracy 0.925.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46685132
Train loss (w/o reg) on all data: 0.46133325
Test loss (w/o reg) on all data: 0.29208383
Train acc on all data:  0.7823972769268174
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.264375e-05
Norm of the params: 10.505325
              Random: fixed 120 labels. Loss 0.29208. Accuracy 0.961.
### Flips: 1230, rs: 26, checks: 615
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33642808
Train loss (w/o reg) on all data: 0.32507244
Test loss (w/o reg) on all data: 0.19879273
Train acc on all data:  0.8475565280816922
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.4817406e-05
Norm of the params: 15.070252
     Influence (LOO): fixed 399 labels. Loss 0.19879. Accuracy 0.969.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20768465
Train loss (w/o reg) on all data: 0.18656163
Test loss (w/o reg) on all data: 0.16560015
Train acc on all data:  0.9110138584974471
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 9.875029e-06
Norm of the params: 20.553843
                Loss: fixed 608 labels. Loss 0.16560. Accuracy 0.944.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45435464
Train loss (w/o reg) on all data: 0.44870394
Test loss (w/o reg) on all data: 0.27855614
Train acc on all data:  0.788475565280817
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 9.99938e-06
Norm of the params: 10.630795
              Random: fixed 166 labels. Loss 0.27856. Accuracy 0.961.
### Flips: 1230, rs: 26, checks: 820
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29099748
Train loss (w/o reg) on all data: 0.27846208
Test loss (w/o reg) on all data: 0.1673412
Train acc on all data:  0.8699246292244104
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.16101955e-05
Norm of the params: 15.833754
     Influence (LOO): fixed 507 labels. Loss 0.16734. Accuracy 0.975.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12654059
Train loss (w/o reg) on all data: 0.104786694
Test loss (w/o reg) on all data: 0.0945053
Train acc on all data:  0.9542912715779237
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 1.9442006e-05
Norm of the params: 20.858522
                Loss: fixed 795 labels. Loss 0.09451. Accuracy 0.972.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44032234
Train loss (w/o reg) on all data: 0.4347438
Test loss (w/o reg) on all data: 0.2554707
Train acc on all data:  0.8037928519328957
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 5.2882064e-05
Norm of the params: 10.562723
              Random: fixed 226 labels. Loss 0.25547. Accuracy 0.972.
### Flips: 1230, rs: 26, checks: 1025
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25108233
Train loss (w/o reg) on all data: 0.23841748
Test loss (w/o reg) on all data: 0.14097314
Train acc on all data:  0.8903476780938487
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 9.361331e-06
Norm of the params: 15.915311
     Influence (LOO): fixed 588 labels. Loss 0.14097. Accuracy 0.980.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06836124
Train loss (w/o reg) on all data: 0.0508085
Test loss (w/o reg) on all data: 0.059967425
Train acc on all data:  0.9795769511305616
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 4.0953146e-06
Norm of the params: 18.736458
                Loss: fixed 923 labels. Loss 0.05997. Accuracy 0.976.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42468914
Train loss (w/o reg) on all data: 0.41915217
Test loss (w/o reg) on all data: 0.23427372
Train acc on all data:  0.8164356917092147
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.40753855e-05
Norm of the params: 10.523293
              Random: fixed 282 labels. Loss 0.23427. Accuracy 0.984.
### Flips: 1230, rs: 26, checks: 1230
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21582532
Train loss (w/o reg) on all data: 0.20301992
Test loss (w/o reg) on all data: 0.114869796
Train acc on all data:  0.9066374908825675
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 2.3686296e-05
Norm of the params: 16.003378
     Influence (LOO): fixed 659 labels. Loss 0.11487. Accuracy 0.986.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039429262
Train loss (w/o reg) on all data: 0.026601344
Test loss (w/o reg) on all data: 0.033433944
Train acc on all data:  0.9900316070994408
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 2.2417698e-06
Norm of the params: 16.01744
                Loss: fixed 986 labels. Loss 0.03343. Accuracy 0.990.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40846324
Train loss (w/o reg) on all data: 0.4029881
Test loss (w/o reg) on all data: 0.22042103
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.199951e-05
Norm of the params: 10.464353
              Random: fixed 337 labels. Loss 0.22042. Accuracy 0.981.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4989986
Train loss (w/o reg) on all data: 0.49445987
Test loss (w/o reg) on all data: 0.33934498
Train acc on all data:  0.7515195720884998
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 2.5453295e-05
Norm of the params: 9.527578
Flipped loss: 0.33934. Accuracy: 0.942
### Flips: 1230, rs: 27, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42929736
Train loss (w/o reg) on all data: 0.4194877
Test loss (w/o reg) on all data: 0.28183958
Train acc on all data:  0.7964989059080962
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 4.176684e-05
Norm of the params: 14.006881
     Influence (LOO): fixed 162 labels. Loss 0.28184. Accuracy 0.948.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3978272
Train loss (w/o reg) on all data: 0.38688585
Test loss (w/o reg) on all data: 0.28663835
Train acc on all data:  0.8040359834670556
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 3.68869e-05
Norm of the params: 14.792808
                Loss: fixed 204 labels. Loss 0.28664. Accuracy 0.917.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48485166
Train loss (w/o reg) on all data: 0.47997898
Test loss (w/o reg) on all data: 0.31857526
Train acc on all data:  0.7670799902747386
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 1.1937392e-05
Norm of the params: 9.871858
              Random: fixed  58 labels. Loss 0.31858. Accuracy 0.949.
### Flips: 1230, rs: 27, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3861494
Train loss (w/o reg) on all data: 0.37576348
Test loss (w/o reg) on all data: 0.23796962
Train acc on all data:  0.8203257962557744
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.5107087e-06
Norm of the params: 14.412438
     Influence (LOO): fixed 285 labels. Loss 0.23797. Accuracy 0.970.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30715212
Train loss (w/o reg) on all data: 0.29262763
Test loss (w/o reg) on all data: 0.23234065
Train acc on all data:  0.8575249209822514
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 4.354177e-06
Norm of the params: 17.043756
                Loss: fixed 407 labels. Loss 0.23234. Accuracy 0.919.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47197402
Train loss (w/o reg) on all data: 0.46695185
Test loss (w/o reg) on all data: 0.2990575
Train acc on all data:  0.7787503039144177
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 3.270608e-05
Norm of the params: 10.022133
              Random: fixed 114 labels. Loss 0.29906. Accuracy 0.959.
### Flips: 1230, rs: 27, checks: 615
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3429921
Train loss (w/o reg) on all data: 0.33112073
Test loss (w/o reg) on all data: 0.20754384
Train acc on all data:  0.8424507658643327
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 4.0599418e-05
Norm of the params: 15.408669
     Influence (LOO): fixed 392 labels. Loss 0.20754. Accuracy 0.970.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21417649
Train loss (w/o reg) on all data: 0.19576797
Test loss (w/o reg) on all data: 0.17884544
Train acc on all data:  0.9080962800875274
Test acc on all data:   0.9378036929057337
Norm of the mean of gradients: 7.846308e-06
Norm of the params: 19.187765
                Loss: fixed 608 labels. Loss 0.17885. Accuracy 0.938.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45751655
Train loss (w/o reg) on all data: 0.4522983
Test loss (w/o reg) on all data: 0.28121018
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9601554907677357
Norm of the mean of gradients: 1.3295311e-05
Norm of the params: 10.215906
              Random: fixed 168 labels. Loss 0.28121. Accuracy 0.960.
### Flips: 1230, rs: 27, checks: 820
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3046902
Train loss (w/o reg) on all data: 0.292634
Test loss (w/o reg) on all data: 0.17548281
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.4651028e-05
Norm of the params: 15.528162
     Influence (LOO): fixed 486 labels. Loss 0.17548. Accuracy 0.982.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12226589
Train loss (w/o reg) on all data: 0.10285473
Test loss (w/o reg) on all data: 0.1129435
Train acc on all data:  0.9555069292487236
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 6.21435e-06
Norm of the params: 19.703382
                Loss: fixed 809 labels. Loss 0.11294. Accuracy 0.959.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4473365
Train loss (w/o reg) on all data: 0.4422339
Test loss (w/o reg) on all data: 0.26289955
Train acc on all data:  0.7986870897155361
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 3.797733e-05
Norm of the params: 10.1020775
              Random: fixed 209 labels. Loss 0.26290. Accuracy 0.974.
### Flips: 1230, rs: 27, checks: 1025
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27222013
Train loss (w/o reg) on all data: 0.2608692
Test loss (w/o reg) on all data: 0.15157883
Train acc on all data:  0.8779479698516898
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 9.482285e-06
Norm of the params: 15.067142
     Influence (LOO): fixed 561 labels. Loss 0.15158. Accuracy 0.987.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04951642
Train loss (w/o reg) on all data: 0.034308944
Test loss (w/o reg) on all data: 0.03304334
Train acc on all data:  0.987600291757841
Test acc on all data:   0.9902818270165209
Norm of the mean of gradients: 6.669324e-06
Norm of the params: 17.439884
                Loss: fixed 960 labels. Loss 0.03304. Accuracy 0.990.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4306203
Train loss (w/o reg) on all data: 0.42539582
Test loss (w/o reg) on all data: 0.2473686
Train acc on all data:  0.8115730610260151
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 4.977895e-05
Norm of the params: 10.222029
              Random: fixed 264 labels. Loss 0.24737. Accuracy 0.978.
### Flips: 1230, rs: 27, checks: 1230
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23921122
Train loss (w/o reg) on all data: 0.22825444
Test loss (w/o reg) on all data: 0.12730113
Train acc on all data:  0.8973984925844882
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 8.107679e-06
Norm of the params: 14.803227
     Influence (LOO): fixed 637 labels. Loss 0.12730. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02811399
Train loss (w/o reg) on all data: 0.01686486
Test loss (w/o reg) on all data: 0.016768368
Train acc on all data:  0.9939217116460005
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 1.2424733e-06
Norm of the params: 14.999419
                Loss: fixed 1006 labels. Loss 0.01677. Accuracy 0.994.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41703182
Train loss (w/o reg) on all data: 0.41184926
Test loss (w/o reg) on all data: 0.23072436
Train acc on all data:  0.8220277169948942
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4961268e-05
Norm of the params: 10.180927
              Random: fixed 312 labels. Loss 0.23072. Accuracy 0.981.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49779424
Train loss (w/o reg) on all data: 0.4932338
Test loss (w/o reg) on all data: 0.3392464
Train acc on all data:  0.75103330902018
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 5.5529643e-05
Norm of the params: 9.550336
Flipped loss: 0.33925. Accuracy: 0.927
### Flips: 1230, rs: 28, checks: 205
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42687702
Train loss (w/o reg) on all data: 0.41710675
Test loss (w/o reg) on all data: 0.28328982
Train acc on all data:  0.7909068806224168
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 3.566712e-05
Norm of the params: 13.97876
     Influence (LOO): fixed 166 labels. Loss 0.28329. Accuracy 0.937.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39680466
Train loss (w/o reg) on all data: 0.38655102
Test loss (w/o reg) on all data: 0.28736892
Train acc on all data:  0.8020909311937758
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 2.8060535e-05
Norm of the params: 14.320371
                Loss: fixed 203 labels. Loss 0.28737. Accuracy 0.913.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48339623
Train loss (w/o reg) on all data: 0.47842807
Test loss (w/o reg) on all data: 0.3171355
Train acc on all data:  0.7641624118648188
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.4591759e-05
Norm of the params: 9.968112
              Random: fixed  59 labels. Loss 0.31714. Accuracy 0.934.
### Flips: 1230, rs: 28, checks: 410
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3870569
Train loss (w/o reg) on all data: 0.37554276
Test loss (w/o reg) on all data: 0.24652429
Train acc on all data:  0.8149769025042548
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 1.5441201e-05
Norm of the params: 15.175073
     Influence (LOO): fixed 275 labels. Loss 0.24652. Accuracy 0.950.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30588734
Train loss (w/o reg) on all data: 0.29073712
Test loss (w/o reg) on all data: 0.23527591
Train acc on all data:  0.8541210795040116
Test acc on all data:   0.9154518950437318
Norm of the mean of gradients: 4.867589e-05
Norm of the params: 17.407028
                Loss: fixed 406 labels. Loss 0.23528. Accuracy 0.915.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47213185
Train loss (w/o reg) on all data: 0.46704605
Test loss (w/o reg) on all data: 0.29956213
Train acc on all data:  0.7780209093119378
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 2.618602e-05
Norm of the params: 10.085435
              Random: fixed 109 labels. Loss 0.29956. Accuracy 0.948.
### Flips: 1230, rs: 28, checks: 615
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3485126
Train loss (w/o reg) on all data: 0.3361665
Test loss (w/o reg) on all data: 0.20828527
Train acc on all data:  0.838317529783613
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 2.733112e-05
Norm of the params: 15.713745
     Influence (LOO): fixed 379 labels. Loss 0.20829. Accuracy 0.967.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21620809
Train loss (w/o reg) on all data: 0.19696419
Test loss (w/o reg) on all data: 0.17282309
Train acc on all data:  0.9088256746900073
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 1.0800641e-05
Norm of the params: 19.618303
                Loss: fixed 605 labels. Loss 0.17282. Accuracy 0.939.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4576745
Train loss (w/o reg) on all data: 0.45240915
Test loss (w/o reg) on all data: 0.27570832
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.1949689e-05
Norm of the params: 10.261921
              Random: fixed 169 labels. Loss 0.27571. Accuracy 0.957.
### Flips: 1230, rs: 28, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3057645
Train loss (w/o reg) on all data: 0.29253796
Test loss (w/o reg) on all data: 0.17953974
Train acc on all data:  0.8640894724045709
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.082606e-05
Norm of the params: 16.264406
     Influence (LOO): fixed 481 labels. Loss 0.17954. Accuracy 0.971.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12869684
Train loss (w/o reg) on all data: 0.10748447
Test loss (w/o reg) on all data: 0.11876505
Train acc on all data:  0.9552637977145636
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 2.697893e-06
Norm of the params: 20.597271
                Loss: fixed 791 labels. Loss 0.11877. Accuracy 0.958.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44383198
Train loss (w/o reg) on all data: 0.4387181
Test loss (w/o reg) on all data: 0.25590363
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 2.5877758e-05
Norm of the params: 10.11324
              Random: fixed 224 labels. Loss 0.25590. Accuracy 0.968.
### Flips: 1230, rs: 28, checks: 1025
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27052823
Train loss (w/o reg) on all data: 0.25799832
Test loss (w/o reg) on all data: 0.15437292
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.2578782e-05
Norm of the params: 15.830287
     Influence (LOO): fixed 567 labels. Loss 0.15437. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06853402
Train loss (w/o reg) on all data: 0.049613737
Test loss (w/o reg) on all data: 0.07146304
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 5.7721977e-06
Norm of the params: 19.45265
                Loss: fixed 928 labels. Loss 0.07146. Accuracy 0.978.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43129224
Train loss (w/o reg) on all data: 0.42607236
Test loss (w/o reg) on all data: 0.23994097
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 4.247952e-05
Norm of the params: 10.217508
              Random: fixed 268 labels. Loss 0.23994. Accuracy 0.971.
### Flips: 1230, rs: 28, checks: 1230
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23311518
Train loss (w/o reg) on all data: 0.22036533
Test loss (w/o reg) on all data: 0.12709539
Train acc on all data:  0.899100413323608
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.1325838e-05
Norm of the params: 15.968629
     Influence (LOO): fixed 648 labels. Loss 0.12710. Accuracy 0.986.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045498464
Train loss (w/o reg) on all data: 0.030575486
Test loss (w/o reg) on all data: 0.039035138
Train acc on all data:  0.9890590809628009
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 2.064691e-06
Norm of the params: 17.275982
                Loss: fixed 984 labels. Loss 0.03904. Accuracy 0.985.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41597304
Train loss (w/o reg) on all data: 0.4106334
Test loss (w/o reg) on all data: 0.2240509
Train acc on all data:  0.8217845854607343
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.1905372e-05
Norm of the params: 10.33407
              Random: fixed 322 labels. Loss 0.22405. Accuracy 0.976.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4937567
Train loss (w/o reg) on all data: 0.4890252
Test loss (w/o reg) on all data: 0.34426504
Train acc on all data:  0.7551665451008995
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 1.3989449e-05
Norm of the params: 9.727813
Flipped loss: 0.34427. Accuracy: 0.943
### Flips: 1230, rs: 29, checks: 205
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4258653
Train loss (w/o reg) on all data: 0.4157815
Test loss (w/o reg) on all data: 0.29430634
Train acc on all data:  0.7933381959640166
Test acc on all data:   0.9465500485908649
Norm of the mean of gradients: 1.859785e-05
Norm of the params: 14.20126
     Influence (LOO): fixed 161 labels. Loss 0.29431. Accuracy 0.947.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3916729
Train loss (w/o reg) on all data: 0.37990192
Test loss (w/o reg) on all data: 0.29536963
Train acc on all data:  0.8091417456844152
Test acc on all data:   0.9193391642371235
Norm of the mean of gradients: 3.185314e-05
Norm of the params: 15.343389
                Loss: fixed 203 labels. Loss 0.29537. Accuracy 0.919.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4836088
Train loss (w/o reg) on all data: 0.4783953
Test loss (w/o reg) on all data: 0.3299656
Train acc on all data:  0.7651349380014588
Test acc on all data:   0.9494655004859086
Norm of the mean of gradients: 2.4955662e-05
Norm of the params: 10.211276
              Random: fixed  45 labels. Loss 0.32997. Accuracy 0.949.
### Flips: 1230, rs: 29, checks: 410
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37838733
Train loss (w/o reg) on all data: 0.3662819
Test loss (w/o reg) on all data: 0.254913
Train acc on all data:  0.8222708485290542
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 3.4495086e-05
Norm of the params: 15.559841
     Influence (LOO): fixed 287 labels. Loss 0.25491. Accuracy 0.948.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30454898
Train loss (w/o reg) on all data: 0.28764847
Test loss (w/o reg) on all data: 0.24646401
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 6.3144767e-06
Norm of the params: 18.38506
                Loss: fixed 406 labels. Loss 0.24646. Accuracy 0.924.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4722303
Train loss (w/o reg) on all data: 0.46699843
Test loss (w/o reg) on all data: 0.3098675
Train acc on all data:  0.7758327255044979
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 4.477176e-05
Norm of the params: 10.22922
              Random: fixed  98 labels. Loss 0.30987. Accuracy 0.952.
### Flips: 1230, rs: 29, checks: 615
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33540043
Train loss (w/o reg) on all data: 0.32229543
Test loss (w/o reg) on all data: 0.21997859
Train acc on all data:  0.8429370289326525
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 2.8940154e-05
Norm of the params: 16.189505
     Influence (LOO): fixed 391 labels. Loss 0.21998. Accuracy 0.955.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21221733
Train loss (w/o reg) on all data: 0.19174762
Test loss (w/o reg) on all data: 0.18850823
Train acc on all data:  0.912472647702407
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 4.431731e-06
Norm of the params: 20.233496
                Loss: fixed 609 labels. Loss 0.18851. Accuracy 0.941.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45948502
Train loss (w/o reg) on all data: 0.45423248
Test loss (w/o reg) on all data: 0.2915837
Train acc on all data:  0.7855579868708972
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.3885877e-05
Norm of the params: 10.249417
              Random: fixed 157 labels. Loss 0.29158. Accuracy 0.953.
### Flips: 1230, rs: 29, checks: 820
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29963627
Train loss (w/o reg) on all data: 0.28602052
Test loss (w/o reg) on all data: 0.18077289
Train acc on all data:  0.862387551665451
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4286533e-05
Norm of the params: 16.50198
     Influence (LOO): fixed 486 labels. Loss 0.18077. Accuracy 0.979.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12777653
Train loss (w/o reg) on all data: 0.10667431
Test loss (w/o reg) on all data: 0.109175935
Train acc on all data:  0.9542912715779237
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.10813135e-05
Norm of the params: 20.54372
                Loss: fixed 795 labels. Loss 0.10918. Accuracy 0.956.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44417134
Train loss (w/o reg) on all data: 0.43898913
Test loss (w/o reg) on all data: 0.2654327
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 6.416596e-05
Norm of the params: 10.180569
              Random: fixed 219 labels. Loss 0.26543. Accuracy 0.964.
### Flips: 1230, rs: 29, checks: 1025
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25949436
Train loss (w/o reg) on all data: 0.24617544
Test loss (w/o reg) on all data: 0.1468683
Train acc on all data:  0.886700705081449
Test acc on all data:   0.9863945578231292
Norm of the mean of gradients: 1.8000002e-05
Norm of the params: 16.321098
     Influence (LOO): fixed 584 labels. Loss 0.14687. Accuracy 0.986.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058181856
Train loss (w/o reg) on all data: 0.041795615
Test loss (w/o reg) on all data: 0.050128616
Train acc on all data:  0.9854121079504011
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 2.6888722e-06
Norm of the params: 18.103172
                Loss: fixed 946 labels. Loss 0.05013. Accuracy 0.982.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42849743
Train loss (w/o reg) on all data: 0.42315197
Test loss (w/o reg) on all data: 0.2496211
Train acc on all data:  0.8132749817651349
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.0957587e-05
Norm of the params: 10.339687
              Random: fixed 275 labels. Loss 0.24962. Accuracy 0.969.
### Flips: 1230, rs: 29, checks: 1230
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22486594
Train loss (w/o reg) on all data: 0.21219562
Test loss (w/o reg) on all data: 0.122301154
Train acc on all data:  0.9051787016776076
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 7.046022e-06
Norm of the params: 15.918748
     Influence (LOO): fixed 663 labels. Loss 0.12230. Accuracy 0.991.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031791523
Train loss (w/o reg) on all data: 0.01877751
Test loss (w/o reg) on all data: 0.032501757
Train acc on all data:  0.9944079747143204
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.302823e-06
Norm of the params: 16.133204
                Loss: fixed 998 labels. Loss 0.03250. Accuracy 0.991.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40956977
Train loss (w/o reg) on all data: 0.40363258
Test loss (w/o reg) on all data: 0.23391981
Train acc on all data:  0.8261609530756139
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.1284012e-05
Norm of the params: 10.896961
              Random: fixed 327 labels. Loss 0.23392. Accuracy 0.967.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4995212
Train loss (w/o reg) on all data: 0.49544656
Test loss (w/o reg) on all data: 0.33927613
Train acc on all data:  0.75079017748602
Test acc on all data:   0.9397473275024295
Norm of the mean of gradients: 3.2295702e-05
Norm of the params: 9.027328
Flipped loss: 0.33928. Accuracy: 0.940
### Flips: 1230, rs: 30, checks: 205
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4367519
Train loss (w/o reg) on all data: 0.42746592
Test loss (w/o reg) on all data: 0.29569155
Train acc on all data:  0.7901774860199368
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 3.0034895e-05
Norm of the params: 13.6279
     Influence (LOO): fixed 158 labels. Loss 0.29569. Accuracy 0.937.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40052038
Train loss (w/o reg) on all data: 0.39044714
Test loss (w/o reg) on all data: 0.29485002
Train acc on all data:  0.8011184050571359
Test acc on all data:   0.9037900874635568
Norm of the mean of gradients: 3.2142376e-05
Norm of the params: 14.193843
                Loss: fixed 205 labels. Loss 0.29485. Accuracy 0.904.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48710585
Train loss (w/o reg) on all data: 0.48269048
Test loss (w/o reg) on all data: 0.31629243
Train acc on all data:  0.7644055433989788
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 4.880533e-05
Norm of the params: 9.397195
              Random: fixed  56 labels. Loss 0.31629. Accuracy 0.954.
### Flips: 1230, rs: 30, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39054674
Train loss (w/o reg) on all data: 0.3801371
Test loss (w/o reg) on all data: 0.2529475
Train acc on all data:  0.8171650863116946
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 5.0563387e-05
Norm of the params: 14.428894
     Influence (LOO): fixed 281 labels. Loss 0.25295. Accuracy 0.948.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30557334
Train loss (w/o reg) on all data: 0.29068354
Test loss (w/o reg) on all data: 0.24936411
Train acc on all data:  0.8531485533673717
Test acc on all data:   0.9067055393586005
Norm of the mean of gradients: 2.336057e-05
Norm of the params: 17.256763
                Loss: fixed 406 labels. Loss 0.24936. Accuracy 0.907.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47362438
Train loss (w/o reg) on all data: 0.46916777
Test loss (w/o reg) on all data: 0.29666156
Train acc on all data:  0.7758327255044979
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 9.616377e-06
Norm of the params: 9.440988
              Random: fixed 112 labels. Loss 0.29666. Accuracy 0.964.
### Flips: 1230, rs: 30, checks: 615
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3448
Train loss (w/o reg) on all data: 0.33333528
Test loss (w/o reg) on all data: 0.21297781
Train acc on all data:  0.8400194505227327
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.0302642e-05
Norm of the params: 15.142475
     Influence (LOO): fixed 393 labels. Loss 0.21298. Accuracy 0.971.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2112486
Train loss (w/o reg) on all data: 0.19283128
Test loss (w/o reg) on all data: 0.20357037
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 9.181721e-06
Norm of the params: 19.192354
                Loss: fixed 607 labels. Loss 0.20357. Accuracy 0.930.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46036163
Train loss (w/o reg) on all data: 0.45561326
Test loss (w/o reg) on all data: 0.28045446
Train acc on all data:  0.787989302212497
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 7.441597e-05
Norm of the params: 9.745138
              Random: fixed 161 labels. Loss 0.28045. Accuracy 0.962.
### Flips: 1230, rs: 30, checks: 820
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3047061
Train loss (w/o reg) on all data: 0.29322547
Test loss (w/o reg) on all data: 0.17778072
Train acc on all data:  0.8631169462679309
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.1705451e-05
Norm of the params: 15.152972
     Influence (LOO): fixed 495 labels. Loss 0.17778. Accuracy 0.978.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1280759
Train loss (w/o reg) on all data: 0.10831608
Test loss (w/o reg) on all data: 0.12485453
Train acc on all data:  0.9533187454412837
Test acc on all data:   0.9523809523809523
Norm of the mean of gradients: 5.160275e-06
Norm of the params: 19.879549
                Loss: fixed 795 labels. Loss 0.12485. Accuracy 0.952.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44821468
Train loss (w/o reg) on all data: 0.44337
Test loss (w/o reg) on all data: 0.26479554
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 4.030803e-05
Norm of the params: 9.843454
              Random: fixed 205 labels. Loss 0.26480. Accuracy 0.968.
### Flips: 1230, rs: 30, checks: 1025
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26891458
Train loss (w/o reg) on all data: 0.25690138
Test loss (w/o reg) on all data: 0.1558916
Train acc on all data:  0.8825674690007294
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.847232e-06
Norm of the params: 15.500457
     Influence (LOO): fixed 574 labels. Loss 0.15589. Accuracy 0.982.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06441425
Train loss (w/o reg) on all data: 0.047689483
Test loss (w/o reg) on all data: 0.06070624
Train acc on all data:  0.9810357403355215
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.713037e-06
Norm of the params: 18.289215
                Loss: fixed 937 labels. Loss 0.06071. Accuracy 0.981.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43259242
Train loss (w/o reg) on all data: 0.42762494
Test loss (w/o reg) on all data: 0.24202543
Train acc on all data:  0.812059324094335
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 4.6046094e-05
Norm of the params: 9.967417
              Random: fixed 262 labels. Loss 0.24203. Accuracy 0.981.
### Flips: 1230, rs: 30, checks: 1230
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23049888
Train loss (w/o reg) on all data: 0.21856464
Test loss (w/o reg) on all data: 0.12886399
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.0409905e-05
Norm of the params: 15.449424
     Influence (LOO): fixed 657 labels. Loss 0.12886. Accuracy 0.983.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03672892
Train loss (w/o reg) on all data: 0.025025254
Test loss (w/o reg) on all data: 0.02856893
Train acc on all data:  0.9922197909068806
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 9.5212346e-07
Norm of the params: 15.299456
                Loss: fixed 1003 labels. Loss 0.02857. Accuracy 0.994.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41240984
Train loss (w/o reg) on all data: 0.406654
Test loss (w/o reg) on all data: 0.22689527
Train acc on all data:  0.8259178215414539
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 1.31486495e-05
Norm of the params: 10.729264
              Random: fixed 323 labels. Loss 0.22690. Accuracy 0.978.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.5023365
Train loss (w/o reg) on all data: 0.49731272
Test loss (w/o reg) on all data: 0.338991
Train acc on all data:  0.7473863360077803
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 7.765179e-05
Norm of the params: 10.023762
Flipped loss: 0.33899. Accuracy: 0.935
### Flips: 1230, rs: 31, checks: 205
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44051266
Train loss (w/o reg) on all data: 0.43048295
Test loss (w/o reg) on all data: 0.2848076
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 1.3737362e-05
Norm of the params: 14.163131
     Influence (LOO): fixed 161 labels. Loss 0.28481. Accuracy 0.948.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4065127
Train loss (w/o reg) on all data: 0.39529634
Test loss (w/o reg) on all data: 0.28422514
Train acc on all data:  0.7979576951130561
Test acc on all data:   0.9105928085519922
Norm of the mean of gradients: 1.7261424e-05
Norm of the params: 14.9775715
                Loss: fixed 204 labels. Loss 0.28423. Accuracy 0.911.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49271017
Train loss (w/o reg) on all data: 0.48755106
Test loss (w/o reg) on all data: 0.32294917
Train acc on all data:  0.762217359591539
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 1.4689318e-05
Norm of the params: 10.157852
              Random: fixed  49 labels. Loss 0.32295. Accuracy 0.946.
### Flips: 1230, rs: 31, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39341283
Train loss (w/o reg) on all data: 0.38249934
Test loss (w/o reg) on all data: 0.24199493
Train acc on all data:  0.8140043763676149
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 1.5503252e-05
Norm of the params: 14.773963
     Influence (LOO): fixed 287 labels. Loss 0.24199. Accuracy 0.962.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31447035
Train loss (w/o reg) on all data: 0.2992842
Test loss (w/o reg) on all data: 0.22142364
Train acc on all data:  0.8512035010940919
Test acc on all data:   0.923226433430515
Norm of the mean of gradients: 2.0859474e-05
Norm of the params: 17.427662
                Loss: fixed 409 labels. Loss 0.22142. Accuracy 0.923.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.478429
Train loss (w/o reg) on all data: 0.47304615
Test loss (w/o reg) on all data: 0.3034792
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 3.518546e-05
Norm of the params: 10.3757715
              Random: fixed 113 labels. Loss 0.30348. Accuracy 0.950.
### Flips: 1230, rs: 31, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.35200936
Train loss (w/o reg) on all data: 0.34058893
Test loss (w/o reg) on all data: 0.20603973
Train acc on all data:  0.8341842937028933
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 4.799545e-05
Norm of the params: 15.113194
     Influence (LOO): fixed 392 labels. Loss 0.20604. Accuracy 0.969.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22350708
Train loss (w/o reg) on all data: 0.20439573
Test loss (w/o reg) on all data: 0.17099112
Train acc on all data:  0.9005592025285679
Test acc on all data:   0.9358600583090378
Norm of the mean of gradients: 8.205286e-06
Norm of the params: 19.550625
                Loss: fixed 609 labels. Loss 0.17099. Accuracy 0.936.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4654337
Train loss (w/o reg) on all data: 0.45971867
Test loss (w/o reg) on all data: 0.28684524
Train acc on all data:  0.7823972769268174
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 2.765101e-05
Norm of the params: 10.691141
              Random: fixed 163 labels. Loss 0.28685. Accuracy 0.957.
### Flips: 1230, rs: 31, checks: 820
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3132633
Train loss (w/o reg) on all data: 0.30175084
Test loss (w/o reg) on all data: 0.17937016
Train acc on all data:  0.8543642110381716
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 1.4952479e-05
Norm of the params: 15.173957
     Influence (LOO): fixed 484 labels. Loss 0.17937. Accuracy 0.981.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1376935
Train loss (w/o reg) on all data: 0.116658255
Test loss (w/o reg) on all data: 0.11066254
Train acc on all data:  0.9472404570872842
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 3.891799e-06
Norm of the params: 20.511087
                Loss: fixed 796 labels. Loss 0.11066. Accuracy 0.958.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45021662
Train loss (w/o reg) on all data: 0.44453457
Test loss (w/o reg) on all data: 0.2677652
Train acc on all data:  0.7967420374422562
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 4.8051923e-05
Norm of the params: 10.660258
              Random: fixed 222 labels. Loss 0.26777. Accuracy 0.967.
### Flips: 1230, rs: 31, checks: 1025
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27488878
Train loss (w/o reg) on all data: 0.26355392
Test loss (w/o reg) on all data: 0.14939685
Train acc on all data:  0.87503039144177
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 2.3508383e-05
Norm of the params: 15.05648
     Influence (LOO): fixed 572 labels. Loss 0.14940. Accuracy 0.991.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07214545
Train loss (w/o reg) on all data: 0.05424843
Test loss (w/o reg) on all data: 0.061428722
Train acc on all data:  0.9783612934597617
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.5116246e-06
Norm of the params: 18.919313
                Loss: fixed 944 labels. Loss 0.06143. Accuracy 0.978.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43062544
Train loss (w/o reg) on all data: 0.42480487
Test loss (w/o reg) on all data: 0.2473769
Train acc on all data:  0.8108436664235351
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 5.0528077e-05
Norm of the params: 10.789403
              Random: fixed 286 labels. Loss 0.24738. Accuracy 0.975.
### Flips: 1230, rs: 31, checks: 1230
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2414008
Train loss (w/o reg) on all data: 0.22997838
Test loss (w/o reg) on all data: 0.12886375
Train acc on all data:  0.8922927303671286
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 9.434316e-06
Norm of the params: 15.114505
     Influence (LOO): fixed 648 labels. Loss 0.12886. Accuracy 0.993.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039585926
Train loss (w/o reg) on all data: 0.026771525
Test loss (w/o reg) on all data: 0.026779672
Train acc on all data:  0.9902747386336008
Test acc on all data:   0.9922254616132167
Norm of the mean of gradients: 6.4906567e-07
Norm of the params: 16.008997
                Loss: fixed 1016 labels. Loss 0.02678. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41328183
Train loss (w/o reg) on all data: 0.40732056
Test loss (w/o reg) on all data: 0.22917184
Train acc on all data:  0.8237296377340141
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 9.567671e-06
Norm of the params: 10.919025
              Random: fixed 343 labels. Loss 0.22917. Accuracy 0.976.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49124414
Train loss (w/o reg) on all data: 0.48634067
Test loss (w/o reg) on all data: 0.33884898
Train acc on all data:  0.7568684658400194
Test acc on all data:   0.924198250728863
Norm of the mean of gradients: 4.3174154e-05
Norm of the params: 9.902984
Flipped loss: 0.33885. Accuracy: 0.924
### Flips: 1230, rs: 32, checks: 205
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4199861
Train loss (w/o reg) on all data: 0.40938145
Test loss (w/o reg) on all data: 0.291188
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 1.4555367e-05
Norm of the params: 14.563412
     Influence (LOO): fixed 163 labels. Loss 0.29119. Accuracy 0.930.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38452983
Train loss (w/o reg) on all data: 0.373131
Test loss (w/o reg) on all data: 0.2997286
Train acc on all data:  0.8154631655725748
Test acc on all data:   0.8882410106899903
Norm of the mean of gradients: 1.0161334e-05
Norm of the params: 15.098891
                Loss: fixed 204 labels. Loss 0.29973. Accuracy 0.888.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48100248
Train loss (w/o reg) on all data: 0.4758281
Test loss (w/o reg) on all data: 0.3187852
Train acc on all data:  0.7699975686846584
Test acc on all data:   0.9368318756073858
Norm of the mean of gradients: 1.8991714e-05
Norm of the params: 10.172879
              Random: fixed  51 labels. Loss 0.31879. Accuracy 0.937.
### Flips: 1230, rs: 32, checks: 410
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.37484717
Train loss (w/o reg) on all data: 0.363415
Test loss (w/o reg) on all data: 0.24480727
Train acc on all data:  0.8273766107464138
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.052927e-05
Norm of the params: 15.120964
     Influence (LOO): fixed 283 labels. Loss 0.24481. Accuracy 0.945.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28881082
Train loss (w/o reg) on all data: 0.27235052
Test loss (w/o reg) on all data: 0.24929996
Train acc on all data:  0.8633600778020909
Test acc on all data:   0.8999028182701652
Norm of the mean of gradients: 1.0011334e-05
Norm of the params: 18.14404
                Loss: fixed 407 labels. Loss 0.24930. Accuracy 0.900.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4682342
Train loss (w/o reg) on all data: 0.4626756
Test loss (w/o reg) on all data: 0.29856187
Train acc on all data:  0.7797228300510576
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 1.1719516e-05
Norm of the params: 10.543832
              Random: fixed 107 labels. Loss 0.29856. Accuracy 0.948.
### Flips: 1230, rs: 32, checks: 615
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33109093
Train loss (w/o reg) on all data: 0.31888083
Test loss (w/o reg) on all data: 0.21282637
Train acc on all data:  0.8536348164356917
Test acc on all data:   0.9475218658892128
Norm of the mean of gradients: 7.487102e-06
Norm of the params: 15.626971
     Influence (LOO): fixed 391 labels. Loss 0.21283. Accuracy 0.948.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1990944
Train loss (w/o reg) on all data: 0.17803141
Test loss (w/o reg) on all data: 0.18829364
Train acc on all data:  0.9161196207148067
Test acc on all data:   0.9300291545189504
Norm of the mean of gradients: 1.0570621e-05
Norm of the params: 20.52461
                Loss: fixed 605 labels. Loss 0.18829. Accuracy 0.930.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4559131
Train loss (w/o reg) on all data: 0.45016432
Test loss (w/o reg) on all data: 0.2786817
Train acc on all data:  0.7887186968149769
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.7655395e-05
Norm of the params: 10.722658
              Random: fixed 161 labels. Loss 0.27868. Accuracy 0.955.
### Flips: 1230, rs: 32, checks: 820
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29769176
Train loss (w/o reg) on all data: 0.28497806
Test loss (w/o reg) on all data: 0.18540578
Train acc on all data:  0.8643326039387309
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 8.46646e-06
Norm of the params: 15.945969
     Influence (LOO): fixed 475 labels. Loss 0.18541. Accuracy 0.965.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12383376
Train loss (w/o reg) on all data: 0.1016701
Test loss (w/o reg) on all data: 0.12878935
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 2.340011e-06
Norm of the params: 21.054056
                Loss: fixed 782 labels. Loss 0.12879. Accuracy 0.964.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44261718
Train loss (w/o reg) on all data: 0.43682697
Test loss (w/o reg) on all data: 0.26062116
Train acc on all data:  0.8035497203987357
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.602447e-05
Norm of the params: 10.761231
              Random: fixed 217 labels. Loss 0.26062. Accuracy 0.961.
### Flips: 1230, rs: 32, checks: 1025
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25973323
Train loss (w/o reg) on all data: 0.24694988
Test loss (w/o reg) on all data: 0.15995105
Train acc on all data:  0.8852419158764891
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 1.13381575e-05
Norm of the params: 15.989585
     Influence (LOO): fixed 561 labels. Loss 0.15995. Accuracy 0.969.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078045696
Train loss (w/o reg) on all data: 0.057722654
Test loss (w/o reg) on all data: 0.07843419
Train acc on all data:  0.9776318988572817
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 5.2916407e-06
Norm of the params: 20.160873
                Loss: fixed 896 labels. Loss 0.07843. Accuracy 0.976.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42525566
Train loss (w/o reg) on all data: 0.4191081
Test loss (w/o reg) on all data: 0.24165933
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 7.122282e-05
Norm of the params: 11.088346
              Random: fixed 273 labels. Loss 0.24166. Accuracy 0.967.
### Flips: 1230, rs: 32, checks: 1230
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22605458
Train loss (w/o reg) on all data: 0.21284454
Test loss (w/o reg) on all data: 0.13119553
Train acc on all data:  0.9037199124726477
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 2.3486265e-05
Norm of the params: 16.254255
     Influence (LOO): fixed 639 labels. Loss 0.13120. Accuracy 0.976.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050207973
Train loss (w/o reg) on all data: 0.03388216
Test loss (w/o reg) on all data: 0.042429492
Train acc on all data:  0.9880865548261609
Test acc on all data:   0.9873663751214772
Norm of the mean of gradients: 6.064138e-06
Norm of the params: 18.069761
                Loss: fixed 965 labels. Loss 0.04243. Accuracy 0.987.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4123535
Train loss (w/o reg) on all data: 0.40582234
Test loss (w/o reg) on all data: 0.22645253
Train acc on all data:  0.825674690007294
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 2.1596687e-05
Norm of the params: 11.429053
              Random: fixed 318 labels. Loss 0.22645. Accuracy 0.973.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49388552
Train loss (w/o reg) on all data: 0.48937652
Test loss (w/o reg) on all data: 0.33003572
Train acc on all data:  0.7554096766350595
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 3.758195e-05
Norm of the params: 9.496315
Flipped loss: 0.33004. Accuracy: 0.944
### Flips: 1230, rs: 33, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4301131
Train loss (w/o reg) on all data: 0.42004097
Test loss (w/o reg) on all data: 0.27692622
Train acc on all data:  0.7921225382932167
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 6.508679e-05
Norm of the params: 14.193054
     Influence (LOO): fixed 158 labels. Loss 0.27693. Accuracy 0.959.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39290598
Train loss (w/o reg) on all data: 0.38137928
Test loss (w/o reg) on all data: 0.28146833
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 1.3086363e-05
Norm of the params: 15.183344
                Loss: fixed 201 labels. Loss 0.28147. Accuracy 0.917.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4832718
Train loss (w/o reg) on all data: 0.47865954
Test loss (w/o reg) on all data: 0.30866626
Train acc on all data:  0.7658643326039387
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 2.5198935e-05
Norm of the params: 9.604453
              Random: fixed  57 labels. Loss 0.30867. Accuracy 0.951.
### Flips: 1230, rs: 33, checks: 410
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3824302
Train loss (w/o reg) on all data: 0.37107423
Test loss (w/o reg) on all data: 0.23494115
Train acc on all data:  0.8198395331874544
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 2.4753446e-05
Norm of the params: 15.070487
     Influence (LOO): fixed 287 labels. Loss 0.23494. Accuracy 0.971.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30410868
Train loss (w/o reg) on all data: 0.2885801
Test loss (w/o reg) on all data: 0.23434065
Train acc on all data:  0.8599562363238512
Test acc on all data:   0.9203109815354713
Norm of the mean of gradients: 1.0087673e-05
Norm of the params: 17.623043
                Loss: fixed 402 labels. Loss 0.23434. Accuracy 0.920.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4738538
Train loss (w/o reg) on all data: 0.46947023
Test loss (w/o reg) on all data: 0.28997117
Train acc on all data:  0.7772915147094578
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.7836423e-05
Norm of the params: 9.363305
              Random: fixed 112 labels. Loss 0.28997. Accuracy 0.964.
### Flips: 1230, rs: 33, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3374335
Train loss (w/o reg) on all data: 0.3256276
Test loss (w/o reg) on all data: 0.19863798
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.6374997e-05
Norm of the params: 15.366129
     Influence (LOO): fixed 398 labels. Loss 0.19864. Accuracy 0.972.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21094523
Train loss (w/o reg) on all data: 0.19149356
Test loss (w/o reg) on all data: 0.18006402
Train acc on all data:  0.911986384634087
Test acc on all data:   0.93488824101069
Norm of the mean of gradients: 5.0870085e-06
Norm of the params: 19.723936
                Loss: fixed 605 labels. Loss 0.18006. Accuracy 0.935.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46062908
Train loss (w/o reg) on all data: 0.4560617
Test loss (w/o reg) on all data: 0.2706888
Train acc on all data:  0.7896912229516169
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 5.8439764e-05
Norm of the params: 9.557609
              Random: fixed 165 labels. Loss 0.27069. Accuracy 0.971.
### Flips: 1230, rs: 33, checks: 820
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29235423
Train loss (w/o reg) on all data: 0.27984637
Test loss (w/o reg) on all data: 0.16242039
Train acc on all data:  0.8696814976902504
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.0439265e-05
Norm of the params: 15.816363
     Influence (LOO): fixed 503 labels. Loss 0.16242. Accuracy 0.975.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12670222
Train loss (w/o reg) on all data: 0.10590188
Test loss (w/o reg) on all data: 0.109298676
Train acc on all data:  0.9535618769754437
Test acc on all data:   0.9591836734693877
Norm of the mean of gradients: 1.3016809e-05
Norm of the params: 20.396248
                Loss: fixed 792 labels. Loss 0.10930. Accuracy 0.959.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4438829
Train loss (w/o reg) on all data: 0.43919507
Test loss (w/o reg) on all data: 0.24783702
Train acc on all data:  0.8018477996596158
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.36603785e-05
Norm of the params: 9.682813
              Random: fixed 227 labels. Loss 0.24784. Accuracy 0.976.
### Flips: 1230, rs: 33, checks: 1025
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25658453
Train loss (w/o reg) on all data: 0.24414952
Test loss (w/o reg) on all data: 0.13842241
Train acc on all data:  0.8832968636032094
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.8571436e-05
Norm of the params: 15.77022
     Influence (LOO): fixed 582 labels. Loss 0.13842. Accuracy 0.983.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06396146
Train loss (w/o reg) on all data: 0.046116393
Test loss (w/o reg) on all data: 0.043378867
Train acc on all data:  0.9834670556771213
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.4732452e-06
Norm of the params: 18.891836
                Loss: fixed 939 labels. Loss 0.04338. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4261313
Train loss (w/o reg) on all data: 0.42131793
Test loss (w/o reg) on all data: 0.22657011
Train acc on all data:  0.8176513493800146
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.4416238e-05
Norm of the params: 9.811606
              Random: fixed 291 labels. Loss 0.22657. Accuracy 0.983.
### Flips: 1230, rs: 33, checks: 1230
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22633703
Train loss (w/o reg) on all data: 0.21458475
Test loss (w/o reg) on all data: 0.11886266
Train acc on all data:  0.9015317286652079
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 1.6144204e-05
Norm of the params: 15.3312
     Influence (LOO): fixed 651 labels. Loss 0.11886. Accuracy 0.984.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037931584
Train loss (w/o reg) on all data: 0.025154954
Test loss (w/o reg) on all data: 0.019278672
Train acc on all data:  0.9919766593727206
Test acc on all data:   0.9970845481049563
Norm of the mean of gradients: 1.081236e-06
Norm of the params: 15.985387
                Loss: fixed 994 labels. Loss 0.01928. Accuracy 0.997.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4090394
Train loss (w/o reg) on all data: 0.40410212
Test loss (w/o reg) on all data: 0.2101274
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9854227405247813
Norm of the mean of gradients: 1.1242416e-05
Norm of the params: 9.937081
              Random: fixed 345 labels. Loss 0.21013. Accuracy 0.985.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49958402
Train loss (w/o reg) on all data: 0.49481544
Test loss (w/o reg) on all data: 0.33550337
Train acc on all data:  0.7588135181132993
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 3.1340704e-05
Norm of the params: 9.765842
Flipped loss: 0.33550. Accuracy: 0.954
### Flips: 1230, rs: 34, checks: 205
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43278873
Train loss (w/o reg) on all data: 0.42327338
Test loss (w/o reg) on all data: 0.28353068
Train acc on all data:  0.7928519328956966
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.5605552e-05
Norm of the params: 13.795177
     Influence (LOO): fixed 163 labels. Loss 0.28353. Accuracy 0.955.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39497617
Train loss (w/o reg) on all data: 0.38208082
Test loss (w/o reg) on all data: 0.28642252
Train acc on all data:  0.8096280087527352
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 5.189327e-05
Norm of the params: 16.059483
                Loss: fixed 205 labels. Loss 0.28642. Accuracy 0.921.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48450527
Train loss (w/o reg) on all data: 0.47930133
Test loss (w/o reg) on all data: 0.30664065
Train acc on all data:  0.7729151470945782
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 4.676086e-05
Norm of the params: 10.20189
              Random: fixed  70 labels. Loss 0.30664. Accuracy 0.957.
### Flips: 1230, rs: 34, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38199496
Train loss (w/o reg) on all data: 0.370566
Test loss (w/o reg) on all data: 0.24501155
Train acc on all data:  0.8193532701191345
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 8.967632e-06
Norm of the params: 15.1188345
     Influence (LOO): fixed 291 labels. Loss 0.24501. Accuracy 0.954.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30237204
Train loss (w/o reg) on all data: 0.28472653
Test loss (w/o reg) on all data: 0.2330977
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 6.439759e-05
Norm of the params: 18.785896
                Loss: fixed 409 labels. Loss 0.23310. Accuracy 0.922.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4694229
Train loss (w/o reg) on all data: 0.4638869
Test loss (w/o reg) on all data: 0.286272
Train acc on all data:  0.7806953561876976
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 2.1826749e-05
Norm of the params: 10.522376
              Random: fixed 130 labels. Loss 0.28627. Accuracy 0.965.
### Flips: 1230, rs: 34, checks: 615
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33867013
Train loss (w/o reg) on all data: 0.32604745
Test loss (w/o reg) on all data: 0.20891052
Train acc on all data:  0.8446389496717724
Test acc on all data:   0.966958211856171
Norm of the mean of gradients: 1.4411634e-05
Norm of the params: 15.888793
     Influence (LOO): fixed 395 labels. Loss 0.20891. Accuracy 0.967.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21440385
Train loss (w/o reg) on all data: 0.19386907
Test loss (w/o reg) on all data: 0.16953687
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9446064139941691
Norm of the mean of gradients: 1.746625e-05
Norm of the params: 20.265629
                Loss: fixed 607 labels. Loss 0.16954. Accuracy 0.945.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45809117
Train loss (w/o reg) on all data: 0.4526959
Test loss (w/o reg) on all data: 0.27075338
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 3.0710627e-05
Norm of the params: 10.38776
              Random: fixed 181 labels. Loss 0.27075. Accuracy 0.968.
### Flips: 1230, rs: 34, checks: 820
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30452704
Train loss (w/o reg) on all data: 0.29209095
Test loss (w/o reg) on all data: 0.17595914
Train acc on all data:  0.8604424993921712
Test acc on all data:   0.9776482021379981
Norm of the mean of gradients: 2.305954e-05
Norm of the params: 15.77091
     Influence (LOO): fixed 483 labels. Loss 0.17596. Accuracy 0.978.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12827885
Train loss (w/o reg) on all data: 0.105866164
Test loss (w/o reg) on all data: 0.1126955
Train acc on all data:  0.9557500607828835
Test acc on all data:   0.9620991253644315
Norm of the mean of gradients: 4.170463e-06
Norm of the params: 21.172003
                Loss: fixed 795 labels. Loss 0.11270. Accuracy 0.962.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44330165
Train loss (w/o reg) on all data: 0.43777585
Test loss (w/o reg) on all data: 0.25769958
Train acc on all data:  0.8006321419888159
Test acc on all data:   0.967930029154519
Norm of the mean of gradients: 6.417212e-05
Norm of the params: 10.512646
              Random: fixed 232 labels. Loss 0.25770. Accuracy 0.968.
### Flips: 1230, rs: 34, checks: 1025
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2696461
Train loss (w/o reg) on all data: 0.257314
Test loss (w/o reg) on all data: 0.14941657
Train acc on all data:  0.8801361536591296
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 7.5921603e-06
Norm of the params: 15.704838
     Influence (LOO): fixed 567 labels. Loss 0.14942. Accuracy 0.981.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06611189
Train loss (w/o reg) on all data: 0.048120998
Test loss (w/o reg) on all data: 0.062326986
Train acc on all data:  0.9820082664721614
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 1.3560691e-06
Norm of the params: 18.968868
                Loss: fixed 939 labels. Loss 0.06233. Accuracy 0.975.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42864582
Train loss (w/o reg) on all data: 0.4231151
Test loss (w/o reg) on all data: 0.24118157
Train acc on all data:  0.8137612448334549
Test acc on all data:   0.9718172983479106
Norm of the mean of gradients: 3.110228e-05
Norm of the params: 10.517333
              Random: fixed 281 labels. Loss 0.24118. Accuracy 0.972.
### Flips: 1230, rs: 34, checks: 1230
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23688985
Train loss (w/o reg) on all data: 0.22504291
Test loss (w/o reg) on all data: 0.12863329
Train acc on all data:  0.8986141502552881
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 7.1086806e-06
Norm of the params: 15.392824
     Influence (LOO): fixed 635 labels. Loss 0.12863. Accuracy 0.982.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03742917
Train loss (w/o reg) on all data: 0.025057036
Test loss (w/o reg) on all data: 0.028626561
Train acc on all data:  0.9910041332360807
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.3554786e-06
Norm of the params: 15.73031
                Loss: fixed 1008 labels. Loss 0.02863. Accuracy 0.991.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.41227487
Train loss (w/o reg) on all data: 0.40616038
Test loss (w/o reg) on all data: 0.2236634
Train acc on all data:  0.8271334792122538
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 9.613304e-06
Norm of the params: 11.0584755
              Random: fixed 334 labels. Loss 0.22366. Accuracy 0.981.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.50116456
Train loss (w/o reg) on all data: 0.4956009
Test loss (w/o reg) on all data: 0.332468
Train acc on all data:  0.74981765134938
Test acc on all data:   0.9484936831875608
Norm of the mean of gradients: 3.8364724e-05
Norm of the params: 10.548577
Flipped loss: 0.33247. Accuracy: 0.948
### Flips: 1230, rs: 35, checks: 205
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4356273
Train loss (w/o reg) on all data: 0.42541358
Test loss (w/o reg) on all data: 0.2891233
Train acc on all data:  0.788232433746657
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 2.033923e-05
Norm of the params: 14.292479
     Influence (LOO): fixed 155 labels. Loss 0.28912. Accuracy 0.946.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.405161
Train loss (w/o reg) on all data: 0.3936957
Test loss (w/o reg) on all data: 0.28535396
Train acc on all data:  0.8059810357403355
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 1.9954154e-05
Norm of the params: 15.142843
                Loss: fixed 204 labels. Loss 0.28535. Accuracy 0.913.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4904836
Train loss (w/o reg) on all data: 0.4849638
Test loss (w/o reg) on all data: 0.31770393
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.1027299e-05
Norm of the params: 10.506953
              Random: fixed  51 labels. Loss 0.31770. Accuracy 0.954.
### Flips: 1230, rs: 35, checks: 410
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39145485
Train loss (w/o reg) on all data: 0.37991992
Test loss (w/o reg) on all data: 0.24563749
Train acc on all data:  0.8152200340384148
Test acc on all data:   0.9572400388726919
Norm of the mean of gradients: 1.6934087e-05
Norm of the params: 15.18876
     Influence (LOO): fixed 280 labels. Loss 0.24564. Accuracy 0.957.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.31105968
Train loss (w/o reg) on all data: 0.2951779
Test loss (w/o reg) on all data: 0.23734199
Train acc on all data:  0.8546073425723316
Test acc on all data:   0.9067055393586005
Norm of the mean of gradients: 8.416849e-06
Norm of the params: 17.822334
                Loss: fixed 406 labels. Loss 0.23734. Accuracy 0.907.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4790701
Train loss (w/o reg) on all data: 0.47337064
Test loss (w/o reg) on all data: 0.29921797
Train acc on all data:  0.7736445416970581
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 6.884887e-05
Norm of the params: 10.676561
              Random: fixed 104 labels. Loss 0.29922. Accuracy 0.965.
### Flips: 1230, rs: 35, checks: 615
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34542006
Train loss (w/o reg) on all data: 0.3331065
Test loss (w/o reg) on all data: 0.20164764
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 1.310395e-05
Norm of the params: 15.693048
     Influence (LOO): fixed 401 labels. Loss 0.20165. Accuracy 0.983.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21988058
Train loss (w/o reg) on all data: 0.20037606
Test loss (w/o reg) on all data: 0.18084247
Train acc on all data:  0.9071237539508874
Test acc on all data:   0.9329446064139941
Norm of the mean of gradients: 9.731289e-06
Norm of the params: 19.750708
                Loss: fixed 609 labels. Loss 0.18084. Accuracy 0.933.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46685788
Train loss (w/o reg) on all data: 0.46112758
Test loss (w/o reg) on all data: 0.28086397
Train acc on all data:  0.7848285922684172
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 5.0365656e-05
Norm of the params: 10.705409
              Random: fixed 155 labels. Loss 0.28086. Accuracy 0.970.
### Flips: 1230, rs: 35, checks: 820
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30722558
Train loss (w/o reg) on all data: 0.29468018
Test loss (w/o reg) on all data: 0.17166902
Train acc on all data:  0.8601993678580112
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.4156241e-05
Norm of the params: 15.840073
     Influence (LOO): fixed 490 labels. Loss 0.17167. Accuracy 0.980.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13714422
Train loss (w/o reg) on all data: 0.116749994
Test loss (w/o reg) on all data: 0.111831374
Train acc on all data:  0.9508874300996839
Test acc on all data:   0.9650145772594753
Norm of the mean of gradients: 1.3190127e-05
Norm of the params: 20.196156
                Loss: fixed 796 labels. Loss 0.11183. Accuracy 0.965.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4557319
Train loss (w/o reg) on all data: 0.45003846
Test loss (w/o reg) on all data: 0.26435995
Train acc on all data:  0.7952832482372963
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.040317e-05
Norm of the params: 10.670938
              Random: fixed 201 labels. Loss 0.26436. Accuracy 0.980.
### Flips: 1230, rs: 35, checks: 1025
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2676608
Train loss (w/o reg) on all data: 0.25590858
Test loss (w/o reg) on all data: 0.14364241
Train acc on all data:  0.8818380743982495
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.3010611e-05
Norm of the params: 15.331148
     Influence (LOO): fixed 584 labels. Loss 0.14364. Accuracy 0.988.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06969115
Train loss (w/o reg) on all data: 0.052500837
Test loss (w/o reg) on all data: 0.062051345
Train acc on all data:  0.9798200826647216
Test acc on all data:   0.9805636540330418
Norm of the mean of gradients: 3.0575793e-06
Norm of the params: 18.542011
                Loss: fixed 942 labels. Loss 0.06205. Accuracy 0.981.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43897724
Train loss (w/o reg) on all data: 0.43299413
Test loss (w/o reg) on all data: 0.24656658
Train acc on all data:  0.8076829564794554
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 1.4347812e-05
Norm of the params: 10.93903
              Random: fixed 254 labels. Loss 0.24657. Accuracy 0.979.
### Flips: 1230, rs: 35, checks: 1230
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23824352
Train loss (w/o reg) on all data: 0.22638188
Test loss (w/o reg) on all data: 0.12541945
Train acc on all data:  0.8976416241186482
Test acc on all data:   0.9883381924198251
Norm of the mean of gradients: 1.7625227e-05
Norm of the params: 15.402358
     Influence (LOO): fixed 652 labels. Loss 0.12542. Accuracy 0.988.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03801173
Train loss (w/o reg) on all data: 0.025395902
Test loss (w/o reg) on all data: 0.026437035
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 6.769966e-07
Norm of the params: 15.884475
                Loss: fixed 1014 labels. Loss 0.02644. Accuracy 0.994.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.425228
Train loss (w/o reg) on all data: 0.41940916
Test loss (w/o reg) on all data: 0.22803074
Train acc on all data:  0.8159494286408947
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 1.066215e-05
Norm of the params: 10.787801
              Random: fixed 306 labels. Loss 0.22803. Accuracy 0.982.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48982236
Train loss (w/o reg) on all data: 0.48339087
Test loss (w/o reg) on all data: 0.35888204
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9047619047619048
Norm of the mean of gradients: 6.358005e-05
Norm of the params: 11.3415165
Flipped loss: 0.35888. Accuracy: 0.905
### Flips: 1230, rs: 36, checks: 205
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42785856
Train loss (w/o reg) on all data: 0.41672114
Test loss (w/o reg) on all data: 0.30605173
Train acc on all data:  0.7950401167031363
Test acc on all data:   0.9222546161321672
Norm of the mean of gradients: 1.4103181e-05
Norm of the params: 14.924765
     Influence (LOO): fixed 152 labels. Loss 0.30605. Accuracy 0.922.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39070013
Train loss (w/o reg) on all data: 0.3776237
Test loss (w/o reg) on all data: 0.30883667
Train acc on all data:  0.8086554826160953
Test acc on all data:   0.8833819241982507
Norm of the mean of gradients: 2.8538674e-05
Norm of the params: 16.171837
                Loss: fixed 203 labels. Loss 0.30884. Accuracy 0.883.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47728267
Train loss (w/o reg) on all data: 0.47088197
Test loss (w/o reg) on all data: 0.33891168
Train acc on all data:  0.7695113056163384
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 1.9412992e-05
Norm of the params: 11.314339
              Random: fixed  61 labels. Loss 0.33891. Accuracy 0.914.
### Flips: 1230, rs: 36, checks: 410
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38542503
Train loss (w/o reg) on all data: 0.37293035
Test loss (w/o reg) on all data: 0.27063227
Train acc on all data:  0.8183807439824945
Test acc on all data:   0.9251700680272109
Norm of the mean of gradients: 4.455725e-05
Norm of the params: 15.808029
     Influence (LOO): fixed 270 labels. Loss 0.27063. Accuracy 0.925.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3000181
Train loss (w/o reg) on all data: 0.28219923
Test loss (w/o reg) on all data: 0.2584367
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.8950437317784257
Norm of the mean of gradients: 2.8893708e-05
Norm of the params: 18.877966
                Loss: fixed 404 labels. Loss 0.25844. Accuracy 0.895.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4680696
Train loss (w/o reg) on all data: 0.46177366
Test loss (w/o reg) on all data: 0.31378323
Train acc on all data:  0.7792365669827377
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 1.1644101e-05
Norm of the params: 11.221375
              Random: fixed 112 labels. Loss 0.31378. Accuracy 0.944.
### Flips: 1230, rs: 36, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34786832
Train loss (w/o reg) on all data: 0.33461982
Test loss (w/o reg) on all data: 0.2329074
Train acc on all data:  0.8388037928519329
Test acc on all data:   0.9416909620991254
Norm of the mean of gradients: 1.9516649e-05
Norm of the params: 16.277908
     Influence (LOO): fixed 374 labels. Loss 0.23291. Accuracy 0.942.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21559824
Train loss (w/o reg) on all data: 0.194504
Test loss (w/o reg) on all data: 0.19734369
Train acc on all data:  0.9056649647459275
Test acc on all data:   0.9076773566569485
Norm of the mean of gradients: 2.7660473e-05
Norm of the params: 20.539837
                Loss: fixed 595 labels. Loss 0.19734. Accuracy 0.908.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45458928
Train loss (w/o reg) on all data: 0.4485399
Test loss (w/o reg) on all data: 0.29055628
Train acc on all data:  0.7899343544857768
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 1.637391e-05
Norm of the params: 10.9994135
              Random: fixed 171 labels. Loss 0.29056. Accuracy 0.954.
### Flips: 1230, rs: 36, checks: 820
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30934623
Train loss (w/o reg) on all data: 0.2957059
Test loss (w/o reg) on all data: 0.18891068
Train acc on all data:  0.8614150255288111
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 7.940118e-06
Norm of the params: 16.516855
     Influence (LOO): fixed 478 labels. Loss 0.18891. Accuracy 0.970.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13411416
Train loss (w/o reg) on all data: 0.11076585
Test loss (w/o reg) on all data: 0.12261112
Train acc on all data:  0.9506442985655239
Test acc on all data:   0.9543245869776482
Norm of the mean of gradients: 2.382975e-05
Norm of the params: 21.609404
                Loss: fixed 780 labels. Loss 0.12261. Accuracy 0.954.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.44020218
Train loss (w/o reg) on all data: 0.43402648
Test loss (w/o reg) on all data: 0.26788118
Train acc on all data:  0.8003890104546559
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 3.3040433e-05
Norm of the params: 11.113686
              Random: fixed 230 labels. Loss 0.26788. Accuracy 0.964.
### Flips: 1230, rs: 36, checks: 1025
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27336186
Train loss (w/o reg) on all data: 0.25985852
Test loss (w/o reg) on all data: 0.15119024
Train acc on all data:  0.8803792851932896
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 2.9030021e-05
Norm of the params: 16.433718
     Influence (LOO): fixed 571 labels. Loss 0.15119. Accuracy 0.984.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08151442
Train loss (w/o reg) on all data: 0.060802467
Test loss (w/o reg) on all data: 0.06983696
Train acc on all data:  0.9790906880622416
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 1.5117763e-06
Norm of the params: 20.352861
                Loss: fixed 906 labels. Loss 0.06984. Accuracy 0.974.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42452818
Train loss (w/o reg) on all data: 0.4183037
Test loss (w/o reg) on all data: 0.24408822
Train acc on all data:  0.8161925601750547
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.5567115e-05
Norm of the params: 11.157489
              Random: fixed 288 labels. Loss 0.24409. Accuracy 0.977.
### Flips: 1230, rs: 36, checks: 1230
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23961914
Train loss (w/o reg) on all data: 0.22715643
Test loss (w/o reg) on all data: 0.1227308
Train acc on all data:  0.8959397033795283
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 8.485154e-06
Norm of the params: 15.787782
     Influence (LOO): fixed 652 labels. Loss 0.12273. Accuracy 0.993.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05246837
Train loss (w/o reg) on all data: 0.035453636
Test loss (w/o reg) on all data: 0.044427525
Train acc on all data:  0.9883296863603209
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 2.7430924e-06
Norm of the params: 18.447079
                Loss: fixed 971 labels. Loss 0.04443. Accuracy 0.983.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40956616
Train loss (w/o reg) on all data: 0.40325406
Test loss (w/o reg) on all data: 0.22584185
Train acc on all data:  0.8266472161439339
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 5.4753833e-05
Norm of the params: 11.235756
              Random: fixed 341 labels. Loss 0.22584. Accuracy 0.984.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4983947
Train loss (w/o reg) on all data: 0.4938593
Test loss (w/o reg) on all data: 0.32902798
Train acc on all data:  0.7580841235108193
Test acc on all data:   0.9407191448007775
Norm of the mean of gradients: 1.0694764e-05
Norm of the params: 9.524074
Flipped loss: 0.32903. Accuracy: 0.941
### Flips: 1230, rs: 37, checks: 205
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4305344
Train loss (w/o reg) on all data: 0.42058128
Test loss (w/o reg) on all data: 0.27575234
Train acc on all data:  0.7916362752248967
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 6.261971e-05
Norm of the params: 14.10894
     Influence (LOO): fixed 161 labels. Loss 0.27575. Accuracy 0.958.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39975318
Train loss (w/o reg) on all data: 0.38801655
Test loss (w/o reg) on all data: 0.281313
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9125364431486881
Norm of the mean of gradients: 5.6376928e-05
Norm of the params: 15.32099
                Loss: fixed 202 labels. Loss 0.28131. Accuracy 0.913.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48954225
Train loss (w/o reg) on all data: 0.48476723
Test loss (w/o reg) on all data: 0.31414497
Train acc on all data:  0.7661074641380987
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 3.3325712e-05
Norm of the params: 9.772434
              Random: fixed  42 labels. Loss 0.31414. Accuracy 0.953.
### Flips: 1230, rs: 37, checks: 410
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3829165
Train loss (w/o reg) on all data: 0.37138048
Test loss (w/o reg) on all data: 0.23345812
Train acc on all data:  0.8208120593240943
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 1.8429639e-05
Norm of the params: 15.189501
     Influence (LOO): fixed 287 labels. Loss 0.23346. Accuracy 0.970.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30597752
Train loss (w/o reg) on all data: 0.29012293
Test loss (w/o reg) on all data: 0.22940914
Train acc on all data:  0.8594699732555312
Test acc on all data:   0.9173955296404276
Norm of the mean of gradients: 8.99647e-06
Norm of the params: 17.80707
                Loss: fixed 404 labels. Loss 0.22941. Accuracy 0.917.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.47818083
Train loss (w/o reg) on all data: 0.47364974
Test loss (w/o reg) on all data: 0.2940376
Train acc on all data:  0.775589593970338
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 3.1303796e-05
Norm of the params: 9.51956
              Random: fixed  99 labels. Loss 0.29404. Accuracy 0.961.
### Flips: 1230, rs: 37, checks: 615
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.33974978
Train loss (w/o reg) on all data: 0.32746464
Test loss (w/o reg) on all data: 0.19458841
Train acc on all data:  0.8453683442742523
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 2.0403086e-05
Norm of the params: 15.67492
     Influence (LOO): fixed 401 labels. Loss 0.19459. Accuracy 0.979.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21160507
Train loss (w/o reg) on all data: 0.19219504
Test loss (w/o reg) on all data: 0.1647182
Train acc on all data:  0.9117432530999271
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 6.632645e-06
Norm of the params: 19.702803
                Loss: fixed 605 labels. Loss 0.16472. Accuracy 0.943.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4677565
Train loss (w/o reg) on all data: 0.46334168
Test loss (w/o reg) on all data: 0.2755941
Train acc on all data:  0.7843423292000973
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 1.9401108e-05
Norm of the params: 9.396623
              Random: fixed 149 labels. Loss 0.27559. Accuracy 0.966.
### Flips: 1230, rs: 37, checks: 820
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29909623
Train loss (w/o reg) on all data: 0.28688237
Test loss (w/o reg) on all data: 0.16740932
Train acc on all data:  0.8662776562120107
Test acc on all data:   0.9825072886297376
Norm of the mean of gradients: 8.590358e-06
Norm of the params: 15.629361
     Influence (LOO): fixed 491 labels. Loss 0.16741. Accuracy 0.983.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12189251
Train loss (w/o reg) on all data: 0.10052915
Test loss (w/o reg) on all data: 0.1043218
Train acc on all data:  0.9572088499878434
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 4.468628e-06
Norm of the params: 20.67044
                Loss: fixed 795 labels. Loss 0.10432. Accuracy 0.958.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45407137
Train loss (w/o reg) on all data: 0.44965148
Test loss (w/o reg) on all data: 0.2592704
Train acc on all data:  0.7955263797714563
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 1.644455e-05
Norm of the params: 9.402007
              Random: fixed 200 labels. Loss 0.25927. Accuracy 0.971.
### Flips: 1230, rs: 37, checks: 1025
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2635212
Train loss (w/o reg) on all data: 0.25183803
Test loss (w/o reg) on all data: 0.13998163
Train acc on all data:  0.8859713104789692
Test acc on all data:   0.9961127308066083
Norm of the mean of gradients: 2.1449268e-05
Norm of the params: 15.286049
     Influence (LOO): fixed 577 labels. Loss 0.13998. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05863171
Train loss (w/o reg) on all data: 0.042140592
Test loss (w/o reg) on all data: 0.037272133
Train acc on all data:  0.9841964502796012
Test acc on all data:   0.989310009718173
Norm of the mean of gradients: 3.6731358e-06
Norm of the params: 18.16101
                Loss: fixed 940 labels. Loss 0.03727. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43987277
Train loss (w/o reg) on all data: 0.43538737
Test loss (w/o reg) on all data: 0.24380527
Train acc on all data:  0.8074398249452954
Test acc on all data:   0.9786200194363459
Norm of the mean of gradients: 7.0464754e-05
Norm of the params: 9.47143
              Random: fixed 250 labels. Loss 0.24381. Accuracy 0.979.
### Flips: 1230, rs: 37, checks: 1230
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2260722
Train loss (w/o reg) on all data: 0.2143842
Test loss (w/o reg) on all data: 0.11948292
Train acc on all data:  0.9017748601993678
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 8.095088e-06
Norm of the params: 15.289216
     Influence (LOO): fixed 649 labels. Loss 0.11948. Accuracy 0.991.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035784997
Train loss (w/o reg) on all data: 0.023035983
Test loss (w/o reg) on all data: 0.020292846
Train acc on all data:  0.9914903963044007
Test acc on all data:   0.9941690962099126
Norm of the mean of gradients: 3.242003e-07
Norm of the params: 15.968102
                Loss: fixed 996 labels. Loss 0.02029. Accuracy 0.994.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42647234
Train loss (w/o reg) on all data: 0.42199525
Test loss (w/o reg) on all data: 0.22600108
Train acc on all data:  0.8181376124483345
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 2.9567236e-05
Norm of the params: 9.462647
              Random: fixed 298 labels. Loss 0.22600. Accuracy 0.980.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.496441
Train loss (w/o reg) on all data: 0.49121258
Test loss (w/o reg) on all data: 0.33280525
Train acc on all data:  0.75127644055434
Test acc on all data:   0.9339164237123421
Norm of the mean of gradients: 1.7631848e-05
Norm of the params: 10.225868
Flipped loss: 0.33281. Accuracy: 0.934
### Flips: 1230, rs: 38, checks: 205
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.43348774
Train loss (w/o reg) on all data: 0.42409012
Test loss (w/o reg) on all data: 0.27896887
Train acc on all data:  0.7904206175540968
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 2.2923297e-05
Norm of the params: 13.709585
     Influence (LOO): fixed 163 labels. Loss 0.27897. Accuracy 0.953.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39707905
Train loss (w/o reg) on all data: 0.38522157
Test loss (w/o reg) on all data: 0.28097427
Train acc on all data:  0.8057379042061755
Test acc on all data:   0.9115646258503401
Norm of the mean of gradients: 2.6224388e-05
Norm of the params: 15.399658
                Loss: fixed 203 labels. Loss 0.28097. Accuracy 0.912.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48681074
Train loss (w/o reg) on all data: 0.4815133
Test loss (w/o reg) on all data: 0.3185777
Train acc on all data:  0.7634330172623389
Test acc on all data:   0.9426627793974732
Norm of the mean of gradients: 2.0734224e-05
Norm of the params: 10.293157
              Random: fixed  50 labels. Loss 0.31858. Accuracy 0.943.
### Flips: 1230, rs: 38, checks: 410
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3846953
Train loss (w/o reg) on all data: 0.3743868
Test loss (w/o reg) on all data: 0.23165525
Train acc on all data:  0.8166788232433747
Test acc on all data:   0.9611273080660836
Norm of the mean of gradients: 1.1782524e-05
Norm of the params: 14.35862
     Influence (LOO): fixed 293 labels. Loss 0.23166. Accuracy 0.961.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3068385
Train loss (w/o reg) on all data: 0.29080486
Test loss (w/o reg) on all data: 0.22758958
Train acc on all data:  0.8587405786530513
Test acc on all data:   0.9144800777453839
Norm of the mean of gradients: 9.625435e-06
Norm of the params: 17.907345
                Loss: fixed 406 labels. Loss 0.22759. Accuracy 0.914.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4755021
Train loss (w/o reg) on all data: 0.47023755
Test loss (w/o reg) on all data: 0.30316082
Train acc on all data:  0.7724288840262582
Test acc on all data:   0.9533527696793003
Norm of the mean of gradients: 1.9954407e-05
Norm of the params: 10.261135
              Random: fixed  99 labels. Loss 0.30316. Accuracy 0.953.
### Flips: 1230, rs: 38, checks: 615
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34131333
Train loss (w/o reg) on all data: 0.33006105
Test loss (w/o reg) on all data: 0.19888595
Train acc on all data:  0.838317529783613
Test acc on all data:   0.9640427599611273
Norm of the mean of gradients: 1.20427885e-05
Norm of the params: 15.001517
     Influence (LOO): fixed 398 labels. Loss 0.19889. Accuracy 0.964.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21488318
Train loss (w/o reg) on all data: 0.19502029
Test loss (w/o reg) on all data: 0.1656057
Train acc on all data:  0.9083394116216873
Test acc on all data:   0.9387755102040817
Norm of the mean of gradients: 8.424822e-06
Norm of the params: 19.931324
                Loss: fixed 608 labels. Loss 0.16561. Accuracy 0.939.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46370867
Train loss (w/o reg) on all data: 0.45835736
Test loss (w/o reg) on all data: 0.28453392
Train acc on all data:  0.7850717238025772
Test acc on all data:   0.9552964042759962
Norm of the mean of gradients: 1.3302766e-05
Norm of the params: 10.345352
              Random: fixed 147 labels. Loss 0.28453. Accuracy 0.955.
### Flips: 1230, rs: 38, checks: 820
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30039802
Train loss (w/o reg) on all data: 0.2888262
Test loss (w/o reg) on all data: 0.17281231
Train acc on all data:  0.862630683199611
Test acc on all data:   0.9747327502429544
Norm of the mean of gradients: 2.1831242e-05
Norm of the params: 15.213042
     Influence (LOO): fixed 497 labels. Loss 0.17281. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12904389
Train loss (w/o reg) on all data: 0.10732734
Test loss (w/o reg) on all data: 0.118251145
Train acc on all data:  0.9521030877704838
Test acc on all data:   0.956268221574344
Norm of the mean of gradients: 1.0334132e-05
Norm of the params: 20.840609
                Loss: fixed 796 labels. Loss 0.11825. Accuracy 0.956.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4510054
Train loss (w/o reg) on all data: 0.44567624
Test loss (w/o reg) on all data: 0.26804253
Train acc on all data:  0.7962557743739364
Test acc on all data:   0.9698736637512148
Norm of the mean of gradients: 9.149243e-05
Norm of the params: 10.32391
              Random: fixed 199 labels. Loss 0.26804. Accuracy 0.970.
### Flips: 1230, rs: 38, checks: 1025
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25603753
Train loss (w/o reg) on all data: 0.24348696
Test loss (w/o reg) on all data: 0.14580086
Train acc on all data:  0.8837831266715293
Test acc on all data:   0.9757045675413022
Norm of the mean of gradients: 1.5983916e-05
Norm of the params: 15.843345
     Influence (LOO): fixed 589 labels. Loss 0.14580. Accuracy 0.976.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07366288
Train loss (w/o reg) on all data: 0.055430334
Test loss (w/o reg) on all data: 0.06277737
Train acc on all data:  0.9764162411864818
Test acc on all data:   0.9815354713313897
Norm of the mean of gradients: 4.6206396e-06
Norm of the params: 19.095835
                Loss: fixed 929 labels. Loss 0.06278. Accuracy 0.982.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4386274
Train loss (w/o reg) on all data: 0.43325466
Test loss (w/o reg) on all data: 0.25047922
Train acc on all data:  0.8088986141502553
Test acc on all data:   0.9708454810495627
Norm of the mean of gradients: 3.4923483e-05
Norm of the params: 10.366025
              Random: fixed 249 labels. Loss 0.25048. Accuracy 0.971.
### Flips: 1230, rs: 38, checks: 1230
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23068735
Train loss (w/o reg) on all data: 0.21843316
Test loss (w/o reg) on all data: 0.12775181
Train acc on all data:  0.8981278871869681
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 4.9132714e-06
Norm of the params: 15.655153
     Influence (LOO): fixed 651 labels. Loss 0.12775. Accuracy 0.983.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043865073
Train loss (w/o reg) on all data: 0.029698633
Test loss (w/o reg) on all data: 0.025150634
Train acc on all data:  0.9888159494286409
Test acc on all data:   0.9931972789115646
Norm of the mean of gradients: 1.0415664e-06
Norm of the params: 16.832375
                Loss: fixed 1000 labels. Loss 0.02515. Accuracy 0.993.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42670226
Train loss (w/o reg) on all data: 0.4213241
Test loss (w/o reg) on all data: 0.23395318
Train acc on all data:  0.8203257962557744
Test acc on all data:   0.9766763848396501
Norm of the mean of gradients: 3.6158293e-05
Norm of the params: 10.371265
              Random: fixed 296 labels. Loss 0.23395. Accuracy 0.977.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.49135104
Train loss (w/o reg) on all data: 0.48592773
Test loss (w/o reg) on all data: 0.32554746
Train acc on all data:  0.7549234135667396
Test acc on all data:   0.9436345966958212
Norm of the mean of gradients: 2.3872873e-05
Norm of the params: 10.414711
Flipped loss: 0.32555. Accuracy: 0.944
### Flips: 1230, rs: 39, checks: 205
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4311805
Train loss (w/o reg) on all data: 0.42118245
Test loss (w/o reg) on all data: 0.2678537
Train acc on all data:  0.7923656698273767
Test acc on all data:   0.9514091350826045
Norm of the mean of gradients: 1.4521683e-05
Norm of the params: 14.14075
     Influence (LOO): fixed 155 labels. Loss 0.26785. Accuracy 0.951.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.39030755
Train loss (w/o reg) on all data: 0.3780982
Test loss (w/o reg) on all data: 0.27138308
Train acc on all data:  0.8084123510819353
Test acc on all data:   0.9212827988338192
Norm of the mean of gradients: 1.0227705e-05
Norm of the params: 15.62648
                Loss: fixed 205 labels. Loss 0.27138. Accuracy 0.921.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.48011944
Train loss (w/o reg) on all data: 0.4747212
Test loss (w/o reg) on all data: 0.3079663
Train acc on all data:  0.7680525164113785
Test acc on all data:   0.9504373177842566
Norm of the mean of gradients: 2.8680024e-05
Norm of the params: 10.390627
              Random: fixed  50 labels. Loss 0.30797. Accuracy 0.950.
### Flips: 1230, rs: 39, checks: 410
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.38141647
Train loss (w/o reg) on all data: 0.3704089
Test loss (w/o reg) on all data: 0.22189157
Train acc on all data:  0.8200826647216144
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.57155e-05
Norm of the params: 14.837508
     Influence (LOO): fixed 291 labels. Loss 0.22189. Accuracy 0.966.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.3003719
Train loss (w/o reg) on all data: 0.28416187
Test loss (w/o reg) on all data: 0.22160406
Train acc on all data:  0.8584974471188913
Test acc on all data:   0.9271137026239067
Norm of the mean of gradients: 8.379068e-06
Norm of the params: 18.005568
                Loss: fixed 406 labels. Loss 0.22160. Accuracy 0.927.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.46793088
Train loss (w/o reg) on all data: 0.46249118
Test loss (w/o reg) on all data: 0.29136768
Train acc on all data:  0.7799659615852176
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.432849e-05
Norm of the params: 10.430425
              Random: fixed 105 labels. Loss 0.29137. Accuracy 0.958.
### Flips: 1230, rs: 39, checks: 615
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.34099638
Train loss (w/o reg) on all data: 0.3285815
Test loss (w/o reg) on all data: 0.18862765
Train acc on all data:  0.8443958181376124
Test acc on all data:   0.9727891156462585
Norm of the mean of gradients: 1.5144349e-05
Norm of the params: 15.757461
     Influence (LOO): fixed 392 labels. Loss 0.18863. Accuracy 0.973.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20755178
Train loss (w/o reg) on all data: 0.18778929
Test loss (w/o reg) on all data: 0.16102996
Train acc on all data:  0.912715779236567
Test acc on all data:   0.9455782312925171
Norm of the mean of gradients: 4.0691666e-05
Norm of the params: 19.880886
                Loss: fixed 608 labels. Loss 0.16103. Accuracy 0.946.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.45869386
Train loss (w/o reg) on all data: 0.45303497
Test loss (w/o reg) on all data: 0.27958742
Train acc on all data:  0.7889618283491369
Test acc on all data:   0.9582118561710399
Norm of the mean of gradients: 1.1232976e-05
Norm of the params: 10.6385
              Random: fixed 144 labels. Loss 0.27959. Accuracy 0.958.
### Flips: 1230, rs: 39, checks: 820
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29593137
Train loss (w/o reg) on all data: 0.28281066
Test loss (w/o reg) on all data: 0.16170003
Train acc on all data:  0.8684658400194505
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 5.4714284e-05
Norm of the params: 16.19921
     Influence (LOO): fixed 493 labels. Loss 0.16170. Accuracy 0.974.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11808233
Train loss (w/o reg) on all data: 0.09803259
Test loss (w/o reg) on all data: 0.09467609
Train acc on all data:  0.9586676391928033
Test acc on all data:   0.9659863945578231
Norm of the mean of gradients: 3.5604382e-06
Norm of the params: 20.024858
                Loss: fixed 797 labels. Loss 0.09468. Accuracy 0.966.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.4446972
Train loss (w/o reg) on all data: 0.43916517
Test loss (w/o reg) on all data: 0.25813603
Train acc on all data:  0.8033065888645757
Test acc on all data:   0.9689018464528668
Norm of the mean of gradients: 2.7767443e-05
Norm of the params: 10.518566
              Random: fixed 204 labels. Loss 0.25814. Accuracy 0.969.
### Flips: 1230, rs: 39, checks: 1025
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2593936
Train loss (w/o reg) on all data: 0.24695668
Test loss (w/o reg) on all data: 0.13803475
Train acc on all data:  0.8891320204230488
Test acc on all data:   0.9844509232264335
Norm of the mean of gradients: 9.036121e-06
Norm of the params: 15.771449
     Influence (LOO): fixed 577 labels. Loss 0.13803. Accuracy 0.984.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057989314
Train loss (w/o reg) on all data: 0.0420788
Test loss (w/o reg) on all data: 0.049998485
Train acc on all data:  0.9839533187454412
Test acc on all data:   0.9834791059280855
Norm of the mean of gradients: 5.040412e-06
Norm of the params: 17.83845
                Loss: fixed 931 labels. Loss 0.05000. Accuracy 0.983.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.42604384
Train loss (w/o reg) on all data: 0.42029428
Test loss (w/o reg) on all data: 0.2359904
Train acc on all data:  0.8164356917092147
Test acc on all data:   0.9737609329446064
Norm of the mean of gradients: 4.3691332e-05
Norm of the params: 10.723376
              Random: fixed 267 labels. Loss 0.23599. Accuracy 0.974.
### Flips: 1230, rs: 39, checks: 1230
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23308335
Train loss (w/o reg) on all data: 0.22077598
Test loss (w/o reg) on all data: 0.11869938
Train acc on all data:  0.9022611232676878
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.370483e-05
Norm of the params: 15.689087
     Influence (LOO): fixed 642 labels. Loss 0.11870. Accuracy 0.991.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037686154
Train loss (w/o reg) on all data: 0.024281492
Test loss (w/o reg) on all data: 0.029780503
Train acc on all data:  0.9917335278385606
Test acc on all data:   0.9912536443148688
Norm of the mean of gradients: 1.0200449e-06
Norm of the params: 16.373554
                Loss: fixed 977 labels. Loss 0.02978. Accuracy 0.991.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.40707776
Train loss (w/o reg) on all data: 0.40131584
Test loss (w/o reg) on all data: 0.21438533
Train acc on all data:  0.8307804522246536
Test acc on all data:   0.9795918367346939
Norm of the mean of gradients: 1.663689e-05
Norm of the params: 10.734905
              Random: fixed 332 labels. Loss 0.21439. Accuracy 0.980.
